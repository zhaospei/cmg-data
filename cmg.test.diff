init_failed : <nl> static gboolean <nl> plugin_init ( GstPlugin * plugin ) <nl> { <nl> - if (! gst_element_register ( plugin , " mpeg2dec ", GST_RANK_MARGINAL , <nl> + if (! gst_element_register ( plugin , " mpeg2dec ", GST_RANK_PRIMARY , <nl> GST_TYPE_MPEG2DEC )) <nl> return FALSE ; <nl> 
create_request_failed : <nl> { <nl> GST_ELEMENT_ERROR ( ctx , LIBRARY , INIT , <nl> (" Could not create request ."), ( NULL )); <nl> + g_free ( req_url ); <nl> goto reset ; <nl> } <nl> send_error :
gst_rdt_depay_push ( GstRDTDepay * rdtdepay , GstBuffer * buffer ) <nl> rdtdepay -> need_newsegment = FALSE ; <nl> } <nl>  <nl> + buffer = gst_buffer_make_metadata_writable ( buffer ); <nl> gst_buffer_set_caps ( buffer , GST_PAD_CAPS ( rdtdepay -> srcpad )); <nl>  <nl> if ( rdtdepay -> discont ) {
guestfs___check_package_management ( guestfs_h * g , struct inspect_fs * fs ) <nl> char * <nl> guestfs___first_line_of_file ( guestfs_h * g , const char * filename ) <nl> { <nl> - CLEANUP_FREE char ** lines = NULL ; /* sic : not CLEANUP_FREE_STRING_LIST */ <nl> + char ** lines = NULL ; /* sic : not CLEANUP_FREE_STRING_LIST */ <nl> int64_t size ; <nl> char * ret ; <nl>  <nl> guestfs___first_line_of_file ( guestfs_h * g , const char * filename ) <nl>  <nl> ret = lines [ 0 ]; /* caller frees */ <nl>  <nl> + free ( lines ); <nl> + <nl> return ret ; <nl> } <nl> 
main ( int argc , char * argv []) <nl> name = strrchr ( drvs -> a . filename , '/'); <nl> if ( name == NULL ) <nl> name = drvs -> a . filename ; <nl> + else <nl> + name ++; /* skip '/' character */ <nl> break ; <nl> case drv_d : <nl> name = drvs -> d . guest ;
do_ldmtool_diskgroup_volumes ( const char * diskgroup ) <nl> reply_with_error ("% s ", err ); <nl> return NULL ; <nl> } <nl> - free ( err ); <nl>  <nl> return parse_json_get_object_string_list ( out , " volumes ", <nl> __func__ , " ldmtool show diskgroup ");
guestfs___free_inspect_info ( guestfs_h * g ) <nl> free ( g -> fses [ i ]. device ); <nl> free ( g -> fses [ i ]. product_name ); <nl> free ( g -> fses [ i ]. arch ); <nl> + free ( g -> fses [ i ]. hostname ); <nl> free ( g -> fses [ i ]. windows_systemroot ); <nl> size_t j ; <nl> for ( j = 0 ; j < g -> fses [ i ]. nr_fstab ; ++ j ) {
do_vgcreate ( const char * volgroup , char * const * physvols ) <nl> if ( r == - 1 ) { <nl> reply_with_error ("% s ", err ); <nl> free ( err ); <nl> + free ( argv ); <nl> return - 1 ; <nl> } <nl>  <nl> free ( err ); <nl> + free ( argv ); <nl>  <nl> udev_settle (); <nl>  <nl> do_vg_activate ( int activate , char * const * volgroups ) <nl> if ( r == - 1 ) { <nl> reply_with_error (" vgchange : % s ", err ); <nl> free ( err ); <nl> + free ( argv ); <nl> return - 1 ; <nl> } <nl>  <nl> free ( err ); <nl> + free ( argv ); <nl>  <nl> udev_settle (); <nl> 
inspect_mount_handle ( guestfs_h * g ) <nl> exit ( EXIT_FAILURE ); <nl> } <nl>  <nl> + /* Free old global if there is one . */ <nl> + free ( root ); <nl> + <nl> root = roots [ 0 ]; <nl> free ( roots ); <nl> 
guestfs___send_file ( guestfs_h * g , const char * filename ) <nl> if ( err < 0 ) { <nl> if ( err == - 2 ) /* daemon sent cancellation */ <nl> send_file_cancellation ( g ); <nl> + close ( fd ); <nl> return err ; <nl> } <nl> } <nl> guestfs___send_file ( guestfs_h * g , const char * filename ) <nl> if ( r == - 1 ) { <nl> perrorf ( g , " read : % s ", filename ); <nl> send_file_cancellation ( g ); <nl> + close ( fd ); <nl> return - 1 ; <nl> } <nl>  <nl> guestfs___send_file ( guestfs_h * g , const char * filename ) <nl> error ( g , _ (" operation cancelled by user ")); <nl> g -> last_errnum = EINTR ; <nl> send_file_cancellation ( g ); <nl> + close ( fd ); <nl> return - 1 ; <nl> } <nl> 
static char * replace_str ( const char * str , const char * old , /* {{{ */ <nl> } else <nl> retlen = strlen ( str ); <nl>  <nl> - ret = malloc ( retlen + 1 ); <nl> + ret = calloc ( 1 , retlen + 1 ); <nl> if ( ret == NULL ) <nl> return NULL ; <nl> // added to original : not optimized , but keeps valgrind happy . <nl> - memset ( ret , 0 , retlen + 1 ); <nl>  <nl> r = ret ; <nl> p = str ;
static int parse_packet ( sockent_t * se , /* {{{ */ <nl> printed_ignore_warning = 1 ; <nl> } <nl> buffer = (( char *) buffer ) + pkg_length ; <nl> + buffer_size -= ( size_t ) pkg_length ; <nl> continue ; <nl> } <nl> # endif /* HAVE_LIBGCRYPT */ <nl> static int parse_packet ( sockent_t * se , /* {{{ */ <nl> printed_ignore_warning = 1 ; <nl> } <nl> buffer = (( char *) buffer ) + pkg_length ; <nl> + buffer_size -= ( size_t ) pkg_length ; <nl> continue ; <nl> } <nl> # endif /* HAVE_LIBGCRYPT */ <nl> static int parse_packet ( sockent_t * se , /* {{{ */ <nl> DEBUG (" network plugin : parse_packet : Unknown part " <nl> " type : 0x % 04hx ", pkg_type ); <nl> buffer = (( char *) buffer ) + pkg_length ; <nl> + buffer_size -= ( size_t ) pkg_length ; <nl> } <nl> } /* while ( buffer_size > sizeof ( part_header_t )) */ <nl> 
static int rrd_shutdown ( void ) <nl> rrd_cache_flush (- 1 ); <nl> pthread_mutex_unlock (& cache_lock ); <nl>  <nl> + /* Wait for all the values to be written to disk before returning . */ <nl> + if ( queue_thread != 0 ) <nl> + { <nl> + pthread_join ( queue_thread , NULL ); <nl> + queue_thread = 0 ; <nl> + DEBUG (" rrdtool plugin : queue_thread exited ."); <nl> + } <nl> + <nl> pthread_mutex_lock (& queue_lock ); <nl> do_shutdown = 1 ; <nl> pthread_cond_signal (& queue_cond );
krb5_ldap_get_password_policy_from_dn ( krb5_context context , char * pol_name , <nl> LDAP_SEARCH ( pol_dn , LDAP_SCOPE_BASE , "( objectclass = krbPwdPolicy )", password_policy_attributes ); <nl>  <nl> ent = ldap_first_entry ( ld , result ); <nl> - if ( ent != NULL ) { <nl> - if (( st = populate_policy ( context , ld , ent , pol_name , * policy )) != 0 ) <nl> - goto cleanup ; <nl> + if ( ent == NULL ) { <nl> + st = KRB5_KDB_NOENTRY ; <nl> + goto cleanup ; <nl> } <nl> + st = populate_policy ( context , ld , ent , pol_name , * policy ); <nl>  <nl> cleanup : <nl> ldap_msgfree ( result );
check_1_6_dummy ( kadm5_principal_ent_t entry , long mask , <nl> char * password = * passptr ; <nl>  <nl> /* Old - style randkey operations disallowed tickets to start . */ <nl> - if (!( mask & KADM5_ATTRIBUTES ) || <nl> + if ( password == NULL || !( mask & KADM5_ATTRIBUTES ) || <nl> !( entry -> attributes & KRB5_KDB_DISALLOW_ALL_TIX )) <nl> return ; <nl> 
process_tgs_req ( krb5_data * pkt , const krb5_fulladdr * from , <nl> status =" UNEXPECTED NULL in header_ticket "; <nl> goto cleanup ; <nl> } <nl> + errcode = kdc_make_rstate (& state ); <nl> + if ( errcode != 0 ) { <nl> + status = " making state "; <nl> + goto cleanup ; <nl> + } <nl> scratch . length = pa_tgs_req -> length ; <nl> scratch . data = ( char *) pa_tgs_req -> contents ; <nl> errcode = kdc_find_fast (& request , & scratch , subkey , header_ticket -> enc_part2 -> session , state );
krb5_fcc_retrieve ( id , whichfields , mcreds , creds ) <nl> authdata_match ( mcreds -> authdata , fetchcreds . authdata )) <nl> && <nl> (! set ( KRB5_TC_MATCH_2ND_TKT ) || <nl> - data_match ( mcreds -> second_ticket , fetchcreds . second_ticket )) <nl> + data_match (& mcreds -> second_ticket , & fetchcreds . second_ticket )) <nl> ) <nl> { <nl> krb5_fcc_end_seq_get ( id , & cursor );
void Log :: Initialize () { <nl> // one character so we can escape the loop properly . <nl> p --; <nl> break ; <nl> + case ' p ': <nl> + // % p expands to the process ID . <nl> + stream . Add ("% d ", OS :: GetCurrentProcessId ()); <nl> + break ; <nl> case ' t ': { <nl> // % t expands to the current time in milliseconds . <nl> double time = OS :: TimeCurrentMillis ();
void TLSWrap :: EncOut () { <nl> for ( size_t i = 0 ; i < count ; i ++) <nl> buf [ i ] = uv_buf_init ( data [ i ], size [ i ]); <nl> int r = stream_ -> DoWrite ( write_req , buf , count , nullptr ); <nl> + write_req -> Dispatched (); <nl>  <nl> // Ignore errors , this should be already handled in js <nl> if (! r ) <nl> void TLSWrap :: EncOut () { <nl>  <nl> void TLSWrap :: EncOutCb ( WriteWrap * req_wrap , int status ) { <nl> TLSWrap * wrap = req_wrap -> wrap ()-> Cast < TLSWrap >(); <nl> + req_wrap ->~ WriteWrap (); <nl> + delete [] reinterpret_cast < char *>( req_wrap ); <nl>  <nl> // Handle error <nl> if ( status ) {
int main ( int argc , char * argv []) <nl> f = fopen ( argv [ 1 ], " r "); <nl> if (! f ) { <nl> fprintf ( stderr , " Unable to load % s \ n ", argv [ 1 ]); <nl> + optfree ( opts ); <nl> exit ( 2 ); <nl> } <nl>  <nl> bc = malloc ( sizeof (* bc )); <nl> if (! bc ) { <nl> fprintf ( stderr , " Out of memory \ n "); <nl> + optfree ( opts ); <nl> exit ( 3 ); <nl> } <nl>  <nl> int main ( int argc , char * argv []) <nl> rc = cli_bytecode_load ( bc , f , NULL ); <nl> if ( rc != CL_SUCCESS ) { <nl> fprintf ( stderr ," Unable to load bytecode : % s \ n ", cl_strerror ( rc )); <nl> + optfree ( opts ); <nl> exit ( 4 ); <nl> } <nl> fclose ( f ); <nl> int main ( int argc , char * argv []) <nl> cli_bytecode_destroy_context ( ctx ); <nl> cli_bytecode_destroy ( bc ); <nl> free ( bc ); <nl> + optfree ( opts ); <nl> return 0 ; <nl> }
static char * sha256file ( const char * file , unsigned int * size ) <nl> sha256_final (& ctx , digest ); <nl> sha = ( char *) malloc ( 65 ); <nl> if (! sha ) <nl> + { <nl> + fclose ( fh ); <nl> return NULL ; <nl> + } <nl> for ( i = 0 ; i < 32 ; i ++) <nl> sprintf ( sha + i * 2 , "% 02x ", digest [ i ]); <nl> + <nl> + fclose ( fh ); <nl> return sha ; <nl> } <nl> 
static int read_tables ( int fd , unpack_data_t * unpack_data ) <nl> rar_addbits ( unpack_data , 7 ); <nl> } <nl> if ( i == 0 ) { <nl> - rar_dbgmsg (" We cannot have repeat previous code at the first position "); <nl> + rar_dbgmsg (" We cannot have repeat previous code at the first position \ n "); <nl> return FALSE ; <nl> } <nl> while ( n -- > 0 && i < table_size ) {
static int multiscan ( const char * dirname , const struct cl_node * root , const stru <nl> closedir ( dd ); <nl> return - 1 ; <nl> } <nl> + free ( fname ); <nl> } else { <nl> if ( S_ISREG ( statbuf . st_mode ) || ( S_ISLNK ( statbuf . st_mode ) && ( checksymlink ( fname ) == 2 ) && cfgopt ( copt , " FollowFileSymlinks ")-> enabled )) { <nl>  <nl> static int multiscan ( const char * dirname , const struct cl_node * root , const stru <nl> } <nl> } <nl> } <nl> + } else { <nl> + free ( fname ); <nl> } <nl> } <nl> }
static int td_isutf8 ( const unsigned char * buf , unsigned int len ) <nl>  <nl> static int td_isutf16 ( const unsigned char * buf , unsigned int len ) <nl> { <nl> - unsigned int be = 1 , nobom = 0 , i , c , bad = 0 ; <nl> + unsigned int be = 1 , nobom = 0 , i , c , bad = 0 , high = 0 ; <nl>  <nl>  <nl> if ( len < 2 ) <nl> static int td_isutf16 ( const unsigned char * buf , unsigned int len ) <nl> return 0 ; <nl> else <nl> bad ++; <nl> - } <nl> + } else if ( c >= 128 ) { <nl> + high ++; <nl> + } <nl> } <nl>  <nl> + if ( nobom && high >= len / 4 ) <nl> + return 0 ; <nl> + <nl> if (! nobom && bad >= len / 2 ) <nl> return 0 ; <nl> 
try_pacemaker ( int command , enum cluster_type_e stack ) <nl> case ' p ': <nl> /* Go to pacemakerd */ <nl> { <nl> - GMainLoop * amainloop = g_main_new ( FALSE ); <nl> + GMainLoop * amainloop = g_main_loop_new ( NULL , FALSE ); <nl> mainloop_io_t * ipc = <nl> mainloop_add_ipc_client ( CRM_SYSTEM_MCP , G_PRIORITY_DEFAULT , 0 , NULL , & node_callbacks ); <nl> if ( ipc != NULL ) {
char * get_ais_data ( AIS_Message * msg ) <nl>  <nl> # if SUPPORT_AIS <nl> int ais_fd_sync = - 1 ; <nl> - static int ais_fd_async = - 1 ; /* never send messages via this channel */ <nl> + int ais_fd_async = - 1 ; /* never send messages via this channel */ <nl> GFDSource * ais_source = NULL ; <nl> GFDSource * ais_source_sync = NULL ; <nl>  <nl> static gboolean ais_dispatch ( int sender , gpointer user_data ) <nl>  <nl> if ( rc != BZ_OK ) { <nl> crm_err (" Decompression failed : % d ", rc ); <nl> - crm_free ( uncompressed ); <nl> goto badmsg ; <nl> } <nl> 
-/* $ Id : utils . c , v 1 . 9 2004 / 07 / 30 15 : 31 : 05 andrew Exp $ */ <nl> +/* $ Id : utils . c , v 1 . 10 2004 / 08 / 03 08 : 51 : 36 andrew Exp $ */ <nl> /* <nl> * Copyright ( C ) 2004 Andrew Beekhof < andrew @ beekhof . net > <nl> * <nl> alter_debug ( int nsig ) <nl> set_crm_log_level ( level + 1 ); <nl> level = get_crm_log_level (); <nl> fprintf ( stderr , " Upped log level to % d \ n ", level ); <nl> + cl_log ( LOG_INFO , " Upped log level to % d \ n ", level ); <nl>  <nl> if ( level > LOG_INFO ) { <nl> cl_log_enable_stderr ( TRUE ); <nl> } <nl> break ; <nl> + <nl> case DEBUG_DEC : <nl> level = get_crm_log_level (); <nl> set_crm_log_level ( level - 1 ); <nl> level = get_crm_log_level (); <nl> fprintf ( stderr , " Reduced log level to % d \ n ", level ); <nl> + cl_log ( LOG_INFO , " Reduced log level to % d \ n ", level ); <nl>  <nl> if ( level < LOG_DEBUG ) { <nl> cl_log_enable_stderr ( FALSE ); <nl> } <nl> + break ; <nl> + <nl> default : <nl> fprintf ( stderr , " Unknown signal % d \ n ", nsig ); <nl> + cl_log ( LOG_ERR , " Unknown signal % d \ n ", nsig ); <nl> break ; <nl> } <nl> }
find_topology_for_host ( const char * host ) <nl> crm_info (" Bad regex '% s ' for fencing level ", tp -> node ); <nl> } else { <nl> status = regexec (& r_patt , host , 0 , NULL , 0 ); <nl> + regfree (& r_patt ); <nl> } <nl>  <nl> if ( status == 0 ) {
crm_graph_functions_t te_graph_fns = { <nl> te_fence_node <nl> }; <nl>  <nl> + extern GMainLoop * mainloop ; <nl> + <nl> void <nl> notify_crmd ( crm_graph_t * graph ) <nl> { <nl> notify_crmd ( crm_graph_t * graph ) <nl>  <nl> case tg_shutdown : <nl> crm_info (" Exiting after transition "); <nl> + if ( mainloop != NULL && g_main_is_running ( mainloop )) { <nl> + g_main_quit ( mainloop ); <nl> + return ; <nl> + } <nl> exit ( LSB_EXIT_OK ); <nl> } <nl> 
do_election_count_vote ( long long action , <nl> crm_debug (" Election % d , owner : % s ", election_id , election_owner ); <nl>  <nl> /* update the list of nodes that have voted */ <nl> - if ( crm_str_eq ( fsa_our_uuid , election_owner )) { <nl> + if ( safe_str_eq ( fsa_our_uuid , election_owner )) { <nl> if ( election_id == current_election_id ) { <nl> char * uname_copy = NULL ; <nl> char * op_copy = crm_strdup ( op ); <nl> do_election_count_vote ( long long action , <nl> } <nl>  <nl> } else { <nl> - CRM_CHECK ( crm_str_neq ( op , CRM_OP_NOVOTE ), return I_NULL ); <nl> + CRM_CHECK ( safe_str_neq ( op , CRM_OP_NOVOTE ), return I_NULL ); <nl> } <nl>  <nl> - if ( vote_from == NULL || crm_str_eq ( vote_from , fsa_our_uname )) { <nl> + if ( vote_from == NULL || safe_str_eq ( vote_from , fsa_our_uname )) { <nl> /* don ' t count our own vote */ <nl> crm_info (" Election ignore : our % s (% s )", op , crm_str ( vote_from )); <nl> return I_NULL ; <nl>  <nl> - } else if ( crm_str_eq ( op , CRM_OP_NOVOTE )) { <nl> + } else if ( safe_str_eq ( op , CRM_OP_NOVOTE )) { <nl> crm_info (" Election ignore : no - vote from % s ", vote_from ); <nl> return I_NULL ; <nl> }
get_rsc_restart_list ( lrm_rsc_t * rsc , lrm_op_t * op ) <nl> } <nl>  <nl> metadata = string2xml ( metadata_str ); <nl> + if ( metadata == NULL ) { <nl> + crm_err (" Metadata for % s ::% s :% s is not valid XML ", <nl> + rsc -> provider , rsc -> class , rsc -> type ); <nl> + return NULL ; <nl> + } <nl> + <nl> actions = find_xml_node ( metadata , " actions ", TRUE ); <nl>  <nl> xml_child_iter_filter (
__xml_log_element ( int log_level , const char * file , const char * function , int lin <nl> } else { <nl> buffer_print ( buffer , max , offset , "/>"); <nl> } <nl> + <nl> do_crm_log_alias ( log_level , file , function , line , "% s % s ", prefix , buffer ); <nl> + free ( buffer ); <nl> + buffer = NULL ; /* Reset the buffer */ <nl> } <nl>  <nl> if ( data -> type == XML_COMMENT_NODE ) { <nl> __xml_log_element ( int log_level , const char * file , const char * function , int lin <nl> } else if ( is_set ( options , xml_log_option_children )) { <nl> offset = 0 ; <nl> max = 0 ; <nl> - free ( buffer ); <nl> - buffer = NULL ; <nl>  <nl> for ( child = __xml_first_child ( data ); child != NULL ; child = __xml_next ( child )) { <nl> __xml_log_element ( log_level , file , function , line , prefix , child , depth + 1 , options | xml_log_option_open | xml_log_option_close ); <nl> } <nl> + free ( buffer ); <nl> + buffer = NULL ; /* Reset the buffer */ <nl> } <nl>  <nl> if ( is_set ( options , xml_log_option_close )) { <nl> __xml_log_element ( int log_level , const char * file , const char * function , int lin <nl> buffer_print ( buffer , max , offset , "</% s >", name ); <nl>  <nl> do_crm_log_alias ( log_level , file , function , line , "% s % s ", prefix , buffer ); <nl> + free ( buffer ); <nl> + buffer = NULL ; /* Reset the buffer */ <nl> } <nl> } <nl> 
__subtract_xml_object ( xmlNode * target , xmlNode * patch ) <nl> xmlNode * target_child = NULL ; <nl> xmlAttrPtr xIter = NULL ; <nl>  <nl> - const char * id = NULL ; <nl> + char * id = NULL ; <nl> const char * name = NULL ; <nl> const char * value = NULL ; <nl>  <nl> __subtract_xml_object ( xmlNode * target , xmlNode * patch ) <nl> subtract_xml_comment ( target -> parent , target , patch , & dummy ); <nl> } <nl>  <nl> - id = ID ( target ); <nl> name = crm_element_name ( target ); <nl> CRM_CHECK ( name != NULL , return ); <nl> CRM_CHECK ( safe_str_eq ( crm_element_name ( target ), crm_element_name ( patch )), return ); <nl> CRM_CHECK ( safe_str_eq ( ID ( target ), ID ( patch )), return ); <nl>  <nl> /* check for XML_DIFF_MARKER in a child */ <nl> + id = crm_element_value_copy ( target , XML_ATTR_ID ); <nl> value = crm_element_value ( patch , XML_DIFF_MARKER ); <nl> if ( value != NULL && strcmp ( value , " removed : top ") == 0 ) { <nl> crm_trace (" We are the root of the deletion : % s . id =% s ", name , id ); <nl> free_xml ( target ); <nl> + free ( id ); <nl> return ; <nl> } <nl>  <nl> __subtract_xml_object ( xmlNode * target , xmlNode * patch ) <nl>  <nl> __subtract_xml_object ( target_child , patch_child ); <nl> } <nl> + free ( id ); <nl> } <nl>  <nl> static void
run_simulation ( pe_working_set_t * data_set ) <nl>  <nl> if ( quiet == FALSE ) { <nl> xmlNode * cib_object = NULL ; <nl> - ha_time_t * a_date = data_set -> now ; <nl> int rc = global_cib -> cmds -> query ( global_cib , NULL , & cib_object , cib_sync_call | cib_scope_local ); <nl> - CRM_ASSERT ( rc == cib_ok ); <nl> + ha_time_t * a_date = data_set -> now ; data_set -> now = NULL ; /* Prevent it being free ' d in cleanup_alloc_calculations () */ <nl>  <nl> + CRM_ASSERT ( rc == cib_ok ); <nl> quiet_log ("\ nRevised cluster status :\ n "); <nl> cleanup_alloc_calculations ( data_set ); <nl> data_set -> input = cib_object ;
common_apply_stickiness ( resource_t * rsc , node_t * node , pe_working_set_t * data_se <nl> value = g_hash_table_lookup ( node -> details -> attrs , fail_attr ); <nl> if ( value != NULL ) { <nl> crm_debug ("% s : % s ", fail_attr , value ); <nl> - fail_count = crm_parse_int ( value , " 0 "); <nl> + fail_count = char2score ( value ); <nl> } <nl> crm_free ( fail_attr ); <nl> 
namespace Checkpoints { <nl> ( 225430 , uint256 (" 0x00000000000001c108384350f74090433e7fcf79a606b8e797f065b130575932 ")) <nl> ( 250000 , uint256 (" 0x000000000000003887df1f29024b06fc2200b55f8af8f35453d7be294df2d214 ")) <nl> ( 279000 , uint256 (" 0x0000000000000001ae8c72a0b0c301f67e3afca10e819efa9041e458e9bd7e40 ")) <nl> + ( 295000 , uint256 (" 0x00000000000000004d9b4ef50f0f9d686fd69db2e03af35a100370c64632a983 ")) <nl> ; <nl> static const CCheckpointData data = { <nl> & mapCheckpoints , <nl> - 1389047471 , // * UNIX timestamp of last checkpoint block <nl> - 30549816 , // * total number of transactions between genesis and last checkpoint <nl> + 1397080064 , // * UNIX timestamp of last checkpoint block <nl> + 36544669 , // * total number of transactions between genesis and last checkpoint <nl> // ( the tx =... number in the SetBestChain debug . log lines ) <nl> 60000 . 0 // * estimated number of transactions per day after checkpoint <nl> };
int main ( int argc , char * argv []) <nl> QString locale = QLocale :: system (). name (); <nl> QTranslator translator ; <nl> translator . load (":/ translations /"+ locale ); <nl> - app . installTranslator (& translator ); <nl> + if (! translator . isEmpty ()) <nl> + app . installTranslator (& translator ); <nl>  <nl> QSplashScreen splash ( QPixmap (":/ images / splash "), 0 ); <nl> splash . show ();
int git_repository_config__weakptr ( git_config ** out , git_repository * repo ) <nl> git_buf_free (& global_buf ); <nl> git_buf_free (& xdg_buf ); <nl> git_buf_free (& system_buf ); <nl> + git_buf_free (& programdata_buf ); <nl> } <nl>  <nl> * out = repo -> _config ;
git_oid_shorten * git_oid_shorten_new ( size_t min_length ) <nl>  <nl> void git_oid_shorten_free ( git_oid_shorten * os ) <nl> { <nl> + if ( os == NULL ) <nl> + return ; <nl> + <nl> git__free ( os -> nodes ); <nl> git__free ( os ); <nl> }
uint32_t git_pool__system_page_size ( void ) <nl> size_t page_size ; <nl> if ( git__page_size (& page_size ) < 0 ) <nl> page_size = 4096 ; <nl> - size = page_size - 2 * sizeof ( void *); /* allow space for malloc overhead */ <nl> + /* allow space for malloc overhead */ <nl> + size = page_size - ( 2 * sizeof ( void *)) - sizeof ( git_pool_page ); <nl> } <nl>  <nl> return size ;
static int crlf_check ( <nl> return GIT_PASSTHROUGH ; <nl>  <nl> if ( ca . crlf_action == GIT_CRLF_GUESS || <nl> - (( ca . crlf_action == GIT_CRLF_AUTO || ca . crlf_action == GIT_CRLF_TEXT ) && <nl> + (( ca . crlf_action == GIT_CRLF_AUTO || <nl> + ca . crlf_action == GIT_CRLF_TEXT ) && <nl> git_filter_source_mode ( src ) == GIT_FILTER_SMUDGE )) { <nl>  <nl> error = git_repository__cvar ( <nl> static int crlf_check ( <nl> ca . auto_crlf == GIT_AUTO_CRLF_FALSE ) <nl> return GIT_PASSTHROUGH ; <nl>  <nl> - if ( ca . auto_crlf == GIT_AUTO_CRLF_INPUT && ca . eol != GIT_EOL_CRLF && <nl> + if ( ca . auto_crlf == GIT_AUTO_CRLF_INPUT && <nl> + ca . eol != GIT_EOL_CRLF && <nl> git_filter_source_mode ( src ) == GIT_FILTER_SMUDGE ) <nl> return GIT_PASSTHROUGH ; <nl> }
const char * make_absolute_path ( const char * path ) <nl> char * last_elem = NULL ; <nl> struct stat st ; <nl>  <nl> + /* We ' ve already done it */ <nl> + if ( path == buf || path == next_buf ) <nl> + return path ; <nl> + <nl> if ( strlcpy ( buf , path , PATH_MAX ) >= PATH_MAX ) <nl> die (" Too long path : %.* s ", 60 , path ); <nl> 
static int prepare_lines ( struct scoreboard * sb ) <nl> bol = 1 ; <nl> } <nl> } <nl> + sb -> lineno = xrealloc ( sb -> lineno , <nl> + sizeof ( int * ) * ( num + incomplete + 1 )); <nl> + sb -> lineno [ num + incomplete ] = buf - sb -> final_buf ; <nl> sb -> num_lines = num + incomplete ; <nl> return sb -> num_lines ; <nl> }
static void setup_progress_signal ( void ) <nl> int main ( int argc , char ** argv ) <nl> { <nl> SHA_CTX ctx ; <nl> - char line [ PATH_MAX + 20 ]; <nl> + char line [ 40 + 1 + PATH_MAX + 2 ]; <nl> int window = 10 , depth = 10 , pack_to_stdout = 0 ; <nl> struct object_entry ** list ; <nl> int num_preferred_base = 0 ;
static void show_line ( struct grep_opt * opt , char * bol , char * eol , <nl>  <nl> * eol = '\ 0 '; <nl> while ( next_match ( opt , bol , eol , ctx , & match , eflags )) { <nl> + if ( match . rm_so == match . rm_eo ) <nl> + break ; <nl> printf ("%.* s % s %.* s % s ", <nl> ( int ) match . rm_so , bol , <nl> opt -> color_match ,
# include " mailmap . h " <nl> # include " gpg - interface . h " <nl> # include " progress . h " <nl> +# include " commit - slab . h " <nl>  <nl> # define MAIL_DEFAULT_WRAP 72 <nl>  <nl> static struct commit * get_base_commit ( const char * base_commit , <nl> return base ; <nl> } <nl>  <nl> + define_commit_slab ( commit_base , int ); <nl> + <nl> static void prepare_bases ( struct base_tree_info * bases , <nl> struct commit * base , <nl> struct commit ** list , <nl> static void prepare_bases ( struct base_tree_info * bases , <nl> struct commit * commit ; <nl> struct rev_info revs ; <nl> struct diff_options diffopt ; <nl> + struct commit_base commit_base ; <nl> int i ; <nl>  <nl> if (! base ) <nl> return ; <nl>  <nl> + init_commit_base (& commit_base ); <nl> diff_setup (& diffopt ); <nl> diffopt . flags . recursive = 1 ; <nl> diff_setup_done (& diffopt ); <nl> static void prepare_bases ( struct base_tree_info * bases , <nl> for ( i = 0 ; i < total ; i ++) { <nl> list [ i ]-> object . flags &= ~ UNINTERESTING ; <nl> add_pending_object (& revs , & list [ i ]-> object , " rev_list "); <nl> - list [ i ]-> util = ( void *) 1 ; <nl> + * commit_base_at (& commit_base , list [ i ]) = 1 ; <nl> } <nl> base -> object . flags |= UNINTERESTING ; <nl> add_pending_object (& revs , & base -> object , " base "); <nl> static void prepare_bases ( struct base_tree_info * bases , <nl> while (( commit = get_revision (& revs )) != NULL ) { <nl> struct object_id oid ; <nl> struct object_id * patch_id ; <nl> - if ( commit -> util ) <nl> + if (* commit_base_at (& commit_base , commit )) <nl> continue ; <nl> if ( commit_patch_id ( commit , & diffopt , & oid , 0 )) <nl> die ( _ (" cannot get patch id ")); <nl> static void prepare_bases ( struct base_tree_info * bases , <nl> oidcpy ( patch_id , & oid ); <nl> bases -> nr_patch_id ++; <nl> } <nl> + clear_commit_base (& commit_base ); <nl> } <nl>  <nl> static void print_bases ( struct base_tree_info * bases , FILE * file )
int xmkstemp ( char * template ) <nl> int saved_errno = errno ; <nl> const char * nonrelative_template ; <nl>  <nl> - if (! template [ 0 ]) <nl> + if ( strlen ( template ) != strlen ( origtemplate )) <nl> template = origtemplate ; <nl>  <nl> nonrelative_template = absolute_path ( template );
static NORETURN void compile_regexp_failed ( const struct grep_pat * p , <nl> char where [ 1024 ]; <nl>  <nl> if ( p -> no ) <nl> - sprintf ( where , " In '% s ' at % d , ", p -> origin , p -> no ); <nl> + xsnprintf ( where , sizeof ( where ), " In '% s ' at % d , ", p -> origin , p -> no ); <nl> else if ( p -> origin ) <nl> - sprintf ( where , "% s , ", p -> origin ); <nl> + xsnprintf ( where , sizeof ( where ), "% s , ", p -> origin ); <nl> else <nl> where [ 0 ] = 0 ; <nl> 
static int merged_entry ( struct cache_entry * merge , struct cache_entry * old , <nl> * a match . <nl> */ <nl> if ( same ( old , merge )) { <nl> - * merge = * old ; <nl> + memcpy ( merge , old , offsetof ( struct cache_entry , name )); <nl> } else { <nl> verify_uptodate ( old , o ); <nl> invalidate_ce_path ( old );
static int estimate_similarity ( struct diff_filespec * src , <nl> /* A delta that has a lot of literal additions would have <nl> * big delta_size no matter what else it does . <nl> */ <nl> - if ( base_size * ( MAX_SCORE - minimum_score ) < delta_size * MAX_SCORE ) <nl> + if ( base_size * ( MAX_SCORE - minimum_score ) < delta_size * MAX_SCORE ) { <nl> + free ( delta ); <nl> return 0 ; <nl> + } <nl>  <nl> /* Estimate the edit size by interpreting delta . */ <nl> if ( count_delta ( delta , delta_size , & src_copied , & literal_added )) {
int fts_expunge_log_uid_count ( struct fts_expunge_log * log , <nl> { <nl> int ret ; <nl>  <nl> - if (( ret = fts_expunge_log_reopen_if_needed ( log , FALSE )) <= 0 ) <nl> + if (( ret = fts_expunge_log_reopen_if_needed ( log , FALSE )) <= 0 ) { <nl> + * expunges_r = 0 ; <nl> return ret ; <nl> + } <nl>  <nl> return fts_expunge_log_read_expunge_count ( log , expunges_r ); <nl> }
static const struct command imap4rev1_commands [] = { <nl> { " APPEND ", cmd_append , COMMAND_FLAG_BREAKS_SEQS }, <nl> { " EXAMINE ", cmd_examine , COMMAND_FLAG_BREAKS_MAILBOX }, <nl> { " CREATE ", cmd_create , 0 }, <nl> - { " DELETE ", cmd_delete , COMMAND_FLAG_USE_NONEXISTENT }, <nl> + { " DELETE ", cmd_delete , COMMAND_FLAG_BREAKS_MAILBOX | <nl> + COMMAND_FLAG_USE_NONEXISTENT }, <nl> { " RENAME ", cmd_rename , COMMAND_FLAG_USE_NONEXISTENT }, <nl> { " LIST ", cmd_list , 0 }, <nl> { " LSUB ", cmd_lsub , 0 },
int o_buffer_have_space ( OBuffer * buf , size_t size ) <nl> return _buf -> have_space ( _buf , size ); <nl> } <nl>  <nl> - ssize_t o_buffer_seek ( OBuffer * buf , uoff_t offset ) <nl> + int o_buffer_seek ( OBuffer * buf , uoff_t offset ) <nl> { <nl> _OBuffer * _buf = buf -> real_buffer ; <nl> 
view_sync_get_expunges ( struct mail_index_view * view , <nl> struct seq_range * src , * src_end , * dest ; <nl> const void * data ; <nl> unsigned int count , expunge_count = 0 ; <nl> + uint32_t prev_seq = 0 ; <nl> int ret ; <nl>  <nl> if ( view_sync_set_log_view_range ( view , TRUE ) < 0 ) <nl> view_sync_get_expunges ( struct mail_index_view * view , <nl> if ( dest -> seq1 == 0 ) <nl> count --; <nl> else { <nl> + i_assert ( dest -> seq1 > prev_seq ); <nl> + prev_seq = dest -> seq2 ; <nl> + <nl> expunge_count += dest -> seq2 - dest -> seq1 + 1 ; <nl> dest ++; <nl> }
mbox_sync_handle_missing_space ( struct mbox_sync_mail_context * mail_ctx ) <nl> needed_space = mail_ctx -> mail . space - sync_ctx -> space_diff ; <nl> if (( uoff_t ) sync_ctx -> space_diff > needed_space + extra_space ) { <nl> /* don ' t waste too much on padding */ <nl> - sync_ctx -> expunged_space = mail_ctx -> mail . space - <nl> - ( needed_space + extra_space ); <nl> - sync_ctx -> space_diff = needed_space + extra_space ; <nl> + move_diff = needed_space + extra_space ; <nl> + sync_ctx -> expunged_space = <nl> + mail_ctx -> mail . space - move_diff ; <nl> } else { <nl> - extra_space = sync_ctx -> space_diff ; <nl> + move_diff = mail_ctx -> mail . space ; <nl> + extra_space = sync_ctx -> space_diff - needed_space ; <nl> } <nl> last_seq = sync_ctx -> seq - 1 ; <nl> buffer_set_used_size ( sync_ctx -> mails , sync_ctx -> mails -> used - <nl> sizeof ( mail_ctx -> mail )); <nl> end_offset = mail_ctx -> mail . from_offset ; <nl> - move_diff = sync_ctx -> space_diff ; <nl> } else { <nl> /* this message gave enough space from headers . rewriting stops <nl> at the end of this message ' s headers . */
fts_backend_solr_update_deinit ( struct fts_backend_update_context * _ctx ) <nl> visible to the following search */ <nl> if ( ctx -> expunges ) <nl> fts_backend_solr_expunge_flush ( ctx ); <nl> - str = t_strdup_printf ("< commit waitFlush =\" false \" " <nl> - " waitSearcher =\"% s \"/>", <nl> + str = t_strdup_printf ("< commit waitSearcher =\"% s \"/>", <nl> ctx -> documents_added ? " true " : " false "); <nl> if ( solr_connection_post ( solr_conn , str ) < 0 ) <nl> ret = - 1 ;
prevfile : <nl> str_append ( str , key ); <nl> str_append_c ( str , '='); <nl>  <nl> - if ( type != CONFIG_LINE_TYPE_KEYFILE || ! expand_files ) <nl> + if ( type != CONFIG_LINE_TYPE_KEYFILE ) <nl> str_append ( str , value ); <nl> - else if ( str_append_file ( str , key , value , & errormsg ) < 0 ) { <nl> + else if (! expand_files ) { <nl> + str_append_c ( str , '<'); <nl> + str_append ( str , value ); <nl> + } else if ( str_append_file ( str , key , value , & errormsg ) < 0 ) { <nl> /* file reading failed */ <nl> break ; <nl> }
struct maildir_uidlist_sync_ctx { <nl>  <nl> unsigned int first_unwritten_pos , first_nouid_pos ; <nl> unsigned int new_files_count ; <nl> + unsigned int finish_change_counter ; <nl>  <nl> unsigned int partial : 1 ; <nl> unsigned int finished : 1 ; <nl> static int maildir_uidlist_write_fd ( struct maildir_uidlist * uidlist , int fd , <nl> } <nl>  <nl> iter = maildir_uidlist_iter_init ( uidlist ); <nl> + i_assert ( first_idx <= array_count (& uidlist -> records )); <nl> iter -> next += first_idx ; <nl>  <nl> while ( maildir_uidlist_iter_next_rec ( iter , & rec )) { <nl> static int maildir_uidlist_sync_update ( struct maildir_uidlist_sync_ctx * ctx ) <nl>  <nl> if ( ctx -> uidlist -> recreate || uidlist -> fd == - 1 || <nl> uidlist -> version != 3 || <nl> + ctx -> finish_change_counter != ctx -> uidlist -> change_counter || <nl> ( uidlist -> read_records_count + ctx -> new_files_count ) * <nl> UIDLIST_COMPRESS_PERCENTAGE / 100 >= array_count (& uidlist -> records )) <nl> return maildir_uidlist_recreate ( uidlist ); <nl> static void maildir_uidlist_assign_uids ( struct maildir_uidlist_sync_ctx * ctx ) <nl>  <nl> ctx -> uidlist -> last_seen_uid = ctx -> uidlist -> next_uid - 1 ; <nl> ctx -> uidlist -> change_counter ++; <nl> + ctx -> finish_change_counter = ctx -> uidlist -> change_counter ; <nl> } <nl>  <nl> static void maildir_uidlist_swap ( struct maildir_uidlist_sync_ctx * ctx ) <nl> static void maildir_uidlist_swap ( struct maildir_uidlist_sync_ctx * ctx ) <nl> if ( ctx -> new_files_count != 0 ) { <nl> ctx -> first_nouid_pos = count - ctx -> new_files_count ; <nl> maildir_uidlist_assign_uids ( ctx ); <nl> + } else { <nl> + ctx -> uidlist -> change_counter ++; <nl> } <nl> - <nl> - ctx -> uidlist -> change_counter ++; <nl> } <nl>  <nl> void maildir_uidlist_sync_finish ( struct maildir_uidlist_sync_ctx * ctx )
penalty_bump_checksum ( struct penalty_rec * rec , unsigned int checksum ) <nl> for ( i = 0 ; i < count ; i ++) { <nl> if ( checksums [ i ] == checksum ) { <nl> if ( i > 0 ) { <nl> - memcpy ( checksums + 1 , checksums , <nl> - sizeof ( checksums [ 0 ]) * i ); <nl> + memmove ( checksums + 1 , checksums , <nl> + sizeof ( checksums [ 0 ]) * i ); <nl> checksums [ 0 ] = checksum ; <nl> } <nl> return TRUE ;
static void mail_transaction_log_2_unlink_old ( struct mail_transaction_log * log ) <nl> return ; <nl> } <nl>  <nl> - if ( st . st_mtime + log -> index -> log_rotate_log2_stale_secs <= ioloop_time && <nl> + if ( ioloop_time - st . st_mtime >= ( time_t ) log -> index -> log_rotate_log2_stale_secs && <nl> ! log -> index -> readonly ) <nl> i_unlink_if_exists ( log -> filepath2 ); <nl> }
struct http_client * http_client_init ( const struct http_client_settings * set ) <nl> client -> set . user_agent = p_strdup_empty ( pool , set -> user_agent ); <nl> client -> set . rawlog_dir = p_strdup_empty ( pool , set -> rawlog_dir ); <nl>  <nl> - client -> set . ssl = ssl_iostream_settings_dup ( client -> pool , set -> ssl ); <nl> + if ( set -> ssl != NULL ) <nl> + client -> set . ssl = ssl_iostream_settings_dup ( client -> pool , set -> ssl ); <nl>  <nl> if ( set -> proxy_socket_path != NULL && * set -> proxy_socket_path != '\ 0 ') { <nl> client -> set . proxy_socket_path = p_strdup ( pool , set -> proxy_socket_path ); <nl> int http_client_init_ssl_ctx ( struct http_client * client , const char ** error_r ) <nl> if ( client -> ssl_ctx != NULL ) <nl> return 0 ; <nl>  <nl> + if ( client -> set . ssl == NULL ) { <nl> + * error_r = " Requested https connection , but no SSL settings given "; <nl> + return - 1 ; <nl> + } <nl> if ( ssl_iostream_context_init_client ( client -> set . ssl , & client -> ssl_ctx , & error ) < 0 ) { <nl> * error_r = t_strdup_printf (" Couldn ' t initialize SSL context : % s ", <nl> error );
static void push_notification_event_mailboxunsubscribe_event ( <nl>  <nl> data = p_new ( ptxn -> pool , <nl> struct push_notification_event_mailboxunsubscribe_data , 1 ); <nl> - data -> subscribe = TRUE ; <nl> + data -> subscribe = FALSE ; <nl>  <nl> push_notification_txn_mbox_set_eventdata ( ptxn , mbox , ec , data ); <nl> }
int mountpoint_get ( const char * path , pool_t pool , struct mountpoint * point_r ) <nl> if ( device_path == NULL ) <nl> return 0 ; <nl>  <nl> + memset ( point_r , 0 , sizeof (* point_r )); <nl> point_r -> device_path = p_strdup ( pool , device_path ); <nl> point_r -> mount_path = p_strdup ( pool , mount_path ); <nl> point_r -> type = p_strdup ( pool , type );
int mail_cache_expunge_handler ( struct mail_index_sync_map_ctx * sync_ctx , <nl> if (* cache_offset == 0 ) <nl> return 1 ; <nl>  <nl> + if ( MAIL_CACHE_IS_UNUSABLE ( cache )) <nl> + return 1 ; <nl> + <nl> ret = mail_cache_handler_init (& ctx , cache ); <nl> * context = ctx ; <nl> if ( ret <= 0 ) <nl> int mail_cache_sync_handler ( struct mail_index_sync_map_ctx * sync_ctx , <nl> return 1 ; <nl> } <nl>  <nl> + if ( MAIL_CACHE_IS_UNUSABLE ( cache )) <nl> + return 1 ; <nl> + <nl> if ( cache -> file_cache != NULL ) { <nl> file_cache_invalidate ( cache -> file_cache , * new_cache_offset , <nl> ( size_t )- 1 );
message_parser_init_from_parts ( struct message_part * parts , <nl> { <nl> struct message_parser_ctx * ctx ; <nl>  <nl> + i_assert ( parts != NULL ); <nl> + <nl> ctx = message_parser_init_int ( input , hdr_flags , flags ); <nl> ctx -> parts = ctx -> part = parts ; <nl> ctx -> parse_next_block = preparsed_parse_next_header_init ; <nl> int message_parser_deinit ( struct message_parser_ctx ** _ctx , <nl> message_parse_header_deinit (& ctx -> hdr_parser_ctx ); <nl> i_stream_unref (& ctx -> input ); <nl> pool_unref (& ctx -> parser_pool ); <nl> + i_assert ( ret < 0 || * parts_r != NULL ); <nl> return ret ; <nl> } <nl> 
mbox_sync_read_next_mail ( struct mbox_sync_context * sync_ctx , <nl> i_assert ( sync_ctx -> input -> v_offset != mail_ctx -> mail . from_offset || <nl> sync_ctx -> input -> eof ); <nl>  <nl> + if ( istream_raw_mbox_is_corrupted ( sync_ctx -> input )) <nl> + return - 1 ; <nl> + <nl> mail_ctx -> mail . body_size = <nl> istream_raw_mbox_get_body_size ( sync_ctx -> input , <nl> mail_ctx -> content_length ); <nl> static int mbox_sync_loop ( struct mbox_sync_context * sync_ctx , <nl> ret = mbox_sync_partial_seek_next ( sync_ctx , uid + 1 , <nl> & partial , <nl> & skipped_mails ); <nl> - if ( ret <= 0 ) { <nl> - if ( ret < 0 ) <nl> - return - 1 ; <nl> + if ( ret <= 0 ) <nl> break ; <nl> - } <nl> } <nl> } <nl> + if ( ret < 0 ) <nl> + return - 1 ; <nl>  <nl> if ( istream_raw_mbox_is_eof ( sync_ctx -> input )) { <nl> /* rest of the messages in index don ' t exist -> expunge them */
static int imap_parser_read_atom ( struct imap_parser * parser , <nl> imap_parser_save_arg ( parser , data , i ); <nl> break ; <nl> } else if ( data [ i ] == ')') { <nl> - if ( parser -> list_arg != NULL || <nl> - ( parser -> flags & <nl> - IMAP_PARSE_FLAG_ATOM_ALLCHARS ) == 0 ) { <nl> + if ( parser -> list_arg != NULL ) { <nl> imap_parser_save_arg ( parser , data , i ); <nl> break ; <nl> + } else if (( parser -> flags & <nl> + IMAP_PARSE_FLAG_ATOM_ALLCHARS ) == 0 ) { <nl> + parser -> error = " Unexpected ')'"; <nl> + return FALSE ; <nl> } <nl> /* assume it ' s part of the atom */ <nl> } else if (! is_valid_atom_char ( parser , data [ i ]))
add_conf_item ( const char * topconf , const char * name , int type , void (* func ) ( voi <nl> if (( tc = find_top_conf ( topconf )) == NULL ) <nl> return - 1 ; <nl>  <nl> - if (( cf = find_conf_item ( tc , name )) != NULL ) <nl> + if ( find_conf_item ( tc , name ) != NULL ) <nl> return - 1 ; <nl>  <nl> cf = rb_malloc ( sizeof ( struct ConfEntry ));
ms_sjoin ( struct Client * client_p , struct Client * source_p , int parc , const char <nl> static char empty [] = ""; <nl> rb_dlink_node * ptr , * next_ptr ; <nl>  <nl> + if ( parc < 5 ) <nl> + return 0 ; <nl> + <nl> if (! IsChannelName ( parv [ 2 ]) || ! check_channel_name ( parv [ 2 ])) <nl> return 0 ; <nl> 
* Copyright ( C ) 1990 Jarkko Oikarinen and University of Oulu , Co Center <nl> * Copyright ( C ) 1996 - 2002 Hybrid Development Team <nl> * Copyright ( C ) 2002 - 2008 ircd - ratbox development team <nl> - * Copyright ( C ) 2005 - 2008 charybdis development team <nl> + * Copyright ( C ) 2005 - 2013 charybdis development team <nl> * <nl> * This program is free software ; you can redistribute it and / or modify <nl> * it under the terms of the GNU General Public License as published by <nl> main ( int argc , char * argv []) <nl> inotice (" starting % s ...", ircd_version ); <nl> inotice ("% s ", rb_lib_version ()); <nl> } <nl> + <nl> + /* Make sure config file exists -- Quora */ <nl> + if ( access ( CPATH , F_OK ) == - 1 ) { <nl> + inotice (" FATAL : No config file found at % s , exiting ", CPATH ); <nl> + exit (- 1 ); <nl> + } <nl>  <nl> /* Init the event subsystem */ <nl> rb_lib_init ( ircd_log_cb , ircd_restart_cb , ircd_die_cb , ! server_state_foreground , maxconnections , DNODE_HEAP_SIZE , FD_HEAP_SIZE );
m_authenticate ( struct Client * client_p , struct Client * source_p , <nl> return 0 ; <nl> } <nl>  <nl> + if (* parv [ 1 ] == ':' || strchr ( parv [ 1 ], ' ')) <nl> + { <nl> + exit_client ( client_p , client_p , client_p , " Malformed AUTHENTICATE "); <nl> + return 0 ; <nl> + } <nl> + <nl> saslserv_p = find_named_client ( ConfigFileEntry . sasl_service ); <nl> if ( saslserv_p == NULL || ! IsService ( saslserv_p )) <nl> {
static CURLcode file_range ( struct connectdata * conn ) <nl> else { <nl> /* X - Y */ <nl> totalsize = to - from ; <nl> + if ( totalsize == CURL_OFF_T_MAX ) <nl> + /* this is too big to increase , so bail out */ <nl> + return CURLE_RANGE_ERROR ; <nl> data -> req . maxdownload = totalsize + 1 ; /* include last byte */ <nl> data -> state . resume_from = from ; <nl> DEBUGF ( infof ( data , " RANGE from %" CURL_FORMAT_CURL_OFF_T
int Curl_resolv_timeout ( struct connectdata * conn , <nl>  <nl> * entry = NULL ; <nl>  <nl> + if ( timeoutms < 0 ) <nl> + /* got an already expired timeout */ <nl> + return CURLRESOLV_TIMEDOUT ; <nl> + <nl> # ifdef USE_ALARM_TIMEOUT <nl> if ( data -> set . no_signal ) <nl> /* Ignore the timeout when signals are disabled */
Curl_cookie_add ( struct Curl_easy * data , <nl> /* too long individual name or contents , or too long combination of <nl> name + contents . Chrome and Firefox support 4095 or 4096 bytes <nl> combo . */ <nl> - free ( co ); <nl> + freecookie ( co ); <nl> infof ( data , " oversized cookie dropped , name / val % d + % d bytes \ n ", <nl> nlen , len ); <nl> return NULL ;
void Curl_sasl_digest_cleanup ( struct digestdata * digest ) <nl> * This is used to generate an already encoded NTLM type - 1 message ready for <nl> * sending to the recipient . <nl> * <nl> -* Note : This is a simple wrapper of the NTLM function which means that any <nl> -* SASL based protocols don ' t have to include the NTLM functions directly . <nl> -* <nl> * Parameters : <nl> * <nl> * userp [ in ] - The user name in the format User or Domain \ User .
CURLcode Curl_perform ( struct SessionHandle * data ) <nl> * an error , use the strerror () string or if things are so bad that not <nl> * even that is good , set a bad string that mentions the error code . <nl> */ <nl> - char * str = curl_easy_strerror ( res ); <nl> + const char * str = curl_easy_strerror ( res ); <nl> if (! str ) <nl> failf ( data , " unspecified error % d ", ( int ) res ); <nl> else
CURLcode Curl_conncache_add_conn ( struct conncache * connc , <nl> return result ; <nl>  <nl> key = hashkey ( conn ); <nl> - if (! key ) <nl> + if (! key ) { <nl> + bundle_destroy ( new_bundle ); <nl> return CURLE_OUT_OF_MEMORY ; <nl> + } <nl>  <nl> rc = conncache_add_bundle ( data -> state . conn_cache , key , new_bundle ); <nl> free ( key );
CURLMcode Curl_pipeline_set_site_blacklist ( char ** sites , <nl> bool Curl_pipeline_server_blacklisted ( struct SessionHandle * handle , <nl> char * server_name ) <nl> { <nl> - if ( handle -> multi ) { <nl> + if ( handle -> multi && server_name ) { <nl> struct curl_llist * blacklist = <nl> Curl_multi_pipelining_server_bl ( handle -> multi ); <nl> 
pango_item_free ( PangoItem * item ) <nl> g_slist_free ( item -> analysis . extra_attrs ); <nl> } <nl>  <nl> - if ( result -> analysis . font ) <nl> + if ( item -> analysis . font ) <nl> g_object_unref ( item -> analysis . font ); <nl>  <nl> g_free ( item );
int TMomFinalizeChild ( <nl> /* Put the script ' s arguments on the command line ( see configure option -- enable - shell - use - argv ). */ <nl> if ( TJE -> is_interactive == FALSE ) <nl> { <nl> - arg [ aindex ] = calloc ( 1 , <nl> + arg [ aindex ] = ( char *) calloc ( 1 , <nl> strlen ( path_jobs ) + <nl> strlen ( pjob -> ji_qs . ji_fileprefix ) + <nl> strlen ( JOB_SCRIPT_SUFFIX ) + 6 );
void * queue_route ( void * vp ) <nl> exit ( 1 ); <nl> } <nl>  <nl> - void acct_close ( void ) <nl> + void acct_close ( bool ) <nl> { <nl> fprintf ( stderr , " The call to acct_close needs to be mocked !!\ n "); <nl> exit ( 1 );
int add_mic_status ( <nl> return ( PBSE_NONE ); <nl> # endif <nl>  <nl> - for ( unsigned int i = 0 ; i < MAX_ENGINES ; i ++) <nl> - { <nl> - memset ( engine [ i ], 0 , sizeof ( COIENGINE )); <nl> - memset ( mic_stat [ i ], 0 , sizeof ( COIENGINE )); <nl> - } <nl> + memset (& engine , 0 , sizeof ( engine )); <nl> + memset (& mic_stat , 0 , sizeof ( mic_stat )); <nl>  <nl> if ( COIEngineGetCount ( COI_ISA_MIC , & num_engines ) != COI_SUCCESS ) <nl> {
static int chk_save_file ( <nl> { <nl> struct stat sb ; <nl>  <nl> + if (* filename == '.') <nl> + { <nl> + return (- 1 ); <nl> + } <nl> + <nl> if ( stat ( filename ,& sb ) == - 1 ) <nl> { <nl> return ( errno );
int read_config ( <nl> return ( 0 ); <nl> } <nl>  <nl> + void free_pwnam ( struct passwd * pwdp , char * buf ) <nl> + {}
error : <nl>  <nl> static int mod_init ( void ) <nl> { <nl> + memset (& xhttp_api , 0 , sizeof ( xhttp_api_t )); <nl>  <nl> /* bind the XHTTP API */ <nl> if ( jsonrpc_transport == 0 || ( jsonrpc_transport & 1 )) { <nl> if ( xhttp_load_api (& xhttp_api ) < 0 ) { <nl> - if ( jsonrpc_transport == 1 ) { <nl> + if ( jsonrpc_transport & 1 ) { <nl> LM_ERR (" cannot bind to XHTTP API \ n "); <nl> return - 1 ; <nl> } else { <nl> static int jsonrpc_dispatch ( sip_msg_t * msg , char * s1 , char * s2 ) <nl> return NONSIP_MSG_PASS ; <nl> } <nl>  <nl> + if ( xhttp_api . reply == NULL ) { <nl> + LM_ERR (" jsonrpc over http not initialized - check transport param \ n "); <nl> + return NONSIP_MSG_ERROR ; <nl> + } <nl> + <nl> /* initialize jsonrpc context */ <nl> ctx = & _jsonrpc_ctx ; <nl> memset ( ctx , 0 , sizeof ( jsonrpc_ctx_t ));
int db_postgres_val2str ( const db_con_t * _con , const db_val_t * _v , char * _s , int * <nl>  <nl> case DB_BLOB : <nl> l = VAL_BLOB ( _v ). len ; <nl> + /* this estimation is not always correct , thus we need to check later again */ <nl> if (* _len < ( l * 2 + 3 )) { <nl> LM_ERR (" destination buffer too short for blob \ n "); <nl> return - 7 ; <nl> int db_postgres_val2str ( const db_con_t * _con , const db_val_t * _v , char * _s , int * <nl> LM_ERR (" PQescapeBytea failed \ n "); <nl> return - 7 ; <nl> } <nl> + if ( tmp_len > * _len ) { <nl> + LM_ERR (" escaped result too long \ n "); <nl> + return - 7 ; <nl> + } <nl> memcpy ( _s , tmp_s , tmp_len ); <nl> PQfreemem ( tmp_s ); <nl> tmp_len = strlen ( _s );
# include "../../ mem / mem . h " <nl> # include "../../ md5utils . h " <nl> # include "../../ ip_addr . h " <nl> +# include "../../ parser / parse_uri . h " <nl>  <nl> # include " config . h " <nl> # include " lock . h "
static int rtpproxy_set_store ( modparam_t type , void * val ){ <nl> return - 1 ; <nl> } <nl> } else {/* realloc to make room for the current set */ <nl> - rtpp_strings = ( char **) pkg_realloc ( rtpp_strings , <nl> + rtpp_strings = ( char **) pkg_reallocxf ( rtpp_strings , <nl> ( rtpp_sets + 1 )* sizeof ( char *)); <nl> if (! rtpp_strings ){ <nl> LM_ERR (" no pkg memory left \ n ");
void dtrie_delete ( struct dtrie_node_t * root , struct dtrie_node_t * node , <nl> if ( delete_payload ) { <nl> delete_payload ( node -> data ); <nl> } <nl> + <nl> node -> data = NULL ; <nl> - <nl> + <nl> if ( node != root ) { <nl> LM_DBG (" free node at % p \ n ", node ); <nl> + shm_free ( node -> child ); <nl> + node -> child = NULL ; <nl> shm_free ( node ); <nl> } <nl> } <nl> void dtrie_destroy ( struct dtrie_node_t ** root , dt_delete_func_t delete_payload , <nl> if (( root != NULL ) && (* root != NULL )) { <nl> dtrie_delete (* root , * root , delete_payload , branches ); <nl> LM_DBG (" free root at % p \ n ", root ); <nl> + shm_free ((* root )-> child ); <nl> shm_free (* root ); <nl> * root = NULL ; <nl> }
static inline int fake_req ( struct sip_msg * faked_req , <nl> /* if we set msg_id to something different from current ' s message <nl> * id , the first t_fork will properly clean new branch URIs */ <nl> faked_req -> id = shmem_msg -> id - 1 ; <nl> + /* msg -> parsed_uri_ok must be reset since msg_parsed_uri is <nl> + * not cloned ( and cannot be cloned ) */ <nl> + faked_req -> parsed_uri_ok = 0 ; <nl>  <nl> /* new_uri can change -- make a private copy */ <nl> if ( shmem_msg -> new_uri . s != 0 && shmem_msg -> new_uri . len != 0 ) {
int rval_get_int ( struct run_act_ctx * h , struct sip_msg * msg , <nl> break ; <nl> case RV_ACTION_ST : <nl> if ( rv -> v . action ) <nl> - * i = run_actions ( h , rv -> v . action , msg ); <nl> - else <nl> + * i =( run_actions ( h , rv -> v . action , msg )> 0 ); <nl> + else <nl> * i = 0 ; <nl> break ; <nl> case RV_SEL : <nl> int rval_get_tmp_str ( struct run_act_ctx * h , struct sip_msg * msg , <nl> break ; <nl> case RV_ACTION_ST : <nl> if ( rv -> v . action ) <nl> - i = run_actions ( h , rv -> v . action , msg ); <nl> - else <nl> + i =( run_actions ( h , rv -> v . action , msg )> 0 ); <nl> + else <nl> i = 0 ; <nl> tmpv -> s = int2str ( i , & tmpv -> len ); <nl> break ;
int get_to_uid ( str * uid , struct sip_msg * msg ) <nl> DBG (" get_to_uid : Username too long \ n "); <nl> return - 1 ; <nl> } <nl> + if ( p == NULL || uid -> len == 0 ) { <nl> + DBG (" get_to_uid : Username is empty \ n "); <nl> + return - 1 ; <nl> + } <nl> memcpy ( buf , p , uid -> len ); <nl> uid -> s = buf ; <nl> strlower ( uid );
static int init_mi_uptime ( void ) <nl> { <nl> char * p ; <nl>  <nl> + if ( kmi_up_since_ctime . s != 0 ) <nl> + return 0 ; <nl> time (& kmi_up_since ); <nl> p = ctime (& kmi_up_since ); <nl> kmi_up_since_ctime . len = strlen ( p )- 1 ;
struct module_exports exports = { <nl> static int mod_init ( void ) { <nl> struct stat fs ; <nl>  <nl> + if ( register_mi_mod ( exports . name , mi_cmds )!= 0 ) <nl> + { <nl> + LM_ERR (" failed to register MI commands \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> subscriber_table . len = strlen ( subscriber_table . s ); <nl> subscriber_username_col . len = strlen ( subscriber_username_col . s ); <nl> subscriber_domain_col . len = strlen ( subscriber_domain_col . s );
gui_save_file_callback ( gpointer callback_data , guint callback_action , GtkWidget <nl> if ( filetype_str ) <nl> if ( g_str_equal ( savers [ n ]. extension , filetype_str )) <nl> gtk_combo_box_set_active ( GTK_COMBO_BOX ( filetype ), n ); <nl> + g_free ( filetype_str ); <nl> n ++; <nl> } <nl> if ( gtk_combo_box_get_active ( GTK_COMBO_BOX ( filetype )) == - 1 )
bool fbotex_change ( struct fbotex * fbo , GL * gl , struct mp_log * log , int w , int h , <nl>  <nl> GLenum filter = fbo -> tex_filter ; <nl>  <nl> + fbotex_uninit ( fbo ); <nl> + <nl> * fbo = ( struct fbotex ) { <nl> . gl = gl , <nl> . rw = w ,
int vixProbe ( int verbose , int force ){ <nl> # define VID_RD08 ( p , i ) ({ MEM_BARRIER (); (( uint8_t *)( p ))[( i )]; }) <nl>  <nl> # undef VID_WR32 <nl> -# define VID_WR32 ( p , i , val ) ({ MEM_BARRIER (); (( uint32_t *)( p ))[( i )/ 4 ]= le2me_32 ( val ); }) <nl> +# define VID_WR32 ( p , i , val ) ({ MEM_BARRIER (); (( uint32_t *)( p ))[( i )/ 4 ]= val ; }) <nl> # undef VID_RD32 <nl> -# define VID_RD32 ( p , i ) ({ MEM_BARRIER (); le2me_32 ((( uint32_t *)( p ))[( i )/ 4 ]); }) <nl> +# define VID_RD32 ( p , i ) ({ MEM_BARRIER (); (( uint32_t *)( p ))[( i )/ 4 ]; }) <nl>  <nl> # define VID_AND32 ( p , i , val ) VID_WR32 ( p , i , VID_RD32 ( p , i )&( val )) <nl> # define VID_OR32 ( p , i , val ) VID_WR32 ( p , i , VID_RD32 ( p , i )|( val ))
static void uninit ( sh_video_t * sh ) <nl>  <nl> if ( context ) <nl> { <nl> + theora_info_clear (& context -> inf ); <nl> + theora_comment_clear (& context -> cc ); <nl> theora_clear (& context -> st ); <nl> free ( context ); <nl> }
struct m_config_option * m_config_get_co ( const struct m_config * config , <nl> return NULL ; <nl> } <nl>  <nl> - const char * m_config_get_positional_option ( const struct m_config * config , int n ) <nl> + const char * m_config_get_positional_option ( const struct m_config * config , int p ) <nl> { <nl> int pos = 0 ; <nl> for ( int n = 0 ; n < config -> num_opts ; n ++) { <nl> struct m_config_option * co = & config -> opts [ n ]; <nl> if (! co -> is_generated ) { <nl> - if ( pos == n ) <nl> + if ( pos == p ) <nl> return co -> name ; <nl> pos ++; <nl> }
dvb_config_t * dvb_get_config ( void ) <nl> } <nl>  <nl> if (( access ( conf_file , F_OK | R_OK ) != 0 )) <nl> + { <nl> + if ( conf_file ) <nl> + free ( conf_file ); <nl> conf_file = get_path (" channels . conf "); <nl> + } <nl>  <nl> list = dvb_get_channels ( conf_file , type ); <nl> + if ( conf_file ) <nl> + free ( conf_file ); <nl> if ( list == NULL ) <nl> continue ; <nl> 
void m_config_print_option_list ( const struct m_config * config ) <nl> continue ; <nl> if ( co -> is_generated ) <nl> continue ; <nl> + if ( opt -> type == & m_option_type_alias || <nl> + opt -> type == & m_option_type_removed ) <nl> + continue ; <nl> MP_INFO ( config , " % s %- 30s ", prefix , co -> name ); <nl> if ( opt -> type == & m_option_type_choice ) { <nl> MP_INFO ( config , " Choices :");
int streaming_bufferize ( streaming_ctrl_t * streaming_ctrl , char * buffer , int siz <nl> int nop_streaming_read ( int fd , char * buffer , int size , streaming_ctrl_t * stream_ctrl ); <nl> int nop_streaming_seek ( int fd , off_t pos , streaming_ctrl_t * stream_ctrl ); <nl>  <nl> + int connect2Server ( char * host , int port ); <nl> + <nl> # endif
static int gen_sh_video ( sh_video_t * sh , mov_track_t * trak , int timescale ) { <nl>  <nl> sh -> disp_w = trak -> stdata [ 25 ]|( trak -> stdata [ 24 ]<< 8 ); <nl> sh -> disp_h = trak -> stdata [ 27 ]|( trak -> stdata [ 26 ]<< 8 ); <nl> + if ( trak -> tkdata_len > 81 ) { <nl> // if image size is zero , fallback to display size <nl> if (! sh -> disp_w && ! sh -> disp_h ) { <nl> sh -> disp_w = trak -> tkdata [ 77 ]|( trak -> tkdata [ 76 ]<< 8 ); <nl> static int gen_sh_video ( sh_video_t * sh , mov_track_t * trak , int timescale ) { <nl> sh -> aspect = trak -> tkdata [ 77 ]|( trak -> tkdata [ 76 ]<< 8 ); <nl> sh -> aspect /= trak -> tkdata [ 81 ]|( trak -> tkdata [ 80 ]<< 8 ); <nl> } <nl> + } <nl>  <nl> if ( depth > 32 + 8 ) mp_msg ( MSGT_DEMUX , MSGL_INFO ,"*** depth = 0x % X \ n ", depth ); <nl> 
int decode_avsub ( struct sh_sub * sh , uint8_t ** data , int * size , <nl> * endpts = * pts + sub . end_display_time / 1000 . 0 ; <nl> * pts += sub . start_display_time / 1000 . 0 ; <nl> } <nl> + if ( got_sub && vo_spudec && sub . num_rects == 0 ) <nl> + spudec_set_paletted ( vo_spudec , NULL , 0 , NULL , 0 , 0 , 0 , 0 , * pts , * endpts ); <nl> if ( got_sub && sub . num_rects > 0 ) { <nl> switch ( sub . rects [ 0 ]-> type ) { <nl> case SUBTITLE_BITMAP :
static off_t ts_detect_streams ( demuxer_t * demuxer , tsdemux_init_t * param ) <nl> init_pos = stream_tell ( demuxer -> stream ); <nl> mp_msg ( MSGT_DEMUXER , MSGL_V , " PROBING UP TO %" PRIu64 ", PROG : % d \ n ", ( uint64_t ) param -> probe , param -> prog ); <nl> end_pos = init_pos + ( param -> probe ? param -> probe : TS_MAX_PROBE_SIZE ); <nl> - while (( pos <= end_pos ) && (! demuxer -> stream -> eof )) <nl> + while ( 1 ) <nl> { <nl> pos = stream_tell ( demuxer -> stream ); <nl> + if ( pos > end_pos || demuxer -> stream -> eof ) <nl> + break ; <nl> + <nl> if ( ts_parse ( demuxer , & es , tmp , 1 )) <nl> { <nl> // Non PES - aligned A52 audio may escape detection if PMT is not present ;
static void free_entry ( list_entry_t * entry ) { <nl> free ( entry -> ok ); <nl> if ( entry -> cancel ) <nl> free ( entry -> cancel ); <nl> + if ( entry -> left ) <nl> + free ( entry -> left ); <nl> + if ( entry -> right ) <nl> + free ( entry -> right ); <nl> free ( entry -> p . txt ); <nl> free ( entry ); <nl> }
static int af_find_output_conversion ( struct af_stream * s , struct mp_audio * cfg ) <nl> ! mp_chmap_equals_reordered (& af -> fmt_in . channels , & af -> fmt_out . channels )) <nl> return AF_ERROR ; <nl> } <nl> + // And not if it ' s the only filter . <nl> + if ( conv -> prev == s -> first && conv -> next == s -> last ) <nl> + return AF_ERROR ; <nl>  <nl> * cfg = s -> output ; <nl> return AF_OK ;
static int pngRead ( unsigned char * fname , txSample * bf ) <nl> fseek ( fp , 0 , SEEK_END ); <nl> len = ftell ( fp ); <nl> if ( len > 50 * 1024 * 1024 ) return 2 ; <nl> - data = malloc ( len + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + data = av_malloc ( len + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> fseek ( fp , 0 , SEEK_SET ); <nl> fread ( data , len , 1 , fp ); <nl> fclose ( fp ); <nl> static int pngRead ( unsigned char * fname , txSample * bf ) <nl> avcodec_close ( avctx ); <nl> av_freep (& frame ); <nl> av_freep (& avctx ); <nl> + av_freep (& data ); <nl>  <nl> mp_dbg ( MSGT_GPLAYER , MSGL_DBG2 ,"[ png ] filename : % s .\ n ", fname ); <nl> mp_dbg ( MSGT_GPLAYER , MSGL_DBG2 ,"[ png ] size : % dx % d bits : % d \ n ", bf -> Width , bf -> Height , bf -> BPP );
static int cached_demux_control ( struct demux_internal * in , int cmd , void * arg ) <nl> struct demux_stream * ds = in -> d_user -> streams [ n ]-> ds ; <nl> if ( ds -> active ) { <nl> r -> underrun |= ! ds -> head && ! ds -> eof ; <nl> - r -> ts_range [ 0 ] = MP_PTS_MAX ( r -> ts_range [ 0 ], ds -> base_ts ); <nl> - r -> ts_range [ 1 ] = MP_PTS_MIN ( r -> ts_range [ 1 ], ds -> last_ts ); <nl> + if (! ds -> eof ) { <nl> + r -> ts_range [ 0 ] = MP_PTS_MAX ( r -> ts_range [ 0 ], ds -> base_ts ); <nl> + r -> ts_range [ 1 ] = MP_PTS_MIN ( r -> ts_range [ 1 ], ds -> last_ts ); <nl> + } <nl> } <nl> } <nl> r -> idle = ( in -> idle && ! r -> underrun ) || r -> eof ;
void ft232r_close ( struct ft232r_device_handle * dev ) <nl> libusb_release_interface ( dev -> h , 0 ); <nl> libusb_reset_device ( dev -> h ); <nl> libusb_close ( dev -> h ); <nl> + free ( dev ); <nl> } <nl>  <nl> bool ft232r_purge_buffers ( struct ft232r_device_handle * dev , enum ft232r_reset_purge purge )
void hashmeter2 ( struct thr_info * thr ) <nl>  <nl> gettimeofday (& tv_now , NULL ); <nl> timersub (& tv_now , & thr -> tv_lastupdate , & tv_elapsed ); <nl> - if ( tv_elapsed . tv_sec >= opt_log_interval ) { <nl> + /* Update the hashmeter at most 5 times per second */ <nl> + if ( tv_elapsed . tv_sec > 0 || tv_elapsed . tv_usec > 200 ) { <nl> hashmeter ( thr -> id , & tv_elapsed , thr -> hashes_done ); <nl> thr -> hashes_done = 0 ; <nl> thr -> tv_lastupdate = tv_now ;
int _usb_read ( struct cgpu_info * cgpu , int ep , char * buf , size_t bufsiz , int * pro <nl> * if we don ' t sleep here , but do it only if we ' re not <nl> * receiving any data . */ <nl> timeout = initial_timeout - ( done * 1000 ); <nl> + if (! timeout ) <nl> + break ; <nl> if (! got && sleep_time ) { <nl> if ( timeout <= sleep_time ) <nl> sleep_time = timeout - 1 ; <nl> int _usb_read ( struct cgpu_info * cgpu , int ep , char * buf , size_t bufsiz , int * pro <nl> break ; <nl>  <nl> timeout = initial_timeout - ( done * 1000 ); <nl> + if (! timeout ) <nl> + break ; <nl> if (! got && sleep_time ) { <nl> if ( timeout <= sleep_time ) <nl> sleep_time = timeout - 1 ;
bool extract_sockaddr ( struct pool * pool , char * url ) <nl>  <nl> if ( url_len < 1 ) <nl> return false ; <nl> + <nl> + if ( url_len >= sizeof ( url_address )) <nl> + { <nl> + applog ( LOG_WARNING , "% s : Truncating overflowed address '%.* s '", <nl> + __func__ , url_len , url_begin ); <nl> + url_len = sizeof ( url_address ) - 1 ; <nl> + } <nl>  <nl> sprintf ( url_address , "%.* s ", url_len , url_begin ); <nl> 
static void test_work_current ( struct work * work ) <nl>  <nl> uint32_t block_id = (( uint32_t *)( work -> data ))[ 1 ]; <nl>  <nl> - hexstr = bin2hex ( work -> data , 18 ); <nl> + hexstr = bin2hex (& work -> data [ 4 ], 18 ); <nl> if ( unlikely (! hexstr )) { <nl> applog ( LOG_ERR , " stage_thread OOM "); <nl> return ;
void gpu_autotune ( int gpu ) <nl> if ( opt_debug ) <nl> applog ( LOG_DEBUG , " Temperature over target , increasing fanspeed "); <nl> newpercent = fanpercent + 5 ; <nl> + if ( newpercent > 85 ) <nl> + newpercent = 85 ; <nl> } else if ( fanpercent && temp < ga -> targettemp - opt_hysteresis ) { <nl> if ( opt_debug ) <nl> applog ( LOG_DEBUG , " Temperature % d degrees below target , decreasing fanspeed ", opt_hysteresis );
static bool work_decode ( const json_t * val , struct work * work ) <nl> ae = cbappendsz ; <nl> truncatewarning = false ; <nl> } else { <nl> - char * tmp = strndup ( opt_coinbase_sig , ae ); <nl> + char * tmp = malloc ( ae + 1 ); <nl> + memcpy ( tmp , opt_coinbase_sig , ae ); <nl> + tmp [ ae ] = '\ 0 '; <nl> applog (( truncatewarning ? LOG_DEBUG : LOG_WARNING ), <nl> " Pool % u truncating appended coinbase signature at % d bytes : % s (% s )", <nl> work -> pool -> pool_no , ae , tmp , & opt_coinbase_sig [ ae ]);
bool hashbusterusb_vrm_unlock ( struct cgpu_info * const proc , const char * const <nl> hex2bin (& buf [ 1 ], code , size ); <nl>  <nl> hashbusterusb_io ( h , buf , buf ); <nl> - return memcmp ( buf , "\ x12 \ 0 ", 2 ); <nl> + return ! memcmp ( buf , "\ x12 \ 0 ", 2 ); <nl> } <nl>  <nl> static
static uint64_t bitforce_get_temp ( struct cgpu_info * bitforce ) <nl> } <nl> } <nl> } <nl> + return 1 ; <nl> } <nl>  <nl>  <nl> static uint64_t bitforce_send_work ( struct cgpu_info * bitforce , struct work * work <nl> applog ( LOG_ERR , " BitForce block data reports : % s ", pdevbuf ); <nl> return 0 ; <nl> } <nl> + return 1 ; <nl> } <nl>  <nl> static uint64_t bitforce_get_result ( struct thr_info * thr , struct work * work )
void write_config ( FILE * fcfg ) <nl>  <nl> /* Write pool values in priority order */ <nl> fputs ("{\ n \" pools \" : [", fcfg ); <nl> - while ( j < total_pools ) { <nl> + while (( j < total_pools ) && ( i < total_pools )) { <nl> if ( pools [ i ]-> prio == j ) { <nl> fprintf ( fcfg , "% s \ n \ t {\ n \ t \ t \" url \" : \"% s \",", i > 0 ? "," : "", pools [ i ]-> rpc_url ); <nl> fprintf ( fcfg , "\ n \ t \ t \" user \" : \"% s \",", pools [ i ]-> rpc_user );
_clState * initCl ( unsigned int gpu , char * name , size_t nameSize ) <nl> find = strstr ( extensions , camo ); <nl> if ( find ) <nl> clState -> hasBitAlign = true ; <nl> + free ( extensions ); <nl>  <nl> /* Check for OpenCL >= 1 . 0 support , needed for global offset parameter usage . */ <nl> char * devoclver = malloc ( 1024 ); <nl> _clState * initCl ( unsigned int gpu , char * name , size_t nameSize ) <nl> find = strstr ( devoclver , ocl10 ); <nl> if (! find ) <nl> clState -> hasOpenCL11plus = true ; <nl> + free ( devoclver ); <nl>  <nl> status = clGetDeviceInfo ( devices [ gpu ], CL_DEVICE_PREFERRED_VECTOR_WIDTH_INT , sizeof ( cl_uint ), ( void *)& preferred_vwidth , NULL ); <nl> if ( status != CL_SUCCESS ) { <nl> build : <nl> char * log = malloc ( logSize ); <nl> status = clGetProgramBuildInfo ( clState -> program , devices [ gpu ], CL_PROGRAM_BUILD_LOG , logSize , log , NULL ); <nl> applog ( LOG_ERR , "% s ", log ); <nl> + free ( log ); <nl> return NULL ; <nl> } <nl>  <nl> built : <nl> char * log = malloc ( logSize ); <nl> status = clGetProgramBuildInfo ( clState -> program , devices [ gpu ], CL_PROGRAM_BUILD_LOG , logSize , log , NULL ); <nl> applog ( LOG_ERR , "% s ", log ); <nl> + free ( log ); <nl> return NULL ; <nl> } <nl> }
refresh : <nl> wlogprint ("[ E ] nable "); <nl> if ( cgpu -> deven != DEV_DISABLED ) <nl> wlogprint ("[ D ] isable "); <nl> + if ( drv -> identify_device ) <nl> + wlogprint ("[ I ] dentify "); <nl> if ( drv -> proc_tui_wlogprint_choices && likely ( cgpu -> status != LIFE_INIT )) <nl> drv -> proc_tui_wlogprint_choices ( cgpu ); <nl> wlogprint ("\ n "); <nl> refresh : <nl> msg = " Processor being enabled \ n "; <nl> } <nl> goto refresh ; <nl> + case ' i ': case ' I ': <nl> + if ( drv -> identify_device && drv -> identify_device ( cgpu )) <nl> + msg = " Identify command sent \ n "; <nl> + else <nl> + msg = " Error : Identify not supported \ n "; <nl> + goto refresh ; <nl> case KEY_DOWN : <nl> if ( selected_device >= total_devices - 1 ) <nl> break ;
AcpiGetHandle ( <nl> { <nl> ACPI_STATUS Status ; <nl> NAME_TABLE_ENTRY * ThisEntry ; <nl> - ACPI_HANDLE Scope = NULL ; <nl> + NAME_TABLE_ENTRY * Scope = NULL ; <nl>  <nl> if (! RetHandle || ! Pathname ) <nl> { <nl> AcpiGetHandle ( <nl> } <nl>  <nl> if ( Parent ) <nl> - Scope = Parent -> Scope ; <nl> + Scope = (( NAME_TABLE_ENTRY *) Parent )-> Scope ; <nl>  <nl> /* Special case for root , since we can ' t search for it */ <nl> 
AcpiNsEvaluate ( <nl>  <nl> Status = AE_OK ; <nl> } <nl> + else if ( ACPI_FAILURE ( Status )) <nl> + { <nl> + /* If ReturnObject exists , delete it */ <nl> + <nl> + if ( Info -> ReturnObject ) <nl> + { <nl> + AcpiUtRemoveReference ( Info -> ReturnObject ); <nl> + Info -> ReturnObject = NULL ; <nl> + } <nl> + } <nl>  <nl> ACPI_DEBUG_PRINT (( ACPI_DB_NAMES , <nl> "*** Completed evaluation of object % s ***\ n ",
AcpiDmIsResourceTemplate ( <nl> * intialization byte list . Because the resource macros will create <nl> * a buffer of the exact required length ( buffer length will be equal <nl> * to the actual length ). <nl> + * <nl> + * NOTE ( April 2017 ): Resource templates with this issue have been <nl> + * seen in the field . We still don ' t want to attempt to disassemble <nl> + * a buffer like this to a resource template because this output <nl> + * would not match the original input buffer ( it would be shorter <nl> + * than the original when the disassembled code is recompiled ). <nl> + * Basically , a buffer like this appears to be hand crafted in the <nl> + * first place , so just emitting a buffer object instead of a <nl> + * resource template more closely resembles the original ASL code . <nl> */ <nl> if ( DeclaredBufferLength != BufferLength ) <nl> {
AcpiHwClearAcpiStatus ( <nl>  <nl> Status = AcpiHwRegisterWrite ( ACPI_REGISTER_PM1_STATUS , <nl> ACPI_BITMASK_ALL_FIXED_STATUS ); <nl> + <nl> + AcpiOsReleaseLock ( AcpiGbl_HardwareLock , LockFlags ); <nl> + <nl> if ( ACPI_FAILURE ( Status )) <nl> { <nl> - goto UnlockAndExit ; <nl> + goto Exit ; <nl> } <nl>  <nl> /* Clear the GPE Bits in all GPE registers in all GPE blocks */ <nl>  <nl> Status = AcpiEvWalkGpeList ( AcpiHwClearGpeBlock , NULL ); <nl>  <nl> - UnlockAndExit : <nl> - AcpiOsReleaseLock ( AcpiGbl_HardwareLock , LockFlags ); <nl> + Exit : <nl> return_ACPI_STATUS ( Status ); <nl> } <nl> 
/****************************************************************************** <nl> * <nl> * Module Name : aslcompile - top level compile module <nl> - * $ Revision : 1 . 77 $ <nl> + * $ Revision : 1 . 78 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> CmCleanupAndExit ( void ) <nl>  <nl> UtDisplaySummary ( ASL_FILE_STDOUT ); <nl>  <nl> - <nl> if ( Gbl_ExceptionCount [ ASL_ERROR ] > 0 ) <nl> { <nl> exit ( 1 );
AcpiTbInstallTableWithOverride ( <nl> * DESCRIPTION : This function is called to verify and install an ACPI table . <nl> * When this function is called by " Load " or " LoadTable " opcodes , <nl> * or by AcpiLoadTable () API , the " Reload " parameter is set . <nl> - * After sucessfully returning from this function , table is <nl> + * After successfully returning from this function , table is <nl> * " INSTALLED " but not " VALIDATED ". <nl> * <nl> ******************************************************************************/
/****************************************************************************** <nl> * <nl> * Name : acwin . h - OS specific defines , etc . <nl> - * $ Revision : 1 . 27 $ <nl> + * $ Revision : 1 . 28 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> typedef COMPILER_DEPENDENT_UINT64 u64 ; <nl> # define ACPI_SIMPLE_RETURN_MACROS <nl> # endif <nl>  <nl> +/*! [ End ] no source code translation !*/ <nl> + <nl> /* <nl> * Global Lock acquire / release code <nl> *
/****************************************************************************** <nl> * <nl> * Name : hwsleep . c - ACPI Hardware Sleep / Wake Interface <nl> - * $ Revision : 1 . 14 $ <nl> + * $ Revision : 1 . 15 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> AcpiEnterSleepState ( <nl> /* wait a second , then try again */ <nl> AcpiOsStall ( 1000000 ); <nl>  <nl> - AcpiHwRegisterWrite ( ACPI_MTX_LOCK , PM1_CONTROL , <nl> - ( 1 << AcpiHwGetBitShift ( SLP_EN_MASK ))); <nl> + if ( SleepState > ACPI_STATE_S1 ) <nl> + { <nl> + AcpiHwRegisterWrite ( ACPI_MTX_LOCK , PM1_CONTROL , <nl> + ( 1 << AcpiHwGetBitShift ( SLP_EN_MASK ))); <nl> + } <nl>  <nl> enable (); <nl> 
/****************************************************************************** <nl> * <nl> * Module Name : evgpe - General Purpose Event handling and dispatch <nl> - * $ Revision : 1 . 32 $ <nl> + * $ Revision : 1 . 34 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> * <nl> * 1 . Copyright Notice <nl> * <nl> - * Some or all of this work - Copyright ( c ) 1999 - 2003 , Intel Corp . <nl> + * Some or all of this work - Copyright ( c ) 1999 - 2004 , Intel Corp . <nl> * All rights reserved . <nl> * <nl> * 2 . License <nl> AcpiEvGpeDetect ( <nl>  <nl> ACPI_FUNCTION_NAME (" EvGpeDetect "); <nl>  <nl> + /* Check for the case where there are no GPEs */ <nl> + <nl> + if (! GpeXruptList ) <nl> + { <nl> + return ( IntStatus ); <nl> + } <nl>  <nl> /* Examine all GPE blocks attached to this interrupt level */ <nl> 
static const char * AcpiGbl_GenericNotify [ ACPI_GENERIC_NOTIFY_MAX + 1 ] <nl> /* 09 */ " Device PLD Check ", <nl> /* 0A */ " Reserved ", <nl> /* 0B */ " System Locality Update ", <nl> - /* 0C */ " Shutdown Request ", /* Reserved in ACPI 6 . 0 */ <nl> + /* 0C */ " Reserved ( was previously Shutdown Request )", /* Reserved in ACPI 6 . 0 */ <nl> /* 0D */ " System Resource Affinity Update ", <nl> - /* 0E */ " Heterogeneous Memory Attributes Update " /* ACPI 6 . 2 */ <nl> + /* 0E */ " Heterogeneous Memory Attributes Update " /* ACPI 6 . 2 */ <nl> }; <nl>  <nl> static const char * AcpiGbl_DeviceNotify [ 5 ] =
/****************************************************************************** <nl> * <nl> * Module Name : psparse - Parser top level AML parse routines <nl> - * $ Revision : 1 . 124 $ <nl> + * $ Revision : 1 . 125 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> AcpiPsParseLoop ( <nl> { <nl> /* There are arguments ( complex ones ), push Op and prepare for argument */ <nl>  <nl> - AcpiPsPushScope ( ParserState , Op , WalkState -> ArgTypes , WalkState -> ArgCount ); <nl> + Status = AcpiPsPushScope ( ParserState , Op , WalkState -> ArgTypes , WalkState -> ArgCount ); <nl> + if ( ACPI_FAILURE ( Status )) <nl> + { <nl> + return_ACPI_STATUS ( Status ); <nl> + } <nl> Op = NULL ; <nl> continue ; <nl> }
dnode_sync ( dnode_t * dn , dmu_tx_t * tx ) <nl> dn -> dn_free_txg <= tx -> tx_txg ; <nl>  <nl> /* <nl> - * We will either remove a spill block when a file is being removed <nl> - * or we have been asked to remove it . <nl> + * Remove the spill block if we have been explicitly asked to <nl> + * remove it , or if the object is being removed . <nl> */ <nl> - if ( dn -> dn_rm_spillblk [ txgoff ] || <nl> - (( dnp -> dn_flags & DNODE_FLAG_SPILL_BLKPTR ) && freeing_dnode )) { <nl> - if (( dnp -> dn_flags & DNODE_FLAG_SPILL_BLKPTR )) <nl> + if ( dn -> dn_rm_spillblk [ txgoff ] || freeing_dnode ) { <nl> + if ( dnp -> dn_flags & DNODE_FLAG_SPILL_BLKPTR ) <nl> kill_spill = B_TRUE ; <nl> dn -> dn_rm_spillblk [ txgoff ] = 0 ; <nl> }
*/ <nl>  <nl> /* <nl> + * Copyright 2014 Gary Mills <nl> * Copyright ( c ) 2003 , 2010 , Oracle and / or its affiliates . All rights reserved . <nl> */ <nl>  <nl> match_prop ( xmlNodePtr cur , const xmlChar * attr , char * user_prop ) <nl> return ( B_FALSE ); <nl> prop_result = xmlStrcmp ( gotten_prop , ( const xmlChar *) user_prop ); <nl> xmlFree ( gotten_prop ); <nl> - return (( prop_result == 0 )); <nl> + return (( prop_result == 0 )); /* empty strings will match */ <nl> } <nl>  <nl> static int <nl> zonecfg_delete_nwif_core ( zone_dochandle_t handle , struct zone_nwiftab * tabptr ) <nl> phys_match = match_prop ( cur , DTD_ATTR_PHYSICAL , <nl> tabptr -> zone_nwif_physical ); <nl>  <nl> - if (( addr_match || allowed_addr_match ) && phys_match ) { <nl> + if ( addr_match && allowed_addr_match && phys_match ) { <nl> xmlUnlinkNode ( cur ); <nl> xmlFreeNode ( cur ); <nl> return ( Z_OK );
pr_lookup_procdir ( vnode_t * dp , char * comp ) <nl> /* initialize the new prcommon struct */ <nl> if (( p -> p_flag & SSYS ) || p -> p_as == & kas ) <nl> pcp -> prc_flags |= PRC_SYS ; <nl> - if ( p -> p_stat == SZOMB ) <nl> + if ( p -> p_stat == SZOMB || ( p -> p_flag & SEXITING ) != 0 ) <nl> pcp -> prc_flags |= PRC_DESTROY ; <nl> pcp -> prc_proc = p ; <nl> pcp -> prc_datamodel = p -> p_model ;
segvn_setvnode_mpss ( vnode_t * vp ) <nl> int <nl> segvn_create ( struct seg * seg , void * argsp ) <nl> { <nl> + extern lgrp_mem_policy_t lgrp_mem_default_policy ; <nl> struct segvn_crargs * a = ( struct segvn_crargs *) argsp ; <nl> struct segvn_data * svd ; <nl> size_t swresv = 0 ; <nl> segvn_create ( struct seg * seg , void * argsp ) <nl> struct anon_map * amp ; <nl> int error = 0 ; <nl> size_t pgsz ; <nl> - lgrp_mem_policy_t mpolicy = LGRP_MEM_POLICY_DEFAULT ; <nl> + lgrp_mem_policy_t mpolicy = lgrp_mem_default_policy ; <nl> int use_rgn = 0 ; <nl> int trok = 0 ; <nl>  <nl> segvn_create ( struct seg * seg , void * argsp ) <nl> struct segvn_data * psvd , * nsvd ; <nl> lgrp_mem_policy_t ppolicy , npolicy ; <nl> uint_t lgrp_mem_policy_flags = 0 ; <nl> - extern lgrp_mem_policy_t lgrp_mem_default_policy ; <nl>  <nl> /* <nl> * Memory policy flags ( lgrp_mem_policy_flags ) is valid when
*/ <nl>  <nl> /* <nl> - * Copyright 2010 Sun Microsystems , Inc . All rights reserved . <nl> - * Use is subject to license terms . <nl> + * Copyright ( c ) 2008 , 2010 , Oracle and / or its affiliates . All rights reserved . <nl> */ <nl>  <nl> # include < sys / conf . h > <nl> idm_parse_login_rsp ( idm_conn_t * ic , idm_pdu_t * login_rsp_pdu , boolean_t rx ) <nl> } else { <nl> new_event = ( rx ? CE_MISC_RX : CE_MISC_TX ); <nl> } <nl> + } else if ( rx && login_rsp -> status_class == <nl> + ISCSI_STATUS_CLASS_REDIRECT ) { <nl> + new_event = CE_MISC_RX ; <nl> } else { <nl> new_event = ( rx ? CE_LOGIN_FAIL_RCV : CE_LOGIN_FAIL_SND ); <nl> }
static char * usb_get_fw_dev_path ( DeviceState * qdev ) <nl> nr = strtol ( in , & in , 10 ); <nl> if ( in [ 0 ] == '.') { <nl> /* some hub between root port and device */ <nl> - pos += snprintf ( fw_path + pos , fw_len - pos , " hub @% ld /", nr ); <nl> + pos += snprintf ( fw_path + pos , fw_len - pos , " hub @% lx /", nr ); <nl> in ++; <nl> } else { <nl> /* the device itself */ <nl> - pos += snprintf ( fw_path + pos , fw_len - pos , "% s @% ld ", <nl> + pos += snprintf ( fw_path + pos , fw_len - pos , "% s @% lx ", <nl> qdev_fw_name ( qdev ), nr ); <nl> break ; <nl> }
static inline DATA_TYPE glue ( io_read , SUFFIX )( target_phys_addr_t physaddr , <nl> cpu_io_recompile ( env , retaddr ); <nl> } <nl>  <nl> + env -> mem_io_vaddr = addr ; <nl> # if SHIFT <= 2 <nl> res = io_mem_read [ index ][ SHIFT ]( io_mem_opaque [ index ], physaddr ); <nl> # else
static void do_key_event ( VncState * vs , int down , int keycode , int sym ) <nl> break ; <nl> } <nl>  <nl> - if ( vs -> vd -> lock_key_sync && <nl> + if ( down && vs -> vd -> lock_key_sync && <nl> keycode_is_keypad ( vs -> vd -> kbd_layout , keycode )) { <nl> /* If the numlock state needs to change then simulate an additional <nl> keypress before sending this one . This will happen if the user <nl> static void do_key_event ( VncState * vs , int down , int keycode , int sym ) <nl> } <nl> } <nl>  <nl> - if ( vs -> vd -> lock_key_sync && <nl> + if ( down && vs -> vd -> lock_key_sync && <nl> (( sym >= ' A ' && sym <= ' Z ') || ( sym >= ' a ' && sym <= ' z '))) { <nl> /* If the capslock state needs to change then simulate an additional <nl> keypress before sending this one . This will happen if the user
DriveInfo * drive_init ( QemuOpts * all_opts , BlockInterfaceType block_default_type ) <nl> & error_abort ); <nl> qemu_opts_absorb_qdict ( legacy_opts , bs_opts , & local_err ); <nl> if ( local_err ) { <nl> - qerror_report_err ( local_err ); <nl> + error_report ("% s ", error_get_pretty ( local_err )); <nl> error_free ( local_err ); <nl> goto fail ; <nl> } <nl> DriveInfo * drive_init ( QemuOpts * all_opts , BlockInterfaceType block_default_type ) <nl> dinfo = blockdev_init ( filename , bs_opts , & local_err ); <nl> if ( dinfo == NULL ) { <nl> if ( local_err ) { <nl> - qerror_report_err ( local_err ); <nl> + error_report ("% s ", error_get_pretty ( local_err )); <nl> error_free ( local_err ); <nl> } <nl> goto fail ;
int qdev_device_help ( QemuOpts * opts ) <nl> * for removal . This conditional should be removed along with <nl> * it . <nl> */ <nl> - if (! prop -> info -> parse ) { <nl> + if (! prop -> info -> set ) { <nl> continue ; /* no way to set it , don ' t show */ <nl> } <nl> error_printf ("% s .% s =% s \ n ", driver , prop -> name , <nl> int qdev_device_help ( QemuOpts * opts ) <nl> } <nl> if ( info -> bus_info ) { <nl> for ( prop = info -> bus_info -> props ; prop && prop -> name ; prop ++) { <nl> - if (! prop -> info -> parse ) { <nl> + if (! prop -> info -> set ) { <nl> continue ; /* no way to set it , don ' t show */ <nl> } <nl> error_printf ("% s .% s =% s \ n ", driver , prop -> name ,
static int qemu_event_init ( void ) <nl>  <nl> static void qemu_event_increment ( void ) <nl> { <nl> - SetEvent ( qemu_event_handle ); <nl> + if (! SetEvent ( qemu_event_handle )) { <nl> + fprintf ( stderr , " qemu_event_increment : SetEvent failed : % d \ n ", <nl> + GetLastError ()); <nl> + exit ( 1 ); <nl> + } <nl> } <nl> # endif <nl> 
int queue_signal ( CPUState * env , int sig , target_siginfo_t * info ) <nl> k = & ts -> sigtab [ sig - 1 ]; <nl> handler = sigact_table [ sig - 1 ]. _sa_handler ; <nl> if ( handler == TARGET_SIG_DFL ) { <nl> + if ( sig == TARGET_SIGTSTP || sig == TARGET_SIGTTIN || sig == TARGET_SIGTTOU ) { <nl> + kill ( getpid (), SIGSTOP ); <nl> + return 0 ; <nl> + } else <nl> /* default handler : ignore some signal . The other are fatal */ <nl> if ( sig != TARGET_SIGCHLD && <nl> sig != TARGET_SIGURG && <nl> - sig != TARGET_SIGWINCH ) { <nl> + sig != TARGET_SIGWINCH && <nl> + sig != TARGET_SIGCONT ) { <nl> force_sig ( sig ); <nl> } else { <nl> return 0 ; /* indicate ignored */
out : <nl> g_free ( dummy ); <nl> if ( err ) { <nl> qerror_report_err ( err ); <nl> + error_free ( err ); <nl> return - 1 ; <nl> } <nl> return 0 ;
static bool is_version_0 ( void * opaque , int version_id ) <nl> return version_id == 0 ; <nl> } <nl>  <nl> + static bool vmstate_scoop_validate ( void * opaque , int version_id ) <nl> +{ <nl> + ScoopInfo * s = opaque ; <nl> + <nl> + return !( s -> prev_level & 0xffff0000 ) && <nl> + !( s -> gpio_level & 0xffff0000 ) && <nl> + !( s -> gpio_dir & 0xffff0000 ); <nl> +} <nl> + <nl> static const VMStateDescription vmstate_scoop_regs = { <nl> . name = " scoop ", <nl> . version_id = 1 , <nl> static const VMStateDescription vmstate_scoop_regs = { <nl> VMSTATE_UINT32 ( gpio_level , ScoopInfo ), <nl> VMSTATE_UINT32 ( gpio_dir , ScoopInfo ), <nl> VMSTATE_UINT32 ( prev_level , ScoopInfo ), <nl> + VMSTATE_VALIDATE (" irq levels are 16 bit ", vmstate_scoop_validate ), <nl> VMSTATE_UINT16 ( mcr , ScoopInfo ), <nl> VMSTATE_UINT16 ( cdr , ScoopInfo ), <nl> VMSTATE_UINT16 ( ccr , ScoopInfo ),
static void decode_opc ( CPUMIPSState * env , DisasContext * ctx , int * is_branch ) <nl> } <nl>  <nl> static inline void <nl> - gen_intermediate_code_internal ( CPUMIPSState * env , TranslationBlock * tb , <nl> - int search_pc ) <nl> + gen_intermediate_code_internal ( MIPSCPU * cpu , TranslationBlock * tb , <nl> + bool search_pc ) <nl> { <nl> + CPUMIPSState * env = & cpu -> env ; <nl> DisasContext ctx ; <nl> target_ulong pc_start ; <nl> uint16_t * gen_opc_end ; <nl> done_generating : <nl>  <nl> void gen_intermediate_code ( CPUMIPSState * env , struct TranslationBlock * tb ) <nl> { <nl> - gen_intermediate_code_internal ( env , tb , 0 ); <nl> + gen_intermediate_code_internal ( mips_env_get_cpu ( env ), tb , false ); <nl> } <nl>  <nl> void gen_intermediate_code_pc ( CPUMIPSState * env , struct TranslationBlock * tb ) <nl> { <nl> - gen_intermediate_code_internal ( env , tb , 1 ); <nl> + gen_intermediate_code_internal ( mips_env_get_cpu ( env ), tb , true ); <nl> } <nl>  <nl> static void fpu_dump_state ( CPUMIPSState * env , FILE * f , fprintf_function fpu_fprintf ,
static void framebuffer_update_request ( VncState * vs , int incremental , <nl> return ; <nl> } <nl>  <nl> + vs -> force_update = 1 ; <nl> vnc_set_area_dirty ( vs -> dirty , width , height , x , y , w , h ); <nl> } <nl> 
char ** breakline ( char * input , int * count ) <nl> int c = 0 ; <nl> char * p ; <nl> char ** rval = calloc ( sizeof ( char *), 1 ); <nl> + char ** tmp ; <nl>  <nl> while ( rval && ( p = qemu_strsep (& input , " ")) != NULL ) { <nl> if (!* p ) { <nl> continue ; <nl> } <nl> c ++; <nl> - rval = realloc ( rval , sizeof (* rval ) * ( c + 1 )); <nl> - if (! rval ) { <nl> + tmp = realloc ( rval , sizeof (* rval ) * ( c + 1 )); <nl> + if (! tmp ) { <nl> + free ( rval ); <nl> + rval = NULL ; <nl> c = 0 ; <nl> break ; <nl> + } else { <nl> + rval = tmp ; <nl> } <nl> rval [ c - 1 ] = p ; <nl> rval [ c ] = NULL ;
static void coroutine_fn backup_run ( void * opaque ) <nl>  <nl> if ( job -> sync_bitmap ) { <nl> BdrvDirtyBitmap * bm ; <nl> - if ( ret < 0 ) { <nl> + if ( ret < 0 || block_job_is_cancelled (& job -> common )) { <nl> /* Merge the successor back into the parent , delete nothing . */ <nl> bm = bdrv_reclaim_dirty_bitmap ( bs , job -> sync_bitmap , NULL ); <nl> assert ( bm );
static void decode_opc ( DisasContext * ctx ) <nl> } <nl>  <nl> static inline void <nl> - gen_intermediate_code_internal ( CPUSH4State * env , TranslationBlock * tb , <nl> - int search_pc ) <nl> + gen_intermediate_code_internal ( SuperHCPU * cpu , TranslationBlock * tb , <nl> + bool search_pc ) <nl> { <nl> + CPUSH4State * env = & cpu -> env ; <nl> DisasContext ctx ; <nl> target_ulong pc_start ; <nl> static uint16_t * gen_opc_end ; <nl> gen_intermediate_code_internal ( CPUSH4State * env , TranslationBlock * tb , <nl>  <nl> void gen_intermediate_code ( CPUSH4State * env , struct TranslationBlock * tb ) <nl> { <nl> - gen_intermediate_code_internal ( env , tb , 0 ); <nl> + gen_intermediate_code_internal ( sh_env_get_cpu ( env ), tb , false ); <nl> } <nl>  <nl> void gen_intermediate_code_pc ( CPUSH4State * env , struct TranslationBlock * tb ) <nl> { <nl> - gen_intermediate_code_internal ( env , tb , 1 ); <nl> + gen_intermediate_code_internal ( sh_env_get_cpu ( env ), tb , true ); <nl> } <nl>  <nl> void restore_state_to_opc ( CPUSH4State * env , TranslationBlock * tb , int pc_pos )
static int qcow2_read_extensions ( BlockDriverState * bs , uint64_t start_offset , <nl> # ifdef DEBUG_EXT <nl> printf (" ext . magic = 0x % x \ n ", ext . magic ); <nl> # endif <nl> + if ( ext . len > end_offset - offset ) { <nl> + error_report (" Header extension too large "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> switch ( ext . magic ) { <nl> case QCOW2_EXT_MAGIC_END : <nl> return 0 ;
static int qemu_savevm_state ( QEMUFile * f ) <nl> qemu_put_be64 ( f , 0 ); /* total size */ <nl>  <nl> for ( se = first_se ; se != NULL ; se = se -> next ) { <nl> + if ( se -> save_state == NULL ) <nl> + /* this one has a loader only , for backwards compatibility */ <nl> + continue ; <nl> + <nl> /* ID string */ <nl> len = strlen ( se -> idstr ); <nl> qemu_put_byte ( f , len );
static uint32_t pic_poll_read ( PicState * s ) <nl> pic_update_irq ( s -> pics_state ); <nl> } else { <nl> ret = 0x07 ; <nl> - pic_update_irq ( s -> pics_state ); <nl> } <nl>  <nl> return ret ;
static int cpu_x86_find_by_name ( x86_def_t * x86_cpu_def , const char * cpu_model ) <nl> unsigned int i ; <nl> x86_def_t * def ; <nl>  <nl> - char * s = strdup ( cpu_model ); <nl> + char * s = g_strdup ( cpu_model ); <nl> char * featurestr , * name = strtok ( s , ","); <nl> /* Features to be added */ <nl> uint32_t plus_features = 0 , plus_ext_features = 0 ; <nl> static int cpu_x86_find_by_name ( x86_def_t * x86_cpu_def , const char * cpu_model ) <nl> if ( check_features_against_host ( x86_cpu_def ) && enforce_cpuid ) <nl> goto error ; <nl> } <nl> - free ( s ); <nl> + g_free ( s ); <nl> return 0 ; <nl>  <nl> error : <nl> - free ( s ); <nl> + g_free ( s ); <nl> return - 1 ; <nl> } <nl>  <nl> static int cpudef_setfield ( const char * name , const char * str , void * opaque ) <nl> int err = 0 ; <nl>  <nl> if (! strcmp ( name , " name ")) { <nl> - def -> name = strdup ( str ); <nl> + def -> name = g_strdup ( str ); <nl> } else if (! strcmp ( name , " model_id ")) { <nl> strncpy ( def -> model_id , str , sizeof ( def -> model_id )); <nl> } else if (! strcmp ( name , " level ")) {
tight_detect_smooth_image24 ( VncState * vs , int w , int h ) <nl> } <nl> } <nl>  <nl> + if ( pixels == 0 ) { <nl> + return 0 ; <nl> + } <nl> + <nl> /* 95 % smooth or more ... */ <nl> if ( stats [ 0 ] * 33 / pixels >= 95 ) { <nl> return 0 ; <nl> tight_detect_smooth_image24 ( VncState * vs , int w , int h ) <nl> y += w ; \ <nl> } \ <nl> } \ <nl> - \ <nl> + if ( pixels == 0 ) { \ <nl> + return 0 ; \ <nl> + } \ <nl> if (( stats [ 0 ] + stats [ 1 ]) * 100 / pixels >= 90 ) { \ <nl> return 0 ; \ <nl> } \
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl>  <nl> if (( txdw0 & CP_TX_LGSEN ) && ip_protocol == IP_PROTO_TCP ) <nl> { <nl> + /* Large enough for the TCP header ? */ <nl> + if ( ip_data_len < sizeof ( tcp_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> int large_send_mss = ( txdw0 >> 16 ) & CP_TC_LGSEN_MSS_MASK ; <nl>  <nl> DPRINTF ("+++ C + mode offloaded task TSO MTU =% d IP data % d "
static bool all_cpu_threads_idle ( void ) <nl>  <nl> /* Protected by TimersState seqlock */ <nl>  <nl> - static int64_t vm_clock_warp_start ; <nl> + static int64_t vm_clock_warp_start = - 1 ; <nl> /* Conversion factor from emulated instructions to virtual clock ticks . */ <nl> static int icount_time_shift ; <nl> /* Arbitrarily pick 1MIPS as the minimum allowable speed . */
void cpu_reset ( CPUS390XState * env ) <nl> tlb_flush ( env , 1 ); <nl> } <nl>  <nl> + target_phys_addr_t cpu_get_phys_page_debug ( CPUState * env , target_ulong addr ) <nl> +{ <nl> + return 0 ; <nl> +} <nl> + <nl> # ifndef CONFIG_USER_ONLY <nl>  <nl> int cpu_s390x_handle_mmu_fault ( CPUState * env , target_ulong address , int rw ,
struct USBPacket { <nl> USBCallback * complete_cb ; <nl> void * complete_opaque ; <nl> USBCallback * cancel_cb ; <nl> - void * * cancel_opaque ; <nl> + void * cancel_opaque ; <nl> }; <nl>  <nl> /* Defer completion of a USB packet . The hadle_packet routine should then
static inline void mips_vpe_wake ( CPUMIPSState * c ) <nl> cpu_interrupt ( c , CPU_INTERRUPT_WAKE ); <nl> } <nl>  <nl> - static inline void mips_vpe_sleep ( CPUMIPSState * c ) <nl> + static inline void mips_vpe_sleep ( MIPSCPU * cpu ) <nl> { <nl> + CPUMIPSState * c = & cpu -> env ; <nl> + <nl> /* The VPE was shut off , really go to bed . <nl> Reset any old _WAKE requests . */ <nl> c -> halted = 1 ; <nl> static inline void mips_tc_sleep ( MIPSCPU * cpu , int tc ) <nl>  <nl> /* FIXME : TC reschedule . */ <nl> if (! mips_vpe_active ( c )) { <nl> - mips_vpe_sleep ( c ); <nl> + mips_vpe_sleep ( cpu ); <nl> } <nl> } <nl>  <nl> target_ulong helper_dvpe ( CPUMIPSState * env ) <nl> do { <nl> /* Turn off all VPEs except the one executing the dvpe . */ <nl> if ( other_cpu_env != env ) { <nl> + MIPSCPU * other_cpu = mips_env_get_cpu ( other_cpu_env ); <nl> + <nl> other_cpu_env -> mvp -> CP0_MVPControl &= ~( 1 << CP0MVPCo_EVP ); <nl> - mips_vpe_sleep ( other_cpu_env ); <nl> + mips_vpe_sleep ( other_cpu ); <nl> } <nl> other_cpu_env = other_cpu_env -> next_cpu ; <nl> } while ( other_cpu_env );
static int vmdk_open_vmdk4 ( BlockDriverState * bs , <nl> } <nl> extent -> compressed = <nl> le16_to_cpu ( header . compressAlgorithm ) == VMDK4_COMPRESSION_DEFLATE ; <nl> + if ( extent -> compressed ) { <nl> + g_free ( s -> create_type ); <nl> + s -> create_type = g_strdup (" streamOptimized "); <nl> + } <nl> extent -> has_marker = le32_to_cpu ( header . flags ) & VMDK4_FLAG_MARKER ; <nl> extent -> version = le32_to_cpu ( header . version ); <nl> extent -> has_zero_grain = le32_to_cpu ( header . flags ) & VMDK4_FLAG_ZERO_GRAIN ;
static void xhci_xfer_report ( XHCITransfer * xfer ) <nl> XHCIState * xhci = xfer -> xhci ; <nl> int i ; <nl>  <nl> - left = xfer -> packet . status ? 0 : xfer -> packet . actual_length ; <nl> + left = xfer -> packet . actual_length ; <nl>  <nl> for ( i = 0 ; i < xfer -> trb_count ; i ++) { <nl> XHCITRB * trb = & xfer -> trbs [ i ]; <nl> static void xhci_xfer_report ( XHCITransfer * xfer ) <nl>  <nl> if (! reported && (( trb -> control & TRB_TR_IOC ) || <nl> ( shortpkt && ( trb -> control & TRB_TR_ISP )) || <nl> - ( xfer -> status != CC_SUCCESS ))) { <nl> + ( xfer -> status != CC_SUCCESS && left == 0 ))) { <nl> event . slotid = xfer -> slotid ; <nl> event . epid = xfer -> epid ; <nl> event . length = ( trb -> status & 0x1ffff ) - chunk ;
int mew_lzma ( char * orgsource , const char * buf , uint32_t size_sum , uint32_t vma , <nl> if (! special ) <nl> { <nl> source = pushed_ebx ; <nl> + if (! CLI_ISCONTAINED ( orgsource , size_sum , source , 16 )) <nl> + return - 1 ; <nl> + <nl> if ( cli_readint32 ( source ) == 0 ) <nl> { <nl> return 0 ; <nl> } <nl> + } else { <nl> + if (! CLI_ISCONTAINED ( orgsource , size_sum , source , 12 )) <nl> + return - 1 ; <nl> } <nl> + <nl> var28 = cli_readint32 ( source ); <nl> source += 4 ; <nl> temp = cli_readint32 ( source ) - vma ;
int cli_hashset_init ( struct cli_hashset * hs , size_t initial_capacity , uint8_t lo <nl> if (! hs -> keys ) { <nl> return CL_EMEM ; <nl> } <nl> - hs -> bitmap = cli_calloc ( initial_capacity / 8 , sizeof (* hs -> bitmap )); <nl> + hs -> bitmap = cli_calloc ( initial_capacity >> 5 , sizeof (* hs -> bitmap )); <nl> if (! hs -> bitmap ) { <nl> free ( hs -> keys ); <nl> return CL_EMEM ;
int cli_url_canon ( const char * inurl , size_t len , char * urlbuff , size_t dest_len , <nl> ++ host_begin ; <nl>  <nl> /* ignore username in URL */ <nl> - p = strchr ( host_begin , '@'); <nl> + while (( host_begin < urlend ) && * host_begin == '/') ++ host_begin ; <nl> + host_len = strcspn ( host_begin , ":/?"); <nl> + p = memchr ( host_begin , '@', host_len ); <nl> if ( p ) <nl> host_begin = p + 1 ; <nl> url = host_begin ;
int rar_unpack ( int fd , int method , int solid , unpack_data_t * unpack_data ) <nl> default : <nl> retval = rar_unpack29 ( fd , solid , unpack_data ); <nl> if ( retval == FALSE ) { <nl> + rarvm_free (& unpack_data -> rarvm_data ); <nl> retval = rar_unpack20 ( fd , solid , unpack_data ); <nl> if ( retval == FALSE ) <nl> retval = rar_unpack15 ( fd , solid , unpack_data );
static char * parse_yara_hex_string ( YR_STRING * string , int * ret ) <nl> if (( ovr = strchr ( ovr , '}'))) <nl> * ovr = ']'; <nl> else { <nl> + free ( res ); <nl> if ( ret ) * ret = CL_EMALFDB ; <nl> return NULL ; <nl> } <nl> static char * parse_yara_hex_string ( YR_STRING * string , int * ret ) <nl> if (( ovr = strrchr ( res , '{'))) <nl> * ovr = '['; <nl> else { <nl> + free ( res ); <nl> if ( ret ) * ret = CL_EMALFDB ; <nl> return NULL ; <nl> }
static int fincore_fd ( struct fincore_control * ctl , <nl> off_t * count_incore ) <nl> { <nl> size_t window_size = N_PAGES_IN_WINDOW * ctl -> pagesize ; <nl> - off_t file_offset ; <nl> + off_t file_offset , len ; <nl> int rc = 0 ; <nl> int warned_once = 0 ; <nl>  <nl> - for ( file_offset = 0 ; file_offset < file_size ; file_offset += window_size ) { <nl> - off_t len ; <nl> + for ( file_offset = 0 ; file_offset < file_size ; file_offset += len ) { <nl> void * window = NULL ; <nl>  <nl> len = file_size - file_offset ;
struct libmnt_fs * mnt_table_find_target ( struct libmnt_table * tb , const char * pat <nl> /* native @ target */ <nl> mnt_reset_iter (& itr , direction ); <nl> while ( mnt_table_next_fs ( tb , & itr , & fs ) == 0 ) { <nl> - if ( fs -> target && strcmp ( fs -> target , path ) == 0 ) <nl> + if ( fs -> target && streq_except_trailing_slash ( fs -> target , path )) <nl> return fs ; <nl> } <nl> if (! tb -> cache || !( cn = mnt_resolve_path ( path , tb -> cache )))
int read_hypervisor_dmi ( void ) <nl> if ( rc ) <nl> goto done ; <nl> free ( buf ); <nl> - <nl> + buf = NULL ; <nl> memory_scan : <nl> # if defined ( __x86_64__ ) || defined ( __i386__ ) <nl> /* Fallback to memory scan ( x86 , x86_64 ) */
int main ( int argc , char ** argv ) <nl> if (( pwd = getrootpwent ( opt_e )) == NULL ) { <nl> warnx ( _ (" cannot open password database .")); <nl> sleep ( 2 ); <nl> + return EXIT_FAILURE ; <nl> } <nl>  <nl> /*
static int magic ( FILE * f , char * fs ) <nl> case 0x457f : /* simple ELF detection */ <nl> printf ( _ ("\ n ******** % s : Not a text file ********\ n \ n "), <nl> fs ); <nl> - fclose ( f ); <nl> return 1 ; <nl> } <nl> }
void MainWindow :: createKeyboardShortcuts () <nl> connect ( switchRSSShortcut , & QShortcut :: activated , this , static_cast < Func >(& MainWindow :: displayRSSTab )); <nl> QShortcut * switchExecutionLogShortcut = new QShortcut ( Qt :: ALT + Qt :: Key_4 , this ); <nl> connect ( switchExecutionLogShortcut , & QShortcut :: activated , this , & MainWindow :: displayExecutionLogTab ); <nl> - QShortcut * switchSearchFilterShortcut = new QShortcut ( QKeySequence :: Find , this ); <nl> + QShortcut * switchSearchFilterShortcut = new QShortcut ( QKeySequence :: Find , m_transferListWidget ); <nl> connect ( switchSearchFilterShortcut , & QShortcut :: activated , this , & MainWindow :: focusSearchFilter ); <nl>  <nl> m_ui -> actionDocumentation -> setShortcut ( QKeySequence :: HelpContents );
void Bittorrent :: addConsoleMessage ( QString msg , QString ) { <nl> if (! defaultTempPath . isEmpty ()) { <nl> // Check if directory is different <nl> QDir current_dir ( h . save_path ()); <nl> - QDir save_dir ( getSavePath ( h . hash ())); <nl> + QDir save_dir ( getSavePath ( hash )); <nl> if ( current_dir != save_dir ) { <nl> h . move_storage ( save_dir . path ()); <nl> } <nl> void Bittorrent :: addConsoleMessage ( QString msg , QString ) { <nl> } <nl> } <nl> emit torrentFinishedChecking ( h ); <nl> - emit metadataReceived ( h ); <nl> if ( torrentsToPausedAfterChecking . contains ( hash )) { <nl> torrentsToPausedAfterChecking . removeOne ( hash ); <nl> h . pause ();
void MessageChannelAdd :: process ( Connection * cCon ) { <nl> } <nl>  <nl> Channel * c = ServerDB :: addChannel ( p , qsName ); <nl> + if ( sPlayerId >= 0 ) { <nl> + Group * g = new Group ( c , " admin "); <nl> + g -> qsAdd << sPlayerId ; <nl> + } <nl> + ServerDB :: updateChannel ( c ); <nl>  <nl> iId = c -> iId ; <nl> g_sServer -> sendAll ( this );
void Settings :: save () { <nl> SAVELOAD ( bSuppressMacEventTapWarning , " shortcut / mac / suppresswarning "); <nl> SAVELOAD ( bEnableEvdev , " shortcut / linux / evdev / enable "); <nl> SAVELOAD ( bEnableXInput2 , " shortcut / x11 / xinput2 / enable "); <nl> + SAVELOAD ( bEnableGKey , " shortcut / gkey "); <nl> SAVELOAD ( bEnableXboxInput , " shortcut / windows / xbox / enable "); <nl> SAVELOAD ( bEnableWinHooks , " winhooks "); <nl> SAVELOAD ( bDirectInputVerboseLogging , " shortcut / windows / directinput / verboselogging ");
int rpmVerifyScript ( char * root , Header h , int err ) { <nl> exit (- 1 ); <nl> } <nl>  <nl> - close ( out ); <nl> - close ( err ); <nl> + if ( out > 2 ) close ( out ); <nl> + if ( err > 2 ) close ( err ); <nl> close ( fd ); <nl> if (! rpmIsVerbose ()) close ( out ); <nl> 
struct rpmte_s { <nl> char * release ; /*!< Release : */ <nl> char * arch ; /*!< Architecture hint . */ <nl> char * os ; /*!< Operating system hint . */ <nl> - int archScore ; /*!< ( TR_ADDED ) Arch score . */ <nl> - int osScore ; /*!< ( TR_ADDED ) Os score . */ <nl> int isSource ; /*!< ( TR_ADDED ) source rpm ? */ <nl>  <nl> rpmte depends ; /*!< Package updated by this package ( ERASE te ) */ <nl> static void addTE ( rpmte p , Header h , fnpyKey key , rpmRelocation * relocs ) <nl> p -> epoch = headerGetAsString ( h , RPMTAG_EPOCH ); <nl>  <nl> p -> arch = headerGetAsString ( h , RPMTAG_ARCH ); <nl> - p -> archScore = p -> arch ? rpmMachineScore ( RPM_MACHTABLE_INSTARCH , p -> arch ) : 0 ; <nl> - <nl> p -> os = headerGetAsString ( h , RPMTAG_OS ); <nl> - p -> osScore = p -> os ? rpmMachineScore ( RPM_MACHTABLE_INSTOS , p -> os ) : 0 ; <nl>  <nl> p -> isSource = headerIsSource ( h ); <nl> 
char * headerSprintf ( Header h , const char * origFmt , <nl>  <nl> strcat ( answer , piece ); <nl> answerLength += pieceLength ; <nl> + free ( piece ); <nl> } <nl> } <nl> 
findEntry ( MacroContext mc , const char * name , size_t namelen ) <nl> { <nl> MacroEntry key , * ret ; <nl> struct MacroEntry_s keybuf ; <nl> - char namebuf [ 1024 ]; <nl> + char * namebuf = NULL ; <nl>  <nl> /*@- globs @*/ <nl> if ( mc == NULL ) mc = rpmGlobalMacroContext ; <nl> findEntry ( MacroContext mc , const char * name , size_t namelen ) <nl>  <nl> /*@- branchstate @*/ <nl> if ( namelen > 0 ) { <nl> + namebuf = alloca ( namelen + 1 ); <nl> + memset ( namebuf , 0 , ( namelen + 1 )); <nl> strncpy ( namebuf , name , namelen ); <nl> namebuf [ namelen ] = '\ 0 '; <nl> name = namebuf ;
 <nl> # include " system . h " <nl>  <nl> +# include < assert . h > <nl> # include < netinet / in . h > <nl>  <nl> # include " rpmmacro . h " <nl> # define IPPORT_PGPKEYSERVER 11371 <nl> # endif <nl>  <nl> +# define URLMAGIC 0xd00b1ed0 <nl> +# define URLSANE ( u ) assert ( u && u -> magic == URLMAGIC ) <nl> + <nl> /** <nl> */ <nl> int _url_debug = 0 ; <nl>  <nl> -# define URLDBG ( _f , _m , _x ) if (( _url_debug | ( _f )) & ( _m )) fprintf _x <nl> - <nl> urlinfo urlNew () <nl> { <nl> urlinfo u ;
int pkgReadHeader ( int fd , Header * hdr , int * isSource ) { <nl> readOldHeader ( fd , hdr , isSource ); <nl> arch = getArchNum (); <nl> addEntry (* hdr , RPMTAG_ARCH , INT8_TYPE , & arch , 1 ); <nl> - } else if ( lead . major == 2 ) { <nl> + } else if ( lead . major == 2 || lead . major == 3 ) { <nl> + if ( lead . minor ) { <nl> + error ( RPMERR_NEWPACKAGE , " only packages with minor numbers = 0 " <nl> + " are supported by this version of RPM "); <nl> + return 2 ; <nl> + } <nl> if (! readSignature ( fd , lead . signature_type , NULL )) { <nl> return 2 ; <nl> }
* <nl> */ <nl> /* <nl> - * $ Id : mboxlist . c , v 1 . 183 2002 / 04 / 08 01 : 46 : 38 leg Exp $ <nl> + * $ Id : mboxlist . c , v 1 . 184 2002 / 04 / 08 21 : 01 : 04 rjs3 Exp $ <nl> */ <nl>  <nl> # include < config . h > <nl> int mboxlist_deletemailbox ( const char * name , int isadmin , char * userid , <nl>  <nl> /* remove from mupdate */ <nl> /* xxx this can lead to inconsistancies if the later stuff fails */ <nl> - if (! r && ! local_only && config_mupdate_server ) { <nl> + if (! r && ! isremote && ! local_only && config_mupdate_server ) { <nl> /* delete the mailbox in MUPDATE */ <nl> r = mupdate_connect ( config_mupdate_server , NULL , & mupdate_h , NULL ); <nl> if ( r ) {
* OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE . <nl> */ <nl>  <nl> -/* $ Id : proxyd . c , v 1 . 42 2000 / 09 / 30 01 : 17 : 54 leg Exp $ */ <nl> +/* $ Id : proxyd . c , v 1 . 43 2000 / 10 / 09 03 : 28 : 50 leg Exp $ */ <nl>  <nl> # include < config . h > <nl>  <nl> static struct backend * proxyd_findinboxserver ( void ) <nl> { <nl> char inbox [ MAX_MAILBOX_NAME ]; <nl> int r ; <nl> - char * server ; <nl> - struct backend * s ; <nl> + char * server = NULL ; <nl> + struct backend * s = NULL ; <nl>  <nl> if ( strlen ( proxyd_userid ) > MAX_MAILBOX_NAME - 30 ) return NULL ; <nl> strcpy ( inbox , " user ."); <nl> strcat ( inbox , proxyd_userid ); <nl>  <nl> r = mlookup ( inbox , & server , NULL , NULL ); <nl> - s = proxyd_findserver ( server ); <nl> + if (! r ) { <nl> + s = proxyd_findserver ( server ); <nl> + } <nl>  <nl> return s ; <nl> }
EXPORTED void jmap_closembox ( jmap_req_t * req , struct mailbox ** mboxp ) <nl> struct _mboxcache_rec * rec = NULL ; <nl> int i ; <nl>  <nl> + if (! mboxp || !* mboxp ) return ; <nl> + <nl> for ( i = 0 ; i < req -> mboxes -> count ; i ++) { <nl> rec = ( struct _mboxcache_rec *) ptrarray_nth ( req -> mboxes , i ); <nl> if ( rec -> mbox == * mboxp )
EXPORTED int index_snippets ( struct index_state * state , <nl> int nmatches = 0 ; <nl> struct snippet_rock srock ; <nl>  <nl> + /* reload index */ <nl> + r = index_refresh ( state ); <nl> + if ( r ) return r ; <nl> + <nl> bx = search_begin_search ( state -> mailbox , SEARCH_MULTIPLE ); <nl> if (! bx ) { <nl> r = IMAP_INTERNAL ;
void mboxevent_add_flags ( struct mboxevent * event , char * flagnames [ MAX_USER_FLAGS <nl> # ifdef ENABLE_MBOXEVENT <nl> unsigned flag , flagmask ; <nl>  <nl> + if (! event ) <nl> + return ; <nl> + <nl> /* add system flags */ <nl> if ( system_flags & FLAG_DELETED ) { <nl> if ( strarray_find_case ( excluded_flags , "\\ Deleted ", 0 ) < 0 ) <nl> static int filled_params ( enum event_type type , struct mboxevent * event ) <nl> struct buf missing = BUF_INITIALIZER ; <nl> int param , ret = 1 ; <nl>  <nl> + if (! event ) <nl> + return 0 ; <nl> + <nl> for ( param = 0 ; param <= MAX_PARAM ; param ++) { <nl>  <nl> if ( mboxevent_expected_param ( type , param ) &&
EXPORTED void mbname_free ( mbname_t ** mbnamep ) <nl> free ( mbname -> intname ); <nl> free ( mbname -> extname ); <nl> free ( mbname -> extuserid ); <nl> + free ( mbname -> recipient ); <nl>  <nl> /* thing itself */ <nl> free ( mbname );
int mailbox_ensure_cache ( struct mailbox * mailbox , unsigned offset ) <nl> mailbox -> cache_fd = open ( fname , openflags , 0 ); <nl> if ( mailbox -> cache_fd == - 1 ) <nl> goto fail ; <nl> + <nl> + if ( mailbox -> cache_buf . s ) <nl> + map_free (( const char **)& mailbox -> cache_buf . s , & mailbox -> cache_len ); <nl> + mailbox -> cache_buf . len = 0 ; <nl> } <nl>  <nl> if ( offset >= mailbox -> cache_buf . len ) {
/* actions . c -- executes the commands for timsieved <nl> * Tim Martin <nl> - * $ Id : actions . c , v 1 . 14 2000 / 04 / 06 15 : 20 : 10 leg Exp $ <nl> + * $ Id : actions . c , v 1 . 15 2000 / 04 / 07 02 : 45 : 57 leg Exp $ <nl> * <nl> */ <nl> /*********************************************************** <nl> int actions_setuser ( char * userid ) <nl> result = chdir ( sieve_dir ); <nl> if ( result != 0 ) { <nl> result = mkdir ( sieve_dir , 0755 ); <nl> - if ( result != 0 ) { <nl> + if (! result ) result = chdir ( sieve_dir ); <nl> + if (! result ) { <nl> syslog ( LOG_ERR , " mkdir % s : % m ", sieve_dir ); <nl> return TIMSIEVE_FAIL ; <nl> }
void interaction ( int id , const char * prompt , <nl> * tresult = strdup ( getpass ("")); <nl> * tlen = strlen (* tresult ); <nl> return ; <nl> - } else if ((( id == SASL_CB_USER ) || <nl> - ( id == SASL_CB_AUTHNAME )) && ( authname != NULL )) { <nl> - strcpy ( result , authname ); <nl> + } else if (( id == SASL_CB_USER ) || ( id == SASL_CB_AUTHNAME )) { <nl> + if ( authname ) { <nl> + strcpy ( result , authname ); <nl> + } else { <nl> + strcpy ( result , getpwuid ( getuid ())-> pw_name ); <nl> + } <nl> # ifdef SASL_CB_GETREALM <nl> } else if (( id == SASL_CB_GETREALM ) && ( realm != NULL )) { <nl> strcpy ( result , realm );
char * sendSynchronousCommand ( int flags , int fd , ...) { <nl> cmd = sdscat ( cmd , arg ); <nl> } <nl> cmd = sdscatlen ( cmd ,"\ r \ n ", 2 ); <nl> - <nl> + va_end ( ap ); <nl> + <nl> /* Transfer command to the server . */ <nl> if ( syncWrite ( fd , cmd , sdslen ( cmd ), server . repl_syncio_timeout * 1000 ) <nl> == - 1 ) <nl> char * sendSynchronousCommand ( int flags , int fd , ...) { <nl> strerror ( errno )); <nl> } <nl> sdsfree ( cmd ); <nl> - va_end ( ap ); <nl> } <nl>  <nl> /* Read the reply from the server . */
void xclaimCommand ( client * c ) { <nl>  <nl> /* Create the NACK . */ <nl> nack = streamCreateNACK ( NULL ); <nl> + raxInsert ( group -> pel , buf , sizeof ( buf ), nack , NULL ); <nl> } <nl>  <nl> if ( nack != raxNotFound ) {
void xreadCommand ( client * c ) { <nl> } <nl>  <nl> if ( strcmp ( c -> argv [ i ]-> ptr ,"$") == 0 ) { <nl> + if ( xreadgroup ) { <nl> + addReplyError ( c ," The $ ID can be specified only when calling " <nl> + " XREAD without GROUP option ."); <nl> + goto cleanup ; <nl> + } <nl> if ( o ) { <nl> stream * s = o -> ptr ; <nl> ids [ id_idx ] = s -> last_id ;
void clusterCommand ( redisClient * c ) { <nl> addReplyError ( c ," Invalid CLUSTER SETSLOT action or number of arguments "); <nl> return ; <nl> } <nl> + clusterUpdateState (); <nl> clusterSaveConfigOrDie (); <nl> addReply ( c , shared . ok ); <nl> } else if (! strcasecmp ( c -> argv [ 1 ]-> ptr ," info ") && c -> argc == 2 ) {
void clusterSendFailoverAuthIfNeeded ( clusterNode * node , clusterMsg * request ) { <nl> clusterSendFailoverAuth ( node ); <nl> server . cluster -> lastVoteEpoch = server . cluster -> currentEpoch ; <nl> node -> slaveof -> voted_time = mstime (); <nl> + redisLog ( REDIS_WARNING , " Failover auth granted to %. 40s for epoch % llu ", <nl> + node -> name , ( unsigned long long ) server . cluster -> currentEpoch ); <nl> } <nl>  <nl> /* This function returns the " rank " of this instance , a slave , in the context
void zremrangeGenericCommand ( redisClient * c , int rangetype ) { <nl>  <nl> /* Step 2 : Lookup & range sanity checks if needed . */ <nl> if (( zobj = lookupKeyWriteOrReply ( c , key , shared . czero )) == NULL || <nl> - checkType ( c , zobj , REDIS_ZSET )) return ; <nl> + checkType ( c , zobj , REDIS_ZSET )) goto cleanup ; <nl>  <nl> if ( rangetype == ZRANGE_RANK ) { <nl> /* Sanitize indexes . */ <nl> void zremrangeGenericCommand ( redisClient * c , int rangetype ) { <nl> * The range is empty when start > end or start >= length . */ <nl> if ( start > end || start >= llen ) { <nl> addReply ( c , shared . czero ); <nl> - return ; <nl> + goto cleanup ; <nl> } <nl> if ( end >= llen ) end = llen - 1 ; <nl> } <nl> void zremrangeGenericCommand ( redisClient * c , int rangetype ) { <nl> } <nl> server . dirty += deleted ; <nl> addReplyLongLong ( c , deleted ); <nl> + <nl> + cleanup : <nl> + if ( rangetype == ZRANGE_LEX ) zslFreeLexRange (& lexrange ); <nl> } <nl>  <nl> void zremrangebyrankCommand ( redisClient * c ) {
int processCommand ( client * c ) { <nl> * <nl> * First we try to free some memory if possible ( if there are volatile <nl> * keys in the dataset ). If there are not the only thing we can do <nl> - * is returning an error . */ <nl> - if ( server . maxmemory ) { <nl> + * is returning an error . <nl> + * <nl> + * Note that we do not want to reclaim memory if we are here re - entering <nl> + * the event loop since there is a busy Lua script running in timeout <nl> + * condition , to avoid mixing the propagation of scripts with the propagation <nl> + * of DELs due to eviction . */ <nl> + if ( server . maxmemory && ! server . lua_timedout ) { <nl> int out_of_memory = freeMemoryIfNeeded () == C_ERR ; <nl> /* freeMemoryIfNeeded may flush slave output buffers . This may result <nl> * into a slave , that may be the active client , to be freed . */
get_next : <nl> if ( len <= 0 ){ <nl> i = END_NOT_FOUND ; <nl> } else { <nl> + s -> state = 0 ; <nl> i -= s -> header_size - 1 ; <nl> s -> remaining_size = len ; <nl> if (! new_frame_start || pc -> index + i <= 0 ){
static int process_ea_header ( AVFormatContext * s ) { <nl> err = process_video_header_mdec ( s ); <nl> break ; <nl>  <nl> + case MPCh_TAG : <nl> + ea -> video_codec = CODEC_ID_MPEG2VIDEO ; <nl> + break ; <nl> + <nl> case MVhd_TAG : <nl> err = process_video_header_vp6 ( s ); <nl> break ; <nl> static int ea_read_packet ( AVFormatContext * s , <nl> goto get_video_packet ; <nl>  <nl> case MV0K_TAG : <nl> + case MPCh_TAG : <nl> key = PKT_FLAG_KEY ; <nl> case MV0F_TAG : <nl> get_video_packet :
static int mpeg4video_probe ( AVProbeData * probe_packet ) <nl>  <nl> for ( i = 0 ; i < probe_packet -> buf_size ; i ++) { <nl> temp_buffer = ( temp_buffer << 8 ) + probe_packet -> buf [ i ]; <nl> - if (( temp_buffer & 0xffffff00 ) != 0x100 ) <nl> + if ( temp_buffer & 0xfffffe00 ) <nl> + continue ; <nl> + if ( temp_buffer < 2 ) <nl> continue ; <nl>  <nl> if ( temp_buffer == VOP_START_CODE ) <nl> VOP ++; <nl> else if ( temp_buffer == VISUAL_OBJECT_START_CODE ) <nl> VISO ++; <nl> - else if ( temp_buffer < 0x120 ) <nl> + else if ( temp_buffer >= 0x100 && temp_buffer < 0x120 ) <nl> VO ++; <nl> - else if ( temp_buffer < 0x130 ) <nl> + else if ( temp_buffer >= 0x120 && temp_buffer < 0x130 ) <nl> VOL ++; <nl> else if (!( 0x1AF < temp_buffer && temp_buffer < 0x1B7 ) && <nl> !( 0x1B9 < temp_buffer && temp_buffer < 0x1C4 ))
static int process_line ( URLContext * h , char * line , int line_count , <nl>  <nl> /* error codes are 4xx and 5xx , but regard 401 as a success , so we <nl> * don ' t abort until all headers have been parsed . */ <nl> - if ( s -> http_code >= 400 && s -> http_code < 600 && s -> http_code != 401 ) { <nl> + if ( s -> http_code >= 400 && s -> http_code < 600 && ( s -> http_code != 401 <nl> + || s -> auth_state . auth_type != HTTP_AUTH_NONE )) { <nl> end += strspn ( end , SPACE_CHARS ); <nl> av_log ( h , AV_LOG_WARNING , " HTTP error % d % s \ n ", <nl> s -> http_code , end );
static int pal2rgbWrapper ( SwsContext * c , uint8_t * src [], int srcStride [], int sr <nl>  <nl>  <nl> for ( i = 0 ; i < srcSliceH ; i ++) { <nl> - conv ( srcPtr , dstPtr , c -> srcW , c -> pal_rgb ); <nl> + conv ( srcPtr , dstPtr , c -> srcW , ( uint8_t *) c -> pal_rgb ); <nl> srcPtr += srcStride [ 0 ]; <nl> dstPtr += dstStride [ 0 ]; <nl> }
typedef struct { <nl>  <nl> /* Primary audio coding header */ <nl> int subframes ; ///< number of subframes <nl> + int is_channels_set ; ///< check for if the channel number is already set <nl> int total_channels ; ///< number of channels including extensions <nl> int prim_channels ; ///< number of primary audio channels <nl> int subband_activity [ DCA_PRIM_CHANNELS_MAX ]; ///< subband activity count <nl> static int dca_decode_frame ( AVCodecContext * avctx , <nl> unset . Ideally during the first probe for channels the crc should be checked <nl> and only set avctx -> channels when the crc is ok . Right now the decoder could <nl> set the channels based on a broken first frame .*/ <nl> + if ( s -> is_channels_set == 0 ) { <nl> + s -> is_channels_set = 1 ; <nl> avctx -> channels = channels ; <nl> + } <nl> + if ( avctx -> channels != channels ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " DCA decoder does not support number of " <nl> + " channels changing in stream . Skipping frame .\ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> if (* data_size < ( s -> sample_blocks / 8 ) * 256 * sizeof ( int16_t ) * channels ) <nl> return - 1 ;
static int decode_frame ( AVCodecContext * avctx , <nl> get_bits_align32 ( gb ); <nl> } <nl>  <nl> - * data_size = ( uint8_t *) samples - ( uint8_t *) data ; <nl> - if ( reported_size != * data_size ) { <nl> - av_log ( avctx , AV_LOG_WARNING , " reported data size (% d ) does not match output data size (% d )\ n ", <nl> - reported_size , * data_size ); <nl> - } <nl> + * data_size = FFMIN ( reported_size , ( uint8_t *) samples - ( uint8_t *) data ); <nl> return buf_size ; <nl> } <nl> 
static av_cold int oggvorbis_init_encoder ( vorbis_info * vi , AVCodecContext * avcco <nl> avccontext -> global_quality / ( float ) FF_QP2LAMBDA / 10 . 0 )) <nl> return - 1 ; <nl> } else { <nl> + int minrate = avccontext -> rc_min_rate > 0 ? avccontext -> rc_min_rate : - 1 ; <nl> + int maxrate = avccontext -> rc_min_rate > 0 ? avccontext -> rc_max_rate : - 1 ; <nl> + <nl> /* constant bitrate */ <nl> if ( vorbis_encode_setup_managed ( vi , avccontext -> channels , <nl> - avccontext -> sample_rate , - 1 , avccontext -> bit_rate , - 1 )) <nl> + avccontext -> sample_rate , minrate , avccontext -> bit_rate , maxrate )) <nl> return - 1 ; <nl>  <nl> # ifdef OGGVORBIS_VBR_BY_ESTIMATE
static av_cold int vorbis_decode_init ( AVCodecContext * avccontext ) { <nl> hdr_type = get_bits ( gb , 8 ); <nl> if ( hdr_type != 5 ) { <nl> av_log ( avccontext , AV_LOG_ERROR , " Third header is not the setup header .\ n "); <nl> + vorbis_free ( vc ); <nl> return - 1 ; <nl> } <nl> if ( vorbis_parse_setup_hdr ( vc )) {
av_cold int ff_vaapi_encode_close ( AVCodecContext * avctx ) <nl> vaapi_encode_free ( avctx , pic ); <nl> } <nl>  <nl> + av_buffer_pool_uninit (& ctx -> output_buffer_pool ); <nl> + <nl> if ( ctx -> va_context != VA_INVALID_ID ) { <nl> vaDestroyContext ( ctx -> hwctx -> display , ctx -> va_context ); <nl> ctx -> va_context = VA_INVALID_ID ; <nl> av_cold int ff_vaapi_encode_close ( AVCodecContext * avctx ) <nl> ctx -> va_config = VA_INVALID_ID ; <nl> } <nl>  <nl> - av_buffer_pool_uninit (& ctx -> output_buffer_pool ); <nl> - <nl> av_freep (& ctx -> codec_sequence_params ); <nl> av_freep (& ctx -> codec_picture_params ); <nl> 
static int mjpeg_decode_frame ( AVCodecContext * avctx , <nl> *( dst ++) = x ; <nl> if ( x == 0xff ) <nl> { <nl> - while (* src == 0xff ) src ++; <nl> + while ( src < buf_end && x == 0xff ) <nl> + x = *( src ++); <nl>  <nl> - x = *( src ++); <nl> if ( x >= 0xd0 && x <= 0xd7 ) <nl> *( dst ++) = x ; <nl> else if ( x )
SwsContext * sws_getContext ( int srcW , int srcH , int srcFormat , int dstW , int dstH <nl> /* LQ converters if - sws 0 or - sws 4 */ <nl> if ( c -> flags &( SWS_FAST_BILINEAR | SWS_POINT )){ <nl> /* yv12_to_yuy2 */ <nl> - if ( srcFormat == PIX_FMT_YUV420P && <nl> - ( dstFormat == PIX_FMT_YUYV422 || dstFormat == PIX_FMT_UYVY422 )) <nl> + if ( srcFormat == PIX_FMT_YUV420P ) <nl> { <nl> if ( dstFormat == PIX_FMT_YUYV422 ) <nl> c -> swScale = PlanarToYuy2Wrapper ; <nl> - else <nl> + else if ( dstFormat == PIX_FMT_UYVY422 ) <nl> c -> swScale = PlanarToUyvyWrapper ; <nl> } <nl> } <nl>  <nl> # ifdef COMPILE_ALTIVEC <nl> if (( c -> flags & SWS_CPU_CAPS_ALTIVEC ) && <nl> - (( srcFormat == PIX_FMT_YUV420P && <nl> - ( dstFormat == PIX_FMT_YUYV422 || dstFormat == PIX_FMT_UYVY422 )))) { <nl> + srcFormat == PIX_FMT_YUV420P ) { <nl> // unscaled YV12 -> packed YUV , we want speed <nl> if ( dstFormat == PIX_FMT_YUYV422 ) <nl> c -> swScale = yv12toyuy2_unscaled_altivec ; <nl> - else <nl> + else if ( dstFormat == PIX_FMT_UYVY422 ) <nl> c -> swScale = yv12touyvy_unscaled_altivec ; <nl> } <nl> # endif
static int mov_read_stss ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> av_dlog ( c -> fc , " keyframe_count = % d \ n ", entries ); <nl>  <nl> + if (! entries ) <nl> + return 0 ; <nl> if ( entries >= UINT_MAX / sizeof ( int )) <nl> return AVERROR_INVALIDDATA ; <nl> sc -> keyframes = av_malloc ( entries * sizeof ( int ));
static void wiener_denoise ( WMAVoiceContext * s , int fcb_type , <nl> ff_rdft_calc (& s -> rdft , coeffs ); <nl> synth_pf [ 0 ] *= coeffs [ 0 ]; <nl> synth_pf [ 1 ] *= coeffs [ 1 ]; <nl> - for ( n = 1 ; n < 128 ; n ++) { <nl> + for ( n = 1 ; n < 64 ; n ++) { <nl> float v1 = synth_pf [ n * 2 ], v2 = synth_pf [ n * 2 + 1 ]; <nl> synth_pf [ n * 2 ] = v1 * coeffs [ n * 2 ] - v2 * coeffs [ n * 2 + 1 ]; <nl> synth_pf [ n * 2 + 1 ] = v2 * coeffs [ n * 2 ] + v1 * coeffs [ n * 2 + 1 ];
static void mpegts_write_pes ( AVFormatContext * s , AVStream * st , <nl> * q ++ = 0xe0 ; <nl> } else if ( st -> codec -> codec_type == AVMEDIA_TYPE_AUDIO && <nl> ( st -> codec -> codec_id == CODEC_ID_MP2 || <nl> - st -> codec -> codec_id == CODEC_ID_MP3 )) { <nl> + st -> codec -> codec_id == CODEC_ID_MP3 || <nl> + st -> codec -> codec_id == CODEC_ID_AAC )) { <nl> * q ++ = 0xc0 ; <nl> } else { <nl> * q ++ = 0xbd ;
static void mov_build_index ( MOVContext * mov , AVStream * st ) <nl>  <nl> for ( i = 0 ; i < sc -> chunk_count ; i ++) { <nl> current_offset = sc -> chunk_offsets [ i ]; <nl> - if ( stsc_index + 1 < sc -> stsc_count && <nl> + while ( stsc_index + 1 < sc -> stsc_count && <nl> i + 1 == sc -> stsc_data [ stsc_index + 1 ]. first ) <nl> stsc_index ++; <nl> for ( j = 0 ; j < sc -> stsc_data [ stsc_index ]. count ; j ++) {
static int display_end_segment ( AVCodecContext * avctx , void * data , <nl> * not been cleared by a subsequent empty display command . <nl> */ <nl>  <nl> + memset ( sub , 0 , sizeof (* sub )); <nl> sub -> start_display_time = 0 ; <nl> sub -> end_display_time = 20000 ; <nl> sub -> format = 0 ;
static int decode_tag ( AVCodecContext * avctx , <nl> blocks = 2 ; break ; <nl> case 256 : // 22050Hz <nl> blocks = 4 ; break ; <nl> + case 512 : // 44100Hz <nl> + blocks = 8 ; break ; <nl> default : <nl> av_log ( avctx , AV_LOG_ERROR , " Tag size % d unknown , report sample !\ n ", buf_size ); <nl> return buf_size ;
void dsputil_init_bfin ( DSPContext * c , AVCodecContext * avctx ) <nl> c -> put_no_rnd_pixels_tab [ 0 ][ 2 ] = bfin_put_pixels16_y2_nornd ; <nl> c -> put_no_rnd_pixels_tab [ 0 ][ 3 ] = ff_bfin_put_pixels16_xy2_nornd ; <nl>  <nl> + c -> idct_permutation_type = FF_NO_IDCT_PERM ; <nl> c -> fdct = ff_bfin_fdct ; <nl> c -> idct = ff_bfin_idct ; <nl> c -> idct_add = bfin_idct_add ;
static void rtsp_cmd_describe ( HTTPContext * c , const char * url ) <nl> return ; <nl> } <nl> rtsp_reply_header ( c , RTSP_STATUS_OK ); <nl> + url_fprintf ( c -> pb , " Content - Base : % s /\ r \ n ", url ); <nl> url_fprintf ( c -> pb , " Content - Type : application / sdp \ r \ n "); <nl> url_fprintf ( c -> pb , " Content - Length : % d \ r \ n ", content_length ); <nl> url_fprintf ( c -> pb , "\ r \ n ");
static int mpeg_decode_frame ( AVCodecContext * avctx , <nl> if ( s2 -> last_picture_ptr == NULL ){ <nl> /* Skip B - frames if we do not have reference frames . */ <nl> if ( s2 -> pict_type == B_TYPE ) break ; <nl> + } <nl> + if ( s2 -> next_picture_ptr == NULL ){ <nl> /* Skip P - frames if we do not have reference frame no valid header . */ <nl> -// if ( s2 -> pict_type == P_TYPE && s2 -> first_field && ! s2 -> first_slice ) break ; <nl> + if ( s2 -> pict_type == P_TYPE && ( s2 -> first_field || s2 -> picture_structure == PICT_FRAME )) break ; <nl> } <nl> /* Skip B - frames if we are in a hurry . */ <nl> if ( avctx -> hurry_up && s2 -> pict_type == B_TYPE ) break ;
int avfilter_poll_frame ( AVFilterLink * link ) <nl> void avfilter_start_frame ( AVFilterLink * link , AVFilterPicRef * picref ) <nl> { <nl> void (* start_frame )( AVFilterLink *, AVFilterPicRef *); <nl> + AVFilterPad * dst = & link_dpad ( link ); <nl>  <nl> - if (!( start_frame = link_dpad ( link ). start_frame )) <nl> + if (!( start_frame = dst -> start_frame )) <nl> start_frame = avfilter_default_start_frame ; <nl>  <nl> /* prepare to copy the picture if it has insufficient permissions */ <nl> - if (( link_dpad ( link ). min_perms & picref -> perms ) != link_dpad ( link ). min_perms || <nl> - link_dpad ( link ). rej_perms & picref -> perms ) { <nl> + if (( dst -> min_perms & picref -> perms ) != dst -> min_perms || <nl> + dst -> rej_perms & picref -> perms ) { <nl> /* <nl> av_log ( link -> dst , AV_LOG_INFO , <nl> " frame copy needed ( have perms % x , need % x , reject % x )\ n ", <nl> void avfilter_start_frame ( AVFilterLink * link , AVFilterPicRef * picref ) <nl> link_dpad ( link ). min_perms , link_dpad ( link ). rej_perms ); <nl> */ <nl>  <nl> - link -> cur_pic = avfilter_default_get_video_buffer ( link , link_dpad ( link ). min_perms ); <nl> + link -> cur_pic = avfilter_default_get_video_buffer ( link , dst -> min_perms ); <nl> link -> srcpic = picref ; <nl> link -> cur_pic -> pts = link -> srcpic -> pts ; <nl> }
static int tta_read_header ( AVFormatContext * s ) <nl> TTAContext * c = s -> priv_data ; <nl> AVStream * st ; <nl> int i , channels , bps , samplerate , datalen ; <nl> - uint64_t framepos , start_offset ; <nl> + int64_t framepos , start_offset ; <nl>  <nl> if (! av_dict_get ( s -> metadata , "", NULL , AV_DICT_IGNORE_SUFFIX )) <nl> ff_id3v1_read ( s ); <nl>  <nl> start_offset = avio_tell ( s -> pb ); <nl> + if ( start_offset < 0 ) <nl> + return start_offset ; <nl> if ( avio_rl32 ( s -> pb ) != AV_RL32 (" TTA1 ")) <nl> return - 1 ; // not tta file <nl>  <nl> static int tta_read_header ( AVFormatContext * s ) <nl> st -> start_time = 0 ; <nl> st -> duration = datalen ; <nl>  <nl> - framepos = avio_tell ( s -> pb ) + 4 * c -> totalframes + 4 ; <nl> + framepos = avio_tell ( s -> pb ); <nl> + if ( framepos < 0 ) <nl> + return framepos ; <nl> + framepos += 4 * c -> totalframes + 4 ; <nl>  <nl> for ( i = 0 ; i < c -> totalframes ; i ++) { <nl> uint32_t size = avio_rl32 ( s -> pb );
static int64_t rtmp_read_seek ( URLContext * s , int stream_index , <nl> RTMP * r = s -> priv_data ; <nl>  <nl> if ( flags & AVSEEK_FLAG_BYTE ) <nl> - return AVERROR_NOTSUPP ; <nl> + return AVERROR ( ENOSYS ); <nl>  <nl> /* seeks are in milliseconds */ <nl> timestamp = av_rescale ( timestamp , AV_TIME_BASE , 1000 );
static int theora_decode_header ( AVCodecContext * avctx , GetBitContext * gb ) <nl> visible_width = get_bits_long ( gb , 24 ); <nl> visible_height = get_bits_long ( gb , 24 ); <nl>  <nl> - if ( s -> theora >= 0x030200 ) { <nl> - skip_bits ( gb , 8 ); /* offset x */ <nl> - skip_bits ( gb , 8 ); /* offset y */ <nl> - } <nl> + if ( s -> theora >= 0x030200 ) { <nl> + skip_bits ( gb , 8 ); /* offset x */ <nl> + skip_bits ( gb , 8 ); /* offset y */ <nl> + } <nl>  <nl> skip_bits ( gb , 32 ); /* fps numerator */ <nl> skip_bits ( gb , 32 ); /* fps denumerator */
static av_cold int flac_decode_init ( AVCodecContext * avctx ) <nl> FLACContext * s = avctx -> priv_data ; <nl> s -> avctx = avctx ; <nl>  <nl> + avctx -> sample_fmt = SAMPLE_FMT_S16 ; <nl> + <nl> if ( avctx -> extradata_size > 4 ) { <nl> /* initialize based on the demuxer - supplied streamdata header */ <nl> if ( avctx -> extradata_size == FLAC_STREAMINFO_SIZE ) { <nl> static av_cold int flac_decode_init ( AVCodecContext * avctx ) <nl> } <nl> } <nl>  <nl> - avctx -> sample_fmt = SAMPLE_FMT_S16 ; <nl> return 0 ; <nl> } <nl> 
static int rm_read_audio_stream_info ( AVFormatContext * s , AVStream * st , <nl> return - 1 ; <nl> } <nl>  <nl> + if ( sub_packet_size <= 0 ){ <nl> + av_log ( s , AV_LOG_ERROR , " sub_packet_size is invalid \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> if (! strcmp ( buf , " cook ")) st -> codec -> codec_id = CODEC_ID_COOK ; <nl> else if (! strcmp ( buf , " sipr ")) st -> codec -> codec_id = CODEC_ID_SIPR ; <nl> else st -> codec -> codec_id = CODEC_ID_ATRAC3 ;
static int ffm_write_header ( AVFormatContext * s ) <nl> static int ffm_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> { <nl> int64_t dts ; <nl> - uint8_t header [ FRAME_HEADER_SIZE ]; <nl> + uint8_t header [ FRAME_HEADER_SIZE + 4 ]; <nl> int header_size = FRAME_HEADER_SIZE ; <nl>  <nl> dts = s -> timestamp + pkt -> dts ;
int ff_rtmp_packet_read ( URLContext * h , RTMPPacket * p , <nl>  <nl> hdr >>= 6 ; <nl> if ( hdr == RTMP_PS_ONEBYTE ) { <nl> - // todo <nl> - return - 1 ; <nl> + timestamp = prev_pkt [ channel_id ]. timestamp ; <nl> } else { <nl> if ( url_read_complete ( h , buf , 3 ) != 3 ) <nl> return AVERROR ( EIO );
static int xan_unpack ( uint8_t * dest , const int dest_len , <nl> if ( size + size2 > dest_end - dest ) <nl> break ; <nl> } <nl> - if ( src + size > src_end || dest + size + size2 > dest_end ) <nl> + if ( src + size > src_end || dest + size + size2 > dest_end || <nl> + dest - orig_dest + size < back ) <nl> return - 1 ; <nl> bytestream_get_buffer (& src , dest , size ); <nl> dest += size ;
static int poll_frame ( AVFilterLink * link ) <nl> return 1 ; <nl>  <nl> val = avfilter_poll_frame ( link -> src -> inputs [ 0 ]); <nl> + if ( val <= 0 ) <nl> + return val ; <nl>  <nl> if ( val == 1 && ! yadif -> next ) { // FIXME change API to not requre this red tape <nl> if (( ret = avfilter_request_frame ( link -> src -> inputs [ 0 ])) < 0 ) <nl> return ret ; <nl> val = avfilter_poll_frame ( link -> src -> inputs [ 0 ]); <nl> + if ( val <= 0 ) <nl> + return val ; <nl> } <nl> assert ( yadif -> next || ! val ); <nl> 
static void lpc_filter ( const int16_t * lpc_coefs , uint16_t * in , int len ) <nl> new_val = ptr [ 10 ] - sum ; <nl>  <nl> if ( new_val < - 32768 || new_val > 32767 ) { <nl> - memset ( in , 0 , 100 ); <nl> + memset ( in , 0 , 50 * sizeof (* in )); <nl> return ; <nl> } <nl>  <nl> static void do_output_subblock ( RA144Context * ractx , <nl> m [ 2 ] = (( cb2_base [ cb2_idx ] >> 4 ) * gval ) >> 8 ; <nl>  <nl> memmove ( ractx -> adapt_cb , ractx -> adapt_cb + BLOCKSIZE , <nl> - ( BUFFERSIZE - BLOCKSIZE ) * 2 ); <nl> + ( BUFFERSIZE - BLOCKSIZE ) * sizeof (* ractx -> adapt_cb )); <nl>  <nl> block = ractx -> adapt_cb + BUFFERSIZE - BLOCKSIZE ; <nl> 
static void svq3_luma_dc_dequant_idct_c ( DCTELEM * block , int qp ); <nl> static void svq3_add_idct_c ( uint8_t * dst , DCTELEM * block , int stride , int qp , int dc ); <nl> static void filter_mb ( H264Context * h , int mb_x , int mb_y , uint8_t * img_y , uint8_t * img_cb , uint8_t * img_cr , unsigned int linesize , unsigned int uvlinesize ); <nl> static void filter_mb_fast ( H264Context * h , int mb_x , int mb_y , uint8_t * img_y , uint8_t * img_cb , uint8_t * img_cr , unsigned int linesize , unsigned int uvlinesize ); <nl> + static void remove_long_at_index ( H264Context * h , int i ); <nl>  <nl> static av_always_inline uint32_t pack16to32 ( int a , int b ){ <nl> # ifdef WORDS_BIGENDIAN <nl> static void idr ( H264Context * h ){ <nl> for ( i = 0 ; i < 16 ; i ++){ <nl> if ( h -> long_ref [ i ] != NULL ) { <nl> unreference_pic ( h , h -> long_ref [ i ], 0 ); <nl> - h -> long_ref [ i ]= NULL ; <nl> + remove_long_at_index ( h , i ); <nl> } <nl> } <nl> - h -> long_ref_count = 0 ; <nl> + assert ( h -> long_ref_count == 0 ); <nl>  <nl> for ( i = 0 ; i < h -> short_ref_count ; i ++){ <nl> unreference_pic ( h , h -> short_ref [ i ], 0 ); <nl> static Picture * remove_short ( H264Context * h , int frame_num ){ <nl> * @ param i index into h -> long_ref of picture to remove . <nl> */ <nl> static void remove_long_at_index ( H264Context * h , int i ){ <nl> + assert ( h -> long_ref [ i ]-> long_ref == 1 ); <nl> + h -> long_ref [ i ]-> long_ref = 0 ; <nl> h -> long_ref [ i ]= NULL ; <nl> h -> long_ref_count --; <nl> }
int ff_rtmp_packet_write ( URLContext * h , RTMPPacket * pkt , <nl>  <nl> // if channel_id = 0 , this is first presentation of prev_pkt , send full hdr . <nl> if ( prev_pkt [ pkt -> channel_id ]. channel_id && <nl> - pkt -> extra == prev_pkt [ pkt -> channel_id ]. extra ) { <nl> + pkt -> extra == prev_pkt [ pkt -> channel_id ]. extra && <nl> + pkt -> timestamp >= prev_pkt [ pkt -> channel_id ]. timestamp ) { <nl> if ( pkt -> type == prev_pkt [ pkt -> channel_id ]. type && <nl> pkt -> size == prev_pkt [ pkt -> channel_id ]. size ) { <nl> mode = RTMP_PS_FOURBYTES ;
static int mov_read_stsd ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> sc -> stsd_count = entries ; <nl> - sc -> extradata_size = av_mallocz_array ( sc -> stsd_count , sizeof ( sc -> extradata_size )); <nl> + sc -> extradata_size = av_mallocz_array ( sc -> stsd_count , sizeof (* sc -> extradata_size )); <nl> if (! sc -> extradata_size ) <nl> return AVERROR ( ENOMEM ); <nl> 
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> bit_depth = 8 ; <nl> color_type = PNG_COLOR_TYPE_RGB ; <nl> break ; <nl> + case AV_PIX_FMT_GRAY16BE : <nl> + bit_depth = 16 ; <nl> + color_type = PNG_COLOR_TYPE_GRAY ; <nl> + break ; <nl> case AV_PIX_FMT_GRAY8 : <nl> bit_depth = 8 ; <nl> color_type = PNG_COLOR_TYPE_GRAY ; <nl> AVCodec ff_png_encoder = { <nl> . encode2 = encode_frame , <nl> . pix_fmts = ( const enum AVPixelFormat []){ <nl> AV_PIX_FMT_RGB24 , AV_PIX_FMT_RGB32 , AV_PIX_FMT_PAL8 , AV_PIX_FMT_GRAY8 , <nl> + AV_PIX_FMT_GRAY16BE , <nl> AV_PIX_FMT_MONOBLACK , AV_PIX_FMT_NONE <nl> }, <nl> . long_name = NULL_IF_CONFIG_SMALL (" PNG ( Portable Network Graphics ) image "),
static void decode_ac_filter ( WmallDecodeCtx * s ) <nl> s -> acfilter_scaling = get_bits (& s -> gb , 4 ); <nl>  <nl> for ( i = 0 ; i < s -> acfilter_order ; i ++) <nl> - s -> acfilter_coeffs [ i ] = get_bits (& s -> gb , s -> acfilter_scaling ) + 1 ; <nl> + s -> acfilter_coeffs [ i ] = ( s -> acfilter_scaling ? <nl> + get_bits (& s -> gb , s -> acfilter_scaling ) : 0 ) + 1 ; <nl> } <nl>  <nl> static void decode_mclms ( WmallDecodeCtx * s )
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> /* skip B - frames if we don ' t have reference frames */ <nl> if ( s -> last_picture_ptr == NULL && ( s -> pict_type == AV_PICTURE_TYPE_B || s -> droppable )) { <nl> - goto err ; <nl> + goto end ; <nl> } <nl> if (( avctx -> skip_frame >= AVDISCARD_NONREF && s -> pict_type == AV_PICTURE_TYPE_B ) || <nl> ( avctx -> skip_frame >= AVDISCARD_NONKEY && s -> pict_type != AV_PICTURE_TYPE_I ) ||
static int oma_read_header ( AVFormatContext * s , <nl>  <nl> ff_id3v2_read ( s , ID3v2_EA3_MAGIC ); <nl> ret = avio_read ( s -> pb , buf , EA3_HEADER_SIZE ); <nl> + if ( ret < EA3_HEADER_SIZE ) <nl> + return - 1 ; <nl>  <nl> if ( memcmp ( buf , (( const uint8_t []){' E ', ' A ', ' 3 '}), 3 ) || buf [ 4 ] != 0 || buf [ 5 ] != EA3_HEADER_SIZE ) { <nl> av_log ( s , AV_LOG_ERROR , " Couldn ' t find the EA3 header !\ n ");
static int mxf_get_sorted_table_segments ( MXFContext * mxf , int * nb_sorted_segment <nl> if ( mxf -> metadata_sets [ i ]-> type == IndexTableSegment ) <nl> nb_segments ++; <nl>  <nl> + if (! nb_segments ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> * sorted_segments = av_mallocz ( nb_segments * sizeof (** sorted_segments )); <nl> unsorted_segments = av_mallocz ( nb_segments * sizeof (* unsorted_segments )); <nl> if (! sorted_segments || ! unsorted_segments ) {
static int mov_parse_uuid_spherical ( MOVStreamContext * sc , AVIOContext * pb , size_ <nl> goto out ; <nl>  <nl> /* Check for mandatory keys and values , try to support XML as best - effort */ <nl> - if ( av_stristr ( buffer , "< GSpherical : StitchingSoftware >") && <nl> + if (! sc -> spherical && <nl> + av_stristr ( buffer , "< GSpherical : StitchingSoftware >") && <nl> ( val = av_stristr ( buffer , "< GSpherical : Spherical >")) && <nl> av_stristr ( val , " true ") && <nl> ( val = av_stristr ( buffer , "< GSpherical : Stitched >")) && <nl> static int mov_parse_uuid_spherical ( MOVStreamContext * sc , AVIOContext * pb , size_ <nl>  <nl> sc -> spherical -> projection = AV_SPHERICAL_EQUIRECTANGULAR ; <nl>  <nl> - if ( av_stristr ( buffer , "< GSpherical : StereoMode >")) { <nl> + if ( av_stristr ( buffer , "< GSpherical : StereoMode >") && ! sc -> stereo3d ) { <nl> enum AVStereo3DType mode ; <nl>  <nl> if ( av_stristr ( buffer , " left - right "))
static int handle_connection ( HTTPContext * c ) <nl> } <nl> if ( http_send_data ( c ) < 0 ) <nl> return - 1 ; <nl> + /* close connection if trailer sent */ <nl> + if ( c -> state == HTTPSTATE_SEND_DATA_TRAILER ) <nl> + return - 1 ; <nl> break ; <nl> case HTTPSTATE_RECEIVE_DATA : <nl> /* no need to read if no events */
static int lua_websocket_read ( lua_State * L ) <nl> { <nl> apr_socket_t * sock ; <nl> apr_status_t rv ; <nl> + int do_read = 1 ; <nl> int n = 0 ; <nl> apr_size_t len = 1 ; <nl> apr_size_t plen = 0 ; <nl> static int lua_websocket_read ( lua_State * L ) <nl> mask_bytes = apr_pcalloc ( r -> pool , 4 ); <nl> sock = ap_get_conn_socket ( r -> connection ); <nl>  <nl> + while ( do_read ) { <nl> + do_read = 0 ; <nl> /* Get opcode and FIN bit */ <nl> if ( plaintext ) { <nl> rv = apr_socket_recv ( sock , & byte , & len ); <nl> static int lua_websocket_read ( lua_State * L ) <nl> frame [ 0 ] = 0x8A ; <nl> frame [ 1 ] = 0 ; <nl> apr_socket_send ( sock , frame , & plen ); /* Pong ! */ <nl> - lua_websocket_read ( L ); /* read the next frame instead */ <nl> + do_read = 1 ; <nl> } <nl> } <nl> } <nl> + } <nl> return 0 ; <nl> } <nl> 
# include < glib . h > <nl>  <nl> void prof_run ( char * log_level , char * account_name ); <nl> - <nl> - gboolean prof_process_input ( char * inp ); <nl> - <nl> void prof_set_quit ( void ); <nl>  <nl> pthread_mutex_t lock ;
static int sqlcounter_expand ( char * out , int outlen , const char * fmt , void * insta <nl>  <nl> case '%': <nl> * q ++ = * p ; <nl> + break ; <nl> case ' b ': /* last_reset */ <nl> snprintf ( tmpdt , sizeof ( tmpdt ), "% lu ", data -> last_reset ); <nl> strlcpy ( q , tmpdt , freespace );
static int passwd_instantiate ( CONF_SECTION * conf , void ** instance ) <nl> } while (* s ); <nl> if ( keyfield < 0 ) { <nl> radlog ( L_ERR , " rlm_passwd : no field market as key in format : % s ", inst -> format ); <nl> + free ( lf ); <nl> return - 1 ; <nl> } <nl> if (! ( inst -> ht = build_hash_table ( inst -> filename , nfields , keyfield , listable , inst -> hashsize , inst -> ignorenislike , * inst -> delimiter )) ){ <nl> radlog ( L_ERR , " rlm_passwd : can ' t build hashtable from passwd file "); <nl> + free ( lf ); <nl> return - 1 ; <nl> } <nl> if (! ( inst -> pwdfmt = mypasswd_malloc ( inst -> format , nfields , & len )) ){ <nl> radlog ( L_ERR , " rlm_passwd : memory allocation failed "); <nl> release_ht ( inst -> ht ); <nl> + free ( lf ); <nl> return - 1 ; <nl> } <nl> if (! string_to_entry ( inst -> format , nfields , ':', inst -> pwdfmt , len )) { <nl> radlog ( L_ERR , " rlm_passwd : unable to convert format entry "); <nl> release_ht ( inst -> ht ); <nl> + free ( lf ); <nl> return - 1 ; <nl> } <nl> 
void * request_data_get ( REQUEST * request , <nl> { <nl> request_data_t ** last ; <nl>  <nl> + if (! request ) return NULL ; <nl> + <nl> for ( last = &( request -> data ); * last != NULL ; last = &((* last )-> next )) { <nl> if (((* last )-> unique_ptr == unique_ptr ) && <nl> ((* last )-> unique_int == unique_int )) {
autz_redo : <nl>  <nl> tmp = pairfind ( request -> config_items , PW_SESSION_TYPE , 0 ); <nl> if ( tmp ) { <nl> - RDEBUG2 (" Using Session - Type % s ", tmp -> vp_strvalue ); <nl> session_type = tmp -> vp_integer ; <nl> + RDEBUG2 (" Using Session - Type % s ", <nl> + dict_valnamebyattr ( PW_SESSION_TYPE , 0 , session_type )); <nl> } <nl>  <nl> /*
static int WalkNodePostOrder ( rbnode_t * X , <nl> int rbtree_walk ( rbtree_t * tree , RBTREE_ORDER order , <nl> int (* callback )( void *, void *), void * context ) <nl> { <nl> + if ( tree -> Root == NIL ) return 0 ; <nl> + <nl> switch ( order ) { <nl> case PreOrder : <nl> return WalkNodePreOrder ( tree -> Root , callback , context );
static uint8_t * rad_coalesce ( int attribute , size_t length , uint8_t * data , <nl> static VALUE_PAIR * rad_continuation2vp ( const RADIUS_PACKET * packet , <nl> const RADIUS_PACKET * original , <nl> const char * secret , int attribute , <nl> - int length , <nl> + int length , /* CANNOT be zero */ <nl> uint8_t * data , size_t packet_length , <nl> int flag , DICT_ATTR * da ) <nl> { <nl> int rad_decode ( RADIUS_PACKET * packet , RADIUS_PACKET * original , <nl> if ( vendorlen == 0 ) vendorcode = 0 ; <nl> packet_length -= ( vsa_tlen + vsa_llen + vsa_offset ); <nl>  <nl> + /* <nl> + * Ignore VSAs that have no data . <nl> + */ <nl> + if ( attrlen == 0 ) goto next ; <nl> + <nl> /* <nl> * WiMAX attributes of type 0 are ignored . They <nl> * are a secret flag to us that the attribute has
int received_request ( rad_listen_t * listener , <nl> request -> number = request_num_counter ++; <nl> request -> priority = listener -> type ; <nl>  <nl> + /* <nl> + * Status - Server packets go to the head of the queue . <nl> + */ <nl> + if ( request -> packet -> code == PW_STATUS_SERVER ) request -> priority = 0 ; <nl> + <nl> /* <nl> * Set virtual server identity <nl> */
to_do_again : <nl> } <nl> else { <nl> attr_vp = pairmake ( data -> attribute , replace_STR , 0 ); <nl> + if ( attr_vp == NULL ){ <nl> + DEBUG2 (" rlm_attr_rewrite : Could not add new attribute % s with value '% s '", <nl> + data -> attribute , replace_STR ); <nl> + return ret ; <nl> + } <nl> switch ( data -> searchin ){ <nl> case RLM_REGEX_INPACKET : <nl> pairadd (& request -> packet -> vps , attr_vp );
file_transfer_t * imcb_file_send_start ( struct im_connection * ic , char * handle , ch <nl> bee_t * bee = ic -> bee ; <nl> bee_user_t * bu = bee_user_by_handle ( bee , ic , handle ); <nl>  <nl> - if ( bee -> ui -> ft_in_start ) { <nl> + if ( bee -> ui -> ft_in_start && bu ) { <nl> return bee -> ui -> ft_in_start ( bee , bu , file_name , file_size ); <nl> } else { <nl> return NULL ;
dissect_smpp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> guint32 offset = 0 ; <nl> while ( tvb_reported_length_remaining ( tvb , offset ) > 0 ) { <nl> guint16 pdu_len = tvb_get_ntohl ( tvb , offset ); <nl> + if ( pdu_len < 1 ) <nl> + THROW ( ReportedBoundsError ); <nl> gint pdu_real_len = tvb_length_remaining ( tvb , offset ); <nl> tvbuff_t * pdu_tvb ; <nl> 
dissect_smb_command ( tvbuff_t * tvb , packet_info * pinfo , int offset , proto_tree * s <nl> smb_dissector [ cmd ]. request : smb_dissector [ cmd ]. response ; <nl>  <nl> offset = (* dissector )( tvb , pinfo , cmd_tree , offset , smb_tree ); <nl> + <nl> + if (! tvb_offset_exists ( tvb , offset - 1 )) { <nl> + THROW ( ReportedBoundsError ); <nl> + } <nl> proto_item_set_end ( cmd_item , tvb , offset ); <nl> } <nl> return offset ;
* Copyright 2011 , Grzegorz Szczytowski < grzegorz . szczytowski @ gmail . com > <nl> * <nl> * Updates and corrections : <nl> - * Copyright 2011 - 2012 , Anders Broman < anders . broman @ ericsson . com > <nl> + * Copyright 2011 - 2013 , Anders Broman < anders . broman @ ericsson . com > <nl> * <nl> * PDCP PDU number extension header support added by Martin Isaksson < martin . isaksson @ ericsson . com > <nl> * <nl> decode_gtp_priv_ext ( tvbuff_t * tvb , int offset , packet_info * pinfo _U_ , proto_t <nl> offset = offset + 2 ; <nl>  <nl> if ( length > 2 ) { <nl> - next_tvb = tvb_new_subset_remaining ( tvb , offset ); <nl> + next_tvb = tvb_new_subset ( tvb , offset , length - 2 , length - 2 ); <nl> if (! dissector_try_uint ( gtp_priv_ext_dissector_table , ext_id , next_tvb , pinfo , ext_tree_priv_ext )){ <nl> proto_tree_add_item ( ext_tree_priv_ext , hf_gtp_ext_val , tvb , offset , length - 2 , ENC_NA ); <nl> }
capture_all_filter_check_syntax_cb ( GtkWidget * w _U_ , gpointer user_data _U_ ) <nl> } <nl> # ifdef HAVE_EXTCAP <nl> /* Can ' t verify extcap capture filters */ <nl> - if ( device . if_info . extcap != NULL ) <nl> + if ( device . if_info . extcap != NULL && strlen ( device . if_info . extcap ) > 0 ) <nl> continue ; <nl> # endif <nl> filter_text = gtk_combo_box_text_get_active_text ( GTK_COMBO_BOX_TEXT ( filter_cm ));
csv_handle ( GtkTreeModel * model , GtkTreePath * path _U_ , GtkTreeIter * iter , <nl> i == PERCENT_COLUMN || i == PROTECTION_COLUMN ) { <nl> gtk_tree_model_get ( model , iter , i , & table_text , - 1 ); <nl> g_string_append ( CSV_str , table_text ); <nl> + g_free ( table_text ); <nl> } else { <nl> gtk_tree_model_get ( model , iter , i , & table_value , - 1 ); <nl> g_string_append_printf ( CSV_str , "% u ", table_value );
gboolean win32_save_as_statstree ( HWND h_wnd , GString * file_name , int * file_type ) <nl> return gsfn_ok ; <nl> } <nl>  <nl> - <nl> + <nl> gboolean <nl> win32_export_specified_packets_file ( HWND h_wnd , capture_file * cf , <nl> GString * file_name , <nl> win32_export_sslkeys_file ( HWND h_wnd ) { <nl> OPENFILENAME * ofn ; <nl> TCHAR file_name [ MAX_PATH ] = _T (""); <nl> char * dirname ; <nl> - gchar * keylist ; <nl> + gchar * keylist = NULL ; <nl> char * file_name8 ; <nl> int fd ; <nl> int ofnsize ;
/* pppdump . c <nl> * <nl> - * $ Id : pppdump . c , v 1 . 5 2000 / 11 / 19 03 : 47 : 36 guy Exp $ <nl> + * $ Id : pppdump . c , v 1 . 6 2000 / 11 / 19 20 : 56 : 17 gerald Exp $ <nl> * <nl> * Copyright ( c ) 2000 by Gilbert Ramirez < gram @ xiexie . org > <nl> * <nl> process_data ( pppdump_t * state , FILE_T fh , pkt_t * pkt , int n , guint8 * pd , int * er <nl> return 0 ; <nl> } <nl>  <nl> + if ( num_written > sizeof ( pd )) { <nl> + * err = WTAP_ERR_UNC_OVERFLOW ; <nl> + return - 1 ; <nl> + } <nl> + <nl> memcpy ( pd , pkt -> buf , num_written ); <nl>  <nl> num_bytes --;
parse_netscreen_packet ( FILE_T fh , struct wtap_pkthdr * phdr , Buffer * buf , <nl>  <nl> phdr -> rec_type = REC_TYPE_PACKET ; <nl> phdr -> presence_flags = WTAP_HAS_TS | WTAP_HAS_CAP_LEN ; <nl> + /* Suppress compiler warnings */ <nl> + memset ( cap_int , 0 , sizeof ( cap_int )); <nl> + memset ( cap_dst , 0 , sizeof ( cap_dst )); <nl>  <nl> if ( sscanf ( line , "% 9d .% 9d : % 15 [ a - z0 - 9 /:.-](% 1 [ io ]) len =% 9d :% 12s ->% 12s /", <nl> & sec , & dsec , cap_int , direction , & pkt_len , cap_src , cap_dst ) < 5 ) {
PRIVATE void tplt_xfer ( const char * name , FILE * in , FILE * out , int * lineno ) <nl> PRIVATE FILE * tplt_open ( struct lemon * lemp ) <nl> { <nl> static char templatename [] = " lempar . c "; <nl> - char * buf ; <nl> FILE * in ; <nl> char * tpltname = NULL ; <nl> char * cp ; <nl>  <nl> if ( lemp -> templatename ) { <nl> tpltname = strdup ( lemp -> templatename ); <nl> - } <nl> - else { <nl> + } else { <nl> + char * buf ; <nl> + <nl> cp = strrchr ( lemp -> filename ,'.'); <nl> buf = malloc ( 1000 ); <nl> if ( cp ){ <nl> PRIVATE FILE * tplt_open ( struct lemon * lemp ) <nl> sprintf ( buf ,"% s . lt ", lemp -> filename ); <nl> } <nl> if ( access ( buf , 004 )== 0 ){ <nl> - tpltname = buf ; <nl> + tpltname = strdup ( buf ); <nl> } else if ( access ( templatename , 004 )== 0 ){ <nl> tpltname = strdup ( templatename ); <nl> } else { <nl> tpltname = pathsearch ( lemp -> argv0 , templatename , 0 ); <nl> - free ( buf ); <nl> } <nl> + free ( buf ); <nl> } <nl> if ( tpltname == 0 ){ <nl> fprintf ( stderr ," Can ' t find the parser driver template file \"% s \".\ n ",
struct _rtp_conversation_info <nl> { <nl> gchar method [ MAX_RTP_SETUP_METHOD_SIZE + 1 ]; <nl> guint32 frame_number ; <nl> - guint32 rtp_event_pt ; /* this is payload type for dynamic RTP events ( RFC2833 ) */ <nl> + GHashTable * rtp_dyn_payload ; /* a hash table with the dynamic RTP payload */ <nl> }; <nl>  <nl> /* Add an RTP conversation with the given details */ <nl> void rtp_add_address ( packet_info * pinfo , <nl> int other_port , <nl> gchar * setup_method , <nl> guint32 setup_frame_number , <nl> - int rtp_event_pt ); <nl> + GHashTable * rtp_dyn_payload ); <nl> + <nl> +/* Free and destroy the dyn_payload hash table */ <nl> + void rtp_free_hash_dyn_payload ( GHashTable * rtp_dyn_payload ); <nl> +
dissect_gtpv2_mm_context_utms_qq ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tr <nl> proto_tree_add_item ( tree , hf_gtpv2_ik , tvb , offset , 16 , ENC_NA ); <nl> offset += 16 ; <nl>  <nl> - if ( nr_qua ) <nl> - { <nl> - offset = dissect_gtpv2_authentication_quadruplets ( tvb , tree , offset , nr_qui ); <nl> + if ( nr_qua ) { <nl> + offset = dissect_gtpv2_authentication_quadruplets ( tvb , tree , offset , nr_qua ); <nl> } <nl>  <nl> if ( nr_qui ) {
save_decode_as_entries ( gchar ** err ) <nl>  <nl> dissector_all_tables_foreach_changed ( decode_as_write_entry , da_file ); <nl> fclose ( da_file ); <nl> + g_free ( daf_path ); <nl> return 0 ; <nl> } <nl> 
create_conv_and_add_proto_data ( packet_info * pinfo , guint64 service_id , <nl> conversation_add_proto_data ( conv , proto_infiniband , proto_data ); <nl>  <nl> /* next , register the conversation using the LIDs */ <nl> - set_address ( addr , AT_IB , sizeof ( guint16 ), & lid ); <nl> + set_address ( addr , AT_IB , sizeof ( guint16 ), wmem_memdup ( pinfo -> pool , & lid , sizeof lid )); <nl> conv = conversation_new ( pinfo -> num , addr , addr , <nl> PT_IBQP , port , port , options ); <nl> conversation_add_proto_data ( conv , proto_infiniband , proto_data );
* Routines for megaco packet disassembly <nl> * RFC 3015 <nl> * <nl> -* $ Id : packet - megaco . c , v 1 . 15 2004 / 04 / 21 19 : 58 : 14 etxrab Exp $ <nl> +* $ Id : packet - megaco . c , v 1 . 16 2004 / 04 / 23 03 : 20 : 58 guy Exp $ <nl> * <nl> * Christian Falckenberg , 2002 / 10 / 17 <nl> * Copyright ( c ) 2002 by Christian Falckenberg <nl> dissect_megaco_text ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> if ( tree ) <nl> len = tvb_len - tvb_previous_offset ; <nl> proto_tree_add_text ( megaco_tree , tvb , tvb_previous_offset , - 1 , <nl> - "% s ", tvb_format_text ( tvb , tvb_previous_offset , len ), tvb_len , <nl> - tvb_previous_offset ); <nl> + "% s ", tvb_format_text ( tvb , tvb_previous_offset , len )); <nl> if ( global_megaco_raw_text ){ <nl> tvb_raw_text_add ( tvb , megaco_tree ); <nl> }
ng_file_seek_rand ( wtap * wth , long offset , int whence , int * err ) <nl> the uncompressed byte stream , starting with the blob <nl> following the current blob . */ <nl> new = g_list_next ( ngsniffer -> current_blob ); <nl> - for (;;) { <nl> + while ( new ) { <nl> next = g_list_next ( new ); <nl> if ( next == NULL ) { <nl> /* No more blobs ; the current one is it . */ <nl> ng_file_seek_rand ( wtap * wth , long offset , int whence , int * err ) <nl> the uncompressed byte stream , starting with the blob <nl> preceding the current blob . */ <nl> new = g_list_previous ( ngsniffer -> current_blob ); <nl> - for (;;) { <nl> + while ( new ) { <nl> /* Does this blob start at or before the target offset ? <nl> If so , the current blob is the one we want . */ <nl> new_blob = new -> data ;
epan_init ( void (* register_all_protocols_func )( register_cb cb , gpointer client_da <nl> register_cb cb , <nl> gpointer client_data ) <nl> { <nl> - gboolean status = TRUE ; <nl> + volatile gboolean status = TRUE ; <nl>  <nl> /* initialize memory allocation subsystem */ <nl> wmem_init ();
fill_list ( GtkWidget * main_w ) <nl> * and use it later without any crashes . This may not be a <nl> * valid assumption . <nl> */ <nl> + g_free ( l_select ); <nl> l_select = ( GtkTreeIter *) g_memdup (& iter , sizeof ( iter )); <nl> } <nl> fl_entry = g_list_next ( fl_entry );
static const value_string bssap_speech_codec_values [] = { <nl> static guint8 <nl> be_speech_codec_lst ( tvbuff_t * tvb , proto_tree * tree , guint32 offset , guint len _U_ , gchar * add_string _U_ , int string_len _U_ ) <nl> { <nl> - guint32 curr_offset , consumed ; <nl> + guint32 curr_offset , consumed = 0 ; <nl> guint8 codec ; <nl> guint8 number = 0 ; <nl> proto_item * item = NULL ;
static int decode_bgp_link_nlri_prefix_descriptors ( tvbuff_t * tvb , <nl> break ; <nl>  <nl> case BGP_NLRI_TLV_IP_REACHABILITY_INFORMATION : <nl> - decode_prefix4 ( tlv_sub_tree , pinfo , tlv_sub_item , hf_bgp_ls_nlri_ip_reachability_prefix_ip , <nl> - tvb , offset + 4 , 0 , " Reachability "); <nl> + if ( decode_prefix4 ( tlv_sub_tree , pinfo , tlv_sub_item , hf_bgp_ls_nlri_ip_reachability_prefix_ip , <nl> + tvb , offset + 4 , 0 , " Reachability ") == - 1 ) <nl> + return diss_length ; <nl> break ; <nl> } <nl> 
create_main_window ( gint pl_size , gint tv_size , gint bv_size , e_prefs * prefs ) <nl> channel_list = g_list_append ( channel_list , ieee80211_mhz_to_str ( airpcap_if_active -> pSupportedChannels [ i ]. Frequency )); <nl> } <nl> gtk_combo_set_popdown_strings ( GTK_COMBO ( channel_cm ), channel_list ); <nl> + g_list_free ( channel_list ); <nl> } <nl>  <nl> gtk_tooltips_set_tip ( airpcap_tooltips , GTK_WIDGET ( GTK_COMBO ( channel_cm )-> entry ), <nl> create_main_window ( gint pl_size , gint tv_size , gint bv_size , e_prefs * prefs ) <nl> linktype_list = g_list_append ( linktype_list , AIRPCAP_VALIDATION_TYPE_NAME_CORRUPT ); <nl>  <nl> gtk_combo_set_popdown_strings ( GTK_COMBO ( wrong_crc_cm ), linktype_list ) ; <nl> + g_list_free ( linktype_list ); <nl> gtk_tooltips_set_tip ( airpcap_tooltips , GTK_WIDGET ( GTK_COMBO ( wrong_crc_cm )-> entry ), <nl> " Select the 802 . 11 FCS filter that the wireless adapter will apply .", <nl> NULL );
dissect_mle_decrypt ( tvbuff_t * tvb , <nl>  <nl> DISSECTOR_ASSERT ( pinfo -> src . len == 16 ); <nl> DISSECTOR_ASSERT ( pinfo -> dst . len == 16 ); <nl> - memcpy ( d_a , ( guint8 *) pinfo -> src . data , pinfo -> src . len ); <nl> - memcpy ( d_a + 16 , ( guint8 *) pinfo -> dst . data , pinfo -> dst . len ); <nl> + memcpy ( d_a , ( const guint8 *) pinfo -> src . data , pinfo -> src . len ); <nl> + memcpy ( d_a + 16 , ( const guint8 *) pinfo -> dst . data , pinfo -> dst . len ); <nl>  <nl> tvb_memcpy ( tvb , d_a + 32 , payload_info -> aux_offset , payload_info -> aux_length ); <nl> l_a = 32 + payload_info -> aux_length ;
void proto_register_pppoed ( void ) <nl> } <nl> }, <nl> { & hf_pppoed_tag_host_uniq , <nl> - { " Host - Uniq ", " pppoed . tags . host_uniq ", FT_STRING , BASE_NONE , <nl> + { " Host - Uniq ", " pppoed . tags . host_uniq ", FT_BYTES , BASE_NONE , <nl> NULL , 0x0 , "", HFILL <nl> } <nl> }, <nl> { & hf_pppoed_tag_ac_cookie , <nl> - { " AC - Cookie ", " pppoed . tags . ac_cookie ", FT_BYTES , BASE_HEX , <nl> + { " AC - Cookie ", " pppoed . tags . ac_cookie ", FT_BYTES , BASE_NONE , <nl> NULL , 0x0 , "", HFILL <nl> } <nl> }, <nl> void proto_register_pppoed ( void ) <nl> } <nl> }, <nl> { & hf_pppoed_tag_relay_session_id , <nl> - { " Relay - Session - Id ", " pppoed . tags . relay_session_id ", FT_BYTES , BASE_HEX , <nl> + { " Relay - Session - Id ", " pppoed . tags . relay_session_id ", FT_BYTES , BASE_NONE , <nl> NULL , 0x0 , "", HFILL <nl> } <nl> },
static nghttp2_hd_entry * add_hd_table_incremental ( nghttp2_hd_context * context , <nl>  <nl> if ( rv != 0 ) { <nl> -- new_ent -> ref ; <nl> + <nl> + /* nv -> name and nv -> value are managed by caller . */ <nl> + new_ent -> nv . name = NULL ; <nl> + new_ent -> nv . namelen = 0 ; <nl> + new_ent -> nv . value = NULL ; <nl> + new_ent -> nv . valuelen = 0 ; <nl> + <nl> nghttp2_hd_entry_free ( new_ent ); <nl> free ( new_ent ); <nl> 
parse_toshiba_packet ( FILE_T fh , struct wtap_pkthdr * phdr , Buffer * buf , <nl> union wtap_pseudo_header * pseudo_header = & phdr -> pseudo_header ; <nl> char line [ TOSHIBA_LINE_LENGTH ]; <nl> int num_items_scanned ; <nl> - guint pkt_len ; <nl> - int pktnum , hr , min , sec , csec ; <nl> + int pkt_len , pktnum , hr , min , sec , csec ; <nl> char channel [ 10 ], direction [ 10 ]; <nl> int i , hex_lines ; <nl> guint8 * pd ; <nl> parse_toshiba_packet ( FILE_T fh , struct wtap_pkthdr * phdr , Buffer * buf , <nl>  <nl> } while ( strcmp ( line , " OFFSET 0001 - 0203 ") != 0 ); <nl>  <nl> - num_items_scanned = sscanf ( line + 64 , " LEN =% 9u ", & pkt_len ); <nl> + num_items_scanned = sscanf ( line + 64 , " LEN =% 9d ", & pkt_len ); <nl> if ( num_items_scanned != 1 ) { <nl> * err = WTAP_ERR_BAD_FILE ; <nl> * err_info = g_strdup (" toshiba : OFFSET line doesn ' t have valid LEN item "); <nl> return FALSE ; <nl> } <nl> + if ( pkt_len < 0 ) { <nl> + * err = WTAP_ERR_BAD_FILE ; <nl> + * err_info = g_strdup (" toshiba : packet header has a negative packet length "); <nl> + return FALSE ; <nl> + } <nl> if ( pkt_len > WTAP_MAX_PACKET_SIZE ) { <nl> /* <nl> * Probably a corrupt capture file ; don ' t blow up trying
dissect_6lowpan_iphc_nhc ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , gi <nl> /* Get and display the checksum . */ <nl> if (!( udp_flags & LOWPAN_NHC_UDP_CHECKSUM )) { <nl> /* Parse the checksum . */ <nl> - udp . checksum = tvb_get_ntohs ( tvb , offset ); <nl> + tvb_memcpy ( tvb , & udp . checksum , offset , sizeof ( udp . checksum )); <nl> proto_tree_add_checksum ( tree , tvb , offset , hf_6lowpan_udp_checksum , - 1 , NULL , pinfo , 0 , ENC_BIG_ENDIAN , PROTO_CHECKSUM_NO_FLAGS ); <nl> offset += 2 ; <nl> }
proto_register_xot ( void ) <nl> proto_xot = proto_register_protocol (" X . 25 over TCP ", " XOT ", " xot "); <nl> proto_register_field_array ( proto_xot , hf , array_length ( hf )); <nl> proto_register_subtree_array ( ett , array_length ( ett )); <nl> + register_dissector (" xot ", dissect_xot , proto_xot ); <nl>  <nl> xot_module = prefs_register_protocol ( proto_xot , NULL ); <nl> prefs_register_bool_preference ( xot_module , " desegment ",
ssl_find_private_key ( SslDecryptSession * ssl_session , GHashTable * key_hash , GTree <nl> ssl_debug_printf (" ssl_find_private_key server % s :% u \ n ", <nl> ep_address_to_str (& dummy . addr ), dummy . port ); <nl>  <nl> + if ( g_hash_table_size ( key_hash ) == 0 ) { <nl> + ssl_debug_printf (" ssl_find_private_key : no keys found \ n "); <nl> + return ; <nl> + } else { <nl> + ssl_debug_printf (" ssl_find_private_key : testing % i keys \ n ", <nl> + g_hash_table_size ( key_hash )); <nl> + } <nl> + <nl> /* try to retrieve private key for this service . Do it now ' cause pinfo <nl> * is not always available <nl> * Note that with HAVE_LIBGNUTLS undefined private_key is allways 0
dissect_ExpectedSubmoduleBlockReq_block ( tvbuff_t * tvb , int offset , <nl> /* Initial */ <nl> io_data_object = wmem_new0 ( wmem_file_scope (), ioDataObject ); <nl> io_data_object -> profisafeSupported = FALSE ; <nl> - io_data_object -> moduleNameStr = wmem_strdup ( wmem_file_scope (), " Unknown "); <nl> + io_data_object -> moduleNameStr = ( gchar *) wmem_alloc ( wmem_file_scope (), MAX_NAMELENGTH ); <nl> + g_strlcpy ( io_data_object -> moduleNameStr , " Unknown ", MAX_NAMELENGTH ); <nl> vendorMatch = FALSE ; <nl> deviceMatch = FALSE ; <nl> gsdmlFoundFlag = FALSE ; <nl> dissect_ExpectedSubmoduleBlockReq_block ( tvbuff_t * tvb , int offset , <nl> /* Find a String with the saved TextID and with a fitting value for it in the same line . This value is the name of the Module ! */ <nl> if ((( strstr ( temp , tmp_moduletext )) != NULL ) && (( strstr ( temp , moduleValueInfo )) != NULL )) { <nl> pch = strstr ( temp , moduleValueInfo ); <nl> - if ( pch != NULL && sscanf ( pch , " Value =\"%[^\"]", io_data_object -> moduleNameStr ) == 1 ) <nl> + if ( pch != NULL && sscanf ( pch , " Value =\"% 199 [^\"]", io_data_object -> moduleNameStr ) == 1 ) <nl> break ; /* Found the name of the module */ <nl> } <nl> }
capture_loop_init_output ( capture_options * capture_opts , loop_data * ld , char * err <nl> - 1 , /* section_length */ <nl> & ld -> bytes_written , <nl> & err ); <nl> + g_string_free ( cpu_info_str , TRUE ); <nl> g_free ( appname ); <nl>  <nl> for ( i = 0 ; successful && ( i < capture_opts -> ifaces -> len ); i ++) { <nl> do_file_switch_or_stop ( capture_options * capture_opts , <nl> - 1 , /* section_length */ <nl> &( global_ld . bytes_written ), <nl> & global_ld . err ); <nl> + g_string_free ( cpu_info_str , TRUE ); <nl> g_free ( appname ); <nl>  <nl> for ( i = 0 ; successful && ( i < capture_opts -> ifaces -> len ); i ++) {
register_all_protocols ( register_cb cb , gpointer cb_data ) <nl> } <nl> g_thread_join ( rapw_thread ); <nl> if ( cb && ! called_back ) { <nl> - cb ( RA_REGISTER , " Registration finished ", cb_data ); <nl> + cb ( RA_REGISTER , " finished ", cb_data ); <nl> } <nl> } <nl>  <nl> register_all_protocol_handoffs ( register_cb cb , gpointer cb_data ) <nl> } <nl> g_thread_join ( raphw_thread ); <nl> if ( cb && ! called_back ) { <nl> - cb ( RA_HANDOFF , " Registration finished ", cb_data ); <nl> + cb ( RA_HANDOFF , " finished ", cb_data ); <nl> } <nl> g_async_queue_unref ( register_cb_done_q ); <nl> 
proto_register_llcgprs ( void ) <nl> void <nl> proto_reg_handoff_llcgprs ( void ) <nl> { <nl> - dissector_handle_t llcgprs_handle ; <nl> - <nl> - llcgprs_handle = create_dissector_handle ( dissect_llcgprs , <nl> - proto_llcgprs ); <nl> -/* dissector_add (" PARENT_SUBFIELD ", ID_VALUE , llcgprs_handle ); <nl> -*/ <nl> data_handle = find_dissector (" data "); <nl> }
dissect_quic_common ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl>  <nl> /* Diversification Nonce */ <nl> if ( puflags & PUFLAGS_DNONCE && quic_info -> version >= 33 ){ <nl> - proto_tree_add_item ( quic_tree , hf_quic_diversification_nonce , tvb , offset , 32 , ENC_NA ); <nl> - offset += 32 ; <nl> + if ( pinfo -> srcport == 443 ){ /* Diversification nonce is only present from server to client */ <nl> + proto_tree_add_item ( quic_tree , hf_quic_diversification_nonce , tvb , offset , 32 , ENC_NA ); <nl> + offset += 32 ; <nl> + } <nl> } <nl>  <nl> - <nl> /* Packet Number */ <nl>  <nl> /* Get len of packet number ( and packet number ), may be a more easy function to get the length ... */
bool InterfaceTreeWidgetItem :: operator < ( const QTreeWidgetItem & other ) const { <nl> # include < QComboBox > <nl>  <nl> InterfaceTreeDelegate :: InterfaceTreeDelegate ( QObject * parent ) <nl> - : QStyledItemDelegate ( parent ) <nl> + : QStyledItemDelegate ( parent ), tree_ ( NULL ) <nl> { <nl> } <nl> 
char * alloca (); <nl> # else <nl> # define ALLOCA_FLAG ( name ) <nl> # define SET_ALLOCA_FLAG ( name ) <nl> -# define do_alloca ( p , use_heap ) emalloc ( p ) <nl> -# define free_alloca ( p , use_heap ) efree ( p ) <nl> +# define do_alloca ( p , use_heap ) emalloc ( p ), use_heap = 1 <nl> +# define free_alloca ( p , use_heap ) efree ( p ), use_heap = 1 <nl> # endif <nl>  <nl> # if ZEND_DEBUG
static void exif_iif_add_value ( image_info_type * image_info , int section_index , c <nl> if (! length ) <nl> break ; <nl> case TAG_FMT_UNDEFINED : <nl> - if ( tag == TAG_MAKER_NOTE ) { <nl> - length = MIN ( length , strlen ( value )); <nl> - } <nl> - <nl> if ( value ) { <nl> + if ( tag == TAG_MAKER_NOTE ) { <nl> + length = MIN ( length , strlen ( value )); <nl> + } <nl> + <nl> /* do not recompute length here */ <nl> info_value -> s = estrndup ( value , length ); <nl> info_data -> length = length ;
void phpdbg_clean ( zend_bool full TSRMLS_DC ) /* {{{ */ <nl> } <nl>  <nl> if ( full ) { <nl> - phpdbg_exec = strdup ( PHPDBG_G ( exec )); /* preserve exec , don ' t reparse that from cmd */ <nl> + if ( PHPDBG_G ( exec )) { <nl> + phpdbg_exec = strdup ( PHPDBG_G ( exec )); /* preserve exec , don ' t reparse that from cmd */ <nl> + } <nl> + <nl> PHPDBG_G ( flags ) |= PHPDBG_IS_CLEANING ; <nl>  <nl> zend_bailout ();
ZEND_METHOD ( Generator , __wakeup ) <nl> static void zend_generator_iterator_dtor ( zend_object_iterator * iterator TSRMLS_DC ) /* {{{ */ <nl> { <nl> zend_generator * generator = ( zend_generator *) Z_OBJ ( iterator -> data ); <nl> - zval_ptr_dtor (& iterator -> data ); <nl> generator -> iterator = NULL ; <nl> + zval_ptr_dtor (& iterator -> data ); <nl> + zend_iterator_dtor ( iterator TSRMLS_CC ); <nl> } <nl> /* }}} */ <nl> 
int make_http_soap_request ( zval * this_ptr , <nl> smart_str_append_const (& soap_headers , "\", opaque =\""); <nl> smart_str_appendl (& soap_headers , Z_STRVAL_PP ( tmp ), Z_STRLEN_PP ( tmp )); <nl> } <nl> + if ( zend_hash_find ( Z_ARRVAL_PP ( digest ), " algorithm ", sizeof (" algorithm "), ( void **)& tmp ) == SUCCESS && <nl> + Z_TYPE_PP ( tmp ) == IS_STRING ) { <nl> + smart_str_append_const (& soap_headers , "\", algorithm =\""); <nl> + smart_str_appendl (& soap_headers , Z_STRVAL_PP ( tmp ), Z_STRLEN_PP ( tmp )); <nl> + } <nl> smart_str_append_const (& soap_headers , "\"\ r \ n "); <nl> } <nl> } else {
PHPAPI size_t _php_stream_read ( php_stream * stream , char * buf , size_t size TSRMLS <nl> /* EOF , or temporary end of data ( for non - blocking mode ). */ <nl> break ; <nl> } <nl> + <nl> + /* just break anyway , to avoid greedy read */ <nl> + break ; <nl> } <nl>  <nl> if ( didread > 0 ) {
PHP_METHOD ( Phar , copy ) <nl> } <nl> } <nl>  <nl> - if ( phar_path_check (& newfile , & newfile_len , & pcr_error ) > pcr_is_ok ) { <nl> + if ( phar_path_check (& newfile , ( int *) & newfile_len , & pcr_error ) > pcr_is_ok ) { <nl> zend_throw_exception_ex ( spl_ce_UnexpectedValueException , 0 TSRMLS_CC , <nl> " file \"% s \" contains invalid characters % s , cannot be copied from \"% s \" in phar % s ", newfile , pcr_error , oldfile , phar_obj -> archive -> fname ); <nl> RETURN_FALSE ;
CWD_API char * tsrm_realpath ( const char * path , char * real_path TSRMLS_DC ) /* {{{ <nl> /* realpath ("") returns CWD */ <nl> if (!* path ) { <nl> new_state . cwd = ( char *) malloc ( 1 ); <nl> + if ( new_state . cwd == NULL ) { <nl> + return NULL ; <nl> + } <nl> new_state . cwd [ 0 ] = '\ 0 '; <nl> new_state . cwd_length = 0 ; <nl> if ( VCWD_GETCWD ( cwd , MAXPATHLEN )) { <nl> CWD_API char * tsrm_realpath ( const char * path , char * real_path TSRMLS_DC ) /* {{{ <nl> new_state . cwd_length = strlen ( cwd ); <nl> } else { <nl> new_state . cwd = ( char *) malloc ( 1 ); <nl> + if ( new_state . cwd == NULL ) { <nl> + return NULL ; <nl> + } <nl> new_state . cwd [ 0 ] = '\ 0 '; <nl> new_state . cwd_length = 0 ; <nl> }
static void _php_mb_regex_ereg_replace_exec ( INTERNAL_FUNCTION_PARAMETERS , OnigOp <nl> ! ZVAL_IS_UNDEF (& retval )) { <nl> convert_to_string_ex (& retval ); <nl> smart_str_appendl (& out_buf , Z_STRVAL ( retval ), Z_STRLEN ( retval )); <nl> - eval_buf . s -> len = 0 ; <nl> + if ( eval_buf . s ) { <nl> + eval_buf . s -> len = 0 ; <nl> + } <nl> zval_ptr_dtor (& retval ); <nl> } else { <nl> efree ( description );
PHPAPI int php_check_specific_open_basedir ( const char * basedir , const char * path <nl> if (( expand_filepath ( path , resolved_name TSRMLS_CC ) != NULL ) && ( expand_filepath ( local_open_basedir , resolved_basedir TSRMLS_CC ) != NULL )) { <nl> /* Handler for basedirs that end with a / */ <nl> resolved_basedir_len = strlen ( resolved_basedir ); <nl> - if ( basedir [ strlen ( basedir )- 1 ] == PHP_DIR_SEPARATOR && resolved_basedir [ resolved_basedir_len - 1 ] != PHP_DIR_SEPARATOR ) { <nl> - resolved_basedir [ resolved_basedir_len ] = '/'; <nl> + if ( resolved_basedir [ resolved_basedir_len - 1 ] != PHP_DIR_SEPARATOR ) { <nl> + resolved_basedir [ resolved_basedir_len ] = PHP_DIR_SEPARATOR ; <nl> resolved_basedir [++ resolved_basedir_len ] = '\ 0 '; <nl> } <nl>  <nl> if ( path [ strlen ( path )- 1 ] == PHP_DIR_SEPARATOR ) { <nl> resolved_name_len = strlen ( resolved_name ); <nl> if ( resolved_name [ resolved_name_len - 1 ] != PHP_DIR_SEPARATOR ) { <nl> - resolved_name [ resolved_name_len ] = '/'; <nl> + resolved_name [ resolved_name_len ] = PHP_DIR_SEPARATOR ; <nl> resolved_name [++ resolved_name_len ] = '\ 0 '; <nl> } <nl> }
onig_snprintf_with_pattern ( buf , bufsize , enc , pat , pat_end , fmt , va_alist ) <nl>  <nl> va_init_list ( args , fmt ); <nl> n = vsnprintf ( buf , bufsize , fmt , args ); <nl> + if ( n < 0 || n >= bufsize ) { <nl> + n = bufsize - 1 ; <nl> + } <nl> va_end ( args ); <nl>  <nl> need = ( pat_end - pat ) * 4 + 4 ;
PHP_FUNCTION ( parse_url ) <nl> resource = php_url_parse_ex ( str . s , str_len ); <nl> if ( resource == NULL ) { <nl> php_error_docref1 ( NULL TSRMLS_CC , str . s , E_WARNING , " Unable to parse URL "); <nl> + efree ( str . s ); <nl> RETURN_FALSE ; <nl> } <nl> 
static void php_wddx_pop_element ( void * user_data , const XML_Char * name ) <nl>  <nl> new_str = php_base64_decode ( Z_STRVAL_P ( ent1 -> data ), Z_STRLEN_P ( ent1 -> data ), & new_len ); <nl> STR_FREE ( Z_STRVAL_P ( ent1 -> data )); <nl> - Z_STRVAL_P ( ent1 -> data ) = new_str ; <nl> - Z_STRLEN_P ( ent1 -> data ) = new_len ; <nl> + if ( new_str ) { <nl> + Z_STRVAL_P ( ent1 -> data ) = new_str ; <nl> + Z_STRLEN_P ( ent1 -> data ) = new_len ; <nl> + } else { <nl> + ZVAL_EMPTY_STRING ( ent1 -> data ); <nl> + } <nl> } <nl>  <nl> /* Call __wakeup () method on the object . */
int _php_mb_encoding_handler_ex ( int data_type , zval * arg , char * res , char * separ <nl> val_len = len_list [ n ]; <nl> } <nl> n ++; <nl> + /* we need val to be emalloc () ed */ <nl> + val = estrndup ( val , val_len ); <nl> if ( sapi_module . input_filter ( data_type , var , & val , val_len , & new_val_len TSRMLS_CC )) { <nl> /* add variable to symbol table */ <nl> php_register_variable_safe ( var , val , new_val_len , array_ptr TSRMLS_CC ); <nl> } <nl> + efree ( val ); <nl> + <nl> if ( convd != NULL ){ <nl> mbfl_string_clear (& resvar ); <nl> mbfl_string_clear (& resval );
static int php_iconv_string ( char * in_p , unsigned int in_len , <nl> I added 15 extra bytes for safety . < yohgaki @ php . net > <nl> */ <nl> out_size = in_len * sizeof ( ucs4_t ) + 16 ; <nl> - out_buffer = ( char *) emalloc ( out_size ); <nl> + out_buffer = ( char *) ecalloc ( 1 , out_size ); <nl>  <nl> * out = out_buffer ; <nl> out_p = out_buffer ;
static void php_disable_classes ( TSRMLS_D ) <nl> */ <nl> static PHP_INI_MH ( OnUpdateTimeout ) <nl> { <nl> - EG ( timeout_seconds ) = atoi ( new_value ); <nl> if ( stage == PHP_INI_STAGE_STARTUP ) { <nl> /* Don ' t set a timeout on startup , only per - request */ <nl> + EG ( timeout_seconds ) = atoi ( new_value ); <nl> return SUCCESS ; <nl> } <nl> zend_unset_timeout ( TSRMLS_C ); <nl> + EG ( timeout_seconds ) = atoi ( new_value ); <nl> zend_set_timeout ( EG ( timeout_seconds )); <nl> return SUCCESS ; <nl> }
php_stream * php_stream_url_wrap_ftp ( php_stream_wrapper * wrapper , char * path , ch <nl> php_stream_notify_progress_init ( context , 0 , file_size ); <nl>  <nl> # if HAVE_OPENSSL_EXT <nl> - if ( use_ssl_on_data && php_stream_sock_ssl_activate_with_method ( datastream , 1 , SSLv23_method (), reuseid ) == FAILURE ) { <nl> + if ( use_ssl_on_data && php_stream_sock_ssl_activate_with_method ( datastream , 1 , SSLv23_method (), reuseid TSRMLS_CC ) == FAILURE ) { <nl> php_stream_wrapper_log_error ( wrapper , options TSRMLS_CC , " Unable to activate SSL mode "); <nl> php_stream_close ( datastream ); <nl> datastream = NULL ;
static char * zend_parse_arg_impl ( int arg_num , zval ** arg , va_list * va , char ** sp <nl> double d ; <nl> int type ; <nl>  <nl> - if (( type = is_numeric_unicode ( Z_USTRVAL_PP ( arg ), Z_USTRLEN_PP ( arg ), p , & d , 0 )) == 0 ) { <nl> + if (( type = is_numeric_unicode ( Z_USTRVAL_PP ( arg ), Z_USTRLEN_PP ( arg ), p , & d , - 1 )) == 0 ) { <nl> return " long "; <nl> } else if ( type == IS_DOUBLE ) { <nl> * p = ( long ) d ;
static int _valid_var_name ( char * varname ) <nl> /* }}} */ <nl>  <nl>  <nl> -/* {{{ proto void extract ( array var_array , int extract_type [, string prefix ]) <nl> +/* {{{ proto int extract ( array var_array , int extract_type [, string prefix ]) <nl> Imports variables into symbol table from an array */ <nl> PHP_FUNCTION ( extract ) <nl> { <nl> PHP_FUNCTION ( extract ) <nl> zval ** entry , * data ; <nl> char * varname , * finalname ; <nl> ulong lkey ; <nl> - int res , extype ; <nl> + int res , extype , count = 0 ; <nl>  <nl> switch ( ZEND_NUM_ARGS ()) { <nl> case 1 : <nl> PHP_FUNCTION ( extract ) <nl>  <nl> ZEND_SET_SYMBOL ( EG ( active_symbol_table ), finalname , data ); <nl> efree ( finalname ); <nl> + <nl> + count ++; <nl> } <nl> } <nl> } <nl>  <nl> zend_hash_move_forward ( Z_ARRVAL_PP ( var_array )); <nl> } <nl> + <nl> + RETURN_LONG ( count ); <nl> } <nl> /* }}} */ <nl> 
gdImagePtr gdImageCreateFromPngCtx ( gdIOCtx * infile ) <nl> png_read_info ( png_ptr , info_ptr ); /* read all PNG info up to image data */ <nl>  <nl> png_get_IHDR ( png_ptr , info_ptr , & width , & height , & bit_depth , & color_type , & interlace_type , NULL , NULL ); <nl> - if (( color_type == PNG_COLOR_TYPE_RGB ) || ( color_type == PNG_COLOR_TYPE_RGB_ALPHA )) { <nl> + if (( color_type == PNG_COLOR_TYPE_RGB ) || ( color_type == PNG_COLOR_TYPE_RGB_ALPHA ) <nl> + || color_type == PNG_COLOR_TYPE_GRAY_ALPHA ) { <nl> im = gdImageCreateTrueColor (( int ) width , ( int ) height ); <nl> } else { <nl> im = gdImageCreate (( int ) width , ( int ) height ); <nl> gdImagePtr gdImageCreateFromPngCtx ( gdIOCtx * infile ) <nl> } <nl> # endif <nl>  <nl> - <nl> switch ( color_type ) { <nl> case PNG_COLOR_TYPE_PALETTE : <nl> png_get_PLTE ( png_ptr , info_ptr , & palette , & num_palette ); <nl> gdImagePtr gdImageCreateFromPngCtx ( gdIOCtx * infile ) <nl> } <nl> break ; <nl> case PNG_COLOR_TYPE_GRAY : <nl> - case PNG_COLOR_TYPE_GRAY_ALPHA : <nl> /* create a fake palette and check for single - shade transparency */ <nl> if (( palette = ( png_colorp ) gdMalloc ( 256 * sizeof ( png_color ))) == NULL ) { <nl> php_gd_error (" gd - png error : cannot allocate gray palette "); <nl> gdImagePtr gdImageCreateFromPngCtx ( gdIOCtx * infile ) <nl> } <nl> break ; <nl>  <nl> + case PNG_COLOR_TYPE_GRAY_ALPHA : <nl> + png_set_gray_to_rgb ( png_ptr ); <nl> + <nl> case PNG_COLOR_TYPE_RGB : <nl> case PNG_COLOR_TYPE_RGB_ALPHA : <nl> /* gd 2 . 0 : we now support truecolor . See the comment above <nl> gdImagePtr gdImageCreateFromPngCtx ( gdIOCtx * infile ) <nl> } <nl> break ; <nl>  <nl> + case PNG_COLOR_TYPE_GRAY_ALPHA : <nl> case PNG_COLOR_TYPE_RGB_ALPHA : <nl> for ( h = 0 ; h < height ; h ++) { <nl> int boffset = 0 ;
PDO_API int php_pdo_register_driver ( pdo_driver_t * driver ) <nl> driver -> driver_name , driver -> api_version , PDO_DRIVER_API ); <nl> return FAILURE ; <nl> } <nl> - if (! zend_hash_exists (& module_registry , " PDO ", sizeof (" PDO "))) { <nl> + if (! zend_hash_exists (& module_registry , " pdo ", sizeof (" pdo "))) { <nl> zend_error ( E_ERROR , " You MUST load PDO before loading any PDO drivers "); <nl> return FAILURE ; /* NOTREACHED */ <nl> } <nl> PDO_API int php_pdo_register_driver ( pdo_driver_t * driver ) <nl>  <nl> PDO_API void php_pdo_unregister_driver ( pdo_driver_t * driver ) <nl> { <nl> - if (! zend_hash_exists (& module_registry , " PDO ", sizeof (" PDO "))) { <nl> + if (! zend_hash_exists (& module_registry , " pdo ", sizeof (" pdo "))) { <nl> return ; <nl> } <nl> 
PHP_FUNCTION ( fd_set ) <nl> FD_SET ( fd , & readfd ); <nl> if ( fd > max_fd ) max_fd = fd ; <nl> } <nl> + efree ( args ); <nl> } <nl> RETURN_LONG ( 1 ); <nl> }
private const struct { <nl> { " BZh ", 3 , { " bzip2 ", "- cd ", NULL }, 1 }, /* bzip2 - ed */ <nl> }; <nl>  <nl> + private size_t ncompr = sizeof ( compr ) / sizeof ( compr [ 0 ]); <nl> + <nl> # define NODATA (( size_t )~ 0 ) <nl>  <nl>  <nl> sread ( int fd , void * buf , size_t n , int canbepipe ) <nl> # ifdef FIONREAD <nl> if (( canbepipe && ( ioctl ( fd , FIONREAD , & t ) == - 1 )) || ( t == 0 )) { <nl> # ifdef FD_ZERO <nl> + int cnt ; <nl> + <nl> for ( cnt = 0 ;; cnt ++) { <nl> fd_set check ; <nl> struct timeval tout = { 0 , 100 * 1000 };
static void php_cgi_ini_activate_user_config ( char * path , int path_len , const cha <nl>  <nl> /* Check whether cache entry has expired and rescan if it is */ <nl> if ( request_time > entry -> expires ) { <nl> - char * real_path ; <nl> + char * real_path = NULL ; <nl> int real_path_len ; <nl> char * s1 , * s2 ; <nl> int s_len ; <nl> static void php_cgi_ini_activate_user_config ( char * path , int path_len , const cha <nl> php_parse_user_ini_file ( path , PG ( user_ini_filename ), entry -> user_config TSRMLS_CC ); <nl> } <nl>  <nl> + if ( real_path ) { <nl> + free ( real_path ); <nl> + } <nl> entry -> expires = request_time + PG ( user_ini_cache_ttl ); <nl> } <nl> 
PHP_FUNCTION ( tempnam ) <nl> char * d ; <nl> char * opened_path ; <nl> char p [ 64 ]; <nl> - FILE * fp ; <nl> + int fd ; <nl>  <nl> if ( ZEND_NUM_ARGS () != 2 || zend_get_parameters_ex ( 2 , & arg1 , & arg2 ) == FAILURE ) { <nl> WRONG_PARAM_COUNT ; <nl> PHP_FUNCTION ( tempnam ) <nl> d = estrndup ( Z_STRVAL_PP ( arg1 ), Z_STRLEN_PP ( arg1 )); <nl> strlcpy ( p , Z_STRVAL_PP ( arg2 ), sizeof ( p )); <nl>  <nl> - if (( fp = php_open_temporary_file ( d , p , & opened_path TSRMLS_CC ))) { <nl> - fclose ( fp ); <nl> + if (( fd = php_open_temporary_fd ( d , p , & opened_path TSRMLS_CC )) >= 0 ) { <nl> + close ( fd ); <nl> RETVAL_STRING ( opened_path , 0 ); <nl> } else { <nl> RETVAL_FALSE ;
static inline zval * zend_assign_to_variable ( zval * variable_ptr , zval * value TSRM <nl> value = Z_REFVAL_P ( value ); <nl> } <nl> if ( Z_REFCOUNTED_P ( value )) { <nl> + if ( UNEXPECTED ( variable_ptr == value )) { <nl> + return variable_ptr ; <nl> + } <nl> Z_ADDREF_P ( value ); <nl> } <nl> }
PHP_MINFO_FUNCTION ( apache ) <nl>  <nl> PUTS ("< tr >< td valign =\" top \" bgcolor =\"" PHP_ENTRY_NAME_COLOR "\"> Loaded modules </ td >< td bgcolor =\"" PHP_CONTENTS_COLOR "\">"); <nl> for ( modp = top_module ; modp ; modp = modp -> next ) { <nl> - strncpy ( name , modp -> name , sizeof ( name ) - 1 ); <nl> + strlcpy ( name , modp -> name , sizeof ( name )); <nl> if (( p = strrchr ( name , '.'))) { <nl> * p ='\ 0 '; /* Cut off ugly . c extensions on module names */ <nl> }
PHPAPI char * php_stream_get_record ( php_stream * stream , size_t maxlen , size_t * re <nl>  <nl> php_stream_fill_read_buffer ( stream , maxlen TSRMLS_CC ); <nl>  <nl> + if (( stream -> writepos - stream -> readpos )<= 0 ) { <nl> + return NULL ; <nl> + } <nl> + <nl> if ( delim_len == 0 || ! delim ) { <nl> toread = maxlen ; <nl> } else {
static inline int php_tcp_sockop_connect ( php_stream * stream , php_netstream_data_ <nl> if ( xparam -> want_errortext ) { <nl> spprintf (& xparam -> outputs . error_text , 0 , " local_addr context option is not a string ."); <nl> } <nl> + efree ( host ); <nl> return - 1 ; <nl> } <nl> bindto = parse_ip_address_ex ( Z_STRVAL_PP ( tmpzval ), Z_STRLEN_PP ( tmpzval ), & bindport , xparam -> want_errortext , & xparam -> outputs . error_text TSRMLS_CC );
PHP_FUNCTION ( stream_get_meta_data ) <nl>  <nl> add_assoc_long ( return_value , " unread_bytes ", stream -> writepos - stream -> readpos ); <nl>  <nl> + add_assoc_bool ( return_value , " seekable ", ( stream -> ops -> seek ) && ( stream -> flags & PHP_STREAM_FLAG_NO_SEEK ) == 0 ); <nl> + <nl> if (! php_stream_populate_meta_data ( stream , return_value )) { <nl> add_assoc_bool ( return_value , " timed_out ", 0 ); <nl> add_assoc_bool ( return_value , " blocked ", 1 );
PHP_FUNCTION ( imagegammacorrect ) <nl> return ; <nl> } <nl>  <nl> + if ( input <= 0 . 0 || output <= 0 . 0 ) { <nl> + php_error_docref ( NULL TSRMLS_CC , E_WARNING , " Gamma values should be positive "); <nl> + RETURN_FALSE ; <nl> + } <nl> + <nl> ZEND_FETCH_RESOURCE ( im , gdImagePtr , & IM , - 1 , " Image ", le_gd ); <nl>  <nl> if ( gdImageTrueColor ( im )) {
void * fs_get ( size_t size ); <nl> int imap_mail ( char * to , char * subject , char * message , char * headers , char * cc , char * bcc , char * rpath ); <nl>  <nl>  <nl> - void mail_close_it ( zend_rsrc_list_entry * rsrc ); <nl> + void mail_close_it ( zend_rsrc_list_entry * rsrc ); <nl> # ifdef OP_RELOGIN <nl> /* AJS : close persistent connection */ <nl> void mail_userlogout_it ( zend_rsrc_list_entry * rsrc ); <nl> void mail_close_it ( zend_rsrc_list_entry * rsrc ) <nl> { <nl> pils * imap_le_struct = ( pils *) rsrc -> ptr ; <nl> mail_close_full ( imap_le_struct -> imap_stream , imap_le_struct -> flags ); <nl> + <nl> + efree ( IMAPG ( imap_user )); <nl> + efree ( IMAPG ( imap_password )); <nl> efree ( imap_le_struct ); <nl> } <nl>  <nl> void imap_do_open ( INTERNAL_FUNCTION_PARAMETERS , int persistent ) <nl> } else { <nl> # endif <nl> imap_stream = mail_open ( NIL , Z_STRVAL_PP ( mailbox ), flags ); <nl> - efree ( IMAPG ( imap_user )); <nl> - efree ( IMAPG ( imap_password )); <nl>  <nl> if ( imap_stream == NIL ) { <nl> php_error ( E_WARNING , " Couldn ' t open stream % s \ n ", (* mailbox )-> value . str . val );
DIR * opendir ( const char * dir ) <nl>  <nl> dp = ( DIR *) malloc ( sizeof ( DIR )); <nl> if ( dp == NULL ) { <nl> + free ( filespec ); <nl> return NULL ; <nl> } <nl> dp -> offset = 0 ;
static void php_sybase_get_column_content ( sybase_link * sybase_ptr , int offset , pva <nl> switch ( coltype ( offset )) { <nl> case SYBBINARY : <nl> case SYBVARBINARY : <nl> + case SYBIMAGE : <nl> res_length *= 2 ; <nl> break ; <nl> case SYBCHAR : <nl> case SYBVARCHAR : <nl> case SYBTEXT : <nl> - case SYBIMAGE : <nl> break ; <nl> default : <nl> /* take no chances , no telling how big the result would really be */
int mbfl_filt_conv_html_enc ( int c , mbfl_convert_filter * filter ) <nl> } <nl>  <nl> { <nl> - int * p = tmp + sizeof ( tmp ); <nl> + int * p = tmp + sizeof ( tmp ) / sizeof ( tmp [ 0 ]); <nl>  <nl> CK ((* filter -> output_function )('#', filter -> data )); <nl> 
PHP_BZ2_API php_stream * _php_stream_bz2open ( php_stream_wrapper * wrapper , <nl>  <nl> if ( php_check_open_basedir ( path_copy TSRMLS_CC )) { <nl> # ifdef VIRTUAL_DIR <nl> - efree ( path_copy ); <nl> + free ( path_copy ); <nl> # endif <nl> return NULL ; <nl> } <nl> PHP_BZ2_API php_stream * _php_stream_bz2open ( php_stream_wrapper * wrapper , <nl> * opened_path = estrdup ( path_copy ); <nl> } <nl> # ifdef VIRTUAL_DIR <nl> - efree ( path_copy ); <nl> + free ( path_copy ); <nl> # endif <nl> path_copy = NULL ; <nl> 
struct _php_stream { <nl> char * orig_path ; <nl>  <nl> zend_resource * ctx ; <nl> - int flags ; /* PHP_STREAM_FLAG_XXX */ <nl> + uint32_t flags ; /* PHP_STREAM_FLAG_XXX */ <nl>  <nl> int eof ; <nl> 
static php_conv_err_t php_conv_qprint_encode_convert ( php_conv_qprint_encode * ins <nl> CONSUME_CHAR ( ps , icnt , lb_ptr , lb_cnt ); <nl> } <nl> } else if ((!( opts & PHP_CONV_QPRINT_OPT_FORCE_ENCODE_FIRST ) || line_ccnt < inst -> line_len ) && (( c >= 33 && c <= 60 ) || ( c >= 62 && c <= 126 ))) { <nl> - if ( line_ccnt < 2 ) { <nl> + if ( line_ccnt < 2 && inst -> lbchars != NULL ) { <nl> if ( ocnt < inst -> lbchars_len + 1 ) { <nl> err = PHP_CONV_ERR_TOO_BIG ; <nl> break ;
php_oci_bind * php_oci_bind_array_helper_date ( zval * var , long max_table_length , p <nl>  <nl> if ( connection -> errcode != OCI_SUCCESS ) { <nl> /* failed to convert string to date */ <nl> + efree ( bind -> array . element_lengths ); <nl> efree ( bind -> array . elements ); <nl> efree ( bind ); <nl> php_oci_error ( connection -> err , connection -> errcode TSRMLS_CC ); <nl> php_oci_bind * php_oci_bind_array_helper_date ( zval * var , long max_table_length , p <nl>  <nl> if ( connection -> errcode != OCI_SUCCESS ) { <nl> /* failed to convert string to date */ <nl> + efree ( bind -> array . element_lengths ); <nl> efree ( bind -> array . elements ); <nl> efree ( bind ); <nl> php_oci_error ( connection -> err , connection -> errcode TSRMLS_CC );
int flatfile_store ( flatfile * dba , datum key_datum , datum value_datum , int mode T <nl> return 1 ; <nl> } <nl> php_stream_seek ( dba -> fp , 0L , SEEK_END ); <nl> - php_stream_printf ( dba -> fp TSRMLS_CC , "% d \ n ", key_datum . dsize ); <nl> + php_stream_printf ( dba -> fp TSRMLS_CC , "% zu \ n ", key_datum . dsize ); <nl> php_stream_flush ( dba -> fp ); <nl> if ( php_stream_write ( dba -> fp , key_datum . dptr , key_datum . dsize ) < key_datum . dsize ) { <nl> return - 1 ; <nl> } <nl> - php_stream_printf ( dba -> fp TSRMLS_CC , "% d \ n ", value_datum . dsize ); <nl> + php_stream_printf ( dba -> fp TSRMLS_CC , "% zu \ n ", value_datum . dsize ); <nl> php_stream_flush ( dba -> fp ); <nl> if ( php_stream_write ( dba -> fp , value_datum . dptr , value_datum . dsize ) < value_datum . dsize ) { <nl> return - 1 ; <nl> } <nl> } else { /* FLATFILE_REPLACE */ <nl> flatfile_delete ( dba , key_datum TSRMLS_CC ); <nl> - php_stream_printf ( dba -> fp TSRMLS_CC , "% d \ n ", key_datum . dsize ); <nl> + php_stream_printf ( dba -> fp TSRMLS_CC , "% zu \ n ", key_datum . dsize ); <nl> php_stream_flush ( dba -> fp ); <nl> if ( php_stream_write ( dba -> fp , key_datum . dptr , key_datum . dsize ) < key_datum . dsize ) { <nl> return - 1 ; <nl> } <nl> - php_stream_printf ( dba -> fp TSRMLS_CC , "% d \ n ", value_datum . dsize ); <nl> + php_stream_printf ( dba -> fp TSRMLS_CC , "% zu \ n ", value_datum . dsize ); <nl> if ( php_stream_write ( dba -> fp , value_datum . dptr , value_datum . dsize ) < value_datum . dsize ) { <nl> return - 1 ; <nl> }
php_stream * php_stream_url_wrap_http_ex ( php_stream_wrapper * wrapper , char * path , <nl> } <nl> smart_str_0 (& tmpstr ); <nl> /* Remove newlines and spaces from start and end . there ' s at least one extra \ r \ n at the end that needs to go . */ <nl> - tmp = php_trim ( tmpstr . c , strlen ( tmpstr . c ), NULL , 0 , NULL , 3 TSRMLS_CC ); <nl> + if ( tmpstr . c ) { <nl> + tmp = php_trim ( tmpstr . c , strlen ( tmpstr . c ), NULL , 0 , NULL , 3 TSRMLS_CC ); <nl> + smart_str_free (& tmpstr ); <nl> + } <nl> } <nl> if ( Z_TYPE_PP ( tmpzval ) == IS_STRING && Z_STRLEN_PP ( tmpzval )) { <nl> /* Remove newlines and spaces from start and end php_trim will estrndup () */
php_stream_wrapper php_stream_ftp_wrapper = { <nl> */ <nl> php_stream * php_stream_url_wrap_ftp ( php_stream_wrapper * wrapper , char * path , char * mode , int options , char ** opened_path , php_stream_context * context STREAMS_DC TSRMLS_DC ) <nl> { <nl> - php_stream * stream = NULL , * datastream = NULL , * reuseid = NULL ; <nl> + php_stream * stream = NULL , * datastream = NULL ; <nl> php_url * resource = NULL ; <nl> char tmp_line [ 512 ]; <nl> char ip [ sizeof (" 123 . 123 . 123 . 123 ")]; <nl> unsigned short portno ; <nl> char * scratch ; <nl> int result ; <nl> - int i , use_ssl , use_ssl_on_data = 0 ; <nl> + int i , use_ssl ; <nl> +# if HAVE_OPENSSL_EXT <nl> + int use_ssl_on_data = 0 ; <nl> + php_stream * reuseid = NULL ; <nl> +# endif <nl> char * tpath , * ttpath , * hoststart = NULL ; <nl> size_t file_size = 0 ; <nl> 
flatpak_installation_drop_caches ( FlatpakInstallation * self , <nl> { <nl> priv -> dir_unlocked = clone ; <nl> g_object_unref ( old ); <nl> + res = TRUE ; <nl> } <nl>  <nl> G_UNLOCK ( dir );
setup_seccomp ( FlatpakBwrap * bwrap , <nl>  <nl> /* Don ' t allow faking input to the controlling tty ( CVE - 2017 - 5226 ) */ <nl> { SCMP_SYS ( ioctl ), EPERM , & SCMP_A1 ( SCMP_CMP_MASKED_EQ , 0xFFFFFFFFu , ( int ) TIOCSTI )}, <nl> + <nl> + /* seccomp can ' t look into clone3 ()' s struct clone_args to check whether <nl> + * the flags are OK , so we have no choice but to block clone3 (). <nl> + * Return ENOSYS so user - space will fall back to clone (). <nl> + * ( GHSA - 67h7 - w3jq - vh4q ; see also https :// github . com / moby / moby / commit / 9f6b562d ) */ <nl> + { SCMP_SYS ( clone3 ), ENOSYS }, <nl> }; <nl>  <nl> struct
add_related ( GHashTable * all_refs , <nl> g_hash_table_insert ( all_refs , g_steal_pointer (& ext_collection_ref ), c_s ); <nl> } <nl>  <nl> + g_list_free_full ( extensions , ( GDestroyNotify ) flatpak_extension_free ); <nl> + <nl> return TRUE ; <nl> } <nl> 
xdp_fuse_init ( GError ** error ) <nl>  <nl> path = xdp_fuse_get_mountpoint (); <nl> if (( stat ( path , & st ) == - 1 && errno == ENOTCONN ) || <nl> - (( statfs_res = statfs ( path , & stfs )) == - 1 && errno == ENOTCONN || <nl> + ((( statfs_res = statfs ( path , & stfs )) == - 1 && errno == ENOTCONN ) || <nl> ( statfs_res == 0 && stfs . f_type == 0x65735546 /* fuse */))) <nl> { <nl> int count ;
static inline void add_search_pattern ( PossiblyFreedShadaEntry * const ret_pse , <nl> ? defaults . data . search_pattern . place_cursor_at_end <nl> : pat . off . end ), <nl> . offset = ( is_substitute_pattern <nl> - ? pat . off . off <nl> - : defaults . data . search_pattern . offset ), <nl> + ? defaults . data . search_pattern . offset <nl> + : pat . off . off ), <nl> . is_last_used = ( is_substitute_pattern ^ search_last_used ), <nl> . is_substitute_pattern = is_substitute_pattern , <nl> . highlighted = (( is_substitute_pattern ^ search_last_used )
static inline void typval_encode_list_start ( EncodedData * const edata , <nl> const size_t len ) <nl> FUNC_ATTR_ALWAYS_INLINE FUNC_ATTR_NONNULL_ALL <nl> { <nl> - const Object obj = OBJECT_INIT ; <nl> kv_push ( edata -> stack , ARRAY_OBJ ((( Array ) { <nl> . capacity = len , <nl> . size = 0 , <nl> - . items = xmalloc ( len * sizeof (* obj . data . array . items )), <nl> + . items = xmalloc ( len * sizeof (*(( Object *) NULL )-> data . array . items )), <nl> }))); <nl> } <nl>  <nl> static inline void typval_encode_dict_start ( EncodedData * const edata , <nl> const size_t len ) <nl> FUNC_ATTR_ALWAYS_INLINE FUNC_ATTR_NONNULL_ALL <nl> { <nl> - const Object obj = OBJECT_INIT ; <nl> kv_push ( edata -> stack , DICTIONARY_OBJ ((( Dictionary ) { <nl> . capacity = len , <nl> . size = 0 , <nl> - . items = xmalloc ( len * sizeof (* obj . data . dictionary . items )), <nl> + . items = xmalloc ( len * sizeof (*(( Object *) NULL )-> data . dictionary . items )), <nl> }))); <nl> } <nl> 
BOOL license_read_scope_list ( wStream * s , SCOPE_LIST * scopeList ) <nl>  <nl> Stream_Read_UINT32 ( s , scopeCount ); /* ScopeCount ( 4 bytes ) */ <nl>  <nl> + if ( Stream_GetRemainingLength ( s ) / sizeof ( LICENSE_BLOB ) < scopeCount ) <nl> + return FALSE ; /* Avoid overflow in malloc */ <nl> + <nl> scopeList -> count = scopeCount ; <nl> scopeList -> array = ( LICENSE_BLOB *) malloc ( sizeof ( LICENSE_BLOB ) * scopeCount ); <nl> 
void glyph_cache_put ( rdpGlyphCache * glyph_cache , UINT32 id , UINT32 index , rdpGly <nl> if ( prevGlyph != NULL ) <nl> { <nl> Glyph_Free ( glyph_cache -> context , prevGlyph ); <nl> - free ( prevGlyph -> aj ); <nl> + if ( NULL != prevGlyph -> aj ) <nl> + free ( prevGlyph -> aj ); <nl> free ( prevGlyph ); <nl> } <nl>  <nl> void glyph_cache_free ( rdpGlyphCache * glyph_cache ) <nl> if ( glyph != NULL ) <nl> { <nl> Glyph_Free ( glyph_cache -> context , glyph ); <nl> - free ( glyph -> aj ); <nl> + if ( glyph -> aj ) <nl> + free ( glyph -> aj ); <nl> free ( glyph ); <nl> glyph_cache -> glyphCache [ i ]. entries [ j ] = NULL ; <nl> } <nl> } <nl> free ( glyph_cache -> glyphCache [ i ]. entries ); <nl> + glyph_cache -> glyphCache [ i ]. entries = NULL ; <nl> } <nl>  <nl> for ( i = 0 ; i < 255 ; i ++)
void update_free ( rdpUpdate * update ) <nl>  <nl> xfree ( update -> bitmap_update . rectangles ); <nl> xfree ( update -> pointer ); <nl> + xfree ( update -> primary -> polyline . points ); <nl> + xfree ( update -> primary -> polygon_sc . points ); <nl> xfree ( update -> primary ); <nl> xfree ( update -> secondary ); <nl> xfree ( update -> altsec );
along with this program . If not , see < http :// www . gnu . org / licenses />. <nl> */ <nl>  <nl> +# include < algorithm > <nl> # include < QAbstractProxyModel > <nl> # include < QAuthenticator > <nl> # include < QCoreApplication > <nl> void Model :: unsubscribeMailbox ( const QString & name ) <nl>  <nl> void Model :: saveUidMap ( TreeItemMsgList * list ) <nl> { <nl> - Imap :: Uids seqToUid ; <nl> - seqToUid . reserve ( list -> m_children . size ()); <nl> - auto end = list -> m_children . constEnd (); <nl> - for ( auto it = list -> m_children . constBegin (); it != end ; ++ it ) <nl> - seqToUid << static_cast < TreeItemMessage *>(* it )-> uid (); <nl> + Imap :: Uids seqToUid ( list -> m_children . size (), 0 ); <nl> + std :: transform ( list -> m_children . constBegin (), list -> m_children . cend (), seqToUid . begin (), []( TreeItem * item ) { <nl> + return static_cast < TreeItemMessage *>( item )-> uid (); <nl> + }); <nl> cache ()-> setUidMapping ( static_cast < TreeItemMailbox *>( list -> parent ())-> mailbox (), seqToUid ); <nl> } <nl> 
At the same time , some well - known flags are converted to their " canonical " form <nl> QStringList Model :: normalizeFlags ( const QStringList & source ) const <nl> { <nl> QStringList res ; <nl> +# if QT_VERSION >= QT_VERSION_CHECK ( 4 , 7 , 0 ) <nl> + res . reserve ( source . size ()); <nl> +# endif <nl> for ( QStringList :: const_iterator flag = source . constBegin (); flag != source . constEnd (); ++ flag ) { <nl> // At first , perform a case - insensitive lookup in the ( rather short ) list of known special flags <nl> QString lowerCase = flag -> toLower ();
int main ( int argc , char * argv []) { <nl> log_error_errno ( r , " Failed to iterate through journal : % m "); <nl> goto finish ; <nl> } <nl> + if ( r == 0 ) { <nl> + printf ("-- No entries --\ n "); <nl> + goto finish ; <nl> + } <nl>  <nl> if (! arg_follow ) <nl> pager_open_if_enabled ();
void prefix_free ( Prefix * prefix ) { <nl> } <nl>  <nl> int prefix_new ( Prefix ** ret ) { <nl> - Prefix * prefix = NULL ; <nl> + _cleanup_prefix_free_ Prefix * prefix = NULL ; <nl>  <nl> prefix = new0 ( Prefix , 1 ); <nl> if (! prefix )
static int socket_recv_message ( int fd , struct iovec * iov , uint32_t * _group , bool <nl> } <nl> } <nl>  <nl> - if (! auth ) <nl> + if (! auth ) { <nl> /* not from the kernel , ignore */ <nl> + if ( peek ) { <nl> + /* drop the message */ <nl> + r = recvmsg ( fd , & msg , 0 ); <nl> + if ( r < 0 ) <nl> + return ( errno == EAGAIN || errno == EINTR ) ? 0 : - errno ; <nl> + } <nl> + <nl> return 0 ; <nl> + } <nl>  <nl> if ( group ) <nl> * _group = group ;
static int show_one ( <nl> */ <nl> if ( info . pid_file && access ( info . pid_file , F_OK ) == 0 ) <nl> r = 1 ; <nl> + else if ( streq_ptr ( info . load_state , " not - found ") && streq_ptr ( info . active_state , " inactive ")) <nl> + r = 4 ; <nl> else <nl> r = 3 ; <nl> }
static int chase_all_symlinks ( const char * root_directory , BindMount * m , unsigned <nl> * chase the symlinks on our own first . This call wil do so for all entries and remove all entries where we <nl> * can ' t resolve the path , and which have been marked for such removal . */ <nl>  <nl> - for ( f = m , t = m ; f < m +* n ; f ++) { <nl> + for ( f = m , t = m ; f < m + * n ; f ++) { <nl> _cleanup_free_ char * chased = NULL ; <nl> + <nl> r = chase_symlinks ( f -> path , root_directory , & chased ); <nl> if ( r == - ENOENT && f -> ignore ) { <nl> /* Doesn ' t exist ? Then remove it ! */ <nl> static int chase_all_symlinks ( const char * root_directory , BindMount * m , unsigned <nl>  <nl> if (! path_equal ( f -> path , chased )) { <nl> log_debug (" Chased % s → % s ", f -> path , chased ); <nl> - r = free_and_strdup (& f -> path , chased ); <nl> + r = free_and_replace ( f -> path , chased ); <nl> if ( r < 0 ) <nl> return r ; <nl> }
void manager_send_unit_plymouth ( Manager * m , Unit * u ) { <nl> if ( m -> running_as != SYSTEMD_SYSTEM ) <nl> return ; <nl>  <nl> + if ( detect_container ( NULL ) > 0 ) <nl> + return ; <nl> + <nl> if ( u -> type != UNIT_SERVICE && <nl> u -> type != UNIT_MOUNT && <nl> u -> type != UNIT_SWAP )
static void do_kernelname ( struct sysfs_class_device * class_dev , struct udevice * <nl> { <nl> struct config_device * dev ; <nl> struct list_head * tmp ; <nl> + int len ; <nl>  <nl> strfieldcpy ( udev -> name , class_dev -> name ); <nl> list_for_each ( tmp , & config_device_list ) { <nl> dev = list_entry ( tmp , struct config_device , node ); <nl> - int len = strlen ( dev -> name ); <nl> + len = strlen ( dev -> name ); <nl> if ( dev -> name [ len - 1 ] == '*') { <nl> len --; <nl> if ( strncmp ( dev -> name , class_dev -> name , len ))
static int generic_array_bisect_plus_one ( JournalFile * f , <nl>  <nl> /* This bisects the array in object ' first ', but first checks <nl> * an extra */ <nl> - <nl> r = test_object ( f , extra , needle ); <nl> if ( r < 0 ) <nl> return r ; <nl> static int generic_array_bisect_plus_one ( JournalFile * f , <nl>  <nl> if ( offset ) <nl> * offset = extra ; <nl> + <nl> + if ( idx ) <nl> + * idx = 0 ; <nl> + <nl> + return 1 ; <nl> } else if ( r == TEST_RIGHT ) <nl> return 0 ; <nl> 
int manager_read_resolv_conf ( Manager * m ) { <nl> if ( s -> marked ) <nl> dns_server_free ( s ); <nl>  <nl> + /* Whenever / etc / resolv . conf changes , start using the first <nl> + * DNS server of it . This is useful to deal with broken <nl> + * network managing implementations ( like NetworkManager ), <nl> + * that when connecting to a VPN place both the VPN DNS <nl> + * servers and the local ones in / etc / resolv . conf . Without <nl> + * resetting the DNS server to use back to the first entry we <nl> + * will continue to use the local one thus being unable to <nl> + * resolve VPN domains . */ <nl> + manager_set_dns_server ( m , m -> dns_servers ); <nl> + <nl> return 0 ; <nl>  <nl> clear :
static void output_unit_file_list ( const UnitFileList * units , unsigned c ) { <nl> } else <nl> id_cols = max_id_len ; <nl>  <nl> - if (! arg_no_legend ) <nl> + if (! arg_no_legend && c > 0 ) <nl> printf ("%-* s %-* s \ n ", <nl> id_cols , " UNIT FILE ", <nl> state_cols , " STATE ");
static int journal_file_object_verify ( JournalFile * f , Object * o ) { <nl> h1 = le64toh ( o -> data . hash ); <nl>  <nl> if ( o -> object . flags & OBJECT_COMPRESSED ) { <nl> +# ifdef HAVE_XZ <nl> void * b = NULL ; <nl> uint64_t alloc = 0 , b_size ; <nl>  <nl> static int journal_file_object_verify ( JournalFile * f , Object * o ) { <nl>  <nl> h2 = hash64 ( b , b_size ); <nl> free ( b ); <nl> +# else <nl> + return - EPROTONOSUPPORT ; <nl> +# endif <nl> } else <nl> h2 = hash64 ( o -> data . payload , le64toh ( o -> object . size ) - offsetof ( Object , data . payload )); <nl> 
static void bus_method_resolve_address_complete ( DnsQuery * q ) { <nl> goto finish ; <nl> } <nl>  <nl> - /* We don ' t process CNAME for PTR lookups . */ <nl> + r = dns_query_process_cname ( q ); <nl> + if ( r == - ELOOP ) { <nl> + r = sd_bus_reply_method_errorf ( q -> request , BUS_ERROR_CNAME_LOOP , " CNAME loop detected , or CNAME resolving disabled on '% s '", dns_question_name ( q -> question )); <nl> + goto finish ; <nl> + } <nl> + if ( r < 0 ) <nl> + goto finish ; <nl> + if ( r > 0 ) /* This was a cname , and the query was restarted . */ <nl> + return ; <nl>  <nl> r = sd_bus_message_new_method_return ( q -> request , & reply ); <nl> if ( r < 0 )
int config_parse_netdev ( const char * unit , <nl> void * data , <nl> void * userdata ) { <nl> Network * network = userdata ; <nl> - char * kind_string , * p ; <nl> + _cleanup_free_ char * kind_string = NULL ; <nl> + char * p ; <nl> NetDev * netdev ; <nl> NetDevKind kind ; <nl> int r ;
int session_set_controller ( Session * s , const char * sender , bool force ) { <nl> * If logind crashes / restarts , we restore the controller during restart <nl> * or reset the VT in case it crashed / exited , too . */ <nl> r = session_prepare_vt ( s ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( t ); <nl> return r ; <nl> + } <nl>  <nl> session_swap_controller ( s , t ); <nl> 
static DBusHandlerResult locale_message_handler ( <nl> " Locale \ 0 "); <nl> if (! changed ) <nl> goto oom ; <nl> - } <nl> + } else <nl> + strv_free ( l ); <nl> + <nl> } else if ( dbus_message_is_method_call ( message , " org . freedesktop . locale1 ", " SetVConsoleKeyboard ")) { <nl>  <nl> const char * keymap , * keymap_toggle ;
static int outer_child ( <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> + r = determine_uid_shift ( directory ); <nl> + if ( r < 0 ) <nl> + return r ; <nl> + <nl> /* Turn directory into bind mount */ <nl> if ( mount ( directory , directory , NULL , MS_BIND | MS_REC , NULL ) < 0 ) <nl> return log_error_errno ( errno , " Failed to make bind mount : % m "); <nl> int main ( int argc , char * argv []) { <nl> if ( r < 0 ) <nl> goto finish ; <nl>  <nl> - r = determine_uid_shift ( arg_directory ); <nl> - if ( r < 0 ) <nl> - return r ; <nl> - <nl> if ( geteuid () != 0 ) { <nl> log_error (" Need to be root ."); <nl> r = - EPERM ;
static struct uevent_msg * get_msg_from_envbuf ( const char * buf , int buf_size ) <nl> msg -> envp [ i ++] = " UDEVD_EVENT = 1 "; <nl> msg -> envp [ i ] = NULL ; <nl>  <nl> - if (! msg -> devpath ) { <nl> - info (" DEVPATH missing , ignore message "); <nl> + if ( msg -> devpath == NULL || msg -> action == NULL ) { <nl> + info (" DEVPATH or ACTION missing , ignore message "); <nl> free ( msg ); <nl> return NULL ; <nl> }
static int swap_add_device_links ( Swap * s ) { <nl> if (! s -> what ) <nl> return 0 ; <nl>  <nl> + if (! s -> from_fragment ) <nl> + return 0 ; <nl> + <nl> if ( is_device_path ( s -> what )) <nl> return unit_add_node_link ( UNIT ( s ), s -> what , UNIT ( s )-> manager -> running_as == SYSTEMD_SYSTEM ); <nl> else
ManagerState manager_state ( Manager * m ) { <nl>  <nl> /* Is the special shutdown target queued ? If so , we are in shutdown state */ <nl> u = manager_get_unit ( m , SPECIAL_SHUTDOWN_TARGET ); <nl> - if ( u && u -> job && IN_SET ( u -> job -> type , JOB_START , JOB_RESTART , JOB_TRY_RESTART , JOB_RELOAD_OR_START )) <nl> + if ( u && u -> job && IN_SET ( u -> job -> type , JOB_START , JOB_RESTART , JOB_RELOAD_OR_START )) <nl> return MANAGER_STOPPING ; <nl>  <nl> /* Are the rescue or emergency targets active or queued ? If so we are in maintenance state */ <nl> u = manager_get_unit ( m , SPECIAL_RESCUE_TARGET ); <nl> if ( u && ( UNIT_IS_ACTIVE_OR_ACTIVATING ( unit_active_state ( u )) || <nl> - ( u -> job && IN_SET ( u -> job -> type , JOB_START , JOB_RESTART , JOB_TRY_RESTART , JOB_RELOAD_OR_START )))) <nl> + ( u -> job && IN_SET ( u -> job -> type , JOB_START , JOB_RESTART , JOB_RELOAD_OR_START )))) <nl> return MANAGER_MAINTENANCE ; <nl>  <nl> u = manager_get_unit ( m , SPECIAL_EMERGENCY_TARGET ); <nl> if ( u && ( UNIT_IS_ACTIVE_OR_ACTIVATING ( unit_active_state ( u )) || <nl> - ( u -> job && IN_SET ( u -> job -> type , JOB_START , JOB_RESTART , JOB_TRY_RESTART , JOB_RELOAD_OR_START )))) <nl> + ( u -> job && IN_SET ( u -> job -> type , JOB_START , JOB_RESTART , JOB_RELOAD_OR_START )))) <nl> return MANAGER_MAINTENANCE ; <nl>  <nl> /* Are there any failed units ? If so , we are in degraded mode */
void seat_evict_position ( Seat * s , Session * session ) { <nl> * position ( eg ., during gdm -> session transition ), so let ' s look <nl> * for it and set it on the free slot . */ <nl> LIST_FOREACH ( sessions_by_seat , iter , s -> sessions ) { <nl> - if ( iter -> position == pos ) { <nl> + if ( iter -> position == pos && session_get_state ( iter ) != SESSION_CLOSING ) { <nl> s -> positions [ pos ] = iter ; <nl> break ; <nl> }
int parse_timestamp ( const char * t , usec_t * usec ) { <nl>  <nl> x = time ( NULL ); <nl> assert_se ( localtime_r (& x , & tm )); <nl> + tm . tm_isdst = - 1 ; <nl>  <nl> if ( streq ( t , " now ")) <nl> goto finish ;
int chase_symlinks ( const char * path , const char * original_root , unsigned flags , <nl> return - ENOMEM ; <nl> } <nl>  <nl> - * ret = done ; <nl> - done = NULL ; <nl> + if ( ret ) { <nl> + * ret = done ; <nl> + done = NULL ; <nl> + } <nl>  <nl> return exists ; <nl> }
static int add_string ( struct udev_rules * rules , const char * str ) <nl> unsigned short node_off ; <nl> unsigned char key ; <nl> size_t len ; <nl> - int depth ; <nl> + unsigned int depth ; <nl> unsigned int off ; <nl>  <nl> len = strlen ( str );
struct SocketPeer { <nl>  <nl> Socket * socket ; <nl> union sockaddr_union peer ; <nl> + socklen_t peer_salen ; <nl> }; <nl>  <nl> static const UnitActiveState state_translation_table [ _SOCKET_STATE_MAX ] = { <nl> static void peer_address_hash_func ( const void * p , struct siphash * state ) { <nl>  <nl> if ( s -> peer . sa . sa_family == AF_INET ) <nl> siphash24_compress (& s -> peer . in . sin_addr , sizeof ( s -> peer . in . sin_addr ), state ); <nl> - else <nl> + else if ( s -> peer . sa . sa_family == AF_INET6 ) <nl> siphash24_compress (& s -> peer . in6 . sin6_addr , sizeof ( s -> peer . in6 . sin6_addr ), state ); <nl> + else <nl> + assert_not_reached (" Unknown address family ."); <nl> } <nl>  <nl> static int peer_address_compare_func ( const void * a , const void * b ) { <nl> int socket_acquire_peer ( Socket * s , int fd , SocketPeer ** p ) { <nl> return log_oom (); <nl>  <nl> remote -> peer = sa . peer ; <nl> + remote -> peer_salen = salen ; <nl>  <nl> r = set_put ( s -> peers_by_address , remote ); <nl> if ( r < 0 ) <nl> static void socket_enter_running ( Socket * s , int cfd ) { <nl> } else if ( r > 0 && p -> n_ref > s -> max_connections_per_source ) { <nl> _cleanup_free_ char * t = NULL ; <nl>  <nl> - sockaddr_pretty (& p -> peer . sa , FAMILY_ADDRESS_SIZE ( p -> peer . sa . sa_family ), true , false , & t ); <nl> + ( void ) sockaddr_pretty (& p -> peer . sa , p -> peer_salen , true , false , & t ); <nl>  <nl> log_unit_warning ( UNIT ( s ), <nl> " Too many incoming connections (% u ) from source % s , dropping connection .",
int dns_packet_extract ( DnsPacket * p ) { <nl>  <nl> for ( i = 0 ; i < n ; i ++) { <nl> _cleanup_ ( dns_resource_record_unrefp ) DnsResourceRecord * rr = NULL ; <nl> + bool cache_flush ; <nl>  <nl> - r = dns_packet_read_rr ( p , & rr , NULL ); <nl> + r = dns_packet_read_rr ( p , & rr , & cache_flush , NULL ); <nl> if ( r < 0 ) <nl> goto finish ; <nl>  <nl> if ( rr -> key -> type == DNS_TYPE_OPT ) { <nl>  <nl> + if (! dns_name_is_root ( DNS_RESOURCE_KEY_NAME ( rr -> key ))) { <nl> + r = - EBADMSG ; <nl> + goto finish ; <nl> + } <nl> + <nl> /* The OPT RR is only valid in the Additional section */ <nl> if ( i < DNS_PACKET_ANCOUNT ( p ) + DNS_PACKET_NSCOUNT ( p )) { <nl> r = - EBADMSG ;
void xor_buf ( uint8_t out [], <nl> const uint8_t in2 [], <nl> size_t length ) <nl> { <nl> - while ( length >= 8 ) <nl> + while ( length >= 16 ) <nl> { <nl> out [ 0 ] = in [ 0 ] ^ in2 [ 0 ]; <nl> out [ 1 ] = in [ 1 ] ^ in2 [ 1 ];
Library_State & global_state () <nl>  <nl> void set_global_state ( Library_State * new_state ) <nl> { <nl> - delete global_lib_state ; <nl> - global_lib_state = new_state ; <nl> + delete swap_global_state ( new_state ); <nl> } <nl>  <nl> Library_State * swap_global_state ( Library_State * new_state )
SecureVector < byte > generate_dsa_primes ( BigInt & p , BigInt & q , u32bit pbits ) <nl> BigInt random_prime ( u32bit bits , const BigInt & coprime , <nl> u32bit equiv , u32bit modulo ) <nl> { <nl> - if ( bits <= 48 ) <nl> + if ( bits < 48 ) <nl> throw Invalid_Argument (" random_prime : Can ' t make a prime of " + <nl> to_string ( bits ) + " bits "); <nl> 
hdb_next_enctype2key ( krb5_context context , <nl> { <nl> Key * k ; <nl>  <nl> - for ( k = * key ? * key : e -> keys . val ; <nl> + for ( k = * key ? (* key ) + 1 : e -> keys . val ; <nl> k < e -> keys . val + e -> keys . len ; <nl> k ++) <nl> if ( k -> key . keytype == enctype ){
char * <nl> hstrerror ( int herr ) <nl> { <nl> if ( 0 <= herr && herr < h_nerr ) <nl> - return h_errlist [ herr ]; <nl> + return ( char *) h_errlist [ herr ]; <nl> else <nl> return " Error number out of range ( hstrerror )"; <nl> }
krb5_expand_hostname_realms ( krb5_context context , <nl> free (* new_hostname ); <nl> } <nl> } <nl> + freeaddrinfo ( ai ); <nl> return vanilla_hostname ( context , orig_hostname , new_hostname , realms ); <nl> }
EVP_CipherInit_ex ( EVP_CIPHER_CTX * ctx , const EVP_CIPHER * c , ENGINE * engine , <nl> ctx -> cipher = c ; <nl> ctx -> key_len = c -> key_len ; <nl>  <nl> - ctx -> cipher_data = malloc ( c -> ctx_size ); <nl> + ctx -> cipher_data = calloc ( 1 , c -> ctx_size ); <nl> if ( ctx -> cipher_data == NULL && c -> ctx_size != 0 ) <nl> return 0 ; <nl> 
LDAP_message2entry ( krb5_context context , HDB * db , LDAPMessage * msg , <nl> ldap_value_free ( values ); <nl> } <nl>  <nl> - for ( i = 0 ; i < ent -> etypes -> len ; i ++) { <nl> - if ( ent -> etypes -> val [ i ] == ETYPE_ARCFOUR_HMAC_MD5 ) { <nl> + for ( i = 0 ; i < ent -> entry . etypes -> len ; i ++) { <nl> + if ( ent -> entry . etypes -> val [ i ] == ETYPE_ARCFOUR_HMAC_MD5 ) { <nl> have_arcfour = 1 ; <nl> break ; <nl> } <nl> hdb_ldap_common ( krb5_context context , <nl> (* db )-> hdb_nextkey = LDAP_nextkey ; <nl> (* db )-> hdb_lock = LDAP_lock ; <nl> (* db )-> hdb_unlock = LDAP_unlock ; <nl> - (* db )-> hdb_rename = LDAP_rename ; <nl> + (* db )-> hdb_rename = NULL ; <nl> (* db )-> hdb__get = NULL ; <nl> (* db )-> hdb__put = NULL ; <nl> (* db )-> hdb__del = NULL ;
OM_uint32 _gss_ntlm_release_name <nl> { <nl> if ( minor_status ) <nl> * minor_status = 0 ; <nl> - if ( input_name ) <nl> + if ( input_name ) { <nl> + free (* input_name ); <nl> * input_name = GSS_C_NO_NAME ; <nl> + } <nl> return GSS_S_COMPLETE ; <nl> }
krb5_free_ticket ( krb5_context context , <nl> free_EncTicketPart (& ticket -> ticket ); <nl> krb5_free_principal ( context , ticket -> client ); <nl> krb5_free_principal ( context , ticket -> server ); <nl> + free ( ticket ); <nl> return 0 ; <nl> } <nl> 
format_keytype ( krb5_key_data * k , krb5_salt * def_salt , char * buf , size_t buf_len ) <nl> ( char *) k -> key_data_contents [ 1 ]); <nl> strlcat ( buf , s , buf_len ); <nl> free ( s ); <nl> - <nl> + asprintf (& s , "[% d ]", k -> key_data_kvno ); <nl> strlcat ( buf , ")", buf_len ); <nl> + <nl> + strlcat ( buf , s , buf_len ); <nl> + free ( s ); <nl> } <nl>  <nl> static void
# ifndef KRB5_DEPRECATED <nl> # if defined ( __GNUC__ ) && (( __GNUC__ > 3 ) || (( __GNUC__ == 3 ) && ( __GNUC_MINOR__ >= 1 ))) <nl> # define KRB5_DEPRECATED __attribute__ (( deprecated )) <nl> -# elif defined ( _MSC_VER ) <nl> +# elif defined ( _MSC_VER ) && ( _MSC_VER > 1200 ) <nl> # define KRB5_DEPRECATED __declspec ( deprecated ) <nl> # else <nl> # define KRB5_DEPRECATED
krb4_verify_password ( POP * p ) <nl> krb_get_err_text ( status )); <nl> return 1 ; <nl> } <nl> - snprintf ( tkt , sizeof ( tkt ), <nl> - TKT_ROOT " _popper .% u ", ( unsigned ) getpid ()); <nl> + snprintf ( tkt , sizeof ( tkt ), "% s_popper .% u ", TKT_ROOT , ( unsigned ) getpid ()); <nl> krb_set_tkt_string ( tkt ); <nl>  <nl> status = krb_verify_user ( p -> user , "", lrealm ,
# include " config . h " <nl>  <nl> # include < sys / types . h > <nl> +# include < sys / socket . h > <nl> +# ifdef HAVE_SYS_UN_H <nl> # include < sys / un . h > <nl> +# endif <nl>  <nl> # include < sys / poll . h > <nl>  <nl> # include < krb5 - types . h > <nl> # include < asn1 - common . h > <nl>  <nl> -# ifdef HAVE_SYS_UN_H <nl> -# include < sys / un . h > <nl> -# endif <nl> - <nl> # include < heimbase . h > <nl> # include < base64 . h > <nl> 
hx509_revoke_free ( hx509_revoke_ctx * revoke ) <nl> free ((* revoke )-> crls . val [ i ]. path ); <nl> free_CRLCertificateList (&(* revoke )-> crls . val [ i ]. crl ); <nl> } <nl> + free ((* revoke )-> crls . val ); <nl>  <nl> memset (* revoke , 0 , sizeof (** revoke )); <nl> free (* revoke ); <nl> hx509_revoke_add_crl ( hx509_context context , <nl> { <nl> size_t length , size ; <nl> void * data ; <nl> - int ret , i ; <nl> + size_t i ; <nl> + int ret ; <nl>  <nl> if ( strncmp ( path , " FILE :", 5 ) != 0 ) <nl> return EINVAL ;
read_limits_conf ( const char * file , const struct passwd * pwd ) <nl>  <nl> lineno ++; <nl>  <nl> + if ( buf [ 0 ] == '\ 0 ') { <nl> + syslog ( LOG_ERR , "% s : line % d : NUL character ", file , lineno ); <nl> + continue ; <nl> + } <nl> if ( buf [ strlen ( buf ) - 1 ] != '\ n ') { <nl> /* file did not end with a newline , figure out if we ' re at <nl> the EOF , or if our buffer was too small */
del_enctype ( void * opt , int argc , char ** argv ) <nl> goto out2 ; <nl> } <nl>  <nl> + if ( kadm5_all_keys_are_bogus ( princ . n_key_data , princ . key_data )) { <nl> + krb5_warnx ( context , " user lacks get - keys privilege "); <nl> + goto out ; <nl> + } <nl> + <nl> new_key_data = malloc ( princ . n_key_data * sizeof (* new_key_data )); <nl> if ( new_key_data == NULL && princ . n_key_data != 0 ) { <nl> krb5_warnx ( context , " out of memory ");
/* <nl> - * Copyright ( c ) 1997 , 1998 Kungliga Tekniska Hgskolan <nl> + * Copyright ( c ) 1997 - 2000 Kungliga Tekniska Hgskolan <nl> * ( Royal Institute of Technology , Stockholm , Sweden ). <nl> * All rights reserved . <nl> * <nl> void com_err __P (( const char *, long , const char *, ...)); <nl> errf set_com_err_hook __P (( errf )); <nl> errf reset_com_err_hook __P (( void )); <nl>  <nl> - const char * error_table_name ( int num ); <nl> + const char * error_table_name __P (( int num )); <nl>  <nl> # endif /* __COM_ERR_H__ */
DNSlookup_name ( <nl> { <nl> char buffer [ sizeof ( WSAQUERYSET ) + 2048 ]; <nl> WSAQUERYSET query ; <nl> - hostent_t * addr = NULL ; <nl> + struct hostent * addr = NULL ; <nl> char * bufaddr = NULL ; <nl> char ** addrlist = & bufaddr ; <nl> int addrcnt = 0 ; <nl> DNSlookup_name ( <nl> */ <nl> retcode = EAI_NONAME ; <nl> dwLength = sizeof ( buffer ); <nl> - * addrlist = NULL ; <nl>  <nl> while ( err == 0 ) /* Drop out when error */ <nl> { <nl> DNSlookup_name ( <nl> memset ( addr , 0 , sizeof ( struct hostent )); <nl> addr -> h_addrtype = ( short ) results -> lpcsaBuffer -> iSocketType ; <nl> addr -> h_length = sizeof ( struct in_addr ); /* Only passing back the address */ <nl> + addrlist = malloc ( sizeof ( char *)); <nl> + * addrlist = NULL ; <nl> } <nl> for ( i = 0 ; i < results -> dwNumberOfCsAddrs ; i ++) <nl> { <nl> DNSlookup_name ( <nl> addr -> h_name = ( char *) name ; <nl> addr -> h_addr_list = addrlist ; <nl> retcode = 0 ; <nl> + * Addresses = addr ; <nl> } <nl> else <nl> { <nl> DNSlookup_name ( <nl> # endif <nl> retcode = ReturnCode ( errcode ); <nl> } <nl> - * Addresses = addr ; <nl> WSALookupServiceEnd ( handle ); <nl> return ( retcode ); <nl> }
int rdp_redirection_apply_settings ( rdpRdp * rdp ) <nl> settings -> TargetNetAddresses [ i ] = _strdup ( redirection -> TargetNetAddresses [ i ]); <nl> if (! settings -> TargetNetAddresses [ i ]) <nl> { <nl> - for (; i > 0 ; -- i ) <nl> - free ( settings -> TargetNetAddresses [ i ]); <nl> + UINT32 j ; <nl> + <nl> + for ( j = 0 ; j < i ; j ++) <nl> + free ( settings -> TargetNetAddresses [ j ]); <nl> return - 1 ; <nl> } <nl> }
void winpr_HexDump ( const char * tag , int level , const BYTE * data , int length ) <nl> const BYTE * p = data ; <nl> int i , line , offset = 0 ; <nl> const size_t llen = ( length > WINPR_HEXDUMP_LINE_LENGTH ) ? WINPR_HEXDUMP_LINE_LENGTH : length ; <nl> - size_t blen = 5 + llen * 5 ; <nl> + size_t blen = 7 + WINPR_HEXDUMP_LINE_LENGTH * 5 ; <nl> size_t pos = 0 ; <nl> char * buffer = malloc ( blen ); <nl> 
BOOL nla_read_ts_password_creds ( rdpNla * nla , wStream * s ) <nl> nla -> identity -> Password = NULL ; <nl> nla -> identity -> PasswordLength = ( UINT32 ) 0 ; <nl>  <nl> + if (! ber_read_sequence_tag ( s , & length )) <nl> + return FALSE ; <nl> + <nl> /* The sequence is empty , return early , <nl> * TSPasswordCreds ( SEQUENCE ) is optional . */ <nl> - if (! ber_read_sequence_tag ( s , & length )) <nl> + if ( length == 0 ) <nl> return TRUE ; <nl>  <nl> /* [ 0 ] domainName ( OCTET STRING ) */
int makecert_context_process ( MAKECERT_CONTEXT * context , int argc , char ** argv ) <nl> if (! rsa ) <nl> return - 1 ; <nl>  <nl> + context -> rsa = RSA_new (); <nl> + if (! context -> rsa ) <nl> + { <nl> + BN_clear_free ( rsa ); <nl> + return - 1 ; <nl> + } <nl> BN_set_word ( rsa , RSA_F4 ); <nl> rc = RSA_generate_key_ex ( context -> rsa , key_length , rsa , NULL ); <nl> BN_clear_free ( rsa );
static void * rdpei_schedule_thread ( void * arg ) <nl>  <nl> out : <nl>  <nl> - if ( error && rdpei -> rdpcontext ) <nl> + if ( error && rdpei && rdpei -> rdpcontext ) <nl> setChannelError ( rdpei -> rdpcontext , error , <nl> " rdpei_schedule_thread reported an error "); <nl> 
BOOL xf_event_process ( freerdp * instance , XEvent * event ) <nl> if ( event -> xcookie . type == GenericEvent && <nl> event -> xcookie . extension == xfi -> XInputOpcode ) <nl> { <nl> - switch ( cookie . evtype ) <nl> + switch ( cookie -> evtype ) <nl> { <nl> case XI_ButtonPress : <nl> case XI_Motion :
void transport_free ( rdpTransport * transport ) <nl> { <nl> if ( transport ) <nl> { <nl> + if ( transport -> async ) <nl> + { <nl> + assert (! transport -> thread ); <nl> + assert (! transport -> stopEvent ); <nl> + } <nl> + <nl> if ( transport -> ReceiveBuffer ) <nl> Stream_Release ( transport -> ReceiveBuffer ); <nl> 
void glyph_cache_put ( rdpGlyphCache * glyph_cache , UINT32 id , UINT32 index , rdpGly <nl> if ( prevGlyph != NULL ) <nl> { <nl> Glyph_Free ( glyph_cache -> context , prevGlyph ); <nl> - free ( prevGlyph -> aj ); <nl> + if ( NULL != prevGlyph -> aj ) <nl> + free ( prevGlyph -> aj ); <nl> free ( prevGlyph ); <nl> } <nl>  <nl> void glyph_cache_free ( rdpGlyphCache * glyph_cache ) <nl> if ( glyph != NULL ) <nl> { <nl> Glyph_Free ( glyph_cache -> context , glyph ); <nl> - free ( glyph -> aj ); <nl> + if ( glyph -> aj ) <nl> + free ( glyph -> aj ); <nl> free ( glyph ); <nl> glyph_cache -> glyphCache [ i ]. entries [ j ] = NULL ; <nl> } <nl> } <nl> free ( glyph_cache -> glyphCache [ i ]. entries ); <nl> + glyph_cache -> glyphCache [ i ]. entries = NULL ; <nl> } <nl>  <nl> for ( i = 0 ; i < 255 ; i ++)
void tsmf_playback_ack ( IWTSVirtualChannelCallback * pChannelCallback , <nl> if (! callback || ! callback -> channel || ! callback -> channel -> Write ) <nl> { <nl> WLog_ERR ( TAG , " callback =% p , channel =% p , write =% p ", callback , <nl> - callback -> channel , callback -> channel -> Write ); <nl> + callback ? callback -> channel : NULL , <nl> + ( callback && callback -> channel ) ? callback -> channel -> Write : NULL ); <nl> } <nl> else <nl> {
static int libavcodec_compress ( H264_CONTEXT * h264 , BYTE ** ppDstData , UINT32 * pDs <nl> if (! libavcodec_create_encoder ( h264 )) <nl> return - 1 ; <nl>  <nl> +# if LIBAVUTIL_VERSION_INT >= AV_VERSION_INT ( 55 , 39 , 100 ) <nl> av_packet_unref (& sys -> packet ); <nl> +# else <nl> + av_free ( sys -> packet . data ); <nl> +# endif <nl> av_init_packet (& sys -> packet ); <nl> sys -> packet . data = NULL ; <nl> sys -> packet . size = 0 ; <nl> static int libavcodec_compress ( H264_CONTEXT * h264 , BYTE ** ppDstData , UINT32 * pDs <nl> # if LIBAVUTIL_VERSION_INT >= AV_VERSION_INT ( 52 , 48 , 100 ) <nl> sys -> videoFrame -> colorspace = AVCOL_SPC_BT709 ; <nl> # endif <nl> +# if LIBAVUTIL_VERSION_INT >= AV_VERSION_INT ( 52 , 92 , 100 ) <nl> sys -> videoFrame -> chroma_location = AVCHROMA_LOC_LEFT ; <nl> +# endif <nl> sys -> videoFrame -> data [ 0 ] = h264 -> pYUVData [ 0 ]; <nl> sys -> videoFrame -> data [ 1 ] = h264 -> pYUVData [ 1 ]; <nl> sys -> videoFrame -> data [ 2 ] = h264 -> pYUVData [ 2 ];
INT32 progressive_decompress ( PROGRESSIVE_CONTEXT * progressive , <nl>  <nl> if ( progressive -> cTiles < surface -> gridSize ) <nl> { <nl> - progressive -> tiles = ( RFX_PROGRESSIVE_TILE **) realloc ( progressive -> tiles , <nl> + BYTE * tmpBuf = ( RFX_PROGRESSIVE_TILE **) realloc ( progressive -> tiles , <nl> surface -> gridSize * sizeof ( RFX_PROGRESSIVE_TILE *)); <nl> + if (! tmpBuf ) <nl> + return - 1025 ; <nl> + <nl> + progressive -> tiles = tmpBuf ; <nl> progressive -> cTiles = surface -> gridSize ; <nl> } <nl> 
static BOOL update_recv_secondary_order ( rdpUpdate * update , wStream * s , BYTE flag <nl> name , end - start ); <nl> return FALSE ; <nl> } <nl> - diff = start - end ; <nl> + diff = end - start ; <nl> if ( diff > 0 ) <nl> { <nl> WLog_Print ( update -> log , WLOG_DEBUG , <nl> " SECONDARY_ORDER % s : read %" PRIuz " bytes short , skipping ", name , diff ); <nl> - Stream_Seek ( s , diff ); <nl> + if (! Stream_SafeSeek ( s , diff )) <nl> + return FALSE ; <nl> } <nl> return rc ; <nl> }
namespace mongo { <nl> int retMissing ( const ElementMatcher & bm ) { <nl> if ( bm . compareOp != BSONObj :: opEXISTS ) <nl> return 0 ; <nl> - return ( bm . toMatch . boolean () ^ bm . isNot ) ? - 1 : 1 ; <nl> + return bm . toMatch . boolean () ? - 1 : 1 ; <nl> } <nl>  <nl> /* Check if a particular field matches . <nl> namespace mongo { <nl> BSONElement & m = bm . toMatch ; <nl> // - 1 = mismatch . 0 = missing element . 1 = match <nl> int cmp = matchesDotted ( m . fieldName (), m , jsobj , bm . compareOp , bm , false , details ); <nl> - if ( bm . compareOp != BSONObj :: opEXISTS && bm . isNot ) <nl> + if ( bm . isNot ) <nl> cmp = - cmp ; <nl> if ( cmp < 0 ) <nl> return false ;
 <nl> # include " mongo / db / catalog / collection . h " <nl> # include " mongo / db / client . h " <nl> +# include " mongo / db / concurrency / write_conflict_exception . h " <nl> # include " mongo / db / db_raii . h " <nl> # include " mongo / db / dbhelpers . h " <nl> # include " mongo / db / exec / working_set_common . h " <nl> StatusWith < int > CollectionRangeDeleter :: _doDeletion ( OperationContext * opCtx , <nl> break ; <nl> } <nl> invariant ( PlanExecutor :: ADVANCED == state ); <nl> - { <nl> + <nl> + MONGO_WRITE_CONFLICT_RETRY_LOOP_BEGIN { <nl> WriteUnitOfWork wuow ( opCtx ); <nl> if ( saver ) { <nl> saver -> goingToDelete ( obj ); <nl> } <nl> collection -> deleteDocument ( opCtx , rloc , nullptr , true ); <nl> - <nl> wuow . commit (); <nl> } <nl> + MONGO_WRITE_CONFLICT_RETRY_LOOP_END ( opCtx , " delete range ", nss . ns ()); <nl> + <nl> } while (++ numDeleted < maxToDelete ); <nl>  <nl> return numDeleted ;
public : <nl>  <nl> // check if we ' re outputting to stdout <nl> FilePtr out (( mongoDumpGlobalParams . outputDirectory == "-") ? <nl> - stdout : fopen ( root . c_str (), " wb ")); <nl> + stdout : fopen ( root . string (). c_str (), " wb ")); <nl>  <nl> Extent * extent = em . getExtent ( extentLoc ); <nl> while ( extent != NULL ) { <nl> public : <nl>  <nl> // check if we ' re outputting to stdout <nl> FilePtr out (( mongoDumpGlobalParams . outputDirectory == "-") ? <nl> - stdout : fopen ( root . c_str (), " wb ")); <nl> + stdout : fopen ( root . string (). c_str (), " wb ")); <nl> string ns = getNS (); <nl> Client :: ReadContext cx ( ns ); <nl> Database * db = cx . ctx (). db ();
namespace mongo { <nl> kill_wrapper ( pid , signal , port ); <nl>  <nl> int i = 0 ; <nl> - for ( ; i < 65 ; ++ i ) { <nl> - if ( i == 5 ) { <nl> + for ( ; i < 130 ; ++ i ) { <nl> + if ( i == 30 ) { <nl> char now [ 64 ]; <nl> time_t_to_String ( time ( 0 ), now ); <nl> now [ 20 ] = 0 ;
namespace mongo { <nl> if ( boost :: filesystem :: exists ( nsPath ) ) { <nl> p = f . map ( pathString . c_str ()); <nl> len = f . length (); <nl> - uassert ( " bad . ns file length , cannot open database ", len % ( 1024 * 1024 ) == 0 ); <nl> + if ( len % ( 1024 * 1024 ) != 0 ){ <nl> + log () << " bad . ns file : " << pathString << endl ; <nl> + uassert ( " bad . ns file length , cannot open database ", len % ( 1024 * 1024 ) == 0 ); <nl> + } <nl> } <nl> else { <nl> // use lenForNewNsFiles , we are making a new database
namespace mongo { <nl> } <nl> if ( bestmatchlen < INT_MAX && -- extra <= 0 ) <nl> break ; <nl> - if ( ++ chain > 30 && b < MaxBucket ) { <nl> + if ( ++ chain > 30 && b <= MaxBucket ) { <nl> // too slow , force move to next bucket to grab a big chunk <nl> // b ++; <nl> freelistIterations . increment ( chain );
namespace mongo { <nl> if ( p == buf ) { <nl> if ( sz <= SZ ) return buf ; <nl> void * d = malloc ( sz ); <nl> + if ( d == 0 ) <nl> + msgasserted ( 15912 , " out of memory StackAllocator :: Realloc " ); <nl> memcpy ( d , p , SZ ); <nl> return d ; <nl> } <nl> namespace mongo { <nl> if ( maxSize && size > maxSize ) { <nl> al . Free ( data ); <nl> data = ( char *) al . Malloc ( maxSize ); <nl> + if ( data == 0 ) <nl> + msgasserted ( 15913 , " out of memory BufBuilder :: reset " ); <nl> size = maxSize ; <nl> } <nl> }
namespace { <nl> client -> getOperationContext ()-> lockState ()-> getLockerInfo (& lockerInfo ); <nl> fillLockerInfo ( lockerInfo , infoBuilder ); <nl> } <nl> + else { <nl> + // If no operation context , mark the operation as inactive <nl> + infoBuilder . append (" active ", false ); <nl> + } <nl>  <nl> infoBuilder . done (); <nl> 
RuntimeDyldImpl :: loadObjectImpl ( const object :: ObjectFile & Obj ) { <nl> { <nl> GlobalSymbolTable [ Name ] = SymbolInfo ( SectionID , SectOffset , Vis ); <nl> } else { <nl> - if ( object :: SymbolRef :: ST_Data ) { <nl> + if ( SymType == object :: SymbolRef :: ST_Data ) { <nl> WeakDataSymbolTable [ Name ] = SymbolInfo ( SectionID , SectOffset , Vis ); <nl> } else { <nl> WeakFuncSymbolTable [ Name ] = SymbolInfo ( SectionID , SectOffset , Vis );
namespace cling { <nl> Interpreter :: Interpreter ( int argc , const char * const * argv , <nl> const char * llvmdir /*= 0 */) : <nl> m_UniqueCounter ( 0 ), m_PrintDebug ( false ), <nl> - m_DynamicLookupEnabled ( false ), m_RawInputEnabled ( false ) { <nl> + m_DynamicLookupEnabled ( false ), m_RawInputEnabled ( false ), <nl> + m_LastCustomPragmaDiagPopPoint (){ <nl>  <nl> m_LLVMContext . reset ( new llvm :: LLVMContext ); <nl> std :: vector < unsigned > LeftoverArgsIdx ;
public : <nl>  <nl> TTreeReader (): <nl> fDirectory ( 0 ), <nl> - fEntryStatus ( kEntryNoTree ) <nl> + fEntryStatus ( kEntryNoTree ), <nl> + fDirector ( 0 ) <nl> {} <nl>  <nl> TTreeReader ( TTree * tree );
-// @(#) root / geom :$ Name : $:$ Id : TVirtualGeoTrack . h , v 1 . 2 2003 / 06 / 17 09 : 13 : 55 brun Exp $ <nl> +// @(#) root / geom :$ Name : $:$ Id : TVirtualGeoTrack . h , v 1 . 3 2003 / 06 / 17 19 : 46 : 10 brun Exp $ <nl> // Author : Andrei Gheata 2003 / 04 / 10 <nl>  <nl> /************************************************************************* <nl> public : <nl> Bool_t IsInTimeRange () const ; <nl> virtual void Paint ( Option_t * option ="") = 0 ; <nl> virtual void PaintCollect ( Double_t /* time */, Double_t * /* box */) {;} <nl> - virtual void PaintCollectTrack ( Double_t /* time */, Double_t */* box */) {;} <nl> + virtual void PaintCollectTrack ( Double_t /* time */, Double_t * /* box */) {;} <nl> virtual void PaintTrack ( Option_t * option ="") = 0 ; <nl> virtual void ResetTrack () = 0 ; <nl> void SetName ( const char * name );
static void setup_window ( AVFormatContext * s ) <nl> uint32_t values [] = { 1 , <nl> XCB_EVENT_MASK_EXPOSURE | <nl> XCB_EVENT_MASK_STRUCTURE_NOTIFY }; <nl> - xcb_rectangle_t rect = { 0 , 0 , c -> width , c -> height }; <nl> + av_unused xcb_rectangle_t rect = { 0 , 0 , c -> width , c -> height }; <nl>  <nl> c -> window = xcb_generate_id ( c -> conn ); <nl> 
static av_always_inline void predict ( PredictorState * ps , int * coef , <nl> if ( output_enable ) { <nl> int shift = 28 - pv . exp ; <nl>  <nl> - if ( shift < 31 ) <nl> - * coef += ( pv . mant + ( 1 << ( shift - 1 ))) >> shift ; <nl> + if ( shift < 31 ) { <nl> + if ( shift > 0 ) { <nl> + * coef += ( pv . mant + ( 1 << ( shift - 1 ))) >> shift ; <nl> + } else <nl> + * coef += pv . mant << - shift ; <nl> + } <nl> } <nl>  <nl> e0 = av_int2sf (* coef , 2 );
static void render_slice ( Vp3DecodeContext * s , int slice ) <nl> /* invert DCT and place ( or add ) in final output */ <nl>  <nl> if ( s -> all_fragments [ i ]. coding_method == MODE_INTRA ) { <nl> - int index ; <nl> - index = vp3_dequant ( s , s -> all_fragments + i , plane , 0 , block ); <nl> - if ( index > 63 ) <nl> - continue ; <nl> + vp3_dequant ( s , s -> all_fragments + i , plane , 0 , block ); <nl> if ( s -> avctx -> idct_algo != FF_IDCT_VP3 ) <nl> block [ 0 ] += 128 << 3 ; <nl> s -> dsp . idct_put ( <nl> static void render_slice ( Vp3DecodeContext * s , int slice ) <nl> stride , <nl> block ); <nl> } else { <nl> - int index = vp3_dequant ( s , s -> all_fragments + i , plane , 1 , block ); <nl> - if ( index > 63 ) <nl> - continue ; <nl> - if ( index > 0 ) { <nl> + if ( vp3_dequant ( s , s -> all_fragments + i , plane , 1 , block )) { <nl> s -> dsp . idct_add ( <nl> output_plane + first_pixel , <nl> stride ,
static av_cold void uninit ( AVFilterContext * ctx ) <nl> FrameRateContext * s = ctx -> priv ; <nl> int i ; <nl>  <nl> - for ( i = s -> frst + 1 ; i > s -> last ; i ++) { <nl> + for ( i = s -> frst + 1 ; i < s -> last ; i ++) { <nl> if ( s -> srce [ i ] && ( s -> srce [ i ] != s -> srce [ i + 1 ])) <nl> av_frame_free (& s -> srce [ i ]); <nl> }
static void compute_pkt_fields ( AVFormatContext * s , AVStream * st , <nl> if ( pkt -> dts != AV_NOPTS_VALUE && <nl> pkt -> pts == AV_NOPTS_VALUE && <nl> st -> last_IP_duration > 0 && <nl> - ( st -> cur_dts - next_dts ) <= 1 && <nl> + (( uint64_t ) st -> cur_dts - ( uint64_t ) next_dts + 1 ) <= 2 && <nl> next_dts != next_pts && <nl> next_pts != AV_NOPTS_VALUE ) <nl> pkt -> pts = next_dts ;
static void dca_exss_parse_header ( DCAContext * s ) <nl> } <nl> } <nl>  <nl> + av_assert0 ( num_assets > 0 ); // silence a warning <nl> + <nl> for ( i = 0 ; i < num_assets ; i ++) <nl> asset_size [ i ] = get_bits_long (& s -> gb , 16 + 4 * blownup ); <nl> 
static int rprobe ( AVFormatContext * s , uint8_t * enc_header , const uint8_t * r_val ) <nl> return memcmp (& enc_header [ pos ], oc -> sm_val , 8 ) ? - 1 : 0 ; <nl> } <nl>  <nl> - static int nprobe ( AVFormatContext * s , uint8_t * enc_header , const uint8_t * n_val ) <nl> + static int nprobe ( AVFormatContext * s , uint8_t * enc_header , int size , const uint8_t * n_val ) <nl> { <nl> OMAContext * oc = s -> priv_data ; <nl> uint32_t pos , taglen , datalen ; <nl> static int nprobe ( AVFormatContext * s , uint8_t * enc_header , const uint8_t * n_val ) <nl> taglen = AV_RB32 (& enc_header [ pos + 32 ]); <nl> datalen = AV_RB32 (& enc_header [ pos + 36 ]) >> 4 ; <nl>  <nl> + if ( taglen + ((( uint64_t ) datalen )<< 4 ) + 44 > size ) <nl> + return - 1 ; <nl> + <nl> pos += 44 + taglen ; <nl>  <nl> av_des_init (& av_des , n_val , 192 , 1 ); <nl> static int decrypt_init ( AVFormatContext * s , ID3v2ExtraMeta * em , uint8_t * header ) <nl> } <nl> if (! memcmp ( oc -> r_val , ( const uint8_t [ 8 ]){ 0 }, 8 ) || <nl> rprobe ( s , gdata , oc -> r_val ) < 0 && <nl> - nprobe ( s , gdata , oc -> n_val ) < 0 ) { <nl> + nprobe ( s , gdata , geob -> datasize , oc -> n_val ) < 0 ) { <nl> int i ; <nl> for ( i = 0 ; i < FF_ARRAY_ELEMS ( leaf_table ); i += 2 ) { <nl> uint8_t buf [ 16 ]; <nl> AV_WL64 ( buf , leaf_table [ i ]); <nl> AV_WL64 (& buf [ 8 ], leaf_table [ i + 1 ]); <nl> kset ( s , buf , buf , 16 ); <nl> - if (! rprobe ( s , gdata , oc -> r_val ) || ! nprobe ( s , gdata , oc -> n_val )) <nl> + if (! rprobe ( s , gdata , oc -> r_val ) || ! nprobe ( s , gdata , geob -> datasize , oc -> n_val )) <nl> break ; <nl> } <nl> if ( i >= sizeof ( leaf_table )) {
av_cold int ff_MPV_encode_init ( AVCodecContext * avctx ) <nl> if ( avctx -> max_b_frames > MAX_B_FRAMES ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Too many B - frames requested , maximum " <nl> " is % d .\ n ", MAX_B_FRAMES ); <nl> + avctx -> max_b_frames = MAX_B_FRAMES ; <nl> } <nl> s -> max_b_frames = avctx -> max_b_frames ; <nl> s -> codec_id = avctx -> codec -> id ;
static void json_print_section_header ( WriterContext * wctx ) <nl> json -> indent_level ++; <nl> if ( section -> flags & SECTION_FLAG_IS_ARRAY ) { <nl> printf ("\"% s \": [\ n ", buf . str ); <nl> - } else if (!( parent_section -> flags & SECTION_FLAG_IS_ARRAY )) { <nl> + } else if ( parent_section && !( parent_section -> flags & SECTION_FLAG_IS_ARRAY )) { <nl> printf ("\"% s \": {% s ", buf . str , json -> item_start_end ); <nl> } else { <nl> printf ("{% s ", json -> item_start_end ); <nl>  <nl> /* this is required so the parser can distinguish between packets and frames */ <nl> - if ( parent_section -> id == SECTION_ID_PACKETS_AND_FRAMES ) { <nl> + if ( parent_section && parent_section -> id == SECTION_ID_PACKETS_AND_FRAMES ) { <nl> if (! json -> compact ) <nl> JSON_INDENT (); <nl> printf ("\" type \": \"% s \"% s ", section -> name , json -> item_sep );
static const AVOption swscale_options [] = { <nl> { " neighbor ", " nearest neighbor ", 0 , AV_OPT_TYPE_CONST , { . i64 = SWS_POINT }, INT_MIN , INT_MAX , VE , " sws_flags " }, <nl> { " area ", " averaging area ", 0 , AV_OPT_TYPE_CONST , { . i64 = SWS_AREA }, INT_MIN , INT_MAX , VE , " sws_flags " }, <nl> { " bicublin ", " luma bicubic , chroma bilinear ", 0 , AV_OPT_TYPE_CONST , { . i64 = SWS_BICUBLIN }, INT_MIN , INT_MAX , VE , " sws_flags " }, <nl> - { " gauss ", " gaussian ", 0 , AV_OPT_TYPE_CONST , { . i64 = SWS_GAUSS }, INT_MIN , INT_MAX , VE , " sws_flags " }, <nl> + { " gauss ", " Gaussian ", 0 , AV_OPT_TYPE_CONST , { . i64 = SWS_GAUSS }, INT_MIN , INT_MAX , VE , " sws_flags " }, <nl> { " sinc ", " sinc ", 0 , AV_OPT_TYPE_CONST , { . i64 = SWS_SINC }, INT_MIN , INT_MAX , VE , " sws_flags " }, <nl> - { " lanczos ", " lanczos ", 0 , AV_OPT_TYPE_CONST , { . i64 = SWS_LANCZOS }, INT_MIN , INT_MAX , VE , " sws_flags " }, <nl> + { " lanczos ", " Lanczos ", 0 , AV_OPT_TYPE_CONST , { . i64 = SWS_LANCZOS }, INT_MIN , INT_MAX , VE , " sws_flags " }, <nl> { " spline ", " natural bicubic spline ", 0 , AV_OPT_TYPE_CONST , { . i64 = SWS_SPLINE }, INT_MIN , INT_MAX , VE , " sws_flags " }, <nl> { " print_info ", " print info ", 0 , AV_OPT_TYPE_CONST , { . i64 = SWS_PRINT_INFO }, INT_MIN , INT_MAX , VE , " sws_flags " }, <nl> { " accurate_rnd ", " accurate rounding ", 0 , AV_OPT_TYPE_CONST , { . i64 = SWS_ACCURATE_RND }, INT_MIN , INT_MAX , VE , " sws_flags " },
static void celt_denormalize ( CeltFrame * f , CeltBlock * block , float * data ) <nl>  <nl> for ( i = f -> start_band ; i < f -> end_band ; i ++) { <nl> float * dst = data + ( ff_celt_freq_bands [ i ] << f -> size ); <nl> - float norm = exp2f ( block -> energy [ i ] + ff_celt_mean_energy [ i ]); <nl> + float log_norm = block -> energy [ i ] + ff_celt_mean_energy [ i ]; <nl> + float norm = exp2f ( FFMIN ( log_norm , 32 . 0f )); <nl>  <nl> for ( j = 0 ; j < ff_celt_freq_range [ i ] << f -> size ; j ++) <nl> dst [ j ] *= norm ;
static int decode_block ( AVCodecContext * avctx , void * tdata , <nl>  <nl> td -> ysize = FFMIN ( s -> tile_attr . ySize , s -> ydelta - tileY * s -> tile_attr . ySize ); <nl> td -> xsize = FFMIN ( s -> tile_attr . xSize , s -> xdelta - tileX * s -> tile_attr . xSize ); <nl> - uncompressed_size = s -> current_channel_offset * td -> ysize * td -> xsize ; <nl> + uncompressed_size = s -> current_channel_offset * ( uint64_t ) td -> ysize * td -> xsize ; <nl>  <nl> if ( col ) { /* not the first tile of the line */ <nl> bxmin = 0 ; axmax = 0 ; /* doesn ' t add pixel at the left of the datawindow */
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> bytestream2_init (& s -> gb , avctx -> extradata , avctx -> extradata_size ); <nl> - if ( bytestream2_get_bytes_left (& s -> gb ) < 16 * 8 + 4 * 256 ) <nl> + if ( bytestream2_get_bytes_left (& s -> gb ) < 16 * 8 + 4 * 256 ) { <nl> + av_frame_free (& s -> frame ); <nl> return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> bytestream2_skipu (& s -> gb , 16 * 8 ); <nl> for ( i = 0 ; i < 256 ; i ++)
static int tcp_write_packet ( AVFormatContext * s , RTSPStream * rtsp_st ) <nl> interleave_header [ 0 ] = '$'; <nl> interleave_header [ 1 ] = id ; <nl> AV_WB16 ( interleave_header + 2 , packet_len ); <nl> - url_write ( rt -> rtsp_hd , interleaved_packet , 4 + packet_len ); <nl> + url_write ( rt -> rtsp_hd_out , interleaved_packet , 4 + packet_len ); <nl> ptr += packet_len ; <nl> size -= packet_len ; <nl> }
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * in ) <nl> AVFilterLink * outlink = inlink -> dst -> outputs [ 0 ]; <nl>  <nl> AVFilterBufferRef * out ; <nl> - int direct , c ; <nl> + int direct = 0 , c ; <nl>  <nl> if ( in -> perms & AV_PERM_WRITE ) { <nl> direct = 1 ;
static void hls_prediction_unit ( HEVCContext * s , int x0 , int y0 , <nl>  <nl> MvField * tab_mvf = s -> ref -> tab_mvf ; <nl> RefPicList * refPicList = s -> ref -> refPicList ; <nl> - HEVCFrame * ref0 , * ref1 ; <nl> + HEVCFrame * ref0 = NULL , * ref1 = NULL ; <nl> uint8_t * dst0 = POS ( 0 , x0 , y0 ); <nl> uint8_t * dst1 = POS ( 1 , x0 , y0 ); <nl> uint8_t * dst2 = POS ( 2 , x0 , y0 );
double parse_number_or_die ( const char * context , const char * numstr , int type , do <nl> error = " The value for % s was % s which is not within % f - % f \ n "; <nl> else if ( type == OPT_INT64 && ( int64_t ) d != d ) <nl> error = " Expected int64 for % s but found % s \ n "; <nl> + else if ( type == OPT_INT && ( int ) d != d ) <nl> + error = " Expected int for % s but found % s \ n "; <nl> else <nl> return d ; <nl> fprintf ( stderr , error , context , numstr , min , max );
static int parse_chunks ( AVFormatContext * s , int mode , int64_t seekts , int * len_p <nl> } <nl> } else if (! ff_guidcmp ( g , ff_stream2_guid )) { <nl> int stream_index = ff_find_stream_index ( s , sid ); <nl> - if ( stream_index >= 0 && !(( WtvStream *) s -> streams [ stream_index ]-> priv_data )-> seen_data ) { <nl> + if ( stream_index >= 0 && s -> streams [ stream_index ]-> priv_data && !(( WtvStream *) s -> streams [ stream_index ]-> priv_data )-> seen_data ) { <nl> ff_asf_guid mediatype , subtype , formattype ; <nl> int size ; <nl> avio_skip ( pb , 12 );
static int decode_residuals ( FLACContext * s , int32_t * decoded , int pred_order ) <nl> rice_order = get_bits (& s -> gb , 4 ); <nl>  <nl> samples = s -> blocksize >> rice_order ; <nl> + if ( samples << rice_order != s -> blocksize ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " invalid rice order : % i blocksize % i \ n ", <nl> + rice_order , s -> blocksize ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( pred_order > samples ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " invalid predictor order : % i > % i \ n ", <nl> pred_order , samples );
static void gmc_mmx ( uint8_t * dst , uint8_t * src , <nl> const int dyh = ( dyy - ( 1 << ( 16 + shift ))) * ( h - 1 ); <nl> const int dxh = dxy * ( h - 1 ); <nl> const int dyw = dyx * ( w - 1 ); <nl> - int need_emu = ( unsigned ) ix >= width - w || <nl> - ( unsigned ) iy >= height - h ; <nl> + int need_emu = ( unsigned ) ix >= width - w || width < w || <nl> + ( unsigned ) iy >= height - h || height < h <nl> + ; <nl>  <nl> if ( // non - constant fullpel offset ( 3 % of blocks ) <nl> (( ox ^ ( ox + dxw )) | ( ox ^ ( ox + dxh )) | ( ox ^ ( ox + dxw + dxh )) |
static int vqf_read_header ( AVFormatContext * s ) <nl>  <nl> header_size -= len ; <nl>  <nl> - } while ( header_size >= 0 ); <nl> + } while ( header_size >= 0 && ! url_feof ( s -> pb )); <nl>  <nl> switch ( rate_flag ) { <nl> case - 1 :
static int decode_frame ( AVCodecContext * avctx , <nl> y = s -> height - 1 ; <nl> plane = 0 ; <nl> if ( bytestream_get_le16 (& buf )) { <nl> - while ( buf_end - buf >= 6 ) { <nl> + while ( y >= 0 && buf_end - buf >= 6 ) { <nl> const uint8_t * buf_pend = buf + FFMIN ( AV_RL16 ( buf ), buf_end - buf ); <nl> // ignore uncompressed block size reported at buf [ 2 ] <nl> int marker = buf [ 4 ]; <nl> buf += 5 ; <nl>  <nl> - while ( plane < s -> nb_planes && buf_pend - buf >= 1 ) { <nl> + while ( plane < s -> nb_planes && y >= 0 && buf_pend - buf >= 1 ) { <nl> int run = 1 ; <nl> int val = * buf ++; <nl> if ( val == marker ) { <nl> static int decode_frame ( AVCodecContext * avctx , <nl>  <nl> if ( bits_per_plane == 8 ) { <nl> picmemset_8bpp ( s , val , run , & x , & y ); <nl> - if ( y < 0 ) <nl> - break ; <nl> } else { <nl> picmemset ( s , val , run , & x , & y , & plane , bits_per_plane ); <nl> }
static void mxf_free_metadataset ( MXFMetadataSet ** ctx , int freectx ) <nl> case MaterialPackage : <nl> av_freep (&(( MXFPackage *)* ctx )-> tracks_refs ); <nl> av_freep (&(( MXFPackage *)* ctx )-> name ); <nl> + av_freep (&(( MXFPackage *)* ctx )-> comment_refs ); <nl> break ; <nl> case TaggedValue : <nl> av_freep (&(( MXFTaggedValue *)* ctx )-> name );
static av_cold int svq3_decode_init ( AVCodecContext * avctx ) <nl>  <nl> h -> b_stride = 4 * s -> mb_width ; <nl>  <nl> - ff_h264_alloc_tables ( h ); <nl> + if ( ff_h264_alloc_tables ( h ) < 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " svq3 memory allocation failed \ n "); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> } <nl>  <nl> return 0 ;
int ff_h264_decode_extradata ( H264Context * h , const uint8_t * buf , int size ) <nl> p += 6 ; <nl> for ( i = 0 ; i < cnt ; i ++) { <nl> nalsize = AV_RB16 ( p ) + 2 ; <nl> + if ( nalsize > size - ( p - buf )) <nl> + return - 1 ; <nl> if ( decode_nal_units ( h , p , nalsize ) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Decoding sps % d from avcC failed \ n ", i ); <nl> return - 1 ; <nl> int ff_h264_decode_extradata ( H264Context * h , const uint8_t * buf , int size ) <nl> cnt = *( p ++); // Number of pps <nl> for ( i = 0 ; i < cnt ; i ++) { <nl> nalsize = AV_RB16 ( p ) + 2 ; <nl> + if ( nalsize > size - ( p - buf )) <nl> + return - 1 ; <nl> if ( decode_nal_units ( h , p , nalsize ) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Decoding pps % d from avcC failed \ n ", i ); <nl> return - 1 ;
static int rm_read_audio_stream_info ( AVFormatContext * s , AVIOContext * pb , <nl> avio_read ( pb , buf , 4 ); <nl> buf [ 4 ] = 0 ; <nl> } else { <nl> + AV_WL32 ( buf , 0 ); <nl> get_str8 ( pb , buf , sizeof ( buf )); /* desc */ <nl> ast -> deint_id = AV_RL32 ( buf ); <nl> get_str8 ( pb , buf , sizeof ( buf )); /* desc */
static int mxf_read_close ( AVFormatContext * s ) <nl>  <nl> for ( i = 0 ; i < mxf -> nb_index_tables ; i ++) { <nl> av_freep (& mxf -> index_tables [ i ]. segments ); <nl> + av_freep (& mxf -> index_tables [ i ]. ptses ); <nl> av_freep (& mxf -> index_tables [ i ]. fake_index ); <nl> } <nl> av_freep (& mxf -> index_tables );
struct x11grab { <nl> int show_region ; /**< set by a private option . */ <nl> char * framerate ; /**< Set by a private option . */ <nl>  <nl> + Cursor c ; <nl> Window region_win ; /**< This is used by show_region option . */ <nl> }; <nl>  <nl> paint_mouse_pointer ( XImage * image , struct x11grab * s ) <nl> * Anyone who performs further investigation of the xlib API likely risks <nl> * permanent brain damage . */ <nl> uint8_t * pix = image -> data ; <nl> - Cursor c ; <nl> Window w ; <nl> XSetWindowAttributes attr ; <nl>  <nl> paint_mouse_pointer ( XImage * image , struct x11grab * s ) <nl> if ( image -> bits_per_pixel != 24 && image -> bits_per_pixel != 32 ) <nl> return ; <nl>  <nl> - c = XCreateFontCursor ( dpy , XC_left_ptr ); <nl> + if (! s -> c ) <nl> + s -> c = XCreateFontCursor ( dpy , XC_left_ptr ); <nl> w = DefaultRootWindow ( dpy ); <nl> - attr . cursor = c ; <nl> + attr . cursor = s -> c ; <nl> XChangeWindowAttributes ( dpy , w , CWCursor , & attr ); <nl>  <nl> xcim = XFixesGetCursorImage ( dpy );
static int mjpeg_decode_scan ( MJpegDecodeContext * s , int nb_components , int Ah , <nl> " Can not flip image with CODEC_FLAG_EMU_EDGE set !\ n "); <nl> s -> flipped = 0 ; <nl> } <nl> + if ( s -> flipped && s -> avctx -> lowres ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " Can not flip image with lowres \ n "); <nl> + s -> flipped = 0 ; <nl> + } <nl>  <nl> for ( i = 0 ; i < nb_components ; i ++) { <nl> int c = s -> comp_index [ i ];
static void guess_mv ( ERContext * s ) <nl>  <nl> if ( fixed [ mb_xy ] == MV_FROZEN ) <nl> continue ; <nl> - assert (! IS_INTRA ( s -> cur_pic . mb_type [ mb_xy ])); <nl> - assert ( s -> last_pic && s -> last_pic . f -> data [ 0 ]); <nl>  <nl> j = 0 ; <nl> if ( mb_x > 0 && fixed [ mb_xy - 1 ] == MV_FROZEN )
static int ogg_packet ( AVFormatContext * s , int * str , int * dstart , int * dsize , <nl>  <nl> if (! complete && os -> segp == os -> nsegs ) { <nl> ogg -> curidx = - 1 ; <nl> - os -> incomplete = 1 ; <nl> + // Do not set incomplete for empty packets . <nl> + // Together with the code in ogg_read_page <nl> + // that discards all continuation of empty packets <nl> + // we would get an infinite loop . <nl> + os -> incomplete = !! os -> psize ; <nl> } <nl> } while (! complete ); <nl> 
static av_cold int truemotion1_decode_init ( AVCodecContext * avctx ) <nl> /* there is a vertical predictor for each pixel in a line ; each vertical <nl> * predictor is 0 to start with */ <nl> av_fast_malloc (& s -> vert_pred , & s -> vert_pred_size , s -> avctx -> width * sizeof ( unsigned int )); <nl> - if (! s -> vert_pred ) <nl> + if (! s -> vert_pred ) { <nl> + av_frame_free (& s -> frame ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static int mjpeg_decode_sof ( MJpegDecodeContext * s ) <nl>  <nl> s -> width = width ; <nl> s -> height = height ; <nl> + s -> interlaced = 0 ; <nl>  <nl> /* test interlaced mode */ <nl> if ( s -> first_picture && <nl> read_header : <nl> skip_bits (& hgb , 32 ); /* padded field size */ <nl> second_field_offs = get_bits_long (& hgb , 32 ); <nl> av_log ( avctx , AV_LOG_DEBUG , " second field offs : 0x % x \ n ", second_field_offs ); <nl> - if ( second_field_offs ) <nl> - s -> interlaced = 1 ; <nl>  <nl> dqt_offs = get_bits_long (& hgb , 32 ); <nl> av_log ( avctx , AV_LOG_DEBUG , " dqt offs : 0x % x \ n ", dqt_offs );
static int v410_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( pic -> data [ 0 ]) <nl> avctx -> release_buffer ( avctx , pic ); <nl>  <nl> + if ( avpkt -> size < 4 * avctx -> height * avctx -> width ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Insufficient input data .\ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> pic -> reference = 0 ; <nl>  <nl> if ( avctx -> get_buffer ( avctx , pic ) < 0 ) {
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl> av_log ( avctx , AV_LOG_ERROR , " Custom quant matrix encountered !\ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( band -> quant_mat > 21 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid quant matrix encountered !\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl>  <nl> /* decode block huffman codebook */
static int16_t g726_decode ( G726Context * c , int I ) <nl> c -> se += mult ( i2f ( c -> a [ i ] >> 2 , & f ), & c -> sr [ i ]); <nl> c -> se >>= 1 ; <nl>  <nl> - return av_clip ( re_signal << 2 , - 0xffff , 0xffff ); <nl> + return av_clip ( re_signal * 4 , - 0xffff , 0xffff ); <nl> } <nl>  <nl> static av_cold int g726_reset ( G726Context * c )
static int query_formats ( AVFilterContext * ctx ) <nl> static const enum AVPixelFormat out_fmts [] = { AV_PIX_FMT_RGB32 , AV_PIX_FMT_NONE }; <nl> AVFilterFormats * in = ff_make_format_list ( in_fmts ); <nl> AVFilterFormats * out = ff_make_format_list ( out_fmts ); <nl> - if (! in || ! out ) <nl> + if (! in || ! out ) { <nl> + av_freep (& in ); <nl> + av_freep (& out ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl> ff_formats_ref ( in , & ctx -> inputs [ 0 ]-> out_formats ); <nl> ff_formats_ref ( out , & ctx -> outputs [ 0 ]-> in_formats ); <nl> return 0 ;
static int write_adaptation_set ( AVFormatContext * s , int as_index ) <nl> ret = write_representation ( s , s -> streams [ as -> streams [ i ]], <nl> representation_id , ! width_in_as , <nl> ! height_in_as , ! sample_rate_in_as ); <nl> - if ( ret ) return ret ; <nl> av_free ( representation_id ); <nl> + if ( ret ) return ret ; <nl> } <nl> avio_printf ( s -> pb , "</ AdaptationSet >\ n "); <nl> return 0 ;
int ff_wms_parse_sdp_a_line ( AVFormatContext * s , const char * p ) <nl> { <nl> int ret = 0 ; <nl> if ( av_strstart ( p , " pgmpu : data : application / vnd . ms . wms - hdr . asfv1 ; base64 ,", & p )) { <nl> - AVIOContext pb ; <nl> + AVIOContext pb = { 0 }; <nl> RTSPState * rt = s -> priv_data ; <nl> AVDictionary * opts = NULL ; <nl> int len = strlen ( p ) * 6 / 8 ;
int av_find_stream_info ( AVFormatContext * ic ) <nl> && tb_unreliable ( st -> codec ) /*&& <nl> // FIXME we should not special - case MPEG - 2 , but this needs testing with non - MPEG - 2 ... <nl> st -> time_base . num * duration_sum [ i ]/ duration_count [ i ]* 101LL > st -> time_base . den */){ <nl> + int num = 0 ; <nl> double best_error = 2 * av_q2d ( st -> time_base ); <nl> best_error = best_error * best_error * duration_count [ i ]* 1000 * 12 * 30 ; <nl>  <nl> int av_find_stream_info ( AVFormatContext * ic ) <nl> // av_log ( NULL , AV_LOG_ERROR , "% f % f \ n ", get_std_framerate ( j ) / 12 . 0 / 1001 , error ); <nl> if ( error < best_error ){ <nl> best_error = error ; <nl> - av_reduce (& st -> r_frame_rate . num , & st -> r_frame_rate . den , get_std_framerate ( j ), 12 * 1001 , INT_MAX ); <nl> + num = get_std_framerate ( j ); <nl> } <nl> } <nl> + // do not increase frame rate by more than 1 % in order to match a standard rate . <nl> + if ( num && (! st -> r_frame_rate . num || ( double ) num /( 12 * 1001 ) < 1 . 01 * av_q2d ( st -> r_frame_rate ))) <nl> + av_reduce (& st -> r_frame_rate . num , & st -> r_frame_rate . den , num , 12 * 1001 , INT_MAX ); <nl> } <nl>  <nl> if (! st -> r_frame_rate . num ){
static void guess_dc ( MpegEncContext * s , int16_t * dc , int w , <nl> int16_t (* col )[ 4 ] = av_malloc ( stride * h * sizeof ( int16_t )* 4 ); <nl> uint32_t (* dist )[ 4 ] = av_malloc ( stride * h * sizeof ( uint32_t )* 4 ); <nl>  <nl> + if (! col || ! dist ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " guess_dc () is out of memory \ n "); <nl> + goto fail ; <nl> + } <nl> + <nl> for ( b_y = 0 ; b_y < h ; b_y ++){ <nl> int color = 1024 ; <nl> int distance = - 1 ; <nl> static void guess_dc ( MpegEncContext * s , int16_t * dc , int w , <nl> dc [ b_x + b_y * stride ] = guess ; <nl> } <nl> } <nl> + <nl> + fail : <nl> av_freep (& col ); <nl> av_freep (& dist ); <nl> }
static int guess_ni_flag ( AVFormatContext * s ){ <nl> if ( last_start > first_end ) <nl> return 1 ; <nl> idx = av_mallocz ( sizeof (* idx ) * s -> nb_streams ); <nl> - for ( min_pos = pos = 0 ; min_pos != INT64_MAX ; pos = min_pos + 1 ) { <nl> + for ( min_pos = pos = 0 ; min_pos != INT64_MAX ; pos = min_pos + 1LU ) { <nl> int64_t max_dts = INT64_MIN / 2 , min_dts = INT64_MAX / 2 ; <nl> min_pos = INT64_MAX ; <nl> 
void * av_realloc ( void * ptr , FF_INTERNAL_MEM_TYPE size ) <nl>  <nl> void av_free ( void * ptr ) <nl> { <nl> - /* XXX : this test should not be needed on most libcs */ <nl> - if ( ptr ) <nl> # if CONFIG_MEMALIGN_HACK <nl> + if ( ptr ) <nl> free (( char *) ptr - (( char *) ptr )[- 1 ]); <nl> # else <nl> - free ( ptr ); <nl> + free ( ptr ); <nl> # endif <nl> } <nl> 
static int transcode_init ( void ) <nl> * overhead <nl> */ <nl> if (! strcmp ( oc -> oformat -> name , " avi ")) { <nl> - if ( copy_tb < 0 && av_q2d ( ist -> st -> r_frame_rate ) >= av_q2d ( ist -> st -> avg_frame_rate ) <nl> + if ( copy_tb < 0 && ist -> st -> r_frame_rate . num <nl> + && av_q2d ( ist -> st -> r_frame_rate ) >= av_q2d ( ist -> st -> avg_frame_rate ) <nl> && 0 . 5 / av_q2d ( ist -> st -> r_frame_rate ) > av_q2d ( ist -> st -> time_base ) <nl> && 0 . 5 / av_q2d ( ist -> st -> r_frame_rate ) > av_q2d ( dec_ctx -> time_base ) <nl> && av_q2d ( ist -> st -> time_base ) < 1 . 0 / 500 && av_q2d ( dec_ctx -> time_base ) < 1 . 0 / 500
static int imc_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> IMCContext * q = avctx -> priv_data ; <nl>  <nl> - LOCAL_ALIGNED_16 ( uint16_t , buf16 , [ IMC_BLOCK_SIZE / 2 ]); <nl> + LOCAL_ALIGNED_16 ( uint16_t , buf16 , [ IMC_BLOCK_SIZE / 2 + FF_INPUT_BUFFER_PADDING_SIZE / 2 ]); <nl>  <nl> if ( buf_size < IMC_BLOCK_SIZE * avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame too small !\ n ");
typedef struct PerThreadContext { <nl> struct FrameThreadContext * parent ; <nl>  <nl> pthread_t thread ; <nl> + int thread_created ; <nl> pthread_cond_t input_cond ; ///< Used to wait for a new packet from the main thread . <nl> pthread_cond_t progress_cond ; ///< Used by child threads to wait for progress to change . <nl> pthread_cond_t output_cond ; ///< Used by the main thread to wait for frames to finish . <nl> static void frame_thread_free ( AVCodecContext * avctx , int thread_count ) <nl> pthread_cond_signal (& p -> input_cond ); <nl> pthread_mutex_unlock (& p -> mutex ); <nl>  <nl> - if ( p -> thread ) <nl> + if ( p -> thread_created ) <nl> pthread_join ( p -> thread , NULL ); <nl> + p -> thread_created = 0 ; <nl>  <nl> if ( codec -> close ) <nl> codec -> close ( p -> avctx ); <nl> static int frame_thread_init ( AVCodecContext * avctx ) <nl>  <nl> if ( err ) goto error ; <nl>  <nl> - pthread_create (& p -> thread , NULL , frame_worker_thread , p ); <nl> + p -> thread_created = ! pthread_create (& p -> thread , NULL , frame_worker_thread , p ); <nl> } <nl>  <nl> return 0 ;
int ff_lzf_uncompress ( GetByteContext * gb , uint8_t ** buf , int64_t * size ) <nl> ret = av_reallocp ( buf , * size ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + p = * buf + len ; <nl> } <nl>  <nl> bytestream2_get_buffer ( gb , p , s ); <nl> int ff_lzf_uncompress ( GetByteContext * gb , uint8_t ** buf , int64_t * size ) <nl> ret = av_reallocp ( buf , * size ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + p = * buf + len ; <nl> } <nl>  <nl> av_memcpy_backptr ( p , off , l );
int av_find_stream_info ( AVFormatContext * ic ) <nl> ( st -> codec -> codec_id == CODEC_ID_MPEG4 && ! st -> need_parsing ))*/) <nl> try_decode_frame ( st , pkt -> data , pkt -> size ); <nl>  <nl> - if ( av_rescale_q ( codec_info_duration [ st -> index ], st -> time_base , AV_TIME_BASE_Q ) >= ic -> max_analyze_duration ) { <nl> + if ( st -> time_base . den > 0 && av_rescale_q ( codec_info_duration [ st -> index ], st -> time_base , AV_TIME_BASE_Q ) >= ic -> max_analyze_duration ) { <nl> break ; <nl> } <nl> count ++;
static int wavpack_decode_block ( AVCodecContext * avctx , int block_no , <nl> ThreadFrame tframe = { . f = frame }; <nl> WavpackFrameContext * s ; <nl> GetByteContext gb ; <nl> - void * samples_l , * samples_r ; <nl> + void * samples_l = NULL , * samples_r = NULL ; <nl> int ret ; <nl> int got_terms = 0 , got_weights = 0 , got_samples = 0 , <nl> got_entropy = 0 , got_bs = 0 , got_float = 0 , got_hybrid = 0 ;
av_cold int ff_vc1_decode_init_alloc_tables ( VC1Context * v ) <nl> v -> ttblk = v -> ttblk_base + s -> mb_stride ; <nl> v -> is_intra_base = av_mallocz ( sizeof ( v -> is_intra_base [ 0 ]) * 2 * s -> mb_stride ); <nl> v -> is_intra = v -> is_intra_base + s -> mb_stride ; <nl> - v -> luma_mv_base = av_malloc ( sizeof ( v -> luma_mv_base [ 0 ]) * 2 * s -> mb_stride ); <nl> + v -> luma_mv_base = av_mallocz ( sizeof ( v -> luma_mv_base [ 0 ]) * 2 * s -> mb_stride ); <nl> v -> luma_mv = v -> luma_mv_base + s -> mb_stride ; <nl>  <nl> /* allocate block type info in that way so it could be used with s -> block_index [] */
# include < io . h > <nl> # endif <nl>  <nl> +# include " libavutil / avstring . h " <nl> # include " libavutil / time . h " <nl> # include " libavformat / avformat . h " <nl>  <nl> int main ( int argc , char ** argv ) <nl> return usage ( 1 ); <nl> if ( argc > 2 ) <nl> maxpkts = atoi ( argv [ 2 ]); <nl> - strncpy ( fntemplate , argv [ 1 ], sizeof ( fntemplate ) - 1 ); <nl> + av_strlcpy ( fntemplate , argv [ 1 ], sizeof ( fntemplate )); <nl> if ( strrchr ( argv [ 1 ], '/')) <nl> - strncpy ( fntemplate , strrchr ( argv [ 1 ], '/') + 1 , sizeof ( fntemplate ) - 1 ); <nl> + av_strlcpy ( fntemplate , strrchr ( argv [ 1 ], '/') + 1 , sizeof ( fntemplate )); <nl> if ( strrchr ( fntemplate , '.')) <nl> * strrchr ( fntemplate , '.') = '\ 0 '; <nl> if ( strchr ( fntemplate , '%')) {
static void encode_cblk ( Jpeg2000EncoderContext * s , Jpeg2000T1Context * t1 , Jpeg20 <nl> cblk -> npasses = passno ; <nl> cblk -> ninclpasses = passno ; <nl>  <nl> - cblk -> passes [ passno - 1 ]. rate = ff_mqc_flush_to (& t1 -> mqc , cblk -> passes [ passno - 1 ]. flushed , & cblk -> passes [ passno - 1 ]. flushed_len ); <nl> + if ( passno ) <nl> + cblk -> passes [ passno - 1 ]. rate = ff_mqc_flush_to (& t1 -> mqc , cblk -> passes [ passno - 1 ]. flushed , & cblk -> passes [ passno - 1 ]. flushed_len ); <nl> } <nl>  <nl> /* tier - 2 routines : */
static void process_client ( AVIOContext * client , const char * in_uri ) <nl> // may return empty string . <nl> if ( resource && strlen ( resource )) <nl> break ; <nl> + av_freep (& resource ); <nl> } <nl> if ( ret < 0 ) <nl> goto end ; <nl> end : <nl> avio_close ( client ); <nl> fprintf ( stderr , " Closing input \ n "); <nl> avio_close ( input ); <nl> + av_freep (& resource ); <nl> } <nl>  <nl> int main ( int argc , char ** argv )
static int execute_decode_slices ( H264Context * h , int context_count ) <nl> if ( context_count == 1 ) { <nl> return decode_slice ( avctx , & h ); <nl> } else { <nl> + av_assert0 ( context_count > 0 ); <nl> for ( i = 1 ; i < context_count ; i ++) { <nl> hx = h -> thread_context [ i ]; <nl> hx -> s . err_recognition = avctx -> err_recognition ;
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl>  <nl> align_get_bits (& ctx -> gb ); <nl>  <nl> + if (! band -> scan ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " band -> scan not set \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
int ff_flac_parse_picture ( AVFormatContext * s , uint8_t * buf , int buf_size ) <nl> if (!( data = av_buffer_alloc ( len + FF_INPUT_BUFFER_PADDING_SIZE ))) { <nl> RETURN_ERROR ( AVERROR ( ENOMEM )); <nl> } <nl> + memset ( data -> data + len , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if ( avio_read ( pb , data -> data , len ) != len ) { <nl> av_log ( s , AV_LOG_ERROR , " Error reading attached picture data .\ n "); <nl> if ( s -> error_recognition & AV_EF_EXPLODE )
int attribute_align_arg avcodec_open ( AVCodecContext * avctx , AVCodec * codec ) <nl> } <nl> } <nl>  <nl> - if ( avctx -> codec -> max_lowres < avctx -> lowres ) { <nl> + if ( avctx -> codec -> max_lowres < avctx -> lowres || avctx -> lowres < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " The maximum value for lowres supported by the decoder is % d \ n ", <nl> avctx -> codec -> max_lowres ); <nl> ret = AVERROR ( EINVAL );
static void d3d11va_frames_uninit ( AVHWFramesContext * ctx ) <nl>  <nl> if ( frames_hwctx -> texture ) <nl> ID3D11Texture2D_Release ( frames_hwctx -> texture ); <nl> + frames_hwctx -> texture = NULL ; <nl>  <nl> if ( s -> staging_texture ) <nl> ID3D11Texture2D_Release ( s -> staging_texture ); <nl> + s -> staging_texture = NULL ; <nl> } <nl>  <nl> static void free_texture ( void * opaque , uint8_t * data )
static void do_video_out ( AVFormatContext * s , <nl> av_log ( NULL , AV_LOG_VERBOSE , "*** drop !\ n "); <nl> return ; <nl> } else if ( nb_frames > 1 ) { <nl> + if ( nb_frames > dts_error_threshold * 30 ) { <nl> + av_log ( NULL , AV_LOG_ERROR , "% d frame duplication too large , skiping \ n ", nb_frames - 1 ); <nl> + nb_frames_drop ++; <nl> + return ; <nl> + } <nl> nb_frames_dup += nb_frames - 1 ; <nl> av_log ( NULL , AV_LOG_VERBOSE , "*** % d dup !\ n ", nb_frames - 1 ); <nl> }
static int pcx_decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> nplanes = buf [ 65 ]; <nl> bytes_per_scanline = nplanes * bytes_per_line ; <nl>  <nl> - if ( bytes_per_scanline < w * bits_per_pixel * nplanes / 8 || <nl> + if ( bytes_per_scanline < ( w * bits_per_pixel * nplanes + 7 ) / 8 || <nl> (! compressed && bytes_per_scanline > buf_size / h )) { <nl> av_log ( avctx , AV_LOG_ERROR , " PCX data is corrupted \ n "); <nl> return AVERROR_INVALIDDATA ;
static int sap_write_close ( AVFormatContext * s ) <nl> url_fclose ( rtpctx -> pb ); <nl> av_metadata_free (& rtpctx -> streams [ 0 ]-> metadata ); <nl> av_metadata_free (& rtpctx -> metadata ); <nl> + av_free ( rtpctx -> streams [ 0 ]-> info ); <nl> av_free ( rtpctx -> streams [ 0 ]); <nl> av_free ( rtpctx ); <nl> s -> streams [ i ]-> priv_data = NULL ;
int attribute_align_arg avcodec_open2 ( AVCodecContext * avctx , const AVCodec * code <nl> avctx -> time_base = av_inv_q ( av_mul_q ( avctx -> framerate , ( AVRational ){ avctx -> ticks_per_frame , 1 })); <nl> # endif <nl> } <nl> + if ( codec -> priv_data_size > 0 && avctx -> priv_data && codec -> priv_class ) { <nl> + av_assert0 (*( const AVClass **) avctx -> priv_data == codec -> priv_class ); <nl> + } <nl> + <nl> end : <nl> ff_unlock_avcodec (); <nl> if ( options ) {
static int mov_write_tkhd_tag ( AVIOContext * pb , MOVMuxContext * mov , <nl> if (! track_width_1616 || <nl> track -> height != track -> par -> height || <nl> track_width_1616 > UINT32_MAX ) <nl> - track_width_1616 = track -> par -> width * 0x10000U ; <nl> + track_width_1616 = track -> par -> width * 0x10000ULL ; <nl> + if ( track_width_1616 > UINT32_MAX ) { <nl> + av_log ( mov -> fc , AV_LOG_WARNING , " track width too large \ n "); <nl> + track_width_1616 = 0 ; <nl> + } <nl> avio_wb32 ( pb , track_width_1616 ); <nl> avio_wb32 ( pb , track -> height * 0x10000U ); <nl> }
static int decode_frame ( NUTContext * nut , AVPacket * pkt , int frame_code ) <nl> int64_t pts , last_IP_pts ; <nl> StreamContext * stc ; <nl> uint8_t header_idx ; <nl> + int ret ; <nl>  <nl> size = decode_frame_header ( nut , & pts , & stream_id , & header_idx , frame_code ); <nl> if ( size < 0 ) <nl> static int decode_frame ( NUTContext * nut , AVPacket * pkt , int frame_code ) <nl> pkt -> size -= sm_size ; <nl> } <nl>  <nl> - avio_read ( bc , pkt -> data + nut -> header_len [ header_idx ], size ); <nl> + ret = avio_read ( bc , pkt -> data + nut -> header_len [ header_idx ], size ); <nl> + if ( ret != size ) { <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + av_shrink_packet ( pkt , nut -> header_len [ header_idx ] + size ); <nl> + } <nl>  <nl> pkt -> stream_index = stream_id ; <nl> if ( stc -> last_flags & FLAG_KEY )
static int mpeg4_decode_studio_block ( MpegEncContext * s , int32_t block [ 64 ], int n <nl> code >>= 1 ; <nl> run = ( 1 << ( additional_code_len - 1 )) + code ; <nl> idx += run ; <nl> + if ( idx > 63 ) <nl> + return AVERROR_INVALIDDATA ; <nl> j = scantable [ idx ++]; <nl> block [ j ] = sign ? 1 : - 1 ; <nl> } else if ( group >= 13 && group <= 20 ) { <nl> /* Level value ( Table B . 49 ) */ <nl> + if ( idx > 63 ) <nl> + return AVERROR_INVALIDDATA ; <nl> j = scantable [ idx ++]; <nl> block [ j ] = get_xbits (& s -> gb , additional_code_len ); <nl> } else if ( group == 21 ) { <nl> /* Escape */ <nl> + if ( idx > 63 ) <nl> + return AVERROR_INVALIDDATA ; <nl> j = scantable [ idx ++]; <nl> additional_code_len = s -> avctx -> bits_per_raw_sample + s -> dct_precision + 4 ; <nl> flc = get_bits (& s -> gb , additional_code_len );
static int read_header ( AVFormatContext * s ) <nl> if ( caf -> data_size > 0 ) <nl> st -> nb_frames = ( caf -> data_size / caf -> bytes_per_packet ) * caf -> frames_per_packet ; <nl> } else if ( st -> nb_index_entries && st -> duration > 0 ) { <nl> - st -> codecpar -> bit_rate = st -> codecpar -> sample_rate * caf -> data_size * 8 / <nl> - st -> duration ; <nl> + if ( st -> codecpar -> sample_rate && caf -> data_size / st -> duration > INT64_MAX / st -> codecpar -> sample_rate / 8 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Overflow during bit rate calculation % d * 8 * %" PRId64 "\ n ", <nl> + st -> codecpar -> sample_rate , caf -> data_size / st -> duration ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + st -> codecpar -> bit_rate = st -> codecpar -> sample_rate * 8LL * <nl> + ( caf -> data_size / st -> duration ); <nl> } else { <nl> av_log ( s , AV_LOG_ERROR , " Missing packet table . It is required when " <nl> " block size or frame size are variable .\ n ");
 <nl> # define BE_16 ( x ) (((( uint8_t *)( x ))[ 0 ] << 8 ) | (( uint8_t *)( x ))[ 1 ]) <nl>  <nl> -# define BE_32 ( x ) (((( uint8_t *)( x ))[ 0 ] << 24 ) | \ <nl> - ((( uint8_t *)( x ))[ 1 ] << 16 ) | \ <nl> - ((( uint8_t *)( x ))[ 2 ] << 8 ) | \ <nl> - (( uint8_t *)( x ))[ 3 ]) <nl> +# define BE_32 ( x ) ((( uint32_t )((( uint8_t *)( x ))[ 0 ]) << 24 ) | \ <nl> + ((( uint8_t *)( x ))[ 1 ] << 16 ) | \ <nl> + ((( uint8_t *)( x ))[ 2 ] << 8 ) | \ <nl> + (( uint8_t *)( x ))[ 3 ]) <nl>  <nl> # define BE_64 ( x ) ((( uint64_t )((( uint8_t *)( x ))[ 0 ]) << 56 ) | \ <nl> (( uint64_t )((( uint8_t *)( x ))[ 1 ]) << 48 ) | \ <nl> int main ( int argc , char * argv []) <nl> if ( fread ( atom_bytes , ATOM_PREAMBLE_SIZE , 1 , infile ) != 1 ) { <nl> break ; <nl> } <nl> - atom_size = ( uint32_t ) BE_32 (& atom_bytes [ 0 ]); <nl> + atom_size = BE_32 (& atom_bytes [ 0 ]); <nl> atom_type = BE_32 (& atom_bytes [ 4 ]); <nl>  <nl> /* keep ftyp atom */
av_cold int sws_init_context ( SwsContext * c , SwsFilter * srcFilter , <nl> c -> chrDstW = FF_CEIL_RSHIFT ( dstW , c -> chrDstHSubSample ); <nl> c -> chrDstH = FF_CEIL_RSHIFT ( dstH , c -> chrDstVSubSample ); <nl>  <nl> - FF_ALLOC_OR_GOTO ( c , c -> formatConvBuffer , FFALIGN ( srcW * 2 + 78 , 16 ) * 2 , fail ); <nl> + FF_ALLOCZ_OR_GOTO ( c , c -> formatConvBuffer , FFALIGN ( srcW * 2 + 78 , 16 ) * 2 , fail ); <nl>  <nl> c -> srcBpc = 1 + desc_src -> comp [ 0 ]. depth_minus1 ; <nl> if ( c -> srcBpc < 8 )
static void pre_process_video_frame ( InputStream * ist , AVPicture * picture , void * <nl>  <nl> /* create temporary picture */ <nl> size = avpicture_get_size ( dec -> pix_fmt , dec -> width , dec -> height ); <nl> + if ( size < 0 ) <nl> + return ; <nl> buf = av_malloc ( size ); <nl> if (! buf ) <nl> return ;
int ff_hevc_decode_nal_pps ( HEVCContext * s ) <nl> if ( pps -> tiles_enabled_flag ) { <nl> pps -> num_tile_columns = get_ue_golomb_long ( gb ) + 1 ; <nl> pps -> num_tile_rows = get_ue_golomb_long ( gb ) + 1 ; <nl> - if ( pps -> num_tile_columns == 0 || <nl> + if ( pps -> num_tile_columns <= 0 || <nl> pps -> num_tile_columns >= sps -> width ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " num_tile_columns_minus1 out of range : % d \ n ", <nl> pps -> num_tile_columns - 1 ); <nl> ret = AVERROR_INVALIDDATA ; <nl> goto err ; <nl> } <nl> - if ( pps -> num_tile_rows == 0 || <nl> + if ( pps -> num_tile_rows <= 0 || <nl> pps -> num_tile_rows >= sps -> height ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " num_tile_rows_minus1 out of range : % d \ n ", <nl> pps -> num_tile_rows - 1 );
static int parse_cube ( AVFilterContext * ctx , FILE * f ) <nl> int i , j , k ; <nl> const int size = strtol ( line + 12 , NULL , 0 ); <nl>  <nl> - if ( size > MAX_LEVEL ) { <nl> - av_log ( ctx , AV_LOG_ERROR , " Too large 3D LUT \ n "); <nl> + if ( size < 2 || size > MAX_LEVEL ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " Too large or invalid 3D LUT size \ n "); <nl> return AVERROR ( EINVAL ); <nl> } <nl> lut3d -> lutsize = size ; <nl> static int parse_m3d ( AVFilterContext * ctx , FILE * f ) <nl> av_log ( ctx , AV_LOG_ERROR , " in and out must be defined \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( in < 2 || out < 2 || <nl> + in > MAX_LEVEL * MAX_LEVEL * MAX_LEVEL || <nl> + out > MAX_LEVEL * MAX_LEVEL * MAX_LEVEL ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " invalid in (% d ) or out (% d )\ n ", in , out ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> for ( size = 1 ; size * size * size < in ; size ++); <nl> lut3d -> lutsize = size ; <nl> scale = 1 . / ( out - 1 );
static int write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> pkt -> dts += offset ; <nl> if ( pkt -> pts != AV_NOPTS_VALUE ) <nl> pkt -> pts += offset ; <nl> + <nl> + if ( pkt -> dts != AV_NOPTS_VALUE && pkt -> dts < 0 ) { <nl> + av_log ( s , AV_LOG_WARNING , <nl> + " Packets poorly interleaved , failed to avoid negative " <nl> + " timestamp %" PRId64 " in stream % d .\ n " <nl> + " Try - max_interleave_delta 0 as a possible workaround .\ n ", <nl> + pkt -> dts , pkt -> stream_index ); <nl> + } <nl> } <nl> ret = s -> oformat -> write_packet ( s , pkt ); <nl> 
static av_cold int mss2_decode_init ( AVCodecContext * avctx ) <nl> if ( ret = ff_mss12_decode_init ( c , 1 , & ctx -> sc [ 0 ], & ctx -> sc [ 1 ])) <nl> return ret ; <nl> c -> pal_stride = c -> mask_stride ; <nl> - c -> pal_pic = av_malloc ( c -> pal_stride * avctx -> height ); <nl> - c -> last_pal_pic = av_malloc ( c -> pal_stride * avctx -> height ); <nl> + c -> pal_pic = av_mallocz ( c -> pal_stride * avctx -> height ); <nl> + c -> last_pal_pic = av_mallocz ( c -> pal_stride * avctx -> height ); <nl> if (! c -> pal_pic || ! c -> last_pal_pic ) { <nl> mss2_decode_end ( avctx ); <nl> return AVERROR ( ENOMEM );
int avfilter_graph_queue_command ( AVFilterGraph * graph , const char * target , const <nl> queue = &(* queue )-> next ; <nl> next = * queue ; <nl> * queue = av_mallocz ( sizeof ( AVFilterCommand )); <nl> + if (!* queue ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> (* queue )-> command = av_strdup ( command ); <nl> (* queue )-> arg = av_strdup ( arg ); <nl> (* queue )-> time = ts ;
static int dv_decode_video_segment ( AVCodecContext * avctx , void * arg ) <nl> GetBitContext gb ; <nl> BlockInfo mb_data [ 5 * DV_MAX_BPM ], * mb , * mb1 ; <nl> LOCAL_ALIGNED_16 ( DCTELEM , sblock , [ 5 * DV_MAX_BPM ], [ 64 ]); <nl> - LOCAL_ALIGNED_16 ( uint8_t , mb_bit_buffer , [ 80 + 4 ]); /* allow some slack */ <nl> - LOCAL_ALIGNED_16 ( uint8_t , vs_bit_buffer , [ 5 * 80 + 4 ]); /* allow some slack */ <nl> + LOCAL_ALIGNED_16 ( uint8_t , mb_bit_buffer , [ 80 + FF_INPUT_BUFFER_PADDING_SIZE ]); /* allow some slack */ <nl> + LOCAL_ALIGNED_16 ( uint8_t , vs_bit_buffer , [ 5 * 80 + FF_INPUT_BUFFER_PADDING_SIZE ]); /* allow some slack */ <nl> const int log2_blocksize = 3 - s -> avctx -> lowres ; <nl> int is_field_mode [ 5 ]; <nl>  <nl> static int dv_decode_video_segment ( AVCodecContext * avctx , void * arg ) <nl> block = block1 ; <nl> mb = mb1 ; <nl> init_get_bits (& gb , mb_bit_buffer , put_bits_count (& pb )); <nl> + put_bits32 (& pb , 0 ); // padding must be zero ' ed <nl> flush_put_bits (& pb ); <nl> for ( j = 0 ; j < s -> sys -> bpm ; j ++, block += 64 , mb ++) { <nl> if ( mb -> pos < 64 && get_bits_left (& gb ) > 0 ) { <nl> static int dv_decode_video_segment ( AVCodecContext * avctx , void * arg ) <nl> block = & sblock [ 0 ][ 0 ]; <nl> mb = mb_data ; <nl> init_get_bits (& gb , vs_bit_buffer , put_bits_count (& vs_pb )); <nl> + put_bits32 (& vs_pb , 0 ); // padding must be zero ' ed <nl> flush_put_bits (& vs_pb ); <nl> for ( mb_index = 0 ; mb_index < 5 ; mb_index ++) { <nl> for ( j = 0 ; j < s -> sys -> bpm ; j ++) {
unknown_opt : <nl> * po -> u . float_arg = parse_number_or_die ( opt , arg , OPT_FLOAT , - 1 . 0 / 0 . 0 , 1 . 0 / 0 . 0 ); <nl> } else if ( po -> flags & OPT_FUNC2 ) { <nl> if ( po -> u . func2_arg ( opt , arg ) < 0 ) { <nl> - fprintf ( stderr , "% s : invalid value '% s ' for option '% s '\ n ", argv [ 0 ], arg , opt ); <nl> + fprintf ( stderr , "% s : failed to set value '% s ' for option '% s '\ n ", argv [ 0 ], arg , opt ); <nl> exit ( 1 ); <nl> } <nl> } else {
static av_always_inline void decode_subband_internal ( DiracContext * s , SubBand * b <nl>  <nl> top = 0 ; <nl> for ( cb_y = 0 ; cb_y < cb_height ; cb_y ++) { <nl> - bottom = ( b -> height * ( cb_y + 1 )) / cb_height ; <nl> + bottom = ( b -> height * ( cb_y + 1LL )) / cb_height ; <nl> left = 0 ; <nl> for ( cb_x = 0 ; cb_x < cb_width ; cb_x ++) { <nl> - right = ( b -> width * ( cb_x + 1 )) / cb_width ; <nl> + right = ( b -> width * ( cb_x + 1LL )) / cb_width ; <nl> codeblock ( s , b , & gb , & c , left , right , top , bottom , blockcnt_one , is_arith ); <nl> left = right ; <nl> }
static inline void ls_decode_line ( JLSState * state , MJpegDecodeContext * s , <nl>  <nl> if ( x >= w ) { <nl> av_log ( NULL , AV_LOG_ERROR , " run overflow \ n "); <nl> + av_assert0 ( x <= w ); <nl> return ; <nl> } <nl> 
static int matroska_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> return - 1 ; <nl> matroska_execute_seekhead ( matroska ); <nl>  <nl> + if (! matroska -> time_scale ) <nl> + matroska -> time_scale = 1000000 ; <nl> if ( matroska -> duration ) <nl> matroska -> ctx -> duration = matroska -> duration * matroska -> time_scale <nl> * 1000 / AV_TIME_BASE ;
static int svq1_decode_block_intra ( GetBitContext * bitbuf , uint8_t * pixels , <nl> uint8_t * list [ 63 ]; <nl> uint32_t * dst ; <nl> const uint32_t * codebook ; <nl> - int entries [ 6 ]; <nl> + int entries [ 6 ] = { 0 }; <nl> int i , j , m , n ; <nl> int stages ; <nl> unsigned mean ; <nl> static int svq1_decode_block_non_intra ( GetBitContext * bitbuf , uint8_t * pixels , <nl> uint8_t * list [ 63 ]; <nl> uint32_t * dst ; <nl> const uint32_t * codebook ; <nl> - int entries [ 6 ]; <nl> + int entries [ 6 ] = { 0 }; <nl> int i , j , m , n ; <nl> int stages ; <nl> unsigned mean ;
int ff_alsa_get_device_list ( AVDeviceInfoList * device_list , snd_pcm_stream_t stre <nl> & device_list -> nb_devices , new_device )) < 0 ) { <nl> goto fail ; <nl> } <nl> + if (! strcmp ( new_device -> device_name , " default ")) <nl> + device_list -> default_device = device_list -> nb_devices - 1 ; <nl> new_device = NULL ; <nl> } <nl> fail :
int ff_thread_decode_frame ( AVCodecContext * avctx , <nl> FrameThreadContext * fctx = avctx -> internal -> thread_ctx ; <nl> int finished = fctx -> next_finished ; <nl> PerThreadContext * p ; <nl> - int err , ret ; <nl> + int err , ret = 0 ; <nl>  <nl> /* release the async lock , permitting blocked hwaccel threads to <nl> * go forward while we are in this function */
static int ffm_write_packet ( AVFormatContext * s , int stream_index , <nl> /* packet size & key_frame */ <nl> header [ 0 ] = stream_index ; <nl> header [ 1 ] = 0 ; <nl> - if ( st -> codec . coded_picture -> key_frame ) <nl> + if ( st -> codec . coded_picture && st -> codec . coded_picture -> key_frame ) <nl> header [ 1 ] |= FLAG_KEY_FRAME ; <nl> header [ 2 ] = ( size >> 16 ) & 0xff ; <nl> header [ 3 ] = ( size >> 8 ) & 0xff ;
static int roq_dpcm_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> context -> input_frames ++; <nl> return 0 ; <nl> } <nl> - in = context -> frame_buffer ; <nl> } <nl> + if ( context -> input_frames < 8 ) <nl> + in = context -> frame_buffer ; <nl>  <nl> if ( stereo ) { <nl> context -> lastSample [ 0 ] &= 0xFF00 ; <nl> context -> lastSample [ 1 ] &= 0xFF00 ; <nl> } <nl>  <nl> - if ( context -> input_frames == 7 || ! in ) <nl> + if ( context -> input_frames == 7 ) <nl> data_size = avctx -> channels * context -> buffered_samples ; <nl> else <nl> data_size = avctx -> channels * avctx -> frame_size ;
static int decode_header ( SnowContext * s ){ <nl> s -> always_reset = get_rac (& s -> c , s -> header_state ); <nl> s -> temporal_decomposition_type = get_symbol (& s -> c , s -> header_state , 0 ); <nl> s -> temporal_decomposition_count = get_symbol (& s -> c , s -> header_state , 0 ); <nl> - s -> spatial_decomposition_count = get_symbol (& s -> c , s -> header_state , 0 ); <nl> + GET_S ( s -> spatial_decomposition_count , tmp <= ( unsigned ) MAX_DECOMPOSITIONS ) <nl> s -> colorspace_type = get_symbol (& s -> c , s -> header_state , 0 ); <nl> s -> chroma_h_shift = get_symbol (& s -> c , s -> header_state , 0 ); <nl> s -> chroma_v_shift = get_symbol (& s -> c , s -> header_state , 0 ); <nl> static int decode_header ( SnowContext * s ){ <nl> memcpy ( s -> plane [ 2 ]. hcoeff , s -> plane [ 1 ]. hcoeff , sizeof ( s -> plane [ 1 ]. hcoeff )); <nl> } <nl> if ( get_rac (& s -> c , s -> header_state )){ <nl> - s -> spatial_decomposition_count = get_symbol (& s -> c , s -> header_state , 0 ); <nl> + GET_S ( s -> spatial_decomposition_count , tmp <= ( unsigned ) MAX_DECOMPOSITIONS ) <nl> decode_qlogs ( s ); <nl> } <nl> }
int ff_h264_execute_ref_pic_marking ( H264Context * h , MMCO * mmco , int mmco_count ) <nl> */ <nl> if ( h -> short_ref_count && h -> short_ref [ 0 ] == h -> cur_pic_ptr ) { <nl> /* Just mark the second field valid */ <nl> - h -> cur_pic_ptr -> reference = PICT_FRAME ; <nl> + h -> cur_pic_ptr -> reference |= h -> picture_structure ; <nl> } else if ( h -> cur_pic_ptr -> long_ref ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , " illegal short term reference " <nl> " assignment for second field "
static int codec_reinit ( AVCodecContext * avctx , int width , int height , int qualit <nl> return 0 ; <nl> avctx -> width = c -> width = width ; <nl> avctx -> height = c -> height = height ; <nl> - c -> decomp_size = c -> height * c -> width * 3 / 2 ; <nl> - c -> decomp_buf = av_realloc ( c -> decomp_buf , c -> decomp_size + AV_LZO_OUTPUT_PADDING ); <nl> + av_fast_malloc (& c -> decomp_buf , & c -> decomp_size , c -> height * c -> width * 3 / 2 ); <nl> if (! c -> decomp_buf ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Can ' t allocate decompression buffer .\ n "); <nl> return 0 ;
static void expand_filename_template ( AVBPrint * bp , const char * template , <nl>  <nl> static int init_report ( const char * env ) <nl> { <nl> - const char * filename_template = "% p -% t . log "; <nl> + char * filename_template = NULL ; <nl> char * key , * val ; <nl> int ret , count = 0 ; <nl> time_t now ; <nl> static int init_report ( const char * env ) <nl> env ++; <nl> count ++; <nl> if (! strcmp ( key , " file ")) { <nl> + av_free ( filename_template ); <nl> filename_template = val ; <nl> val = NULL ; <nl> } else { <nl> static int init_report ( const char * env ) <nl> } <nl>  <nl> av_bprint_init (& filename , 0 , 1 ); <nl> - expand_filename_template (& filename , filename_template , tm ); <nl> + expand_filename_template (& filename , <nl> + av_x_if_null ( filename_template , "% p -% t . log "), tm ); <nl> + av_free ( filename_template ); <nl> if (! av_bprint_is_complete (& filename )) { <nl> av_log ( NULL , AV_LOG_ERROR , " Out of memory building report file name \ n "); <nl> return AVERROR ( ENOMEM );
int av_parse_color ( uint8_t * rgba_color , const char * color_string , int slen , <nl> if (! strncmp ( alpha_string , " 0x ", 2 )) { <nl> alpha = strtoul ( alpha_string , & tail , 16 ); <nl> } else { <nl> - alpha = 255 * strtod ( alpha_string , & tail ); <nl> + double norm_alpha = strtod ( alpha_string , & tail ); <nl> + if ( norm_alpha < 0 . 0 || norm_alpha > 1 . 0 ) <nl> + alpha = 256 ; <nl> + else <nl> + alpha = 255 * norm_alpha ; <nl> } <nl>  <nl> if ( tail == alpha_string || * tail || alpha > 255 ) {
static int get_siz ( Jpeg2000DecoderContext * s ) <nl> s -> cdy [ i ] = bytestream2_get_byteu (& s -> g ); <nl> if ( s -> cdx [ i ] != 1 || s -> cdy [ i ] != 1 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " unsupported / CDxy values % d % d for component % d \ n ", s -> cdx [ i ], s -> cdy [ i ], i ); <nl> + if (! s -> cdx [ i ] || ! s -> cdy [ i ]) <nl> + return AVERROR_INVALIDDATA ; <nl> } <nl> } <nl> 
yuv2gbrp_full_X_c ( SwsContext * c , const int16_t * lumFilter , <nl> int hasAlpha = ( desc -> flags & PIX_FMT_ALPHA ) && alpSrc ; <nl> uint16_t ** dest16 = ( uint16_t **) dest ; <nl> int SH = 22 + 7 - desc -> comp [ 0 ]. depth_minus1 ; <nl> + int A = 0 ; // init to silence warning <nl>  <nl> for ( i = 0 ; i < dstW ; i ++) { <nl> int j ; <nl> int Y = 1 << 9 ; <nl> int U = ( 1 << 9 ) - ( 128 << 19 ); <nl> int V = ( 1 << 9 ) - ( 128 << 19 ); <nl> - int R , G , B , A ; <nl> + int R , G , B ; <nl>  <nl> for ( j = 0 ; j < lumFilterSize ; j ++) <nl> Y += lumSrc [ j ][ i ] * lumFilter [ j ];
static int process_ipmovie_chunk ( IPMVEContext * s , AVIOContext * pb , <nl>  <nl> case OPCODE_INIT_VIDEO_BUFFERS : <nl> av_dlog ( NULL , " initialize video buffers \ n "); <nl> - if (( opcode_version > 2 ) || ( opcode_size > 8 )) { <nl> + if (( opcode_version > 2 ) || ( opcode_size > 8 ) || opcode_size < 4 ) { <nl> av_dlog ( NULL , " bad init_video_buffers opcode \ n "); <nl> chunk_type = CHUNK_BAD ; <nl> break ;
static int g2m_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl> c -> tile_width = bytestream2_get_be32 (& bc ); <nl> c -> tile_height = bytestream2_get_be32 (& bc ); <nl> - if (! c -> tile_width || ! c -> tile_height || <nl> - (( c -> tile_width | c -> tile_height ) & 0xF )) { <nl> + if ( c -> tile_width <= 0 || c -> tile_height <= 0 || <nl> + (( c -> tile_width | c -> tile_height ) & 0xF ) || <nl> + c -> tile_width * 4LL * c -> tile_height >= INT_MAX <nl> + ) { <nl> av_log ( avctx , AV_LOG_ERROR , <nl> " Invalid tile dimensions % dx % d \ n ", <nl> c -> tile_width , c -> tile_height );
typedef struct ThreadContext { <nl> pthread_cond_t current_job_cond ; <nl> pthread_mutex_t current_job_lock ; <nl> int current_job ; <nl> + unsigned int current_execute ; <nl> int done ; <nl> } ThreadContext ; <nl>  <nl> static void * attribute_align_arg worker ( void * v ) <nl> ThreadContext * c = v ; <nl> int our_job = c -> nb_jobs ; <nl> int nb_threads = c -> nb_threads ; <nl> + unsigned int last_execute = 0 ; <nl> int self_id ; <nl>  <nl> pthread_mutex_lock (& c -> current_job_lock ); <nl> static void * attribute_align_arg worker ( void * v ) <nl> if ( c -> current_job == nb_threads + c -> nb_jobs ) <nl> pthread_cond_signal (& c -> last_job_cond ); <nl>  <nl> - if (! c -> done ) <nl> + while ( last_execute == c -> current_execute && ! c -> done ) <nl> pthread_cond_wait (& c -> current_job_cond , & c -> current_job_lock ); <nl> + last_execute = c -> current_execute ; <nl> our_job = self_id ; <nl>  <nl> if ( c -> done ) { <nl> static void slice_thread_uninit ( ThreadContext * c ) <nl>  <nl> static void slice_thread_park_workers ( ThreadContext * c ) <nl> { <nl> - pthread_cond_wait (& c -> last_job_cond , & c -> current_job_lock ); <nl> + while ( c -> current_job != c -> nb_threads + c -> nb_jobs ) <nl> + pthread_cond_wait (& c -> last_job_cond , & c -> current_job_lock ); <nl> pthread_mutex_unlock (& c -> current_job_lock ); <nl> } <nl>  <nl> static int thread_execute ( AVFilterContext * ctx , avfilter_action_func * func , <nl> c -> rets = & dummy_ret ; <nl> c -> nb_rets = 1 ; <nl> } <nl> + c -> current_execute ++; <nl> + <nl> pthread_cond_broadcast (& c -> current_job_cond ); <nl>  <nl> slice_thread_park_workers ( c );
int ff_dwt_encode ( DWTContext * s , void * t ) <nl>  <nl> int ff_dwt_decode ( DWTContext * s , void * t ) <nl> { <nl> + if ( s -> ndeclevels == 0 ) <nl> + return 0 ; <nl> + <nl> switch ( s -> type ) { <nl> case FF_DWT97 : <nl> dwt_decode97_float ( s , t );
static int rtp_write ( URLContext * h , const uint8_t * buf , int size ) <nl> int ret ; <nl> URLContext * hd ; <nl>  <nl> + if ( size < 2 ) <nl> + return AVERROR ( EINVAL ); <nl> + <nl> if ( RTP_PT_IS_RTCP ( buf [ 1 ])) { <nl> /* RTCP payload type */ <nl> hd = s -> rtcp_hd ;
static int dxa_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> } <nl> avio_seek ( s -> pb , c -> vidpos , SEEK_SET ); <nl> while (! url_feof ( s -> pb ) && c -> frames ){ <nl> - avio_read ( s -> pb , buf , 4 ); <nl> + if (( ret = avio_read ( s -> pb , buf , 4 )) != 4 ) { <nl> + av_log ( s , AV_LOG_ERROR , " failed reading chunk type \ n "); <nl> + return ret < 0 ? ret : AVERROR_INVALIDDATA ; <nl> + } <nl> switch ( AV_RL32 ( buf )){ <nl> case MKTAG (' N ', ' U ', ' L ', ' L '): <nl> if ( av_new_packet ( pkt , 4 + pal_size ) < 0 ) <nl> static int dxa_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> avio_read ( s -> pb , pal + 4 , 768 ); <nl> break ; <nl> case MKTAG (' F ', ' R ', ' A ', ' M '): <nl> - avio_read ( s -> pb , buf + 4 , DXA_EXTRA_SIZE - 4 ); <nl> + if (( ret = avio_read ( s -> pb , buf + 4 , DXA_EXTRA_SIZE - 4 )) != DXA_EXTRA_SIZE - 4 ) { <nl> + av_log ( s , AV_LOG_ERROR , " failed reading dxa_extra \ n "); <nl> + return ret < 0 ? ret : AVERROR_INVALIDDATA ; <nl> + } <nl> size = AV_RB32 ( buf + 5 ); <nl> if ( size > 0xFFFFFF ){ <nl> av_log ( s , AV_LOG_ERROR , " Frame size is too big : % d \ n ", size );
static int decode_frame_header ( AVCodecContext * ctx , <nl> w = get_bits (& s -> gb , 16 ) + 1 ; <nl> h = get_bits (& s -> gb , 16 ) + 1 ; <nl> } <nl> - s -> use_last_frame_mvs &= s -> frames [ LAST_FRAME ]. tf . f -> width == w && <nl> - s -> frames [ LAST_FRAME ]. tf . f -> height == h ; <nl> + // Note that in this code , " CUR_FRAME " is actually before we <nl> + // have formally allocated a frame , and thus actually represents <nl> + // the _last_ frame <nl> + s -> use_last_frame_mvs &= s -> frames [ CUR_FRAME ]. tf . f -> width == w && <nl> + s -> frames [ CUR_FRAME ]. tf . f -> height == h ; <nl> if ( get_bits1 (& s -> gb )) // display size <nl> skip_bits (& s -> gb , 32 ); <nl> s -> highprecisionmvs = get_bits1 (& s -> gb );
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> tag = avio_rl32 ( pb ); <nl> size = avio_rl32 ( pb ); <nl>  <nl> - if ( size > avi -> fsize ){ <nl> - av_log ( s , AV_LOG_ERROR , " chunk size is too big during header parsing \ n "); <nl> - goto fail ; <nl> - } <nl> - <nl> print_tag (" tag ", tag , size ); <nl>  <nl> switch ( tag ) { <nl> static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> break ; <nl> } <nl>  <nl> - if ( size > 10 * 4 && size <( 1 << 30 )){ <nl> + if ( size > 10 * 4 && size <( 1 << 30 ) && size < avi -> fsize ){ <nl> st -> codec -> extradata_size = size - 10 * 4 ; <nl> st -> codec -> extradata = av_malloc ( st -> codec -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! st -> codec -> extradata ) {
static av_cold int twin_decode_init ( AVCodecContext * avctx ) <nl> avctx -> channels = AV_RB32 ( avctx -> extradata ) + 1 ; <nl> avctx -> bit_rate = AV_RB32 ( avctx -> extradata + 4 ) * 1000 ; <nl> isampf = AV_RB32 ( avctx -> extradata + 8 ); <nl> + <nl> + if ( isampf < 8 || isampf > 44 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Unsupported sample rate \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> switch ( isampf ) { <nl> case 44 : avctx -> sample_rate = 44100 ; break ; <nl> case 22 : avctx -> sample_rate = 22050 ; break ;
AVFilterFormats * avfilter_merge_formats ( AVFilterFormats * a , AVFilterFormats * b ) <nl> AVFilterFormats * ret ; <nl> unsigned i , j , k = 0 , m_count ; <nl>  <nl> + if ( a == b ) <nl> + return a ; <nl> + <nl> ret = av_mallocz ( sizeof (* ret )); <nl>  <nl> /* merge list of formats */
static int amv_encode_picture ( AVCodecContext * avctx , AVPacket * pkt , <nl> return - 1 ; <nl>  <nl> pic = av_frame_alloc (); <nl> + if (! pic ) <nl> + return AVERROR ( ENOMEM ); <nl> av_frame_ref ( pic , pic_arg ); <nl> // picture should be flipped upside - down <nl> for ( i = 0 ; i < 3 ; i ++) {
static void av_update_stream_timings ( AVFormatContext * ic ) <nl> duration = INT64_MIN ; <nl> for ( i = 0 ; i < ic -> nb_streams ; i ++) { <nl> st = ic -> streams [ i ]; <nl> - if ( st -> start_time != AV_NOPTS_VALUE ) { <nl> + if ( st -> start_time != AV_NOPTS_VALUE && st -> time_base . den ) { <nl> start_time1 = av_rescale_q ( st -> start_time , st -> time_base , AV_TIME_BASE_Q ); <nl> if ( start_time1 < start_time ) <nl> start_time = start_time1 ;
static int h263p_decode_umotion ( MpegEncContext * s , int pred ) <nl> code += get_bits1 (& s -> gb ); <nl> if ( code >= 32768 ) { <nl> avpriv_request_sample ( s -> avctx , " Huge DMV "); <nl> - return AVERROR_INVALIDDATA ; <nl> + return 0xffff ; <nl> } <nl> } <nl> sign = code & 1 ;
static int ism_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> SmoothStreamingContext * c = s -> priv_data ; <nl> AVStream * st = s -> streams [ pkt -> stream_index ]; <nl> OutputStream * os = & c -> streams [ pkt -> stream_index ]; <nl> - int64_t end_dts = ( c -> nb_fragments + 1 ) * c -> min_frag_duration ; <nl> + int64_t end_dts = ( c -> nb_fragments + 1 ) * ( int64_t ) c -> min_frag_duration ; <nl> int ret ; <nl>  <nl> if ( st -> first_dts == AV_NOPTS_VALUE )
static int raw_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> & pal_size ); <nl> int ret ; <nl>  <nl> - if ( pal_size != AVPALETTE_SIZE ) { <nl> + if ( pal && pal_size != AVPALETTE_SIZE ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Palette size % d is wrong \ n ", pal_size ); <nl> pal = NULL ; <nl> }
static int create_filtergraph ( AVFilterContext * ctx , <nl> s -> yuv2yuv_fastmode = s -> rgb2rgb_passthrough && fmt_identical ; <nl> s -> yuv2yuv_passthrough = s -> yuv2yuv_fastmode && s -> in_rng == s -> out_rng && <nl> ! memcmp ( s -> in_lumacoef , s -> out_lumacoef , <nl> - sizeof (* s -> in_lumacoef )); <nl> + sizeof (* s -> in_lumacoef )) && <nl> + in_desc -> comp [ 0 ]. depth == out_desc -> comp [ 0 ]. depth ; <nl> if (! s -> yuv2yuv_passthrough ) { <nl> if ( redo_yuv2rgb ) { <nl> double rgb2yuv [ 3 ][ 3 ], (* yuv2rgb )[ 3 ] = s -> yuv2rgb_dbl_coeffs ; <nl> static int filter_frame ( AVFilterLink * link , AVFrame * in ) <nl> td . in_ss_h = av_pix_fmt_desc_get ( in -> format )-> log2_chroma_h ; <nl> td . out_ss_h = av_pix_fmt_desc_get ( out -> format )-> log2_chroma_h ; <nl> if ( s -> yuv2yuv_passthrough ) { <nl> - av_frame_copy ( out , in ); <nl> + res = av_frame_copy ( out , in ); <nl> + if ( res < 0 ) <nl> + return res ; <nl> } else { <nl> ctx -> internal -> execute ( ctx , convert , & td , NULL , <nl> FFMIN (( in -> height + 1 ) >> 1 , ctx -> graph -> nb_threads ));
static int parse_ffconfig ( const char * filename ) <nl> break ; <nl> } <nl> feed -> feed_max_size = ( int64_t ) fsize ; <nl> + if ( feed -> feed_max_size < FFM_PACKET_SIZE * 4 ) { <nl> + fprintf ( stderr , "% s :% d : Feed max file size is too small , " <nl> + " must be at least % d \ n ", filename , line_num , FFM_PACKET_SIZE * 4 ); <nl> + errors ++; <nl> + } <nl> } <nl> } else if (! strcasecmp ( cmd , "</ Feed >")) { <nl> if (! feed ) {
int av_cold ff_ivi_init_tiles ( IVIPlaneDesc * planes , int tile_width , int tile_hei <nl> t_width >>= 1 ; <nl> t_height >>= 1 ; <nl> } <nl> + if ( t_width <= 0 || t_height <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> for ( b = 0 ; b < planes [ p ]. num_bands ; b ++) { <nl> band = & planes [ p ]. bands [ b ];
static int asf_read_frame_header ( AVFormatContext * s , AVIOContext * pb ){ <nl> } <nl> // printf (" Fragsize % d \ n ", asf -> packet_frag_size ); <nl> } else { <nl> - if ( rsize > asf -> packet_size_left ) { <nl> - av_log ( s , AV_LOG_ERROR , " packet_replic_size is invalid \ n "); <nl> - return - 1 ; <nl> - } <nl> asf -> packet_frag_size = asf -> packet_size_left - rsize ; <nl> // printf (" Using rest % d % d % d \ n ", asf -> packet_frag_size , asf -> packet_size_left , rsize ); <nl> }
static int rsd_read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> par -> channels = avio_rl32 ( pb ); <nl> - if (! par -> channels ) <nl> + if ( par -> channels <= 0 || par -> channels > INT_MAX / 36 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels : % d \ n ", par -> channels ); <nl> return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> avio_skip ( pb , 4 ); // Bit depth <nl> par -> sample_rate = avio_rl32 ( pb );
static void ffmpeg_cleanup ( int ret ) <nl> av_freep (& ost -> audio_channels_map ); <nl> ost -> audio_channels_mapped = 0 ; <nl>  <nl> + av_dict_free (& ost -> sws_dict ); <nl> + <nl> avcodec_free_context (& ost -> enc_ctx ); <nl>  <nl> av_freep (& output_streams [ i ]);
static void calc_input_response ( WMAVoiceContext * s , float * lpcs , <nl> ( 5 . 0 / 14 . 7 )); <nl> angle_mul = gain_mul * ( 8 . 0 * M_LN10 / M_PI ); <nl> for ( n = 0 ; n <= 64 ; n ++) { <nl> - float pow ; <nl> + float pwr ; <nl>  <nl> idx = FFMAX ( 0 , lrint (( max - lpcs [ n ]) * irange ) - 1 ); <nl> - pow = wmavoice_denoise_power_table [ s -> denoise_strength ][ idx ]; <nl> - lpcs [ n ] = angle_mul * pow ; <nl> + pwr = wmavoice_denoise_power_table [ s -> denoise_strength ][ idx ]; <nl> + lpcs [ n ] = angle_mul * pwr ; <nl>  <nl> /* 70 . 57 =~ 1 / log10 ( 1 . 0331663 ) */ <nl> - idx = ( pow * gain_mul - 0 . 0295 ) * 70 . 570526123 ; <nl> + idx = ( pwr * gain_mul - 0 . 0295 ) * 70 . 570526123 ; <nl> if ( idx > 127 ) { // fallback if index falls outside table range <nl> coeffs [ n ] = wmavoice_energy_table [ 127 ] * <nl> powf ( 1 . 0331663 , idx - 127 );
static int decode_mb_info ( IVI4DecContext * ctx , IVIBandDesc * band , <nl> } <nl>  <nl> mb -> mv_x = mb -> mv_y = 0 ; /* no motion vector coded */ <nl> - if ( band -> inherit_mv ) { <nl> + if ( band -> inherit_mv && ref_mb ) { <nl> /* motion vector inheritance */ <nl> if ( mv_scale ) { <nl> mb -> mv_x = ivi_scale_mv ( ref_mb -> mv_x , mv_scale ); <nl> static int decode_mb_info ( IVI4DecContext * ctx , IVIBandDesc * band , <nl> if (! mb -> type ) { <nl> mb -> mv_x = mb -> mv_y = 0 ; /* there is no motion vector in intra - macroblocks */ <nl> } else { <nl> - if ( band -> inherit_mv ) { <nl> + if ( band -> inherit_mv && ref_mb ) { <nl> /* motion vector inheritance */ <nl> if ( mv_scale ) { <nl> mb -> mv_x = ivi_scale_mv ( ref_mb -> mv_x , mv_scale );
static int decode_lowdelay ( DiracContext * s ) <nl> } else { <nl> for ( slice_y = 0 ; bufsize > 0 && slice_y < s -> num_y ; slice_y ++) { <nl> for ( slice_x = 0 ; bufsize > 0 && slice_x < s -> num_x ; slice_x ++) { <nl> - bytes = ( slice_num + 1 ) * s -> lowdelay . bytes . num / s -> lowdelay . bytes . den <nl> - - slice_num * s -> lowdelay . bytes . num / s -> lowdelay . bytes . den ; <nl> + bytes = ( slice_num + 1 ) * ( int64_t ) s -> lowdelay . bytes . num / s -> lowdelay . bytes . den <nl> + - slice_num * ( int64_t ) s -> lowdelay . bytes . num / s -> lowdelay . bytes . den ; <nl> slices [ slice_num ]. bytes = bytes ; <nl> slices [ slice_num ]. slice_x = slice_x ; <nl> slices [ slice_num ]. slice_y = slice_y ;
static int h264_set_parameter_from_sps ( H264Context * h ) <nl> if ( h -> avctx -> has_b_frames < 2 ) <nl> h -> avctx -> has_b_frames = ! h -> low_delay ; <nl>  <nl> + if ( h -> sps . bit_depth_luma != h -> sps . bit_depth_chroma ) { <nl> + av_log_missing_feature ( h -> avctx , <nl> + " Different bit depth between chroma and luma ", 1 ); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> + <nl> if ( h -> avctx -> bits_per_raw_sample != h -> sps . bit_depth_luma || <nl> h -> cur_chroma_format_idc != h -> sps . chroma_format_idc ) { <nl> if ( h -> avctx -> codec &&
static int img_read_packet ( AVFormatContext * s1 , AVPacket * pkt ) <nl> av_free_packet ( pkt ); <nl> return AVERROR_IO ; /* signal EOF */ <nl> } else { <nl> + pkt -> size = ret ; <nl> s -> img_count ++; <nl> s -> img_number ++; <nl> return 0 ;
static const struct algo idct_tab [] = { <nl> # elif ARCH_X86 <nl> # include " x86 / dct - test . c " <nl> # else <nl> - static const struct algo fdct_tab_arch [] = { 0 }; <nl> - static const struct algo idct_tab_arch [] = { 0 }; <nl> + static const struct algo fdct_tab_arch [] = { { 0 } }; <nl> + static const struct algo idct_tab_arch [] = { { 0 } }; <nl> # endif <nl>  <nl> # define AANSCALE_BITS 12
static int alloc_blocks ( SnowContext * s ){ <nl> s -> b_width = w ; <nl> s -> b_height = h ; <nl>  <nl> + av_free ( s -> block ); <nl> s -> block = av_mallocz ( w * h * sizeof ( BlockNode ) << ( s -> block_max_depth * 2 )); <nl> return 0 ; <nl> } <nl> static int decode_frame ( AVCodecContext * avctx , void * data , int * data_size , AVPac <nl> && p -> hcoeff [ 2 ]== 2 ; <nl> } <nl>  <nl> - if (! s -> block ) alloc_blocks ( s ); <nl> + alloc_blocks ( s ); <nl>  <nl> frame_start ( s ); <nl> // keyframe flag duplication mess FIXME
static int request_frame ( AVFilterLink * outlink ) <nl> s -> frames_out ++; <nl> if ( j > 0 ) s -> dup ++; <nl> } <nl> + av_frame_free (& buf ); <nl> } else { <nl> /* for delta less or equal to 0 , we should drop the frame , <nl> * otherwise , we will have one or more extra frames */
static void RENAME ( vertical_compose53iL0 )( uint8_t * _b0 , uint8_t * _b1 , uint8_t * _ <nl> TYPE * b1 = ( TYPE *) _b1 ; <nl> TYPE * b2 = ( TYPE *) _b2 ; <nl> for ( i = 0 ; i < width ; i ++) <nl> - b1 [ i ] -= ( b0 [ i ] + b2 [ i ] + 2 ) >> 2 ; <nl> + b1 [ i ] -= ( int )( b0 [ i ] + ( unsigned ) b2 [ i ] + 2 ) >> 2 ; <nl> } <nl>  <nl> static av_always_inline void RENAME ( interleave )( TYPE * dst , TYPE * src0 , TYPE * src1 , int w2 ,
static char * sdp_write_media_attributes ( char * buff , int size , AVCodecContext * c , <nl> break ; <nl> default : <nl> av_log ( c , AV_LOG_ERROR , " Unsupported pixel format .\ n "); <nl> + av_free ( config ); <nl> return NULL ; <nl> } <nl> 
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> nalsize = 0 ; <nl> for ( i = 0 ; i < h -> nal_length_size ; i ++) <nl> nalsize = ( nalsize << 8 ) | buf [ buf_index ++]; <nl> - if ( nalsize <= 1 || ( nalsize + buf_index > buf_size )){ <nl> + if ( nalsize <= 1 || nalsize > buf_size - buf_index ){ <nl> if ( nalsize == 1 ){ <nl> buf_index ++; <nl> continue ;
static int mp3_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> if ( mp3 -> pics_to_write ) { <nl> /* buffer audio packets until we get all the pictures */ <nl> AVPacketList * pktl = av_mallocz ( sizeof (* pktl )); <nl> + int ret ; <nl> if (! pktl ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - pktl -> pkt = * pkt ; <nl> - pktl -> pkt . buf = av_buffer_ref ( pkt -> buf ); <nl> - if (! pktl -> pkt . buf ) { <nl> + ret = av_copy_packet (& pktl -> pkt , pkt ); <nl> + if ( ret < 0 ) { <nl> av_freep (& pktl ); <nl> - return AVERROR ( ENOMEM ); <nl> + return ret ; <nl> } <nl>  <nl> if ( mp3 -> queue_end )
static int matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , <nl> int offset = 0 , pkt_size = lace_size [ n ]; <nl> uint8_t * pkt_data = data ; <nl>  <nl> + if ( lace_size [ n ] > size ) { <nl> + av_log ( matroska -> ctx , AV_LOG_ERROR , " Invalid packet size \ n "); <nl> + break ; <nl> + } <nl> + <nl> if ( encodings && encodings -> scope & 1 ) { <nl> offset = matroska_decode_buffer (& pkt_data ,& pkt_size , track ); <nl> if ( offset < 0 ) <nl> static int matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , <nl> if ( timecode != AV_NOPTS_VALUE ) <nl> timecode = duration ? timecode + duration : AV_NOPTS_VALUE ; <nl> data += lace_size [ n ]; <nl> + size -= lace_size [ n ]; <nl> } <nl> } <nl> 
static int rtp_mpegts_write_header ( AVFormatContext * s ) <nl> st -> time_base . num = 1 ; <nl> st -> time_base . den = 90000 ; <nl> st -> codec -> codec_id = AV_CODEC_ID_MPEG2TS ; <nl> - chain -> rtp_ctx = rtp_ctx ; <nl> rtp_ctx -> pb = s -> pb ; <nl> if (( ret = avformat_write_header ( rtp_ctx , NULL )) < 0 ) <nl> goto fail ; <nl> - rtp_ctx = NULL ; <nl> + chain -> rtp_ctx = rtp_ctx ; <nl>  <nl> return 0 ; <nl> 
static int ac3_decode_frame ( AVCodecContext * avctx , void * data , <nl> memcpy ( s -> outptr [ channel_map [ ch ]], output [ ch ], sizeof (** output ) * AC3_BLOCK_SIZE ); <nl> for ( ch = 0 ; ch < s -> out_channels ; ch ++) <nl> output [ ch ] = s -> outptr [ channel_map [ ch ]]; <nl> - for ( ch = 0 ; ch < s -> channels ; ch ++) <nl> + for ( ch = 0 ; ch < s -> out_channels ; ch ++) <nl> s -> outptr [ ch ] += AC3_BLOCK_SIZE ; <nl> } <nl> 
static int rv10_decode_frame ( AVCodecContext * avctx , <nl> slice_count = avctx -> slice_count ; <nl>  <nl> for ( i = 0 ; i < slice_count ; i ++){ <nl> - int offset = get_slice_offset ( avctx , slices_hdr , i ); <nl> + unsigned offset = get_slice_offset ( avctx , slices_hdr , i ); <nl> int size , size2 ; <nl>  <nl> + if ( offset >= buf_size ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( i + 1 == slice_count ) <nl> size = buf_size - offset ; <nl> else <nl> static int rv10_decode_frame ( AVCodecContext * avctx , <nl> else <nl> size2 = get_slice_offset ( avctx , slices_hdr , i + 2 ) - offset ; <nl>  <nl> + if ( size <= 0 || size2 <= 0 || <nl> + offset + FFMAX ( size , size2 ) > buf_size ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( rv10_decode_packet ( avctx , buf + offset , size , size2 ) > 8 * size ) <nl> i ++; <nl> }
static av_cold int X264_close ( AVCodecContext * avctx ) <nl> X264Context * x4 = avctx -> priv_data ; <nl>  <nl> av_freep (& avctx -> extradata ); <nl> - av_free ( x4 -> sei ); <nl> + av_freep (& x4 -> sei ); <nl>  <nl> - if ( x4 -> enc ) <nl> + if ( x4 -> enc ) { <nl> x264_encoder_close ( x4 -> enc ); <nl> + x4 -> enc = NULL ; <nl> + } <nl>  <nl> av_frame_free (& avctx -> coded_frame ); <nl>  <nl> AVCodec ff_libx264_encoder = { <nl> . priv_class = & class , <nl> . defaults = x264_defaults , <nl> . init_static_data = X264_init_static , <nl> + . caps_internal = FF_CODEC_CAP_INIT_THREADSAFE | <nl> + FF_CODEC_CAP_INIT_CLEANUP , <nl> };
int ff_vc1_parse_frame_header ( VC1Context * v , GetBitContext * gb ) <nl> { <nl> int pqindex , lowquant , status ; <nl>  <nl> + v -> field_mode = 0 ; <nl> + v -> fcm = 0 ; <nl> if ( v -> finterpflag ) <nl> v -> interpfrm = get_bits1 ( gb ); <nl> if (! v -> s . avctx -> codec )
start : <nl> c -> seek_timestamp = AV_NOPTS_VALUE ; <nl> break ; <nl> } <nl> + av_free_packet (& var -> pkt ); <nl> + reset_packet (& var -> pkt ); <nl> } <nl> } <nl> /* Check if this stream still is on an earlier segment number , or
static int url_alloc_for_protocol ( URLContext ** puc , struct URLProtocol * up , <nl> uc -> max_packet_size = 0 ; /* default : stream file */ <nl> if ( up -> priv_data_size ) { <nl> uc -> priv_data = av_mallocz ( up -> priv_data_size ); <nl> + if (! uc -> priv_data ) { <nl> + err = AVERROR ( ENOMEM ); <nl> + goto fail ; <nl> + } <nl> if ( up -> priv_data_class ) { <nl> int proto_len = strlen ( up -> name ); <nl> char * start = strchr ( uc -> filename , ','); <nl> static int url_alloc_for_protocol ( URLContext ** puc , struct URLProtocol * up , <nl> return 0 ; <nl> fail : <nl> * puc = NULL ; <nl> + if ( uc ) <nl> + av_freep (& uc -> priv_data ); <nl> + av_freep (& uc ); <nl> # if CONFIG_NETWORK <nl> if ( up -> flags & URL_PROTOCOL_FLAG_NETWORK ) <nl> ff_network_close ();
static int matroska_decode_buffer ( uint8_t ** buf , int * buf_size , <nl>  <nl> switch ( encodings [ 0 ]. compression . algo ) { <nl> case MATROSKA_TRACK_ENCODING_COMP_HEADERSTRIP : <nl> + if ( encodings [ 0 ]. compression . settings . size && ! encodings [ 0 ]. compression . settings . data ) { <nl> + av_log ( 0 , AV_LOG_ERROR , " Compression size but no data in headerstrip \ n "); <nl> + return - 1 ; <nl> + } <nl> return encodings [ 0 ]. compression . settings . size ; <nl> case MATROSKA_TRACK_ENCODING_COMP_LZO : <nl> do {
static int query_formats ( AVFilterContext * ctx ) <nl> EvalContext * eval = ctx -> priv ; <nl> enum AVSampleFormat sample_fmts [] = { AV_SAMPLE_FMT_DBL , AV_SAMPLE_FMT_NONE }; <nl> int64_t chlayouts [] = { eval -> chlayout , - 1 }; <nl> + int sample_rates [] = { eval -> sample_rate , - 1 }; <nl>  <nl> avfilter_set_common_sample_formats ( ctx , avfilter_make_format_list ( sample_fmts )); <nl> ff_set_common_channel_layouts ( ctx , avfilter_make_format64_list ( chlayouts )); <nl> + ff_set_common_samplerates ( ctx , avfilter_make_format_list ( sample_rates )); <nl>  <nl> return 0 ; <nl> }
static void lumRangeToJpeg16_c ( int16_t * _dst , int width ) <nl> int i ; <nl> int32_t * dst = ( int32_t *) _dst ; <nl> for ( i = 0 ; i < width ; i ++) <nl> - dst [ i ] = ( FFMIN ( dst [ i ], 30189 << 4 )* 19077 - ( 39057361 << 4 ))>> 14 ; <nl> + dst [ i ] = ( FFMIN ( dst [ i ], 30189 << 4 )* 4769 - ( 39057361 << 2 ))>> 12 ; <nl> } <nl> static void lumRangeFromJpeg16_c ( int16_t * _dst , int width ) <nl> {
static void decode_b ( AVCodecContext * ctx , int row , int col , <nl> int w = FFMIN ( s -> cols - col , w4 ) * 8 >> s -> ss_h ; <nl> int h = FFMIN ( s -> rows - row , h4 ) * 8 >> s -> ss_v , n , o = 0 ; <nl>  <nl> - for ( n = 1 ; o < w ; n ++) { <nl> + for ( n = s -> ss_h ; o < w ; n ++) { <nl> int bw = 64 >> n ; <nl>  <nl> av_assert2 ( n <= 4 );
static void check_luma_dc_wht ( void ) <nl> } <nl>  <nl> # define SRC_BUF_STRIDE 32 <nl> -# define SRC_BUF_SIZE (( size + 5 ) * SRC_BUF_STRIDE ) <nl> +# define SRC_BUF_SIZE ((( size << ( size < 16 )) + 5 ) * SRC_BUF_STRIDE ) <nl> // The mc subpixel interpolation filter needs the 2 previous pixels in either <nl> // direction , the + 1 is to make sure the actual load addresses always are <nl> // unaligned .
int av_write_trailer ( AVFormatContext * s ) <nl> if ( s -> oformat -> write_trailer ) <nl> ret = s -> oformat -> write_trailer ( s ); <nl>  <nl> - if (!( s -> oformat -> flags & AVFMT_NOFILE )) <nl> - avio_flush ( s -> pb ); <nl> - <nl> fail : <nl> if ( s -> pb ) <nl> avio_flush ( s -> pb );
int ff_h264_decode_ref_pic_marking ( H264Context * h , GetBitContext * gb , <nl> h -> mmco_index = mmco_index ; <nl> } else if (! first_slice && mmco_index >= 0 && <nl> ( mmco_index != h -> mmco_index || <nl> - ( i = check_opcodes ( h -> mmco , mmco_temp , mmco_index )))) { <nl> + check_opcodes ( h -> mmco , mmco_temp , mmco_index ))) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , <nl> - " Inconsistent MMCO state between slices [% d , % d , % d ]\ n ", <nl> - mmco_index , h -> mmco_index , i ); <nl> + " Inconsistent MMCO state between slices [% d , % d ]\ n ", <nl> + mmco_index , h -> mmco_index ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
static int parse_channel_name ( char ** arg , int * rchannel , int * rnamed ) <nl> int64_t layout , layout0 ; <nl>  <nl> /* try to parse a channel name , e . g . " FL " */ <nl> - if ( sscanf (* arg , " % 7 [ A - Z ] % n ", buf , & len )) { <nl> + if ( sscanf (* arg , "% 7 [ A - Z ]% n ", buf , & len )) { <nl> layout0 = layout = av_get_channel_layout ( buf ); <nl> /* channel_id <- first set bit in layout */ <nl> for ( i = 32 ; i > 0 ; i >>= 1 ) { <nl> static int parse_channel_name ( char ** arg , int * rchannel , int * rnamed ) <nl> return 0 ; <nl> } <nl> /* try to parse a channel number , e . g . " c2 " */ <nl> - if ( sscanf (* arg , " c % d % n ", & channel_id , & len ) && <nl> + if ( sscanf (* arg , " c % d % n ", & channel_id , & len ) && <nl> channel_id >= 0 && channel_id < MAX_CHANNELS ) { <nl> * rchannel = channel_id ; <nl> * rnamed = 0 ; <nl> static av_cold int init ( AVFilterContext * ctx , const char * args0 ) <nl> " Invalid out channel name \"%. 8s \"\ n ", arg0 ); <nl> return AVERROR ( EINVAL ); <nl> } <nl> + skip_spaces (& arg ); <nl> if (* arg == '=') { <nl> arg ++; <nl> } else if (* arg == '<') { <nl> static av_cold int init ( AVFilterContext * ctx , const char * args0 ) <nl> /* gains */ <nl> while ( 1 ) { <nl> gain = 1 ; <nl> - if ( sscanf ( arg , " % lf % n * % n ", & gain , & len , & len )) <nl> + if ( sscanf ( arg , "% lf % n *% n ", & gain , & len , & len )) <nl> arg += len ; <nl> if ( parse_channel_name (& arg , & in_ch_id , & named )){ <nl> av_log ( ctx , AV_LOG_ERROR , <nl> static av_cold int init ( AVFilterContext * ctx , const char * args0 ) <nl> return AVERROR ( EINVAL ); <nl> } <nl> pan -> gain [ out_ch_id ][ in_ch_id ] = gain ; <nl> + skip_spaces (& arg ); <nl> if (!* arg ) <nl> break ; <nl> if (* arg != '+') { <nl> static av_cold int init ( AVFilterContext * ctx , const char * args0 ) <nl> return AVERROR ( EINVAL ); <nl> } <nl> arg ++; <nl> - skip_spaces (& arg ); <nl> } <nl> } <nl> pan -> need_renumber = !! nb_in_channels [ 1 ];
static int raw_decode ( AVCodecContext * avctx , <nl> uint8_t * dst = context -> buffer ; <nl> buf_size = context -> length - 256 * 4 ; <nl> if ( avctx -> bits_per_coded_sample == 4 ){ <nl> - for ( i = 0 ; 2 * i + 1 < buf_size ; i ++){ <nl> + for ( i = 0 ; 2 * i + 1 < buf_size && i < avpkt -> size ; i ++){ <nl> dst [ 2 * i + 0 ]= buf [ i ]>> 4 ; <nl> dst [ 2 * i + 1 ]= buf [ i ]& 15 ; <nl> } <nl> linesize_align = 8 ; <nl> } else { <nl> - for ( i = 0 ; 4 * i + 3 < buf_size ; i ++){ <nl> + for ( i = 0 ; 4 * i + 3 < buf_size && i < avpkt -> size ; i ++){ <nl> dst [ 4 * i + 0 ]= buf [ i ]>> 6 ; <nl> dst [ 4 * i + 1 ]= buf [ i ]>> 4 & 3 ; <nl> dst [ 4 * i + 2 ]= buf [ i ]>> 2 & 3 ;
static void swap_channel_layouts_on_filter ( AVFilterContext * filter ) <nl>  <nl> for ( i = 0 ; i < filter -> nb_outputs ; i ++) { <nl> AVFilterLink * outlink = filter -> outputs [ i ]; <nl> - int best_idx , best_score = INT_MIN , best_count_diff = INT_MAX ; <nl> + int best_idx = - 1 , best_score = INT_MIN , best_count_diff = INT_MAX ; <nl>  <nl> if ( outlink -> type != AVMEDIA_TYPE_AUDIO || <nl> outlink -> in_channel_layouts -> nb_channel_layouts < 2 ) <nl> static void swap_channel_layouts_on_filter ( AVFilterContext * filter ) <nl> best_count_diff = count_diff ; <nl> } <nl> } <nl> + av_assert0 ( best_idx >= 0 ); <nl> FFSWAP ( uint64_t , outlink -> in_channel_layouts -> channel_layouts [ 0 ], <nl> outlink -> in_channel_layouts -> channel_layouts [ best_idx ]); <nl> }
static av_cold int init ( AVFilterContext * ctx ) <nl>  <nl> mb -> cache_allocated = mb -> w * mb -> h * 3 ; <nl> mb -> cache_used = 0 ; <nl> - mb -> point_cache = av_malloc ( sizeof (* mb -> point_cache )* mb -> cache_allocated ); <nl> - mb -> next_cache = av_malloc ( sizeof (* mb -> next_cache )* mb -> cache_allocated ); <nl> - mb -> zyklus = av_malloc ( sizeof (* mb -> zyklus ) * ( mb -> maxiter + 16 )); <nl> + mb -> point_cache = av_malloc_array ( mb -> cache_allocated , sizeof (* mb -> point_cache )); <nl> + mb -> next_cache = av_malloc_array ( mb -> cache_allocated , sizeof (* mb -> next_cache )); <nl> + mb -> zyklus = av_malloc_array ( mb -> maxiter + 16 , sizeof (* mb -> zyklus )); <nl>  <nl> return 0 ; <nl> }
static int vble_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> if ( pic -> data [ 0 ]) <nl> avctx -> release_buffer ( avctx , pic ); <nl>  <nl> + if ( avpkt -> size < 4 || avpkt -> size - 4 > INT_MAX / 8 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid packet size \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> /* Allocate buffer */ <nl> if ( avctx -> get_buffer ( avctx , pic ) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Could not allocate buffer .\ n ");
static int dfa_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> frame_size = AV_RL32 ( pkt -> data + pkt -> size - 8 ); <nl> if ( frame_size > INT_MAX - 4 ) { <nl> av_log ( s , AV_LOG_ERROR , " Too large chunk size : %" PRIu32 "\ n ", frame_size ); <nl> + av_packet_unref ( pkt ); <nl> return AVERROR ( EIO ); <nl> } <nl> if ( AV_RL32 ( pkt -> data + pkt -> size - 12 ) == MKTAG (' E ', ' O ', ' F ', ' R ')) {
static int mov_read_default ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> left = a . size - avio_tell ( pb ) + start_pos ; <nl> if ( left > 0 ) /* skip garbage at atom end */ <nl> avio_skip ( pb , left ); <nl> + else if ( left < 0 ) { <nl> + av_log ( c -> fc , AV_LOG_WARNING , <nl> + " overread end of atom '%. 4s ' by %" PRId64 " bytes \ n ", <nl> + ( char *)& a . type , - left ); <nl> + avio_seek ( pb , left , SEEK_CUR ); <nl> + } <nl> } <nl>  <nl> total_size += a . size ;
static void pmt_cb ( void * opaque , const uint8_t * section , int section_len ) <nl> int program_info_length , pcr_pid , pid , stream_type ; <nl> int desc_list_len , desc_len , desc_tag ; <nl> int comp_page = 0 , anc_page = 0 ; /* initialize to kill warnings */ <nl> - char language [ 4 ]; <nl> + char language [ 4 ] = { 0 }; /* initialize to kill warnings */ <nl>  <nl> # ifdef DEBUG_SI <nl> av_log ( ts -> stream , AV_LOG_DEBUG , " PMT : len % i \ n ", section_len );
static int flashsv2_prime ( FlashSVContext * s , uint8_t * src , <nl> s -> zstream . avail_out = s -> block_size * 3 ; <nl> inflate (& s -> zstream , Z_SYNC_FLUSH ); <nl>  <nl> - deflateInit (& zs , 0 ); <nl> + if ( deflateInit (& zs , 0 ) != Z_OK ) <nl> + return - 1 ; <nl> zs . next_in = s -> tmpblock ; <nl> zs . avail_in = s -> block_size * 3 - s -> zstream . avail_out ; <nl> zs . next_out = s -> deflate_block ;
static void truncate_ts ( AVStream * st , AVPacket * pkt ){ <nl>  <nl> int av_write_frame ( AVFormatContext * s , AVPacket * pkt ) <nl> { <nl> - int ret ; <nl> + int ret = compute_pkt_fields2 ( s -> streams [ pkt -> stream_index ], pkt ); <nl>  <nl> - ret = compute_pkt_fields2 ( s -> streams [ pkt -> stream_index ], pkt ); <nl> if ( ret < 0 && !( s -> oformat -> flags & AVFMT_NOTIMESTAMPS )) <nl> return ret ; <nl> 
static int matroska_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> ret = matroska_parse_cluster ( matroska ); <nl> } <nl>  <nl> + if ( ret == AVERROR_INVALIDDATA ) { <nl> + pkt -> flags |= AV_PKT_FLAG_CORRUPT ; <nl> + return 0 ; <nl> + } <nl> + <nl> return ret ; <nl> } <nl> 
void ff_update_link_current_pts ( AVFilterLink * link , int64_t pts ) <nl> int avfilter_process_command ( AVFilterContext * filter , const char * cmd , const char * arg , char * res , int res_len , int flags ) <nl> { <nl> if (! strcmp ( cmd , " ping ")){ <nl> + char local_res [ 256 ] = { 0 }; <nl> + <nl> + if (! res ) { <nl> + res = local_res ; <nl> + res_len = sizeof ( local_res ); <nl> + } <nl> av_strlcatf ( res , res_len , " pong from :% s % s \ n ", filter -> filter -> name , filter -> name ); <nl> + if ( res == local_res ) <nl> + av_log ( filter , AV_LOG_INFO , "% s ", res ); <nl> return 0 ; <nl> } else if (! strcmp ( cmd , " enable ")) { <nl> return set_enable_expr ( filter , arg );
typedef struct { <nl>  <nl> typedef struct { <nl> int64_t frames_hdr_strm ; <nl> - int audio_strm_length ; <nl> + int64_t audio_strm_length ; <nl> int packet_count ; <nl> int entry ; <nl> 
static int encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> for ( i = 0 ; i < SUBFRAMES ; i ++) <nl> put_subframe ( c , i ); <nl>  <nl> + <nl> + for ( i = put_bits_count (& c -> pb ); i < 8 * c -> frame_size ; i ++) <nl> + put_bits (& c -> pb , 1 , 0 ); <nl> + <nl> flush_put_bits (& c -> pb ); <nl>  <nl> avpkt -> pts = frame -> pts ;
static int mimic_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + res = ff_set_dimensions ( avctx , width , height ); <nl> + if ( res < 0 ) <nl> + return res ; <nl> + <nl> ctx -> avctx = avctx ; <nl> - avctx -> width = width ; <nl> - avctx -> height = height ; <nl> avctx -> pix_fmt = AV_PIX_FMT_YUV420P ; <nl> for ( i = 0 ; i < 3 ; i ++) { <nl> ctx -> num_vblocks [ i ] = AV_CEIL_RSHIFT ( height , 3 + !! i );
static int filter_frame ( AVFilterLink * link , AVFrame * in ) <nl> FFMIN ( in -> height , ctx -> graph -> nb_threads )); <nl> else if ( in -> format == AV_PIX_FMT_YUV420P ) <nl> ctx -> internal -> execute ( ctx , process_slice_yuv420p , & td , NULL , <nl> - FFMIN ( in -> height , ctx -> graph -> nb_threads )); <nl> + FFMAX ( 1 , FFMIN ( in -> height , ctx -> graph -> nb_threads ) & ~ 1 )); <nl> else <nl> ctx -> internal -> execute ( ctx , process_slice_uyvy422 , & td , NULL , <nl> FFMIN ( in -> height , ctx -> graph -> nb_threads ));
static void vc1_draw_sprites ( VC1Context * v , SpriteData * sd ) <nl> if (!( xoff [ sprite ] & 0xFFFF ) && xadv [ sprite ] == 1 << 16 ) { <nl> src_h [ sprite ][ 0 ] = iplane + ( xoff [ sprite ] >> 16 ) + yline * iline ; <nl> if ( ysub [ sprite ]) <nl> - src_h [ sprite ][ 1 ] = iplane + ( xoff [ sprite ] >> 16 ) + ( yline + 1 ) * iline ; <nl> + src_h [ sprite ][ 1 ] = iplane + ( xoff [ sprite ] >> 16 ) + FFMIN ( yline + 1 , ( v -> sprite_height >>!! plane )- 1 ) * iline ; <nl> } else { <nl> if ( sr_cache [ sprite ][ 0 ] != yline ) { <nl> if ( sr_cache [ sprite ][ 1 ] == yline ) { <nl> static void vc1_draw_sprites ( VC1Context * v , SpriteData * sd ) <nl> } <nl> } <nl> if ( ysub [ sprite ] && sr_cache [ sprite ][ 1 ] != yline + 1 ) { <nl> - v -> vc1dsp . sprite_h ( v -> sr_rows [ sprite ][ 1 ], iplane + ( yline + 1 ) * iline , xoff [ sprite ], xadv [ sprite ], width ); <nl> + v -> vc1dsp . sprite_h ( v -> sr_rows [ sprite ][ 1 ], iplane + FFMIN ( yline + 1 , ( v -> sprite_height >>!! plane )- 1 ) * iline , xoff [ sprite ], xadv [ sprite ], width ); <nl> sr_cache [ sprite ][ 1 ] = yline + 1 ; <nl> } <nl> src_h [ sprite ][ 0 ] = v -> sr_rows [ sprite ][ 0 ];
static int parse_ffconfig ( const char * filename ) <nl> stream = av_mallocz ( sizeof ( FFStream )); <nl> get_arg ( stream -> filename , sizeof ( stream -> filename ), & p ); <nl> q = strrchr ( stream -> filename , '>'); <nl> - if (* q ) <nl> + if ( q ) <nl> * q = '\ 0 '; <nl>  <nl> for ( s = first_stream ; s ; s = s -> next ) {
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> h = ( buf [ 1 ] + 1 ) * 8 ; <nl> buf += 2 ; <nl>  <nl> + if ( avpkt -> size < 2 + w * h / 513 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( w != avctx -> width || h != avctx -> height ) { <nl> av_freep (& c -> frame_buffer ); <nl> av_freep (& c -> last_frame_buffer );
static const uint8_t * read_huffman_tables ( FourXContext * f , <nl> for (;;) { <nl> int i ; <nl>  <nl> - if ( start <= end && ptr_end - ptr < end - start + 1 + 1 ) <nl> + if ( ptr_end - ptr < FFMAX ( end - start + 1 , 0 ) + 1 ) { <nl> + av_log ( f -> avctx , AV_LOG_ERROR , " invalid data in read_huffman_tables \ n "); <nl> return NULL ; <nl> + } <nl> for ( i = start ; i <= end ; i ++) <nl> frequency [ i ] = * ptr ++; <nl> start = * ptr ++; <nl> static const uint8_t * read_huffman_tables ( FourXContext * f , <nl> while (( ptr - buf ) & 3 ) <nl> ptr ++; // 4byte align <nl>  <nl> + if ( ptr > ptr_end ) { <nl> + av_log ( f -> avctx , AV_LOG_ERROR , " ptr overflow in read_huffman_tables \ n "); <nl> + return NULL ; <nl> + } <nl> + <nl> for ( j = 257 ; j < 512 ; j ++) { <nl> int min_freq [ 2 ] = { 256 * 256 , 256 * 256 }; <nl> int smallest [ 2 ] = { 0 , 0 };
typedef struct { <nl>  <nl> static int ape_probe ( AVProbeData * p ) <nl> { <nl> - if ( p -> buf [ 0 ] == ' M ' && p -> buf [ 1 ] == ' A ' && p -> buf [ 2 ] == ' C ' && p -> buf [ 3 ] == ' ') <nl> - return AVPROBE_SCORE_MAX ; <nl> + int version = AV_RL16 ( p -> buf + 4 ); <nl> + if ( AV_RL32 ( p -> buf ) != MKTAG (' M ', ' A ', ' C ', ' ')) <nl> + return 0 ; <nl>  <nl> - return 0 ; <nl> + if ( version < APE_MIN_VERSION || version > APE_MAX_VERSION ) <nl> + return AVPROBE_SCORE_MAX / 4 ; <nl> + <nl> + return AVPROBE_SCORE_MAX ; <nl> } <nl>  <nl> static void ape_dumpinfo ( AVFormatContext * s , APEContext * ape_ctx )
int ff_mov_read_stsd_entries ( MOVContext * c , AVIOContext * pb , int entries ) <nl> avio_rb32 ( pb ); /* reserved */ <nl> avio_rb16 ( pb ); /* reserved */ <nl> dref_id = avio_rb16 ( pb ); <nl> + } else if ( size <= 0 ){ <nl> + av_log ( c -> fc , AV_LOG_ERROR , " invalid size % d in stsd \ n ", size ); <nl> + return - 1 ; <nl> } <nl>  <nl> if ( st -> codec -> codec_tag &&
void ff_thread_report_progress ( ThreadFrame * f , int n , int field ) <nl>  <nl> p = f -> owner [ field ]-> internal -> thread_ctx ; <nl>  <nl> + pthread_mutex_lock (& p -> progress_mutex ); <nl> if ( f -> owner [ field ]-> debug & FF_DEBUG_THREADS ) <nl> av_log ( f -> owner [ field ], AV_LOG_DEBUG , <nl> "% p finished % d field % d \ n ", progress , n , field ); <nl>  <nl> - pthread_mutex_lock (& p -> progress_mutex ); <nl> - <nl> atomic_store_explicit (& progress [ field ], n , memory_order_release ); <nl>  <nl> pthread_cond_broadcast (& p -> progress_cond ); <nl> void ff_thread_await_progress ( ThreadFrame * f , int n , int field ) <nl>  <nl> p = f -> owner [ field ]-> internal -> thread_ctx ; <nl>  <nl> + pthread_mutex_lock (& p -> progress_mutex ); <nl> if ( f -> owner [ field ]-> debug & FF_DEBUG_THREADS ) <nl> av_log ( f -> owner [ field ], AV_LOG_DEBUG , <nl> " thread awaiting % d field % d from % p \ n ", n , field , progress ); <nl> - <nl> - pthread_mutex_lock (& p -> progress_mutex ); <nl> while ( atomic_load_explicit (& progress [ field ], memory_order_relaxed ) < n ) <nl> pthread_cond_wait (& p -> progress_cond , & p -> progress_mutex ); <nl> pthread_mutex_unlock (& p -> progress_mutex );
int ff_h2645_extract_rbsp ( const uint8_t * src , int length , <nl> { <nl> int i , si , di ; <nl> uint8_t * dst ; <nl> - int64_t padding = small_padding ? AV_INPUT_BUFFER_PADDING_SIZE : MAX_MBPAIR_SIZE ; <nl> + int64_t padding = small_padding ? 0 : MAX_MBPAIR_SIZE ; <nl>  <nl> nal -> skipped_bytes = 0 ; <nl> # define STARTCODE_TEST \ <nl> int ff_h2645_extract_rbsp ( const uint8_t * src , int length , <nl> return length ; <nl> } <nl>  <nl> - av_fast_malloc (& nal -> rbsp_buffer , & nal -> rbsp_buffer_size , <nl> - length + padding ); <nl> + av_fast_padded_malloc (& nal -> rbsp_buffer , & nal -> rbsp_buffer_size , <nl> + length + padding ); <nl> if (! nal -> rbsp_buffer ) <nl> return AVERROR ( ENOMEM ); <nl> 
static int nppscale_resize ( AVFilterContext * ctx , NPPScaleStageContext * stage , <nl> NppStatus err ; <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < FF_ARRAY_ELEMS ( in -> data ) && in -> data [ i ]; i ++) { <nl> + for ( i = 0 ; i < FF_ARRAY_ELEMS ( stage -> planes_in ) && i < FF_ARRAY_ELEMS ( in -> data ) && in -> data [ i ]; i ++) { <nl> int iw = stage -> planes_in [ i ]. width ; <nl> int ih = stage -> planes_in [ i ]. height ; <nl> int ow = stage -> planes_out [ i ]. width ;
static int bitpacked_decode_yuv422p10 ( AVCodecContext * avctx , AVFrame * frame , <nl> AVPacket * avpkt ) <nl> { <nl> uint64_t frame_size = ( uint64_t ) avctx -> width * ( uint64_t ) avctx -> height * 20 ; <nl> - uint64_t packet_size = avpkt -> size * 8 ; <nl> + uint64_t packet_size = ( uint64_t ) avpkt -> size * 8 ; <nl> GetBitContext bc ; <nl> uint16_t * y , * u , * v ; <nl> int ret , i ;
DECLARE_ALIGNED ( 16 , static const uint16_t , dither )[ 8 ][ 8 ] = { <nl> void ff_gradfun_filter_line_c ( uint8_t * dst , uint8_t * src , uint16_t * dc , int width , int thresh , const uint16_t * dithers ) <nl> { <nl> int x ; <nl> - for ( x = 0 ; x < width ; x ++, dc += x & 1 ) { <nl> + for ( x = 0 ; x < width ; dc += x & 1 , x ++) { <nl> int pix = src [ x ] << 7 ; <nl> int delta = dc [ 0 ] - pix ; <nl> int m = abs ( delta ) * thresh >> 16 ;
static av_always_inline void predict_slice_buffered ( SnowContext * s , slice_buffer <nl>  <nl> if ( s -> avmv && mb_y < mb_h && plane_index == 0 ) <nl> for ( mb_x = 0 ; mb_x < mb_w ; mb_x ++){ <nl> - AVMotionVector * avmv = s -> avmv + ( s -> avmv_index ++); <nl> + AVMotionVector * avmv = s -> avmv + s -> avmv_index ; <nl> const int b_width = s -> b_width << s -> block_max_depth ; <nl> const int b_stride = b_width ; <nl> BlockNode * bn = & s -> block [ mb_x + mb_y * b_stride ]; <nl> static av_always_inline void predict_slice_buffered ( SnowContext * s , slice_buffer <nl> if ( bn -> type ) <nl> continue ; <nl>  <nl> + s -> avmv_index ++; <nl> + <nl> avmv -> w = block_w ; <nl> avmv -> h = block_h ; <nl> avmv -> dst_x = block_w * mb_x - block_w / 2 ;
int ff_h263_decode_picture_header ( MpegEncContext * s ) <nl> s -> qscale = get_bits (& s -> gb , 5 ); <nl> } <nl>  <nl> + if ( s -> width == 0 || s -> height == 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " dimensions 0 \ n "); <nl> + return - 1 ; <nl> + } <nl> s -> mb_width = ( s -> width + 15 ) / 16 ; <nl> s -> mb_height = ( s -> height + 15 ) / 16 ; <nl> s -> mb_num = s -> mb_width * s -> mb_height ;
static void mpegts_write_pes ( AVFormatContext * s , AVStream * st , <nl> } <nl> if ( len > 0xffff ) <nl> len = 0 ; <nl> + if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO ) { <nl> + len = 0 ; <nl> + } <nl> * q ++ = len >> 8 ; <nl> * q ++ = len ; <nl> val = 0x80 ;
static int svq3_decode_frame ( AVCodecContext * avctx , <nl> } else if ( s -> pict_type == AV_PICTURE_TYPE_B && mb_type >= 4 ) { <nl> mb_type += 4 ; <nl> } <nl> - if ( mb_type > 33 || svq3_decode_mb ( svq3 , mb_type )) { <nl> + if (( unsigned ) mb_type > 33 || svq3_decode_mb ( svq3 , mb_type )) { <nl> av_log ( h -> s . avctx , AV_LOG_ERROR , " error while decoding MB % d % d \ n ", s -> mb_x , s -> mb_y ); <nl> return - 1 ; <nl> }
void ff_h264_direct_ref_list_init ( H264Context * const h ){ <nl> h -> col_fieldoff = 2 * h -> ref_list [ 1 ][ 0 ]. f . reference - 3 ; <nl> } <nl>  <nl> - if ( cur -> f . pict_type != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> + if ( h -> slice_type_nos != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> return ; <nl>  <nl> for ( list = 0 ; list < 2 ; list ++){
static int read_header ( AVFormatContext * s ) <nl> return ret ; <nl> } <nl>  <nl> - avio_seek ( pb , vst -> index_entries [ 0 ]. pos , SEEK_SET ); <nl> + if ( vst -> index_entries ) <nl> + avio_seek ( pb , vst -> index_entries [ 0 ]. pos , SEEK_SET ); <nl> + else <nl> + avio_skip ( pb , 4 ); <nl>  <nl> bink -> current_track = - 1 ; <nl> return 0 ;
static int read_var_block_data ( ALSDecContext * ctx , ALSBlockData * bd ) <nl> int opt_order_length = av_ceil_log2 ( av_clip (( bd -> block_length >> 3 ) - 1 , <nl> 2 , sconf -> max_order + 1 )); <nl> * bd -> opt_order = get_bits ( gb , opt_order_length ); <nl> + if (* bd -> opt_order > sconf -> max_order ) { <nl> + * bd -> opt_order = sconf -> max_order ; <nl> + av_log ( avctx , AV_LOG_ERROR , " Predictor order too large !\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } else { <nl> * bd -> opt_order = sconf -> max_order ; <nl> }
static int decode_header ( SnowContext * s ){ <nl> av_log ( s -> avctx , AV_LOG_ERROR , " spatial_decomposition_count % d too large for size \ n ", s -> spatial_decomposition_count ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( s -> avctx -> width > 65536 - 4 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " Width % d is too large \ n ", s -> avctx -> width ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl>  <nl> s -> qlog += get_symbol (& s -> c , s -> header_state , 1 );
static int asfrtp_parse_packet ( AVFormatContext * s , PayloadContext * asf , <nl> int prev_len = out_len ; <nl> out_len += cur_len ; <nl> asf -> buf = av_realloc ( asf -> buf , out_len ); <nl> - memcpy ( asf -> buf + prev_len , buf + off , cur_len ); <nl> + memcpy ( asf -> buf + prev_len , buf + off , <nl> + FFMIN ( cur_len , len - off )); <nl> url_fskip ( pb , cur_len ); <nl> } <nl> }
*/ <nl>  <nl> # include " libavutil / bswap . h " <nl> +# include " libavcodec / internal . h " <nl> # include " avformat . h " <nl> # include " internal . h " <nl>  <nl> static int xvag_read_header ( AVFormatContext * s ) <nl>  <nl> if ( st -> codecpar -> sample_rate <= 0 ) <nl> return AVERROR_INVALIDDATA ; <nl> - if ( st -> codecpar -> channels <= 0 ) <nl> + if ( st -> codecpar -> channels <= 0 || st -> codecpar -> channels > FF_SANE_NB_CHANNELS ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> switch ( codec ) {
static int decode_frame ( AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_ERROR , " Cannot allocate temporary buffer \ n "); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> + memset ( rbuf + buf_size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> if (( ret = ff_get_buffer ( avctx , p , 0 )) < 0 ) { <nl> av_free ( rbuf );
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> } <nl> break ; <nl> case MKTAG (' s ', ' t ', ' r ', ' d '): <nl> + st = s -> streams [ stream_index ]; <nl> if ( stream_index >= ( unsigned ) s -> nb_streams || st -> codec -> extradata_size ) { <nl> avio_skip ( pb , size ); <nl> } else {
 <nl> FFTContext * av_fft_init ( int nbits , int inverse ) <nl> { <nl> - FFTContext * s = av_malloc ( sizeof (* s )); <nl> + FFTContext * s = av_mallocz ( sizeof (* s )); <nl>  <nl> if ( s && ff_fft_init ( s , nbits , inverse )) <nl> av_freep (& s );
static int vp8_lossless_decode_frame ( AVCodecContext * avctx , AVFrame * p , <nl> unsigned int data_size , int is_alpha_chunk ) <nl> { <nl> WebPContext * s = avctx -> priv_data ; <nl> - int w , h , ret , i ; <nl> + int w , h , ret , i , used ; <nl>  <nl> if (! is_alpha_chunk ) { <nl> s -> lossless = 1 ; <nl> static int vp8_lossless_decode_frame ( AVCodecContext * avctx , AVFrame * p , <nl> /* parse transformations */ <nl> s -> nb_transforms = 0 ; <nl> s -> reduced_width = 0 ; <nl> + used = 0 ; <nl> while ( get_bits1 (& s -> gb )) { <nl> enum TransformType transform = get_bits (& s -> gb , 2 ); <nl> + if ( used & ( 1 << transform )) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Transform % d used more than once \ n ", <nl> + transform ); <nl> + ret = AVERROR_INVALIDDATA ; <nl> + goto free_and_return ; <nl> + } <nl> + used |= ( 1 << transform ); <nl> s -> transforms [ s -> nb_transforms ++] = transform ; <nl> switch ( transform ) { <nl> case PREDICTOR_TRANSFORM :
static int decode_codestream ( J2kDecoderContext * s ) <nl> } <nl>  <nl> marker = bytestream_get_be16 (& s -> buf ); <nl> + if ( s -> avctx -> debug & FF_DEBUG_STARTCODE ) <nl> + av_log ( s -> avctx , AV_LOG_DEBUG , " marker 0x %. 4X at pos 0x % x \ n ", marker , s -> buf - s -> buf_start - 4 ); <nl> oldbuf = s -> buf ; <nl>  <nl> if ( marker == J2K_SOD ){
static int aa_read_header ( AVFormatContext * s ) <nl> AADemuxContext * c = s -> priv_data ; <nl> AVIOContext * pb = s -> pb ; <nl> AVStream * st ; <nl> + int ret ; <nl>  <nl> /* parse . aa header */ <nl> avio_skip ( pb , 4 ); // file size <nl> static int aa_read_header ( AVFormatContext * s ) <nl> header_seed = atoi ( val ); <nl> } else if (! strcmp ( key , " HeaderKey ")) { // this looks like " 1234567890 1234567890 1234567890 1234567890 " <nl> av_log ( s , AV_LOG_DEBUG , " HeaderKey is <% s >\ n ", val ); <nl> - sscanf ( val , "%" SCNu32 "%" SCNu32 "%" SCNu32 "%" SCNu32 , <nl> + <nl> + ret = sscanf ( val , "%" SCNu32 "%" SCNu32 "%" SCNu32 "%" SCNu32 , <nl> & header_key_part [ 0 ], & header_key_part [ 1 ], & header_key_part [ 2 ], & header_key_part [ 3 ]); <nl> + if ( ret != 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> for ( idx = 0 ; idx < 4 ; idx ++) { <nl> AV_WB32 (& header_key [ idx * 4 ], header_key_part [ idx ]); // convert each part to BE ! <nl> }
int MPV_encode_picture ( AVCodecContext * avctx , <nl> h >>= 1 ; <nl> } <nl>  <nl> - if ( s -> intra_only && dest_wrap == src_wrap ){ <nl> - s -> current_picture [ i ] = pict -> data [ i ]; <nl> + if ( dest_wrap == src_wrap ){ <nl> + s -> new_picture [ i ] = pict -> data [ i ]; <nl> } else { <nl> for ( j = 0 ; j < h ; j ++) { <nl> memcpy ( dest , src , w ); <nl> dest += dest_wrap ; <nl> src += src_wrap ; <nl> } <nl> + s -> new_picture [ i ] = s -> current_picture [ i ]; <nl> } <nl> - s -> new_picture [ i ] = s -> current_picture [ i ]; <nl> } <nl>  <nl> encode_picture ( s , s -> picture_number );
static void get_attachment ( AVFormatContext * s , AVIOContext * pb , int length ) <nl> st -> codec -> codec_id = AV_CODEC_ID_MJPEG ; <nl> st -> codec -> codec_type = AVMEDIA_TYPE_ATTACHMENT ; <nl> st -> codec -> extradata = av_mallocz ( filesize ); <nl> + st -> id = - 1 ; <nl> if (! st -> codec -> extradata ) <nl> goto done ; <nl> st -> codec -> extradata_size = filesize ;
static void qdm2_fft_decode_tones ( QDM2Context * q , int duration , GetBitContext * <nl> return ; <nl>  <nl> local_int_14 = ( offset >> local_int_8 ); <nl> + if ( local_int_14 >= FF_ARRAY_ELEMS ( fft_level_index_table )) <nl> + return ; <nl>  <nl> if ( q -> nb_channels > 1 ) { <nl> channel = get_bits1 ( gb );
static int audio_decode_frame ( VideoState * is ) <nl> int len1 , len2 , data_size , resampled_data_size ; <nl> int64_t dec_channel_layout ; <nl> int got_frame ; <nl> - av_unused double pts ; <nl> + av_unused double audio_clock0 ; <nl> int new_packet = 0 ; <nl> int flush_complete = 0 ; <nl> int wanted_nb_samples ; <nl> static int audio_decode_frame ( VideoState * is ) <nl> } <nl>  <nl> /* if no pts , then compute it */ <nl> - pts = is -> audio_clock ; <nl> + audio_clock0 = is -> audio_clock ; <nl> is -> audio_clock += ( double ) data_size / <nl> ( is -> frame -> channels * is -> frame -> sample_rate * av_get_bytes_per_sample ( is -> frame -> format )); <nl> # ifdef DEBUG <nl> { <nl> static double last_clock ; <nl> - printf (" audio : delay =% 0 . 3f clock =% 0 . 3f pts =% 0 . 3f \ n ", <nl> + printf (" audio : delay =% 0 . 3f clock =% 0 . 3f clock0 =% 0 . 3f \ n ", <nl> is -> audio_clock - last_clock , <nl> - is -> audio_clock , pts ); <nl> + is -> audio_clock , audio_clock0 ); <nl> last_clock = is -> audio_clock ; <nl> } <nl> # endif
static int cache_read ( URLContext * h , unsigned char * buf , int size ) <nl> { <nl> Context * c = h -> priv_data ; <nl> CacheEntry * entry , * next [ 2 ] = { NULL , NULL }; <nl> - int r ; <nl> + int64_t r ; <nl>  <nl> entry = av_tree_find ( c -> root , & c -> logical_pos , cmp , ( void **) next ); <nl> 
static int ea_read_packet ( AVFormatContext * s , <nl> int packet_read = 0 ; <nl> unsigned int chunk_type , chunk_size ; <nl> int key = 0 ; <nl> - int num_samples ; <nl> + int av_uninit ( num_samples ); <nl>  <nl> while (! packet_read ) { <nl> chunk_type = get_le32 ( pb );
static int sunrast_decode_frame ( AVCodecContext * avctx , void * data , <nl> type = AV_RB32 ( buf + 20 ); <nl> maptype = AV_RB32 ( buf + 24 ); <nl> maplength = AV_RB32 ( buf + 28 ); <nl> + buf += 32 ; <nl>  <nl> if ( type == RT_FORMAT_TIFF || type == RT_FORMAT_IFF ) { <nl> av_log ( avctx , AV_LOG_ERROR , " unsupported ( compression ) type \ n "); <nl> return - 1 ; <nl> } <nl> - if ( type > RT_FORMAT_IFF ) { <nl> + if ( type < RT_OLD || type > RT_FORMAT_IFF ) { <nl> av_log ( avctx , AV_LOG_ERROR , " invalid ( compression ) type \ n "); <nl> return - 1 ; <nl> } <nl> + if ( av_image_check_size ( w , h , 0 , avctx )) { <nl> + av_log ( avctx , AV_LOG_ERROR , " invalid image size \ n "); <nl> + return - 1 ; <nl> + } <nl> if ( maptype & ~ 1 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " invalid colormap type \ n "); <nl> return - 1 ; <nl> } <nl>  <nl> - buf += 32 ; <nl>  <nl> switch ( depth ) { <nl> case 1 : <nl> static int sunrast_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( p -> data [ 0 ]) <nl> avctx -> release_buffer ( avctx , p ); <nl>  <nl> - if ( av_image_check_size ( w , h , 0 , avctx )) <nl> - return - 1 ; <nl> if ( w != avctx -> width || h != avctx -> height ) <nl> avcodec_set_dimensions ( avctx , w , h ); <nl> if ( avctx -> get_buffer ( avctx , p ) < 0 ) {
typedef struct A64Context { <nl> AVLFG randctx ; <nl> int mc_lifetime ; <nl> int mc_use_5col ; <nl> - int mc_frame_counter ; <nl> + unsigned mc_frame_counter ; <nl> int * mc_meta_charset ; <nl> int * mc_charmap ; <nl> int * mc_best_cb ;
static int xan_decode_frame_type0 ( AVCodecContext * avctx ) <nl> int dec_size ; <nl>  <nl> bytestream2_seek (& s -> gb , 8 + corr_off , SEEK_SET ); <nl> - dec_size = xan_unpack ( s , s -> scratch_buffer , s -> buffer_size ); <nl> + dec_size = xan_unpack ( s , s -> scratch_buffer , s -> buffer_size / 2 ); <nl> if ( dec_size < 0 ) <nl> dec_size = 0 ; <nl> for ( i = 0 ; i < dec_size ; i ++)
static void mpeg4_encode_vol_header ( MpegEncContext * s , <nl> /* write mpeg4 VOP header */ <nl> int ff_mpeg4_encode_picture_header ( MpegEncContext * s , int picture_number ) <nl> { <nl> - int time_incr ; <nl> - int time_div , time_mod ; <nl> + uint64_t time_incr ; <nl> + int64_t time_div , time_mod ; <nl>  <nl> if ( s -> pict_type == AV_PICTURE_TYPE_I ) { <nl> if (!( s -> avctx -> flags & AV_CODEC_FLAG_GLOBAL_HEADER )) { <nl> int ff_mpeg4_encode_picture_header ( MpegEncContext * s , int picture_number ) <nl> time_div = FFUDIV ( s -> time , s -> avctx -> time_base . den ); <nl> time_mod = FFUMOD ( s -> time , s -> avctx -> time_base . den ); <nl> time_incr = time_div - s -> last_time_base ; <nl> - av_assert0 ( time_incr >= 0 ); <nl>  <nl> // This limits the frame duration to max 1 hour <nl> if ( time_incr > 3600 ) { <nl> - av_log ( s -> avctx , AV_LOG_ERROR , " time_incr % d too large \ n ", time_incr ); <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " time_incr %" PRIu64 " too large \ n ", time_incr ); <nl> return AVERROR ( EINVAL ); <nl> } <nl> while ( time_incr --)
static int read_thread ( void * arg ) <nl> } <nl> if ( is -> queue_attachments_req ) { <nl> if ( is -> video_st && is -> video_st -> disposition & AV_DISPOSITION_ATTACHED_PIC ) { <nl> - AVPacket copy ; <nl> + AVPacket copy = { 0 }; <nl> if (( ret = av_copy_packet (& copy , & is -> video_st -> attached_pic )) < 0 ) <nl> goto fail ; <nl> packet_queue_put (& is -> videoq , & copy );
static int asfrtp_parse_packet ( AVFormatContext * s , PayloadContext * asf , <nl>  <nl> int cur_len = start_off + len_off - off ; <nl> int prev_len = out_len ; <nl> + void * newmem ; <nl> out_len += cur_len ; <nl> - asf -> buf = av_realloc ( asf -> buf , out_len ); <nl> + if ( FFMIN ( cur_len , len - off ) < 0 ) <nl> + return - 1 ; <nl> + newmem = av_realloc ( asf -> buf , out_len ); <nl> + if (! newmem ) <nl> + return - 1 ; <nl> + asf -> buf = newmem ; <nl> memcpy ( asf -> buf + prev_len , buf + off , <nl> FFMIN ( cur_len , len - off )); <nl> avio_skip ( pb , cur_len );
static int decode_subframe_fixed ( FLACContext * s , int channel , int pred_order ) <nl> { <nl> const int blocksize = s -> blocksize ; <nl> int32_t * decoded = s -> decoded [ channel ]; <nl> - int a , b , c , d , i ; <nl> + int av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ), i ; <nl>  <nl> /* warm up samples */ <nl> for ( i = 0 ; i < pred_order ; i ++) {
static int tm2_read_stream ( TM2Context * ctx , const uint8_t * buf , int stream_id , i <nl> /* get stream length in dwords */ <nl> bytestream2_init (& gb , buf , buf_size ); <nl> len = bytestream2_get_be32 (& gb ); <nl> - skip = len * 4 + 4 ; <nl>  <nl> if ( len == 0 ) <nl> return 4 ; <nl>  <nl> - if ( len >= INT_MAX / 4 - 1 || len < 0 || skip > buf_size ) { <nl> + if ( len >= INT_MAX / 4 - 1 || len < 0 || len * 4 + 4 > buf_size ) { <nl> av_log ( ctx -> avctx , AV_LOG_ERROR , " Error , invalid stream size .\ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + skip = len * 4 + 4 ; <nl>  <nl> toks = bytestream2_get_be32 (& gb ); <nl> if ( toks & 1 ) {
static int decode_frame ( AVCodecContext * avctx , <nl> int prev_y = 0 , prev_u = 0 , prev_v = 0 ; <nl> uint8_t * rbuf ; <nl>  <nl> + if ( buf_size <= 8 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " buf_size % d is too small \ n ", buf_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> rbuf = av_malloc ( buf_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! rbuf ){ <nl> av_log ( avctx , AV_LOG_ERROR , " Cannot allocate temporary buffer \ n ");
static AVStream * get_subtitle_pkt ( AVFormatContext * s , AVStream * next_st , <nl> for ( i = 0 ; i < s -> nb_streams ; i ++) { <nl> st = s -> streams [ i ]; <nl> ast = st -> priv_data ; <nl> - if ( st -> discard < AVDISCARD_ALL && ast -> sub_pkt . data ) { <nl> + if ( st -> discard < AVDISCARD_ALL && ast && ast -> sub_pkt . data ) { <nl> ts = av_rescale_q ( ast -> sub_pkt . dts , st -> time_base , AV_TIME_BASE_Q ); <nl> if ( ts <= next_ts && ts < ts_min ) { <nl> ts_min = ts ; <nl> static int avi_read_close ( AVFormatContext * s ) <nl> AVStream * st = s -> streams [ i ]; <nl> AVIStream * ast = st -> priv_data ; <nl> av_free ( st -> codec -> palctrl ); <nl> + if ( ast ) { <nl> if ( ast -> sub_ctx ) { <nl> av_freep (& ast -> sub_ctx -> pb ); <nl> av_close_input_stream ( ast -> sub_ctx ); <nl> } <nl> av_free ( ast -> sub_buffer ); <nl> av_free_packet (& ast -> sub_pkt ); <nl> + } <nl> } <nl>  <nl> if ( avi -> dv_demux )
static int open_file ( AVFormatContext * avf , unsigned fileno ) <nl> if (! cat -> avf ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - cat -> avf -> flags |= avf -> flags ; <nl> + cat -> avf -> flags |= avf -> flags & ~ AVFMT_FLAG_CUSTOM_IO ; <nl> cat -> avf -> interrupt_callback = avf -> interrupt_callback ; <nl>  <nl> if (( ret = ff_copy_whiteblacklists ( cat -> avf , avf )) < 0 )
static int bethsoftvid_decode_frame ( AVCodecContext * avctx , <nl> uint8_t * frame_end ; <nl> int line_remaining = avctx -> width ; // number of bytes remaining on a line <nl> const int wrap_to_next_line = vid -> frame . linesize [ 0 ] - avctx -> width ; <nl> - uint8_t rle_num_bytes ; <nl> + int rle_num_bytes ; <nl> int yoffset ; <nl>  <nl> if ( avctx -> reget_buffer ( avctx , & vid -> frame )) {
static void hybrid_synthesis ( PSDSPContext * dsp , float out [ 2 ][ 38 ][ 64 ], <nl> # define DECAY_SLOPE 0 . 05f <nl> /// Number of frequency bands that can be addressed by the parameter index , b ( k ) <nl> static const int NR_PAR_BANDS [] = { 20 , 34 }; <nl> + static const int NR_IPDOPD_BANDS [] = { 11 , 17 }; <nl> /// Number of frequency bands that can be addressed by the sub subband index , k <nl> static const int NR_BANDS [] = { 71 , 91 }; <nl> /// Start frequency band for the all - pass filter decay slope <nl> static void stereo_processing ( PSContext * ps , float (* l )[ 32 ][ 2 ], float (* r )[ 32 ][ 2 <nl> h21 = H_LUT [ iid_mapped [ e ][ b ] + 7 + 23 * ps -> iid_quant ][ icc_mapped [ e ][ b ]][ 2 ]; <nl> h22 = H_LUT [ iid_mapped [ e ][ b ] + 7 + 23 * ps -> iid_quant ][ icc_mapped [ e ][ b ]][ 3 ]; <nl>  <nl> - if (! PS_BASELINE && ps -> enable_ipdopd && 2 * b <= NR_PAR_BANDS [ is34 ]) { <nl> + if (! PS_BASELINE && ps -> enable_ipdopd && b < NR_IPDOPD_BANDS [ is34 ]) { <nl> // The spec say says to only run this smoother when enable_ipdopd <nl> // is set but the reference decoder appears to run it constantly <nl> float h11i , h12i , h21i , h22i ;
static av_cold int vqa_decode_init ( AVCodecContext * avctx ) <nl> } <nl> s -> width = AV_RL16 (& s -> avctx -> extradata [ 6 ]); <nl> s -> height = AV_RL16 (& s -> avctx -> extradata [ 8 ]); <nl> - if (( ret = av_image_check_size ( s -> width , s -> height , 0 , avctx )) < 0 ) { <nl> + if (( ret = ff_set_dimensions ( avctx , s -> width , s -> height )) < 0 ) { <nl> s -> width = s -> height = 0 ; <nl> return ret ; <nl> }
static void fix_bitshift ( ShortenContext * s , int32_t * buffer ) <nl>  <nl> if ( s -> bitshift != 0 ) <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> - buffer [ s -> nwrap + i ] <<= s -> bitshift ; <nl> + buffer [ i ] <<= s -> bitshift ; <nl> } <nl>  <nl> 
static int mov_read_stsz ( MOVContext * c , ByteIOContext * pb , MOVAtom atom ) <nl> return - 1 ; <nl> } <nl>  <nl> - if ( entries >= UINT_MAX / sizeof ( int )) <nl> + if ( entries >= UINT_MAX / sizeof ( int ) || entries >= ( UINT_MAX - 4 ) / field_size ) <nl> return - 1 ; <nl> sc -> sample_sizes = av_malloc ( entries * sizeof ( int )); <nl> if (! sc -> sample_sizes )
static av_cold int vqa_decode_init ( AVCodecContext * avctx ) <nl> /* load up the VQA parameters from the header */ <nl> vqa_header = ( unsigned char *) s -> avctx -> extradata ; <nl> s -> vqa_version = vqa_header [ 0 ]; <nl> + if ( s -> vqa_version < 1 || s -> vqa_version > 3 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " VQA video : unsupported version % d \ n ", s -> vqa_version ); <nl> + return - 1 ; <nl> + } <nl> s -> width = AV_RL16 (& vqa_header [ 6 ]); <nl> s -> height = AV_RL16 (& vqa_header [ 8 ]); <nl> if ( av_image_check_size ( s -> width , s -> height , 0 , avctx )){
static av_cold int rv30_decode_init ( AVCodecContext * avctx ) <nl> RV34DecContext * r = avctx -> priv_data ; <nl> int ret ; <nl>  <nl> + if ( avctx -> extradata_size < 2 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Extradata is too small .\ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> r -> rv30 = 1 ; <nl> if (( ret = ff_rv34_decode_init ( avctx )) < 0 ) <nl> return ret ; <nl> - if ( avctx -> extradata_size < 2 ){ <nl> - av_log ( avctx , AV_LOG_ERROR , " Extradata is too small .\ n "); <nl> - return - 1 ; <nl> - } <nl>  <nl> r -> max_rpr = avctx -> extradata [ 1 ] & 7 ; <nl> if ( avctx -> extradata_size < 2 * r -> max_rpr + 8 ){
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> tag = avio_rl32 ( pb ); <nl> size = avio_rl32 ( pb ); <nl>  <nl> + if ( size > avi -> fsize ){ <nl> + av_log ( s , AV_LOG_ERROR , " chunk size is too big during header parsing \ n "); <nl> + goto fail ; <nl> + } <nl> + <nl> print_tag (" tag ", tag , size ); <nl>  <nl> switch ( tag ) {
static int gxf_packet ( AVFormatContext * s , AVPacket * pkt ) { <nl> } <nl>  <nl> static int gxf_seek ( AVFormatContext * s , int stream_index , int64_t timestamp , int flags ) { <nl> - int res = 0 ; <nl> + int64_t res = 0 ; <nl> uint64_t pos ; <nl> uint64_t maxlen = 100 * 1024 * 1024 ; <nl> AVStream * st = s -> streams [ 0 ];
static int decode_main_header ( NUTContext * nut ) <nl> while ( tmp_fields -- > 8 ) <nl> ffio_read_varlen ( bc ); <nl>  <nl> - if ( count == 0 || i + count > 256 ) { <nl> + if ( count <= 0 || count > 256 - ( i <= ' N ') - i ) { <nl> av_log ( s , AV_LOG_ERROR , " illegal count % d at % d \ n ", count , i ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int hnm_decode_frame ( AVCodecContext * avctx , void * data , <nl> int ret ; <nl> uint16_t chunk_id ; <nl>  <nl> + if ( avpkt -> size < 8 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " packet too small \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (( ret = ff_get_buffer ( avctx , frame , 0 )) < 0 ) <nl> return ret ; <nl> 
static int filter_frame ( AVFilterLink * inlink , AVFrame * insamples ) <nl> break ; <nl> av_assert1 ( input_number < am -> nb_inputs ); <nl> if ( ff_bufqueue_is_full (& am -> in [ input_number ]. queue )) { <nl> - av_log ( ctx , AV_LOG_ERROR , " Buffer queue overflow \ n "); <nl> av_frame_free (& insamples ); <nl> return AVERROR ( ENOMEM ); <nl> }
static void ogg_write_pages ( AVFormatContext * s , int flush ) <nl> if ( oggstream -> page_count < 2 && ! flush ) <nl> break ; <nl> ogg_write_page ( s , & p -> page , <nl> - flush && oggstream -> page_count == 1 ? 4 : 0 ); // eos <nl> + flush == 1 && oggstream -> page_count == 1 ? 4 : 0 ); // eos <nl> next = p -> next ; <nl> av_freep (& p ); <nl> p = next ; <nl> static int ogg_write_header ( AVFormatContext * s ) <nl>  <nl> oggstream -> page . start_granule = AV_NOPTS_VALUE ; <nl>  <nl> - ogg_write_pages ( s , 1 ); <nl> + ogg_write_pages ( s , 2 ); <nl>  <nl> return 0 ; <nl> }
static av_cold int wavesynth_init ( AVCodecContext * avc ) <nl> return 0 ; <nl>  <nl> fail : <nl> - av_free ( ws -> inter ); <nl> - av_free ( ws -> sin ); <nl> + av_freep (& ws -> inter ); <nl> + av_freep (& ws -> sin ); <nl> return r ; <nl> } <nl>  <nl> static av_cold int wavesynth_close ( AVCodecContext * avc ) <nl> { <nl> struct wavesynth_context * ws = avc -> priv_data ; <nl>  <nl> - av_free ( ws -> sin ); <nl> - av_free ( ws -> inter ); <nl> + av_freep (& ws -> sin ); <nl> + av_freep (& ws -> inter ); <nl> return 0 ; <nl> } <nl> 
static av_cold int init ( AVCodecContext * avctx ) <nl> int dummy_int ; <nl>  <nl> /* Back up the extradata so it can be restored at close time . */ <nl> - priv -> orig_extradata = av_malloc ( avctx -> extradata_size ); <nl> + priv -> orig_extradata = av_malloc ( avctx -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! priv -> orig_extradata ) { <nl> av_log ( avctx , AV_LOG_ERROR , <nl> " Failed to allocate copy of extradata \ n ");
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> bit_depth = 8 ; <nl> color_type = PNG_COLOR_TYPE_GRAY_ALPHA ; <nl> break ; <nl> + case AV_PIX_FMT_YA16BE : <nl> + bit_depth = 16 ; <nl> + color_type = PNG_COLOR_TYPE_GRAY_ALPHA ; <nl> + break ; <nl> case AV_PIX_FMT_MONOBLACK : <nl> bit_depth = 1 ; <nl> color_type = PNG_COLOR_TYPE_GRAY ; <nl> AVCodec ff_png_encoder = { <nl> AV_PIX_FMT_RGB48BE , AV_PIX_FMT_RGBA64BE , <nl> AV_PIX_FMT_PAL8 , <nl> AV_PIX_FMT_GRAY8 , AV_PIX_FMT_GRAY8A , <nl> - AV_PIX_FMT_GRAY16BE , <nl> + AV_PIX_FMT_GRAY16BE , AV_PIX_FMT_YA16BE , <nl> AV_PIX_FMT_MONOBLACK , AV_PIX_FMT_NONE <nl> }, <nl> . priv_class = & pngenc_class ,
static int ljpeg_decode_rgb_scan ( MJpegDecodeContext * s , int nb_components , int p <nl> return 0 ; <nl> } <nl>  <nl> - static int ljpeg_decode_yuv_scan ( MJpegDecodeContext * s , int predictor , <nl> + static int ljpeg_decode_yuv_scan ( MJpegDecodeContext * s , int nb_components , int predictor , <nl> int point_transform ) <nl> { <nl> int i , mb_x , mb_y ; <nl> - const int nb_components = s -> nb_components ; <nl> int bits = ( s -> bits + 7 )&~ 7 ; <nl> int resync_mb_y = 0 ; <nl> int resync_mb_x = 0 ; <nl> next_field : <nl> if (( ret = ljpeg_decode_rgb_scan ( s , nb_components , predictor , point_transform )) < 0 ) <nl> return ret ; <nl> } else { <nl> - if (( ret = ljpeg_decode_yuv_scan ( s , predictor , point_transform )) < 0 ) <nl> + if (( ret = ljpeg_decode_yuv_scan ( s , nb_components , predictor , point_transform )) < 0 ) <nl> return ret ; <nl> } <nl> }
void ff_htmlmarkup_to_ass ( void * log_ctx , AVBPrint * dst , const char * in ) <nl> if ( stack [ sptr ]. param [ i ][ 0 ]) <nl> av_bprintf ( dst , "% s ", stack [ sptr ]. param [ i ]); <nl> } <nl> - } else if (! tagname [ 1 ] && strspn ( tagname , " bisu ") == 1 ) { <nl> + } else if ( tagname [ 0 ] && ! tagname [ 1 ] && strspn ( tagname , " bisu ") == 1 ) { <nl> av_bprintf ( dst , "{\\% c % d }", tagname [ 0 ], ! tag_close ); <nl> } else { <nl> unknown = 1 ;
enum SmkBlockTypes { <nl> */ <nl> static int smacker_decode_tree ( GetBitContext * gb , HuffContext * hc , uint32_t prefix , int length ) <nl> { <nl> + if ( length > 32 ) { <nl> + av_log ( NULL , AV_LOG_ERROR , " length too long \ n "); <nl> + return - 1 ; <nl> + } <nl> if (! get_bits1 ( gb )){ // Leaf <nl> if ( hc -> current >= 256 ){ <nl> av_log ( NULL , AV_LOG_ERROR , " Tree size exceeded !\ n ");
static int vqf_read_header ( AVFormatContext * s ) <nl> break ; <nl> default : <nl> st -> codec -> sample_rate = rate_flag * 1000 ; <nl> + if ( st -> codec -> sample_rate <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " sample rate % d is invalid \ n ", st -> codec -> sample_rate ); <nl> + return - 1 ; <nl> + } <nl> break ; <nl> } <nl> 
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> } <nl> s -> frame_len = 1 << frame_len_bits ; <nl>  <nl> - if ( s -> channels > MAX_CHANNELS ) { <nl> - av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + if ( avctx -> channels > MAX_CHANNELS ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", avctx -> channels ); <nl> return - 1 ; <nl> } <nl> 
# include " fft . h " <nl> # include " internal . h " <nl> # include " sinewin . h " <nl> +# include " unary . h " <nl>  <nl> # include " cookdata . h " <nl>  <nl> static void decode_gain_info ( GetBitContext * gb , int * gaininfo ) <nl> { <nl> int i , n ; <nl>  <nl> - while ( get_bits1 ( gb )) { <nl> - /* NOTHING */ <nl> - } <nl> - <nl> - n = get_bits_count ( gb ) - 1 ; // amount of elements * 2 to update <nl> + n = get_unary ( gb , 0 , get_bits_left ( gb )); // amount of elements * 2 to update <nl>  <nl> i = 0 ; <nl> while ( n --) {
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> switch (( buf [ 0 ] >> 1 ) & 7 ) { <nl> case 0 : { // lzo compression <nl> int outlen = c -> decomp_size , inlen = buf_size - 2 ; <nl> - if ( av_lzo1x_decode ( c -> decomp_buf , & outlen , & buf [ 2 ], & inlen )) { <nl> + if ( av_lzo1x_decode ( c -> decomp_buf , & outlen , & buf [ 2 ], & inlen ) || outlen ) { <nl> av_log ( avctx , AV_LOG_ERROR , " error during lzo decompression \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static void writer_close ( WriterContext ** wctx ) <nl> if ((* wctx )-> writer -> priv_class ) <nl> av_opt_free ((* wctx )-> priv ); <nl> av_freep (&((* wctx )-> priv )); <nl> + av_opt_free (* wctx ); <nl> av_freep ( wctx ); <nl> } <nl> 
static void mxf_packet_timestamps ( MXFContext * mxf , AVPacket * pkt ) <nl> return ; <nl>  <nl> /* find mxf -> current_edit_unit so that the next edit unit starts ahead of pkt -> pos */ <nl> - for (;;) { <nl> + while ( mxf -> current_edit_unit >= 0 ) { <nl> if ( mxf_edit_unit_absolute_offset ( mxf , t , mxf -> current_edit_unit + 1 , NULL , & next_ofs , 0 ) < 0 ) <nl> break ; <nl>  <nl> static void mxf_packet_timestamps ( MXFContext * mxf , AVPacket * pkt ) <nl> mxf -> current_edit_unit ++; <nl> } <nl>  <nl> - if ( mxf -> current_edit_unit >= t -> nb_ptses ) <nl> + if ( mxf -> current_edit_unit < 0 || mxf -> current_edit_unit >= t -> nb_ptses ) <nl> return ; <nl>  <nl> pkt -> dts = mxf -> current_edit_unit + t -> first_dts ;
static int xv_write_header ( AVFormatContext * s ) <nl> if ( XvQueryAdaptors ( xv -> display , DefaultRootWindow ( xv -> display ), & num_adaptors , & ai ) != Success ) <nl> return AVERROR_EXTERNAL ; <nl> xv -> xv_port = ai [ 0 ]. base_id ; <nl> + XvFreeAdaptorInfo ( ai ); <nl>  <nl> if ( encctx -> pix_fmt != AV_PIX_FMT_YUV420P ) { <nl> av_log ( s , AV_LOG_ERROR ,
static int mov_write_single_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int64_t frag_duration = 0 ; <nl> int size = pkt -> size ; <nl>  <nl> + int ret = check_pkt ( s , pkt ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + <nl> if ( mov -> flags & FF_MOV_FLAG_FRAG_DISCONT ) { <nl> int i ; <nl> for ( i = 0 ; i < s -> nb_streams ; i ++)
static int parse_cookie ( HTTPContext * s , const char * p , AVDictionary ** cookies ) <nl> } <nl> } <nl> } <nl> + av_dict_free (& new_params ); <nl>  <nl> // duplicate the cookie name ( dict will dupe the value ) <nl> if (!( eql = strchr ( p , '='))) return AVERROR ( EINVAL );
static int decode_audio_specific_config ( AACContext * ac , <nl> */ <nl> static av_always_inline int lcg_random ( int previous_val ) <nl> { <nl> - return previous_val * 1664525 + 1013904223 ; <nl> + union { unsigned u ; int s ; } v = { previous_val * 1664525u + 1013904223 }; <nl> + return v . s ; <nl> } <nl>  <nl> static av_always_inline void reset_predict_state ( PredictorState * ps )
static int pcm_decode_frame ( AVCodecContext * avctx , <nl> src = buf ; <nl>  <nl> n = av_get_bits_per_sample ( avctx -> codec_id )/ 8 ; <nl> - if (( n && buf_size % n ) || avctx -> channels > MAX_CHANNELS ){ <nl> + if ( n && buf_size % n ){ <nl> av_log ( avctx , AV_LOG_ERROR , " invalid PCM packet \ n "); <nl> return - 1 ; <nl> } <nl> + if ( avctx -> channels <= 0 || avctx -> channels > MAX_CHANNELS ){ <nl> + av_log ( avctx , AV_LOG_ERROR , " PCM channels out of bounds \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> buf_size = FFMIN ( buf_size , * data_size / 2 ); <nl> * data_size = 0 ;
static int filter_frame ( AVFilterLink * inlink , AVFrame * buf ) <nl> av_frame_free (& buf ); <nl>  <nl> end : <nl> - vol -> var_values [ VAR_NB_CONSUMED_SAMPLES ] += buf -> nb_samples ; <nl> + vol -> var_values [ VAR_NB_CONSUMED_SAMPLES ] += out_buf -> nb_samples ; <nl> return ff_filter_frame ( outlink , out_buf ); <nl> } <nl> 
static int check_fps ( int fps ) <nl>  <nl> static int check_timecode ( void * log_ctx , AVTimecode * tc ) <nl> { <nl> - if ( tc -> fps <= 0 ) { <nl> + if (( int ) tc -> fps <= 0 ) { <nl> av_log ( log_ctx , AV_LOG_ERROR , " Timecode frame rate must be specified \ n "); <nl> return AVERROR ( EINVAL ); <nl> }
static inline void xan_wc3_copy_pixel_run ( XanContext * s , AVFrame * frame , <nl> prevframe_index = ( y + motion_y ) * stride + x + motion_x ; <nl> prevframe_x = x + motion_x ; <nl>  <nl> - if ( prev_palette_plane == palette_plane && FFABS ( curframe_index - prevframe_index ) < pixel_count ) { <nl> + if ( prev_palette_plane == palette_plane && FFABS ( motion_x + width * motion_y ) < pixel_count ) { <nl> avpriv_request_sample ( s -> avctx , " Overlapping copy "); <nl> return ; <nl> }
AVFILTER_DEFINE_CLASS ( testsrc ); <nl> * @ param w width of the rectangle to draw , expressed as a number of segment_width units <nl> * @ param h height of the rectangle to draw , expressed as a number of segment_width units <nl> */ <nl> - static void draw_rectangle ( unsigned val , uint8_t * dst , int dst_linesize , unsigned segment_width , <nl> - unsigned x , unsigned y , unsigned w , unsigned h ) <nl> + static void draw_rectangle ( unsigned val , uint8_t * dst , int dst_linesize , int segment_width , <nl> + int x , int y , int w , int h ) <nl> { <nl> int i ; <nl> int step = 3 ; <nl> static void draw_rectangle ( unsigned val , uint8_t * dst , int dst_linesize , unsigne <nl> } <nl> } <nl>  <nl> - static void draw_digit ( int digit , uint8_t * dst , unsigned dst_linesize , <nl> - unsigned segment_width ) <nl> + static void draw_digit ( int digit , uint8_t * dst , int dst_linesize , <nl> + int segment_width ) <nl> { <nl> # define TOP_HBAR 1 <nl> # define MID_HBAR 2
int ff_h264_decode_ref_pic_list_reordering ( H264Context * h , H264SliceContext * sl ) <nl>  <nl> long_idx = pic_num_extract ( h , pic_id , & pic_structure ); <nl>  <nl> - if ( long_idx > 31 ) { <nl> + if ( long_idx > 31U ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , <nl> " long_term_pic_idx overflow \ n "); <nl> return AVERROR_INVALIDDATA ;
static int plot_spectrum_column ( AVFilterLink * inlink , AVFrame * insamples ) <nl> a = cbrt ( a ); <nl> break ; <nl> case LOG : <nl> - a = 1 + log10 ( FFMAX ( FFMIN ( 1 , a ), 1e - 6 )) / 6 ; // zero = - 120dBFS <nl> + a = 1 + log10 ( FFMAX ( FFMIN ( 1 , a ), 1e - 6 )) / 5 ; // zero = - 120dBFS <nl> break ; <nl> default : <nl> av_assert0 ( 0 );
static int mp3_write_audio_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> if ( pkt -> data && pkt -> size >= 4 ) { <nl> MPADecodeHeader c ; <nl> int av_unused base ; <nl> + uint32_t head = AV_RB32 ( pkt -> data ); <nl>  <nl> - avpriv_mpegaudio_decode_header (& c , AV_RB32 ( pkt -> data )); <nl> + if ( ff_mpa_check_header ( head ) < 0 ) { <nl> + av_log ( s , AV_LOG_WARNING , " Audio packet of size % d ( starting with % 08X ...) " <nl> + " is invalid , writing it anyway .\ n ", pkt -> size , head ); <nl> + return ff_raw_write_packet ( s , pkt ); <nl> + } <nl> + avpriv_mpegaudio_decode_header (& c , head ); <nl>  <nl> if (! mp3 -> initial_bitrate ) <nl> mp3 -> initial_bitrate = c . bit_rate ;
int64_t av_rescale_rnd ( int64_t a , int64_t b , int64_t c , enum AVRounding rnd ) <nl> else { <nl> int64_t ad = a / c ; <nl> int64_t a2 = ( a % c * b + r ) / c ; <nl> - if ( ad >= INT32_MAX && ad > ( INT64_MAX - a2 ) / b ) <nl> + if ( ad >= INT32_MAX && b && ad > ( INT64_MAX - a2 ) / b ) <nl> return INT64_MIN ; <nl> return ad * b + a2 ; <nl> }
static int read_seek ( AVFormatContext * s , int stream_index , <nl> next_node [ 1 ]-> pos , next_node [ 1 ]-> pos , <nl> next_node [ 0 ]-> ts , next_node [ 1 ]-> ts , <nl> AVSEEK_FLAG_BACKWARD , & ts , nut_read_timestamp ); <nl> + if ( pos < 0 ) <nl> + return pos ; <nl>  <nl> if (!( flags & AVSEEK_FLAG_BACKWARD )) { <nl> dummy . pos = pos + 16 ;
static int decode_fctl_chunk ( AVFormatContext * s , APNGDemuxContext * ctx , AVPacket <nl> static int apng_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> { <nl> APNGDemuxContext * ctx = s -> priv_data ; <nl> - int ret ; <nl> + int64_t ret ; <nl> int64_t size ; <nl> AVIOContext * pb = s -> pb ; <nl> uint32_t len , tag ;
static int update_context_from_thread ( AVCodecContext * dst , AVCodecContext * src , <nl> { <nl> int err = 0 ; <nl>  <nl> - if ( dst != src ) { <nl> + if ( dst != src && ( for_user || !( av_codec_get_codec_descriptor ( src )-> props & AV_CODEC_PROP_INTRA_ONLY ))) { <nl> dst -> time_base = src -> time_base ; <nl> dst -> framerate = src -> framerate ; <nl> dst -> width = src -> width ;
static av_always_inline int wp_exp2 ( int16_t val ) <nl> return neg ? - res : res ; <nl> } <nl>  <nl> - static av_always_inline int wp_log2 ( int32_t val ) <nl> + static av_always_inline int wp_log2 ( uint32_t val ) <nl> { <nl> int bits ; <nl> 
int av_packet_ref ( AVPacket * dst , const AVPacket * src ) <nl> ret = packet_alloc (& dst -> buf , src -> size ); <nl> if ( ret < 0 ) <nl> goto fail ; <nl> - memcpy ( dst -> buf -> data , src -> data , src -> size ); <nl> + if ( src -> size ) <nl> + memcpy ( dst -> buf -> data , src -> data , src -> size ); <nl>  <nl> dst -> data = dst -> buf -> data ; <nl> } else {
int attribute_align_arg avcodec_decode_audio4 ( AVCodecContext * avctx , <nl> * extended_data are doing it correctly */ <nl> if (* got_frame_ptr ) { <nl> planar = av_sample_fmt_is_planar ( frame -> format ); <nl> - channels = av_get_channel_layout_nb_channels ( frame -> channel_layout ); <nl> + channels = frame -> channels ; <nl> if (!( planar && channels > AV_NUM_DATA_POINTERS )) <nl> frame -> extended_data = frame -> data ; <nl> } else {
static char * doubles2str ( double * dp , int count , const char * sep ) <nl> ap [ 0 ] = '\ 0 '; <nl> for ( i = 0 ; i < count ; i ++) { <nl> unsigned l = snprintf ( ap , component_len , "% f % s ", dp [ i ], sep ); <nl> - if ( l >= component_len ) <nl> + if ( l >= component_len ) { <nl> + av_free ( ap0 ); <nl> return NULL ; <nl> + } <nl> ap += l ; <nl> } <nl> ap0 [ strlen ( ap0 ) - strlen ( sep )] = '\ 0 ';
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> s -> current_picture -> linesize [ 2 ], w >> s -> chroma_h_shift , h >> s -> chroma_v_shift , <nl> EDGE_WIDTH >> s -> chroma_h_shift , EDGE_WIDTH >> s -> chroma_v_shift , EDGE_TOP | EDGE_BOTTOM ); <nl> } <nl> + emms_c (); <nl> } <nl>  <nl> ff_snow_frame_start ( s ); <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl>  <nl> } <nl> + emms_c (); <nl>  <nl> update_last_header_values ( s ); <nl> 
static int skip_check ( MpegEncContext * s , Picture * p , Picture * ref ) <nl> switch ( s -> avctx -> frame_skip_exp ) { <nl> case 0 : score = FFMAX ( score , v ); break ; <nl> case 1 : score += FFABS ( v ); break ; <nl> - case 2 : score += v * v ; break ; <nl> - case 3 : score64 += FFABS ( v * v * ( int64_t ) v ); break ; <nl> - case 4 : score64 += v * v * ( int64_t )( v * v ); break ; <nl> + case 2 : score64 += v * ( int64_t ) v ; break ; <nl> + case 3 : score64 += FFABS ( v * ( int64_t ) v * v ); break ; <nl> + case 4 : score64 += ( v * ( int64_t ) v ) * ( v * ( int64_t ) v ); break ; <nl> } <nl> } <nl> }
static av_cold void uninit ( AVFilterContext * ctx ) <nl> { <nl> Frei0rContext * frei0r = ctx -> priv ; <nl>  <nl> - if ( frei0r -> destruct ) <nl> + if ( frei0r -> destruct && frei0r -> instance ) <nl> frei0r -> destruct ( frei0r -> instance ); <nl> if ( frei0r -> deinit ) <nl> frei0r -> deinit ();
static int mpeg_decode_slice ( Mpeg1Context * s1 , int mb_y , <nl> s -> resync_mb_x = <nl> s -> resync_mb_y = - 1 ; <nl>  <nl> - if ( mb_y >= s -> mb_height ){ <nl> - av_log ( s -> avctx , AV_LOG_ERROR , " slice below image (% d >= % d )\ n ", s -> mb_y , s -> mb_height ); <nl> + if ( mb_y << field_pic >= s -> mb_height ){ <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " slice below image (% d >= % d )\ n ", mb_y , s -> mb_height ); <nl> return - 1 ; <nl> } <nl> 
int ff_h264_decode_slice_header ( H264Context * h , H264SliceContext * sl ) <nl>  <nl> if ( first_mb_in_slice == 0 ) { // FIXME better field boundary detection <nl> if ( h -> current_slice ) { <nl> - av_assert0 (! h -> setup_finished ); <nl> + if ( h -> setup_finished ) { <nl> + av_log ( h -> avctx , AV_LOG_ERROR , " Too many fields \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( h -> cur_pic_ptr && FIELD_PICTURE ( h ) && h -> first_field ) { <nl> ret = ff_h264_field_end ( h , h -> slice_ctx , 1 ); <nl> h -> current_slice = 0 ;
static int configure_output_video_filter ( FilterGraph * fg , OutputFilter * ofilter , <nl> snprintf ( name , sizeof ( name ), " output stream % d :% d ", ost -> file_index , ost -> index ); <nl> ret = avfilter_graph_create_filter (& ofilter -> filter , <nl> avfilter_get_by_name (" buffersink "), <nl> - name , NULL , pix_fmts , fg -> graph ); <nl> + name , NULL , NULL , fg -> graph ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> 
int av_grow_packet ( AVPacket * pkt , int grow_by ) <nl> pkt -> buf = av_buffer_alloc ( new_size ); <nl> if (! pkt -> buf ) <nl> return AVERROR ( ENOMEM ); <nl> - memcpy ( pkt -> buf -> data , pkt -> data , pkt -> size ); <nl> + if ( pkt -> size > 0 ) <nl> + memcpy ( pkt -> buf -> data , pkt -> data , pkt -> size ); <nl> pkt -> data = pkt -> buf -> data ; <nl> } <nl> pkt -> size += grow_by ;
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * in ) <nl> AVFilterBufferRef * out ; <nl> int hsub0 = desc -> log2_chroma_w ; <nl> int vsub0 = desc -> log2_chroma_h ; <nl> - int direct ; <nl> + int direct = 0 ; <nl> int plane ; <nl>  <nl> if (( in -> perms & AV_PERM_WRITE ) && !( in -> perms & AV_PERM_PRESERVE )) {
static int mxf_read_primer_pack ( void * arg , AVIOContext * pb , int tag , int size , U <nl> avpriv_request_sample ( pb , " Primer pack item length % d ", item_len ); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> - if ( item_num > 65536 ) { <nl> + if ( item_num > 65536 || item_num < 0 ) { <nl> av_log ( mxf -> fc , AV_LOG_ERROR , " item_num % d is too large \ n ", item_num ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static inline void ls_decode_line ( JLSState * state , MJpegDecodeContext * s , <nl> while ( x < w ) { <nl> int err , pred ; <nl>  <nl> + if ( get_bits_left (& s -> gb ) <= 0 ) <nl> + return ; <nl> + <nl> /* compute gradients */ <nl> Ra = x ? R ( dst , x - stride ) : R ( last , x ); <nl> Rb = R ( last , x );
int avresample_convert ( AVAudioResampleContext * avr , void ** output , <nl> } <nl> } <nl>  <nl> - return handle_buffered_output ( avr , & output_buffer , current_buffer ); <nl> + return handle_buffered_output ( avr , output ? & output_buffer : NULL , <nl> + current_buffer ); <nl> } <nl>  <nl> int avresample_available ( AVAudioResampleContext * avr )
static void mov_metadata_creation_time ( AVDictionary ** metadata , int64_t time ) <nl> if ( time ) { <nl> if ( time >= 2082844800 ) <nl> time -= 2082844800 ; /* seconds between 1904 - 01 - 01 and Epoch */ <nl> + <nl> + if (( int64_t )( time * 1000000ULL ) / 1000000 != time ) { <nl> + av_log ( NULL , AV_LOG_DEBUG , " creation_time is not representable \ n "); <nl> + return ; <nl> + } <nl> + <nl> avpriv_dict_set_timestamp ( metadata , " creation_time ", time * 1000000 ); <nl> } <nl> }
static inline int decode_vui_parameters ( H264Context * h , SPS * sps ){ <nl> sps -> num_reorder_frames = get_ue_golomb (& s -> gb ); <nl> get_ue_golomb (& s -> gb ); /* max_dec_frame_buffering */ <nl>  <nl> - if ( s -> gb . size_in_bits < get_bits_count (& s -> gb )){ <nl> + if ( get_bits_left (& s -> gb ) < 0 ){ <nl> sps -> num_reorder_frames = 0 ; <nl> sps -> bitstream_restriction_flag = 0 ; <nl> } <nl> static inline int decode_vui_parameters ( H264Context * h , SPS * sps ){ <nl> return - 1 ; <nl> } <nl> } <nl> - if ( s -> gb . size_in_bits < get_bits_count (& s -> gb )){ <nl> - av_log ( h -> s . avctx , AV_LOG_ERROR , " Overread VUI by % d bits \ n ", get_bits_count (& s -> gb ) - s -> gb . size_in_bits ); <nl> + if ( get_bits_left (& s -> gb ) < 0 ){ <nl> + av_log ( h -> s . avctx , AV_LOG_ERROR , " Overread VUI by % d bits \ n ", - get_bits_left (& s -> gb )); <nl> return - 1 ; <nl> } <nl>  <nl> int ff_h264_decode_picture_parameter_set ( H264Context * h , int bit_length ){ <nl> memcpy ( pps -> scaling_matrix8 , h -> sps_buffers [ pps -> sps_id ]-> scaling_matrix8 , sizeof ( pps -> scaling_matrix8 )); <nl>  <nl> bits_left = bit_length - get_bits_count (& s -> gb ); <nl> - if ( get_bits_count (& s -> gb ) < bit_length ){ <nl> + if ( bits_left > 0 ){ <nl> pps -> transform_8x8_mode = get_bits1 (& s -> gb ); <nl> decode_scaling_matrices ( h , h -> sps_buffers [ pps -> sps_id ], pps , 0 , pps -> scaling_matrix4 , pps -> scaling_matrix8 ); <nl> pps -> chroma_qp_index_offset [ 1 ]= get_se_golomb (& s -> gb ); // second_chroma_qp_index_offset
static unsigned int mszh_decomp ( unsigned char * srcptr , int srclen , unsigned cha <nl> continue ; <nl> } <nl> if (( mask & ( 1 << (-- maskbit ))) == 0 ) { <nl> - if ( destptr + 4 > destptr_end ) <nl> + if ( destptr_end - destptr < 4 ) <nl> break ; <nl> memcpy ( destptr , srcptr , 4 ); <nl> srclen -= 4 ; <nl> static unsigned int mszh_decomp ( unsigned char * srcptr , int srclen , unsigned cha <nl> ofs &= 0x7ff ; <nl> srclen -= 2 ; <nl> cnt *= 4 ; <nl> - if ( destptr + cnt > destptr_end ) { <nl> + if ( destptr_end - destptr < cnt ) { <nl> cnt = destptr_end - destptr ; <nl> } <nl> for (; cnt > 0 ; cnt --) {
static void vc1_put_blocks_clamped ( VC1Context * v , int put_signed ) <nl> if (( edges & 8 ) && \ <nl> s -> mb_y == (( s -> mb_height >> v -> field_mode ) - 1 )) \ <nl> mquant = - v -> altpq ; \ <nl> - if (! mquant || mquant > 31 ) { \ <nl> + if (! mquant || mquant > 31 || mquant < - 31 ) { \ <nl> av_log ( v -> s . avctx , AV_LOG_ERROR , \ <nl> " Overriding invalid mquant % d \ n ", mquant ); \ <nl> mquant = 1 ; \
int ff_hevc_decode_nal_sps ( HEVCContext * s ) <nl> goto err ; <nl> } <nl>  <nl> + if (! s -> vps_list [ sps -> vps_id ]) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " VPS % d does not exist \ n ", <nl> + sps -> vps_id ); <nl> + ret = AVERROR_INVALIDDATA ; <nl> + goto err ; <nl> + } <nl> + <nl> sps -> max_sub_layers = get_bits ( gb , 3 ) + 1 ; <nl> if ( sps -> max_sub_layers > MAX_SUB_LAYERS ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " sps_max_sub_layers out of range : % d \ n ",
static int mpeg_mux_init ( AVFormatContext * ctx ) <nl> if (! s -> mux_rate ) { <nl> /* we increase slightly the bitrate to take into account the <nl> headers . XXX : compute it exactly */ <nl> - bitrate += bitrate * 5 / 100 ; <nl> + bitrate += bitrate * 5LL / 100 ; <nl> bitrate += 10000 ; <nl> s -> mux_rate = ( bitrate + ( 8 * 50 ) - 1 ) / ( 8 * 50 ); <nl> }
static int mxf_write_header ( AVFormatContext * s ) <nl> mxf -> edit_unit_byte_count += klv_fill_size ( mxf -> edit_unit_byte_count ); <nl>  <nl> sc -> signal_standard = 1 ; <nl> + sc -> color_siting = 0 ; <nl> } <nl> if ( mxf -> signal_standard >= 0 ) <nl> sc -> signal_standard = mxf -> signal_standard ;
static int get_stats ( AVCodecContext * avctx , int eos ) <nl> // libtheora generates a summary header at the end <nl> memcpy ( h -> stats , buf , bytes ); <nl> avctx -> stats_out = av_malloc ( b64_size ); <nl> + if (! avctx -> stats_out ) <nl> + return AVERROR ( ENOMEM ); <nl> av_base64_encode ( avctx -> stats_out , b64_size , h -> stats , h -> stats_offset ); <nl> } <nl> return 0 ;
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int av_uninit ( channels ); <nl> int av_uninit ( sample_rate ); <nl> AVStream * st = NULL ; <nl> + int last = - 1 ; <nl>  <nl> /* pkt size is repeated at end . skip it */ <nl> - for (;; avio_skip ( s -> pb , 4 )) { <nl> + for (;; last = avio_rb32 ( s -> pb )) { <nl> pos = avio_tell ( s -> pb ); <nl> type = ( avio_r8 ( s -> pb ) & 0x1F ); <nl> size = avio_rb24 ( s -> pb ); <nl> dts = avio_rb24 ( s -> pb ); <nl> dts |= avio_r8 ( s -> pb ) << 24 ; <nl> - av_log ( s , AV_LOG_TRACE , " type :% d , size :% d , dts :%" PRId64 " pos :%" PRId64 "\ n ", type , size , dts , avio_tell ( s -> pb )); <nl> + av_log ( s , AV_LOG_TRACE , " type :% d , size :% d , last :% d , dts :%" PRId64 " pos :%" PRId64 "\ n ", type , size , last , dts , avio_tell ( s -> pb )); <nl> if ( avio_feof ( s -> pb )) <nl> return AVERROR_EOF ; <nl> avio_skip ( s -> pb , 3 ); /* stream id , always 0 */
static int64_t dv_frame_offset ( AVFormatContext * s , DVDemuxContext * c , <nl> void ff_dv_offset_reset ( DVDemuxContext * c , int64_t frame_offset ) <nl> { <nl> c -> frames = frame_offset ; <nl> - if ( c -> ach ) <nl> + if ( c -> ach ) { <nl> + if ( c -> sys ) { <nl> c -> abytes = av_rescale_q ( c -> frames , c -> sys -> time_base , <nl> ( AVRational ){ 8 , c -> ast [ 0 ]-> codec -> bit_rate }); <nl> + } else <nl> + av_log ( c -> fctx , AV_LOG_ERROR , " cannot adjust audio bytes \ n "); <nl> + } <nl> c -> audio_pkt [ 0 ]. size = c -> audio_pkt [ 1 ]. size = 0 ; <nl> c -> audio_pkt [ 2 ]. size = c -> audio_pkt [ 3 ]. size = 0 ; <nl> }
static int a64multi_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> } else { <nl> /* fill up mc_meta_charset with data until lifetime exceeds */ <nl> if ( c -> mc_frame_counter < c -> mc_lifetime ) { <nl> - * p = * pict ; <nl> + ret = av_frame_ref ( p , pict ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> p -> pict_type = AV_PICTURE_TYPE_I ; <nl> p -> key_frame = 1 ; <nl> to_meta_with_crop ( avctx , p , meta + 32000 * c -> mc_frame_counter );
static av_cold int init ( AVFilterContext * ctx , const char * args , void * opaque ) <nl> sscanf ( args , "% d :% d :% lf :% d :% d :% lf ", & lmsize_x , & lmsize_y , & lamount , <nl> & cmsize_x , & cmsize_y , & camount ); <nl>  <nl> - if ( lmsize_x < 2 || lmsize_y < 2 || cmsize_x < 2 || cmsize_y < 2 ) { <nl> + if (( lamount && ( lmsize_x < 2 || lmsize_y < 2 )) || <nl> + ( camount && ( cmsize_x < 2 || cmsize_y < 2 ))) { <nl> av_log ( ctx , AV_LOG_ERROR , <nl> " Invalid value < 2 for lmsize_x :% d or lmsize_y :% d or cmsize_x :% d or cmsize_y :% d \ n ", <nl> lmsize_x , lmsize_y , cmsize_x , cmsize_y );
int main ( int argc , char ** argv ) <nl> enum PixelFormat srcFormat = PIX_FMT_NONE ; <nl> enum PixelFormat dstFormat = PIX_FMT_NONE ; <nl> uint8_t * rgb_data = av_malloc ( W * H * 4 ); <nl> - uint8_t * rgb_src [ 3 ] = { rgb_data , NULL , NULL }; <nl> - int rgb_stride [ 3 ] = { 4 * W , 0 , 0 }; <nl> + uint8_t * rgb_src [ 4 ] = { rgb_data , NULL , NULL , NULL }; <nl> + int rgb_stride [ 4 ] = { 4 * W , 0 , 0 , 0 }; <nl> uint8_t * data = av_malloc ( 4 * W * H ); <nl> uint8_t * src [ 4 ] = { data , data + W * H , data + W * H * 2 , data + W * H * 3 }; <nl> int stride [ 4 ] = { W , W , W , W };
retry : <nl> # if HAVE_MMX <nl> if ( s -> codec_id == CODEC_ID_MPEG4 && s -> xvid_build >= 0 && avctx -> idct_algo == FF_IDCT_AUTO && ( av_get_cpu_flags () & AV_CPU_FLAG_MMX )) { <nl> avctx -> idct_algo = FF_IDCT_XVIDMMX ; <nl> - avctx -> coded_width = 0 ; // force reinit <nl> -// dsputil_init (& s -> dsp , avctx ); <nl> + ff_dct_common_init ( s ); <nl> s -> picture_number = 0 ; <nl> } <nl> # endif <nl> retry : <nl> || s -> height != avctx -> coded_height ) { <nl> /* H . 263 could change picture size any time */ <nl> ParseContext pc = s -> parse_context ; // FIXME move these demuxng hack to avformat <nl> + <nl> + if ( HAVE_THREADS && ( s -> avctx -> active_thread_type & FF_THREAD_FRAME )) { <nl> + av_log_missing_feature ( s -> avctx , " Width / height / bit depth / chroma idc changing with threads is ", 0 ); <nl> + return - 1 ; // width / height changed during parallelized decoding <nl> + } <nl> + <nl> s -> parse_context . buffer = 0 ; <nl> MPV_common_end ( s ); <nl> s -> parse_context = pc ;
static void encode_scale_factors ( AVCodecContext * avctx , AACEncContext * s , <nl> for ( i = 0 ; i < sce -> ics . max_sfb ; i ++) { <nl> if (! sce -> zeroes [ w * 16 + i ]) { <nl> diff = sce -> sf_idx [ w * 16 + i ] - off + SCALE_DIFF_ZERO ; <nl> - if ( diff < 0 || diff > 120 ) <nl> - av_log ( avctx , AV_LOG_ERROR , " Scalefactor difference is too big to be coded \ n "); <nl> + av_assert0 ( diff >= 0 && diff <= 120 ); <nl> off = sce -> sf_idx [ w * 16 + i ]; <nl> put_bits (& s -> pb , ff_aac_scalefactor_bits [ diff ], ff_aac_scalefactor_code [ diff ]); <nl> }
again : <nl> " SPS decoding failure , trying again with the complete NAL \ n "); <nl> if ( h -> is_avc ) <nl> av_assert0 ( next_avc - buf_index + consumed == nalsize ); <nl> + if (( next_avc - buf_index + consumed - 1 ) >= INT_MAX / 8 ) <nl> + break ; <nl> init_get_bits (& s -> gb , & buf [ buf_index + 1 - consumed ], <nl> 8 *( next_avc - buf_index + consumed - 1 )); <nl> ff_h264_decode_seq_parameter_set ( h );
static int read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> pkt -> data [ 4 ] = jvf -> video_type ; <nl> if (( size = avio_read ( pb , pkt -> data + JV_PREAMBLE_SIZE , size )) < 0 ) <nl> return AVERROR ( EIO ); <nl> + memset ( pkt -> data + JV_PREAMBLE_SIZE + size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> pkt -> size = size + JV_PREAMBLE_SIZE ; <nl> pkt -> stream_index = 1 ;
static av_cold int tdsc_init ( AVCodecContext * avctx ) <nl> ctx -> jpeg_avctx -> flags2 = avctx -> flags2 ; <nl> ctx -> jpeg_avctx -> dct_algo = avctx -> dct_algo ; <nl> ctx -> jpeg_avctx -> idct_algo = avctx -> idct_algo ;; <nl> - ret = avcodec_open2 ( ctx -> jpeg_avctx , codec , NULL ); <nl> + ret = ff_codec_open2_recursive ( ctx -> jpeg_avctx , codec , NULL ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> 
static int vqf_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> pkt -> data [ 1 ] = c -> last_frame_bits ; <nl> ret = avio_read ( s -> pb , pkt -> data + 2 , size ); <nl>  <nl> - if ( ret <= 0 ) { <nl> + if ( ret != size ) { <nl> av_free_packet ( pkt ); <nl> return AVERROR ( EIO ); <nl> }
static int decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> av_log ( avctx , AV_LOG_ERROR , " Buffer contains IP frames !\ n "); <nl> } <nl>  <nl> + if ( ctx -> frame_type >= FRAMETYPE_NULL_FIRST ) <nl> + return buf_size ; <nl> + <nl> if ( ctx -> frame . data [ 0 ]) <nl> avctx -> release_buffer ( avctx , & ctx -> frame ); <nl> 
void avfilter_set_common_formats ( AVFilterContext * ctx , AVFilterFormats * formats ) <nl>  <nl> int avfilter_default_query_formats ( AVFilterContext * ctx ) <nl> { <nl> - enum AVMediaType type = ctx -> inputs [ 0 ] ? ctx -> inputs [ 0 ]-> type : <nl> - ctx -> outputs [ 0 ] ? ctx -> outputs [ 0 ]-> type : <nl> + enum AVMediaType type = ctx -> inputs && ctx -> inputs [ 0 ] ? ctx -> inputs [ 0 ]-> type : <nl> + ctx -> outputs && ctx -> outputs [ 0 ] ? ctx -> outputs [ 0 ]-> type : <nl> AVMEDIA_TYPE_VIDEO ; <nl>  <nl> avfilter_set_common_formats ( ctx , avfilter_all_formats ( type ));
int attribute_align_arg avcodec_open ( AVCodecContext * avctx , AVCodec * codec ) <nl> else if ( avctx -> width && avctx -> height ) <nl> avcodec_set_dimensions ( avctx , avctx -> width , avctx -> height ); <nl>  <nl> - if (( avctx -> coded_width || avctx -> coded_height ) && avcodec_check_dimensions ( avctx , avctx -> coded_width , avctx -> coded_height )){ <nl> +# define SANE_NB_CHANNELS 128U <nl> + if (( avctx -> coded_width || avctx -> coded_height ) && avcodec_check_dimensions ( avctx , avctx -> coded_width , avctx -> coded_height ) || <nl> + avctx -> channels > SANE_NB_CHANNELS ) { <nl> av_freep (& avctx -> priv_data ); <nl> ret = AVERROR ( EINVAL ); <nl> goto end ;
typedef struct { <nl> int requested_planes ; <nl> int map [ 4 ]; <nl> int linesize [ 4 ]; <nl> - int is_packed_rgb ; <nl> + int is_packed ; <nl> int depth ; <nl> int step ; <nl> } ExtractPlanesContext ; <nl> static int config_input ( AVFilterLink * inlink ) <nl>  <nl> s -> depth = ( desc -> comp [ 0 ]. depth_minus1 + 1 ) >> 3 ; <nl> s -> step = av_get_padded_bits_per_pixel ( desc ) >> 3 ; <nl> - s -> is_packed_rgb = !( desc -> flags & AV_PIX_FMT_FLAG_PLANAR ); <nl> + s -> is_packed = !( desc -> flags & AV_PIX_FMT_FLAG_PLANAR ); <nl> if ( desc -> flags & AV_PIX_FMT_FLAG_RGB ) { <nl> ff_fill_rgba_map ( rgba_map , inlink -> format ); <nl> for ( i = 0 ; i < 4 ; i ++) <nl> static int filter_frame ( AVFilterLink * inlink , AVFrame * frame ) <nl> } <nl> av_frame_copy_props ( out , frame ); <nl>  <nl> - if ( s -> is_packed_rgb ) { <nl> + if ( s -> is_packed ) { <nl> extract_from_packed ( out -> data [ 0 ], out -> linesize [ 0 ], <nl> frame -> data [ 0 ], frame -> linesize [ 0 ], <nl> outlink -> w , outlink -> h ,
static void ff_id3v2_parse ( AVFormatContext * s , int len , uint8_t version , uint8_t <nl> { <nl> int isv34 , tlen , unsync ; <nl> char tag [ 5 ]; <nl> - int64_t next ; <nl> + int64_t next , end = avio_tell ( s -> pb ) + len ; <nl> int taghdrlen ; <nl> const char * reason ; <nl> AVIOContext pb ; <nl> static void ff_id3v2_parse ( AVFormatContext * s , int len , uint8_t version , uint8_t <nl> avio_skip ( s -> pb , len ); <nl> } <nl> if ( version == 4 && flags & 0x10 ) /* Footer preset , always 10 bytes , skip over it */ <nl> - avio_skip ( s -> pb , 10 ); <nl> + end += 10 ; <nl>  <nl> + avio_seek ( s -> pb , end , SEEK_SET ); <nl> av_free ( buffer ); <nl> return ; <nl> 
void help ( void ) <nl> " 2 -> do 3 . test from mpeg4 std \ n " <nl> "- i test IDCT implementations \ n " <nl> "- 4 test IDCT248 implementations \ n "); <nl> - exit ( 1 ); <nl> } <nl>  <nl> int main ( int argc , char ** argv ) <nl> int main ( int argc , char ** argv ) <nl> default : <nl> case ' h ': <nl> help (); <nl> - break ; <nl> + return 0 ; <nl> } <nl> } <nl> 
static int decode_subframe ( TAKDecContext * s , int32_t * decoded , <nl> x = 1 << ( 32 - ( 15 - filter_quant )); <nl> y = 1 << (( 15 - filter_quant ) - 1 ); <nl> for ( i = 0 , j = filter_order - 1 ; i < filter_order / 2 ; i ++, j --) { <nl> - int tmp = y + tfilter [ j ]; <nl> s -> filter [ j ] = x - (( tfilter [ i ] + y ) >> ( 15 - filter_quant )); <nl> s -> filter [ i ] = x - (( tfilter [ j ] + y ) >> ( 15 - filter_quant )); <nl> }
static int decode_subframe ( WMAProDecodeCtx * s ) <nl> int num_bits = av_log2 (( s -> subframe_len + 3 )/ 4 ) + 1 ; <nl> for ( i = 0 ; i < s -> channels_for_cur_subframe ; i ++) { <nl> int c = s -> channel_indexes_for_cur_subframe [ i ]; <nl> - s -> channel [ c ]. num_vec_coeffs = get_bits (& s -> gb , num_bits ) << 2 ; <nl> + int num_vec_coeffs = get_bits (& s -> gb , num_bits ) << 2 ; <nl> + if ( num_vec_coeffs > WMAPRO_BLOCK_MAX_SIZE ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " num_vec_coeffs % d is too large \ n ", num_vec_coeffs ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + s -> channel [ c ]. num_vec_coeffs = num_vec_coeffs ; <nl> } <nl> } else { <nl> for ( i = 0 ; i < s -> channels_for_cur_subframe ; i ++) {
static int http_connect ( URLContext * h , const char * path , const char * hoststr , <nl> int post , err , ch ; <nl> char line [ 1024 ], * q ; <nl> char * auth_b64 ; <nl> - int auth_b64_len = strlen ( auth )* 4 / 3 + 12 ; <nl> + int auth_b64_len = ( strlen ( auth ) + 2 ) / 3 * 4 + 1 ; <nl> int64_t off = s -> off ; <nl>  <nl> 
static int roq_dpcm_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> context -> input_frames ++; <nl> return 0 ; <nl> } <nl> + } <nl> + if ( context -> input_frames < 8 ) { <nl> in = context -> frame_buffer ; <nl> } <nl> 
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl>  <nl> if ( h -> ref_count [ 0 ] > max_refs || h -> ref_count [ 1 ] > max_refs ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , " reference overflow \ n "); <nl> - h -> ref_count [ 0 ] = h -> ref_count [ 1 ] = 1 ; <nl> + h -> ref_count [ 0 ] = h -> ref_count [ 1 ] = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
void ff_parse_specific_params ( AVCodecContext * stream , int * au_rate , int * au_ssiz <nl> void ff_get_guid ( AVIOContext * s , ff_asf_guid * g ) <nl> { <nl> assert ( sizeof (* g ) == 16 ); <nl> - avio_read ( s , * g , sizeof (* g )); <nl> + if ( avio_read ( s , * g , sizeof (* g )) < ( int ) sizeof (* g )) <nl> + memset (* g , 0 , sizeof (* g )); <nl> } <nl>  <nl> enum CodecID ff_codec_guid_get_id ( const AVCodecGuid * guids , ff_asf_guid guid )
static av_cold int wavpack_encode_init ( AVCodecContext * avctx ) <nl> s -> avctx = avctx ; <nl>  <nl> if ( avctx -> channels > 255 ) { <nl> - av_log ( avctx , AV_LOG_ERROR , " Too many channels \ n ", avctx -> channels ); <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid channel count : % d \ n ", avctx -> channels ); <nl> return AVERROR ( EINVAL ); <nl> } <nl> 
static int rtsp_parse_request ( HTTPContext * c ) <nl> if (* p == '\ n ') <nl> p ++; <nl> while (* p != '\ 0 ') { <nl> - p1 = strchr ( p , '\ n '); <nl> + p1 = memchr ( p , '\ n ', ( char *) c -> buffer_ptr - p ); <nl> if (! p1 ) <nl> break ; <nl> p2 = p1 ;
int attribute_align_arg avcodec_encode_audio2 ( AVCodecContext * avctx , <nl> implement encode2 () */ <nl> buf_size = 2 * avctx -> frame_size * avctx -> channels * <nl> av_get_bytes_per_sample ( avctx -> sample_fmt ); <nl> - buf_size += FF_MIN_BUFFER_SIZE ; <nl> + buf_size += 2 * FF_MIN_BUFFER_SIZE ; <nl> } <nl> } <nl> if (( ret = ff_alloc_packet ( avpkt , buf_size )))
fail : <nl> if ( pkt -> stream_index == seg -> reference_stream_index ) <nl> seg -> frame_count ++; <nl>  <nl> - if ( ret < 0 ) { <nl> - if ( seg -> list ) <nl> - avio_close ( seg -> list_pb ); <nl> - avformat_free_context ( oc ); <nl> - } <nl> - <nl> return ret ; <nl> } <nl> 
static int cinepak_decode_vectors ( CinepakContext * s , cvid_strip * strip , <nl> const uint8_t * eod = ( data + size ); <nl> uint32_t flag , mask ; <nl> uint8_t * cb0 , * cb1 , * cb2 , * cb3 ; <nl> - unsigned int x , y ; <nl> + int x , y ; <nl> char * ip0 , * ip1 , * ip2 , * ip3 ; <nl>  <nl> flag = 0 ;
static int decode_subframe ( WMAProDecodeCtx * s ) <nl> int num_bits = av_log2 (( s -> subframe_len + 3 )/ 4 ) + 1 ; <nl> for ( i = 0 ; i < s -> channels_for_cur_subframe ; i ++) { <nl> int c = s -> channel_indexes_for_cur_subframe [ i ]; <nl> - s -> channel [ c ]. num_vec_coeffs = get_bits (& s -> gb , num_bits ) << 2 ; <nl> + int num_vec_coeffs = get_bits (& s -> gb , num_bits ) << 2 ; <nl> + if ( num_vec_coeffs > WMAPRO_BLOCK_MAX_SIZE ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " num_vec_coeffs % d is too large \ n ", num_vec_coeffs ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + s -> channel [ c ]. num_vec_coeffs = num_vec_coeffs ; <nl> } <nl> } else { <nl> for ( i = 0 ; i < s -> channels_for_cur_subframe ; i ++) {
static int msrle_decode_8_16_24_32 ( AVCodecContext * avctx , AVPicture * pic , int de <nl> uint8_t * output , * output_end ; <nl> const uint8_t * src = data ; <nl> int p1 , p2 , line = avctx -> height , pos = 0 , i ; <nl> - uint16_t pix16 ; <nl> - uint32_t pix32 ; <nl> + uint16_t av_uninit ( pix16 ); <nl> + uint32_t av_uninit ( pix32 ); <nl>  <nl> output = pic -> data [ 0 ] + ( avctx -> height - 1 ) * pic -> linesize [ 0 ]; <nl> output_end = pic -> data [ 0 ] + ( avctx -> height ) * pic -> linesize [ 0 ];
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl> } <nl> band -> quant_mat = quant_mat ; <nl> } <nl> - <nl> + if ( quant_index_to_tab [ band -> quant_mat ] > 4 && band -> blk_size == 4 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid quant matrix for 4x4 block encountered !\ n "); <nl> + band -> quant_mat = 0 ; <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> /* decode block huffman codebook */ <nl> if ( ff_ivi_dec_huff_desc (& ctx -> gb , get_bits1 (& ctx -> gb ), IVI_BLK_HUFF , <nl> & band -> blk_vlc , avctx ))
 <nl> typedef struct Hnm4VideoContext { <nl> uint8_t version ; <nl> - uint16_t width ; <nl> - uint16_t height ; <nl> + int width ; <nl> + int height ; <nl> uint8_t * current ; <nl> uint8_t * previous ; <nl> uint8_t * buffer1 ;
typedef struct AudioBitScopeContext { <nl> # define FLAGS AV_OPT_FLAG_FILTERING_PARAM | AV_OPT_FLAG_VIDEO_PARAM <nl>  <nl> static const AVOption abitscope_options [] = { <nl> - { " rate ", " set video rate ", OFFSET ( frame_rate ), AV_OPT_TYPE_VIDEO_RATE , {. str =" 25 "}, 0 , 0 , FLAGS }, <nl> - { " r ", " set video rate ", OFFSET ( frame_rate ), AV_OPT_TYPE_VIDEO_RATE , {. str =" 25 "}, 0 , 0 , FLAGS }, <nl> + { " rate ", " set video rate ", OFFSET ( frame_rate ), AV_OPT_TYPE_VIDEO_RATE , {. str =" 25 "}, 0 , INT_MAX , FLAGS }, <nl> + { " r ", " set video rate ", OFFSET ( frame_rate ), AV_OPT_TYPE_VIDEO_RATE , {. str =" 25 "}, 0 , INT_MAX , FLAGS }, <nl> { " size ", " set video size ", OFFSET ( w ), AV_OPT_TYPE_IMAGE_SIZE , {. str =" 1024x256 "}, 0 , 0 , FLAGS }, <nl> { " s ", " set video size ", OFFSET ( w ), AV_OPT_TYPE_IMAGE_SIZE , {. str =" 1024x256 "}, 0 , 0 , FLAGS }, <nl> { " colors ", " set channels colors ", OFFSET ( colors ), AV_OPT_TYPE_STRING , {. str = " red | green | blue | yellow | orange | lime | pink | magenta | brown " }, 0 , 0 , FLAGS },
void ff_vc1_mc_4mv_luma ( VC1Context * v , int n , int dir , int avg ) <nl> if ( s -> pict_type == AV_PICTURE_TYPE_P && n == 3 && v -> field_mode ) { <nl> int same_count = 0 , opp_count = 0 , k ; <nl> int chosen_mv [ 2 ][ 4 ][ 2 ], f ; <nl> - int tx , ty ; <nl> + int tx = 0 , ty = 0 ; <nl> for ( k = 0 ; k < 4 ; k ++) { <nl> f = v -> mv_f [ 0 ][ s -> block_index [ k ] + v -> blocks_off ]; <nl> chosen_mv [ f ][ f ? opp_count : same_count ][ 0 ] = s -> mv [ 0 ][ k ][ 0 ];
static int ff_filter_frame_framed ( AVFilterLink * link , AVFilterBufferRef * frame ) <nl> } else <nl> out = frame ; <nl>  <nl> - while ( cmd && cmd -> time <= frame -> pts * av_q2d ( link -> time_base )){ <nl> + while ( cmd && cmd -> time <= out -> pts * av_q2d ( link -> time_base )){ <nl> av_log ( link -> dst , AV_LOG_DEBUG , <nl> " Processing command time :% f command :% s arg :% s \ n ", <nl> cmd -> time , cmd -> command , cmd -> arg );
static int compare_ocl_device_desc ( const void * a , const void * b ) <nl>  <nl> int opt_opencl_bench ( void * optctx , const char * opt , const char * arg ) <nl> { <nl> - int i , j , nb_devices = 0 , count = 0 ; <nl> + int i , j , nb_devices = 0 , count = 0 , ret = 0 ; <nl> int64_t score = 0 ; <nl> AVOpenCLDeviceList * device_list ; <nl> AVOpenCLDeviceNode * device_node = NULL ; <nl> OpenCLDeviceBenchmark * devices = NULL ; <nl> cl_platform_id platform ; <nl>  <nl> - av_opencl_get_device_list (& device_list ); <nl> + ret = av_opencl_get_device_list (& device_list ); <nl> + if ( ret < 0 ) { <nl> + return ret ; <nl> + } <nl> for ( i = 0 ; i < device_list -> platform_num ; i ++) <nl> nb_devices += device_list -> platform_node [ i ]-> device_num ; <nl> if (! nb_devices ) { <nl> av_log ( NULL , AV_LOG_ERROR , " No OpenCL device detected !\ n "); <nl> + av_opencl_free_device_list (& device_list ); <nl> return AVERROR ( EINVAL ); <nl> } <nl> if (!( devices = av_malloc_array ( nb_devices , sizeof ( OpenCLDeviceBenchmark )))) { <nl> av_log ( NULL , AV_LOG_ERROR , " Could not allocate buffer \ n "); <nl> + av_opencl_free_device_list (& device_list ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> 
static int rm_assemble_video_frame ( AVFormatContext * s , AVIOContext * pb , <nl> int hdr ; <nl> int seq = 0 , pic_num = 0 , len2 = 0 , pos = 0 ; // init to silcense compiler warning <nl> int type ; <nl> + int ret ; <nl>  <nl> hdr = avio_r8 ( pb ); len --; <nl> type = hdr >> 6 ; <nl> static int rm_assemble_video_frame ( AVFormatContext * s , AVIOContext * pb , <nl> pkt -> data [ 0 ] = 0 ; <nl> AV_WL32 ( pkt -> data + 1 , 1 ); <nl> AV_WL32 ( pkt -> data + 5 , 0 ); <nl> - avio_read ( pb , pkt -> data + 9 , len ); <nl> + if (( ret = avio_read ( pb , pkt -> data + 9 , len )) != len ) { <nl> + av_free_packet ( pkt ); <nl> + return ret < 0 ? ret : AVERROR ( EIO ); <nl> + } <nl> return 0 ; <nl> } <nl> // now we have to deal with single slice <nl> static int rm_assemble_video_frame ( AVFormatContext * s , AVIOContext * pb , <nl> av_free_packet (& vst -> pkt ); // FIXME this should be output . <nl> if ( av_new_packet (& vst -> pkt , vst -> videobufsize ) < 0 ) <nl> return AVERROR ( ENOMEM ); <nl> + memset ( vst -> pkt . data , 0 , vst -> pkt . size ); <nl> vst -> videobufpos = 8 * vst -> slices + 1 ; <nl> vst -> cur_slice = 0 ; <nl> vst -> curpic_num = pic_num ;
static int allocate_buffers ( ShortenContext * s ) <nl>  <nl> static inline unsigned int get_uint ( ShortenContext * s , int k ) <nl> { <nl> - if ( s -> version != 0 ) <nl> + if ( s -> version != 0 ) { <nl> k = get_ur_golomb_shorten (& s -> gb , ULONGSIZE ); <nl> + if ( k > 31U ) <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> return get_ur_golomb_shorten (& s -> gb , k ); <nl> } <nl> 
static inline int get_ue_golomb ( GetBitContext * gb ) <nl> int log = 2 * av_log2 ( buf ) - 31 ; <nl> LAST_SKIP_BITS ( re , gb , 32 - log ); <nl> CLOSE_READER ( re , gb ); <nl> - if ( CONFIG_FTRAPV && log < 0 ) { <nl> + if ( log < 7 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Invalid UE golomb code \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int idcin_read_seek ( AVFormatContext * s , int stream_index , <nl> IdcinDemuxContext * idcin = s -> priv_data ; <nl>  <nl> if ( idcin -> first_pkt_pos > 0 ) { <nl> - int ret = avio_seek ( s -> pb , idcin -> first_pkt_pos , SEEK_SET ); <nl> + int64_t ret = avio_seek ( s -> pb , idcin -> first_pkt_pos , SEEK_SET ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> ff_update_cur_dts ( s , s -> streams [ idcin -> video_stream_index ], 0 );
static int dxva2_device_create9ex ( AVHWDeviceContext * ctx , UINT adapter ) <nl> if ( FAILED ( hr )) <nl> return AVERROR_UNKNOWN ; <nl>  <nl> + modeex . Size = sizeof ( D3DDISPLAYMODEEX ); <nl> hr = IDirect3D9Ex_GetAdapterDisplayModeEx ( d3d9ex , adapter , & modeex , NULL ); <nl> if ( FAILED ( hr )) { <nl> IDirect3D9Ex_Release ( d3d9ex );
int ff_hevc_decode_nal_sps ( HEVCContext * s ) <nl> if ( s -> sps_list [ sps_id ] && s -> sps == ( HEVCSPS *) s -> sps_list [ sps_id ]-> data ) { <nl> av_buffer_unref (& s -> current_sps ); <nl> s -> current_sps = av_buffer_ref ( s -> sps_list [ sps_id ]); <nl> + if (! s -> current_sps ) <nl> + s -> sps = NULL ; <nl> } <nl> av_buffer_unref (& s -> sps_list [ sps_id ]); <nl> s -> sps_list [ sps_id ] = sps_buf ;
static int nuv_packet ( AVFormatContext * s , AVPacket * pkt ) { <nl> pkt -> stream_index = ctx -> v_id ; <nl> memcpy ( pkt -> data , hdr , copyhdrsize ); <nl> ret = get_buffer ( pb , pkt -> data + copyhdrsize , size ); <nl> - if ( ret < 0 ) return ret ; <nl> + if ( ret < 0 ) { <nl> + av_free_packet ( pkt ); <nl> + return ret ; <nl> + } <nl> if ( ret < size ) <nl> av_shrink_packet ( pkt , copyhdrsize + ret ); <nl> return 0 ;
static const int shift1 [ 6 ] = { 0 , 8 , 8 , 8 , 4 , 4 }; <nl> static const int shift2 [ 6 ] = { 0 , 0 , 8 , 4 , 0 , 4 }; <nl>  <nl> static int decode_13 ( AVCodecContext * avctx , DxaDecContext * c , uint8_t * dst , <nl> - int stride , uint8_t * src , uint8_t * ref ) <nl> + int stride , uint8_t * src , int srcsize , uint8_t * ref ) <nl> { <nl> uint8_t * code , * data , * mv , * msk , * tmp , * tmp2 ; <nl> + uint8_t * src_end = src + srcsize ; <nl> int i , j , k ; <nl> int type , x , y , d , d2 ; <nl> uint32_t mask ; <nl>  <nl> + if ( 12ULL + (( avctx -> width * avctx -> height ) >> 4 ) + AV_RB32 ( src + 0 ) + AV_RB32 ( src + 4 ) > srcsize ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> code = src + 12 ; <nl> data = code + (( avctx -> width * avctx -> height ) >> 4 ); <nl> mv = data + AV_RB32 ( src + 0 ); <nl> static int decode_13 ( AVCodecContext * avctx , DxaDecContext * c , uint8_t * dst , <nl>  <nl> for ( j = 0 ; j < avctx -> height ; j += 4 ){ <nl> for ( i = 0 ; i < avctx -> width ; i += 4 ){ <nl> + if ( data > src_end || mv > src_end || msk > src_end ) <nl> + return AVERROR_INVALIDDATA ; <nl> tmp = dst + i ; <nl> tmp2 = ref + i ; <nl> type = * code ++; <nl> static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , AVPac <nl> av_log ( avctx , AV_LOG_ERROR , " Missing reference frame \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> - decode_13 ( avctx , c , frame -> data [ 0 ], frame -> linesize [ 0 ], srcptr , c -> prev -> data [ 0 ]); <nl> + decode_13 ( avctx , c , frame -> data [ 0 ], frame -> linesize [ 0 ], srcptr , dsize , c -> prev -> data [ 0 ]); <nl> break ; <nl> default : <nl> av_log ( avctx , AV_LOG_ERROR , " Unknown / unsupported compression type % d \ n ", compr );
static int decode_slice_header ( H264Context * h , H264Context * h0 ){ <nl> if ( h != h0 ) <nl> return - 1 ; // width / height changed during parallelized decoding <nl> free_tables ( h ); <nl> + flush_dpb ( s -> avctx ); <nl> MPV_common_end ( s ); <nl> } <nl> if (! s -> context_initialized ) {
static int encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> memcpy ( s -> buf + NELLY_BUF_LEN , frame -> data [ 0 ], <nl> frame -> nb_samples * sizeof (* s -> buf )); <nl> if ( frame -> nb_samples < NELLY_SAMPLES ) { <nl> - memset ( s -> buf + NELLY_BUF_LEN + avctx -> frame_size , 0 , <nl> + memset ( s -> buf + NELLY_BUF_LEN + frame -> nb_samples , 0 , <nl> ( NELLY_SAMPLES - frame -> nb_samples ) * sizeof (* s -> buf )); <nl> if ( frame -> nb_samples >= NELLY_BUF_LEN ) <nl> s -> last_frame = 1 ;
yuv2rgb_1_c_template ( SwsContext * c , const int16_t * buf0 , <nl> * b = c -> table_bU [ U + YUVRGB_TABLE_HEADROOM ]; <nl>  <nl> if ( hasAlpha ) { <nl> - A1 = ( abuf0 [ i * 2 ] + 64 ) >> 7 ; <nl> - A2 = ( abuf0 [ i * 2 + 1 ] + 64 ) >> 7 ; <nl> + A1 = abuf0 [ i * 2 ] * 255 + 16384 >> 15 ; <nl> + A2 = abuf0 [ i * 2 + 1 ] * 255 + 16384 >> 15 ; <nl> } <nl>  <nl> yuv2rgb_write ( dest , i , Y1 , Y2 , hasAlpha ? A1 : 0 , hasAlpha ? A2 : 0 ,
static int update_dimensions ( VP8Context * s , int width , int height ) <nl> s -> intra4x4_pred_mode_base = av_mallocz ( s -> b4_stride *( 4 * s -> mb_height + 1 )); <nl> s -> top_nnz = av_mallocz ( s -> mb_width * sizeof (* s -> top_nnz )); <nl>  <nl> + if (! s -> macroblocks_base || ! s -> intra4x4_pred_mode_base || ! s -> top_nnz ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> s -> macroblocks = s -> macroblocks_base + 1 + s -> mb_stride ; <nl> s -> intra4x4_pred_mode = s -> intra4x4_pred_mode_base + 4 + s -> b4_stride ; <nl> 
av_cold int ff_vp8_decode_free ( AVCodecContext * avctx ) <nl> VP8Context * s = avctx -> priv_data ; <nl> int i ; <nl>  <nl> + if (! s ) <nl> + return 0 ; <nl> + <nl> vp8_decode_flush_impl ( avctx , 1 ); <nl> for ( i = 0 ; i < FF_ARRAY_ELEMS ( s -> frames ); i ++) <nl> av_frame_free (& s -> frames [ i ]. tf . f );
static int fraps2_decode_plane ( FrapsContext * s , uint8_t * dst , int stride , int w , <nl> */ <nl> if ( j ) dst [ i ] += dst [ i - stride ]; <nl> else if ( Uoff ) dst [ i ] += 0x80 ; <nl> + if ( get_bits_left (& gb ) < 0 ){ <nl> + free_vlc (& vlc ); <nl> + return - 1 ; <nl> + } <nl> } <nl> dst += stride ; <nl> - if ( get_bits_left (& gb ) < 0 ){ <nl> - free_vlc (& vlc ); <nl> - return - 1 ; <nl> - } <nl> } <nl> free_vlc (& vlc ); <nl> return 0 ;
static void dxva2_frames_uninit ( AVHWFramesContext * ctx ) <nl> } <nl> } <nl>  <nl> + static void dxva2_pool_release_dummy ( void * opaque , uint8_t * data ) <nl> +{ <nl> + // important not to free anything here -- data is a surface object <nl> + // associated with the call to CreateSurface (), and these surfaces are <nl> + // released in dxva2_frames_uninit () <nl> +} <nl> + <nl> static AVBufferRef * dxva2_pool_alloc ( void * opaque , int size ) <nl> { <nl> AVHWFramesContext * ctx = ( AVHWFramesContext *) opaque ; <nl> static AVBufferRef * dxva2_pool_alloc ( void * opaque , int size ) <nl> if ( s -> nb_surfaces_used < hwctx -> nb_surfaces ) { <nl> s -> nb_surfaces_used ++; <nl> return av_buffer_create (( uint8_t *) s -> surfaces_internal [ s -> nb_surfaces_used - 1 ], <nl> - sizeof (* hwctx -> surfaces ), NULL , 0 , 0 ); <nl> + sizeof (* hwctx -> surfaces ), dxva2_pool_release_dummy , 0 , 0 ); <nl> } <nl>  <nl> return NULL ;
static int encode_superframe ( AVCodecContext * avctx , <nl> } <nl> # endif <nl>  <nl> - if (( i = encode_frame ( s , s -> coefs , buf , buf_size , total_gain )) >= 0 ) { <nl> - av_log ( avctx , AV_LOG_ERROR , " required frame size too large . please " <nl> - " use a higher bit rate .\ n "); <nl> - return AVERROR ( EINVAL ); <nl> - } <nl> + encode_frame ( s , s -> coefs , buf , buf_size , total_gain ); <nl> assert (( put_bits_count (& s -> pb ) & 7 ) == 0 ); <nl> - while ( i ++) <nl> + i = s -> block_align - ( put_bits_count (& s -> pb )+ 7 )/ 8 ; <nl> + assert ( i >= 0 ); <nl> + while ( i --) <nl> put_bits (& s -> pb , 8 , ' N '); <nl>  <nl> flush_put_bits (& s -> pb );
static int parse_header ( OutputStream * os , const uint8_t * buf , int buf_size ) <nl> if ( size > buf_size ) <nl> return AVERROR_INVALIDDATA ; <nl> if ( type == 8 || type == 9 ) { <nl> - if ( os -> nb_extra_packets > FF_ARRAY_ELEMS ( os -> extra_packets )) <nl> + if ( os -> nb_extra_packets >= FF_ARRAY_ELEMS ( os -> extra_packets )) <nl> return AVERROR_INVALIDDATA ; <nl> os -> extra_packet_sizes [ os -> nb_extra_packets ] = size ; <nl> os -> extra_packets [ os -> nb_extra_packets ] = av_malloc ( size );
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * insamples ) <nl> } <nl> if ( showwaves -> buf_idx == showwaves -> w ) <nl> push_frame ( outlink ); <nl> + outpicref = showwaves -> outpicref ; <nl> } <nl>  <nl> avfilter_unref_buffer ( insamples );
int64_t ff_gen_search ( AVFormatContext * s , int stream_index , int64_t target_ts , <nl> } <nl>  <nl> if ( ts_max == AV_NOPTS_VALUE ){ <nl> - int step = 1024 ; <nl> + int64_t step = 1024 ; <nl> filesize = avio_size ( s -> pb ); <nl> pos_max = filesize - 1 ; <nl> do {
static void do_subtitle_out ( AVFormatContext * s , <nl>  <nl> if (! subtitle_out ) { <nl> subtitle_out = av_malloc ( subtitle_out_max_size ); <nl> + if (! subtitle_out ) { <nl> + av_log ( NULL , AV_LOG_FATAL , " Failed to allocate subtitle_out \ n "); <nl> + exit_program ( 1 ); <nl> + } <nl> } <nl>  <nl> /* Note : DVB subtitle need one packet to draw them and one other
static int decode_frame ( WmallDecodeCtx * s ) <nl> /* decode tile information */ <nl> if (( ret = decode_tilehdr ( s ))) { <nl> s -> packet_loss = 1 ; <nl> + av_frame_unref ( s -> frame ); <nl> return ret ; <nl> } <nl> 
typedef struct GifState { <nl>  <nl> AVCodecContext * avctx ; <nl> int keyframe ; <nl> + int keyframe_ok ; <nl> int trans_color ; /**< color value that is used instead of transparent color */ <nl> } GifState ; <nl>  <nl> static int gif_decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , A <nl> } <nl>  <nl> if ( s -> keyframe ) { <nl> + s -> keyframe_ok = 0 ; <nl> if (( ret = gif_read_header1 ( s )) < 0 ) <nl> return ret ; <nl>  <nl> static int gif_decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , A <nl>  <nl> s -> picture . pict_type = AV_PICTURE_TYPE_I ; <nl> s -> picture . key_frame = 1 ; <nl> + s -> keyframe_ok = 1 ; <nl> } else { <nl> + if (! s -> keyframe_ok ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " cannot decode frame without keyframe \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (( ret = avctx -> reget_buffer ( avctx , & s -> picture )) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " reget_buffer () failed \ n "); <nl> return ret ;
* <nl> */ <nl>  <nl> +# include " libavutil / avassert . h " <nl> # include " libavutil / avstring . h " <nl> # include " libavutil / base64 . h " <nl>  <nl> static av_cold int hevc_sdp_parse_fmtp_config ( AVFormatContext * s , <nl> } else if (! strcmp ( attr , " sprop - sei ")) { <nl> data_ptr = & hevc_data -> sei ; <nl> size_ptr = & hevc_data -> sei_size ; <nl> - } <nl> + } else <nl> + av_assert0 ( 0 ); <nl>  <nl> while (* value ) { <nl> char base64packet [ 1024 ];
static int decode_frame ( AVCodecContext * avctx , <nl>  <nl> if ( avctx -> pix_fmt == AV_PIX_FMT_RGB555 ) <nl> ret = decode_rle16 ( avctx , p , & gbc ); <nl> - else if ( bpp = 2 ) <nl> + else if ( bpp == 2 ) <nl> ret = decode_rle_bpp2 ( avctx , p , & gbc ); <nl> - else if ( bpp = 4 ) <nl> + else if ( bpp == 4 ) <nl> ret = decode_rle_bpp4 ( avctx , p , & gbc ); <nl> else <nl> ret = decode_rle ( avctx , p , & gbc , bppcnt );
const uint8_t * avpriv_mpv_find_start_code ( const uint8_t * restrict p , <nl> av_cold int ff_dct_common_init ( MpegEncContext * s ) <nl> { <nl> ff_dsputil_init (& s -> dsp , s -> avctx ); <nl> - ff_videodsp_init (& s -> vdsp , 8 ); <nl> + ff_videodsp_init (& s -> vdsp , s -> avctx -> bits_per_raw_sample ); <nl>  <nl> s -> dct_unquantize_h263_intra = dct_unquantize_h263_intra_c ; <nl> s -> dct_unquantize_h263_inter = dct_unquantize_h263_inter_c ;
struct ff_timecode { <nl> char * str ; ///< string following the hh : mm : ss [:;.] ff format <nl> int start ; ///< timecode frame start <nl> int drop ; ///< drop flag ( 1 if drop , else 0 ) <nl> - AVRational rate ; ///< Frame rate in rationnal form <nl> + AVRational rate ; ///< Frame rate in rational form <nl> }; <nl>  <nl> /**
static av_cold int init ( AVFilterContext * ctx , const char * args ) <nl>  <nl> aresample -> next_pts = AV_NOPTS_VALUE ; <nl> aresample -> swr = swr_alloc (); <nl> - if (! aresample -> swr ) <nl> - return AVERROR ( ENOMEM ); <nl> + if (! aresample -> swr ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl>  <nl> if ( args ) { <nl> char * ptr = argd , * token ;
static int FUNC ( extension_data )( CodedBitstreamContext * ctx , RWContext * rw , <nl> BitstreamContext start ; <nl> uint8_t bit ; <nl> start = * rw ; <nl> - for ( k = 0 ; cbs_h2645_read_more_rbsp_data ( rw ); k ++); <nl> + for ( k = 0 ; cbs_h2645_read_more_rbsp_data ( rw ); k ++) <nl> + bitstream_skip ( rw , 1 ); <nl> current -> bit_length = k ; <nl> if ( k > 0 ) { <nl> * rw = start ;
static int config_input ( AVFilterLink * inlink ) <nl> StereoWidenContext * s = ctx -> priv ; <nl>  <nl> s -> length = 2 * s -> delay * inlink -> sample_rate / 1000 ; <nl> - s -> buffer = av_calloc ( s -> length , sizeof ( s -> buffer )); <nl> + s -> buffer = av_calloc ( s -> length , sizeof (* s -> buffer )); <nl> if (! s -> buffer ) <nl> return AVERROR ( ENOMEM ); <nl> s -> write = s -> buffer ;
static int decode_ihdr_chunk ( AVCodecContext * avctx , PNGDecContext * s , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> s -> bit_depth = bytestream2_get_byte (& s -> gb ); <nl> + if ( s -> bit_depth != 1 && s -> bit_depth != 2 && s -> bit_depth != 4 && <nl> + s -> bit_depth != 8 && s -> bit_depth != 16 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid bit depth \ n "); <nl> + goto error ; <nl> + } <nl> s -> color_type = bytestream2_get_byte (& s -> gb ); <nl> s -> compression_type = bytestream2_get_byte (& s -> gb ); <nl> s -> filter_type = bytestream2_get_byte (& s -> gb ); <nl> static int decode_ihdr_chunk ( AVCodecContext * avctx , PNGDecContext * s , <nl> s -> compression_type , s -> filter_type , s -> interlace_type ); <nl>  <nl> return 0 ; <nl> + error : <nl> + s -> cur_w = s -> cur_h = s -> width = s -> height = 0 ; <nl> + s -> bit_depth = 8 ; <nl> + return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> static int decode_phys_chunk ( AVCodecContext * avctx , PNGDecContext * s )
av_cold void ff_msmpeg4_encode_init ( MpegEncContext * s ) <nl>  <nl> for ( i = 0 ; i < NB_RL_TABLES ; i ++){ <nl> int level ; <nl> - for ( level = 0 ; level <= MAX_LEVEL ; level ++){ <nl> + for ( level = 1 ; level <= MAX_LEVEL ; level ++) { <nl> int run ; <nl> for ( run = 0 ; run <= MAX_RUN ; run ++){ <nl> int last ;
static int Stagefright_decode_frame ( AVCodecContext * avctx , void * data , <nl> AVFrame * ret_frame ; <nl>  <nl> if (! s -> thread_started ) { <nl> - pthread_create (& s -> decode_thread_id , NULL , & decode_thread , avctx ); <nl> + if ( pthread_create (& s -> decode_thread_id , NULL , & decode_thread , avctx )) <nl> + return AVERROR ( ENOMEM ); <nl> s -> thread_started = true ; <nl> } <nl> 
static av_cold int vc1_decode_init ( AVCodecContext * avctx ) <nl> avctx -> idct_algo = FF_IDCT_WMV2 ; <nl> } <nl>  <nl> - if ( ff_h263_decode_init ( avctx ) < 0 ) <nl> + if ( ff_msmpeg4_decode_init ( avctx ) < 0 ) <nl> return - 1 ; <nl> if ( vc1_init_common ( v ) < 0 ) return - 1 ; <nl> - // only for ff_msmp4_mb_i_table <nl> - if ( ff_msmpeg4_decode_init ( avctx ) < 0 ) return - 1 ; <nl>  <nl> avctx -> coded_width = avctx -> width ; <nl> avctx -> coded_height = avctx -> height ;
static int sdp_parse_fmtp_config_h264 ( AVStream * stream , <nl> } <nl> } else if (! strcmp ( attr , " sprop - parameter - sets ")) { <nl> codec -> extradata_size = 0 ; <nl> - codec -> extradata = NULL ; <nl> + av_freep (& codec -> extradata ); <nl>  <nl> while (* value ) { <nl> char base64packet [ 1024 ];
int attribute_align_arg avcodec_encode_audio ( AVCodecContext * avctx , <nl> const short * samples ) <nl> { <nl> AVPacket pkt ; <nl> - AVFrame frame0 = { 0 }; <nl> + AVFrame frame0 = {{ 0 }}; <nl> AVFrame * frame ; <nl> int ret , samples_size , got_packet ; <nl>  <nl> int attribute_align_arg avcodec_decode_audio3 ( AVCodecContext * avctx , int16_t * sa <nl> int * frame_size_ptr , <nl> AVPacket * avpkt ) <nl> { <nl> - AVFrame frame = { 0 }; <nl> + AVFrame frame = {{ 0 }}; <nl> int ret , got_frame = 0 ; <nl>  <nl> if ( avctx -> get_buffer != avcodec_default_get_buffer ) {
static int poll_filters ( void ) <nl> } <nl> break ; <nl> } <nl> + frame_pts = AV_NOPTS_VALUE ; <nl> if ( ost -> enc -> type == AVMEDIA_TYPE_VIDEO ) <nl> filtered_frame -> pts = frame_pts = av_rescale_q ( picref -> pts , ist_pts_tb , AV_TIME_BASE_Q ); <nl> else if ( picref -> pts != AV_NOPTS_VALUE )
int av_write_trailer ( AVFormatContext * s ) <nl> fail : <nl> if ( ret == 0 ) <nl> ret = url_ferror ( s -> pb ); <nl> - for ( i = 0 ; i < s -> nb_streams ; i ++) <nl> + for ( i = 0 ; i < s -> nb_streams ; i ++) { <nl> av_freep (& s -> streams [ i ]-> priv_data ); <nl> + av_freep (& s -> streams [ i ]-> index_entries ); <nl> + } <nl> av_freep (& s -> priv_data ); <nl> return ret ; <nl> }
int32_t ff_mlp_pack_output ( int32_t lossless_check_data , <nl> ( 1U << output_shift [ mat_ch ]); <nl> lossless_check_data ^= ( sample & 0xffffff ) << mat_ch ; <nl> if ( is32 ) <nl> - * data_32 ++ = sample * 256 ; <nl> + * data_32 ++ = sample * 256U ; <nl> else <nl> * data_16 ++ = sample >> 8 ; <nl> }
void ff_msmpeg4_encode_mb ( MpegEncContext * s , <nl> static void msmpeg4_encode_dc ( MpegEncContext * s , int level , int n , int * dir_ptr ) <nl> { <nl> int sign , code ; <nl> - int pred , extquant ; <nl> + int pred , av_uninit ( extquant ); <nl> int extrabits = 0 ; <nl>  <nl> int16_t * dc_val ;
static int mkv_write_trailer ( AVFormatContext * s ) <nl> MatroskaMuxContext * mkv = s -> priv_data ; <nl> ByteIOContext * pb = & s -> pb ; <nl> offset_t currentpos , second_seekhead , cuespos ; <nl> + int ret ; <nl>  <nl> end_ebml_master ( pb , mkv -> cluster ); <nl>  <nl> cuespos = mkv_write_cues ( pb , mkv -> cues , s -> nb_streams ); <nl> second_seekhead = mkv_write_seekhead ( pb , mkv -> cluster_seekhead ); <nl>  <nl> - mkv_add_seekhead_entry ( mkv -> main_seekhead , MATROSKA_ID_CUES , cuespos ); <nl> - mkv_add_seekhead_entry ( mkv -> main_seekhead , MATROSKA_ID_SEEKHEAD , second_seekhead ); <nl> + ret = mkv_add_seekhead_entry ( mkv -> main_seekhead , MATROSKA_ID_CUES , cuespos ); <nl> + if ( ret < 0 ) return ret ; <nl> + ret = mkv_add_seekhead_entry ( mkv -> main_seekhead , MATROSKA_ID_SEEKHEAD , second_seekhead ); <nl> + if ( ret < 0 ) return ret ; <nl> mkv_write_seekhead ( pb , mkv -> main_seekhead ); <nl>  <nl> // update the duration
static int dc1394_read_header ( AVFormatContext * c ) <nl>  <nl> /* FIXME : To select a specific camera I need to search in list its guid */ <nl> dc1394 -> camera = dc1394_camera_new ( dc1394 -> d , list -> ids [ 0 ]. guid ); <nl> + <nl> + if (! dc1394 -> camera ) { <nl> + av_log ( c , AV_LOG_ERROR , " Unable to open camera with guid 0x %" PRIx64 "\ n ", <nl> + list -> ids [ 0 ]. guid ); <nl> + dc1394_camera_free_list ( list ); <nl> + goto out ; <nl> + } <nl> + <nl> if ( list -> num > 1 ) { <nl> av_log ( c , AV_LOG_INFO , " Working with the first camera found \ n "); <nl> }
int av_frame_copy_props ( AVFrame * dst , const AVFrame * src ) <nl> dst -> pict_type = src -> pict_type ; <nl> dst -> sample_aspect_ratio = src -> sample_aspect_ratio ; <nl> dst -> pts = src -> pts ; <nl> + dst -> repeat_pict = src -> repeat_pict ; <nl> dst -> interlaced_frame = src -> interlaced_frame ; <nl> dst -> top_field_first = src -> top_field_first ; <nl> + dst -> palette_has_changed = src -> palette_has_changed ; <nl> dst -> sample_rate = src -> sample_rate ; <nl> dst -> opaque = src -> opaque ; <nl> dst -> pkt_pts = src -> pkt_pts ; <nl> int av_frame_copy_props ( AVFrame * dst , const AVFrame * src ) <nl> dst -> coded_picture_number = src -> coded_picture_number ; <nl> dst -> display_picture_number = src -> display_picture_number ; <nl>  <nl> + memcpy ( dst -> error , src -> error , sizeof ( dst -> error )); <nl> + <nl> for ( i = 0 ; i < src -> nb_side_data ; i ++) { <nl> const AVFrameSideData * sd_src = src -> side_data [ i ]; <nl> AVFrameSideData * sd_dst = av_frame_new_side_data ( dst , sd_src -> type ,
int ff_hevc_decode_nal_pps ( HEVCContext * s ) <nl> int pps_range_extensions_flag = get_bits1 ( gb ); <nl> /* int pps_extension_7bits = */ get_bits ( gb , 7 ); <nl> if ( sps -> ptl . general_ptl . profile_idc == FF_PROFILE_HEVC_REXT && pps_range_extensions_flag ) { <nl> - pps_range_extensions ( s , pps , sps ); <nl> + if (( ret = pps_range_extensions ( s , pps , sps )) < 0 ) <nl> + goto err ; <nl> } <nl> } <nl> 
static int swf_probe ( AVProbeData * p ) <nl> && AV_RB24 ( p -> buf ) != AV_RB24 (" FWS ")) <nl> return 0 ; <nl>  <nl> + if ( AV_RB24 ( p -> buf ) == AV_RB24 (" CWS ") <nl> + && p -> buf [ 3 ] <= 20 ) <nl> + return AVPROBE_SCORE_MAX / 4 + 1 ; <nl> + <nl> init_get_bits8 (& gb , p -> buf + 3 , p -> buf_size - 3 ); <nl>  <nl> skip_bits (& gb , 40 );
static av_cold int X264_close ( AVCodecContext * avctx ) <nl> if ( x4 -> enc ) <nl> x264_encoder_close ( x4 -> enc ); <nl>  <nl> - av_free ( x4 -> preset ); <nl> - av_free ( x4 -> tune ); <nl> - av_free ( x4 -> profile ); <nl> - av_free ( x4 -> level ); <nl> - av_free ( x4 -> stats ); <nl> - av_free ( x4 -> weightp ); <nl> - av_free ( x4 -> x264opts ); <nl> - <nl> return 0 ; <nl> } <nl> 
void FUNCC ( ff_h264_idct8_add )( uint8_t * _dst , int16_t * _block , int stride ){ <nl> } <nl> for ( i = 0 ; i < 8 ; i ++ ) <nl> { <nl> - const unsigned a0 = block [ 0 + i * 8 ] + block [ 4 + i * 8 ]; <nl> - const unsigned a2 = block [ 0 + i * 8 ] - block [ 4 + i * 8 ]; <nl> - const unsigned a4 = ( block [ 2 + i * 8 ]>> 1 ) - block [ 6 + i * 8 ]; <nl> - const unsigned a6 = ( block [ 6 + i * 8 ]>> 1 ) + block [ 2 + i * 8 ]; <nl> + const unsigned a0 = block [ 0 + i * 8 ] + ( unsigned ) block [ 4 + i * 8 ]; <nl> + const unsigned a2 = block [ 0 + i * 8 ] - ( unsigned ) block [ 4 + i * 8 ]; <nl> + const unsigned a4 = ( block [ 2 + i * 8 ]>> 1 ) - ( unsigned ) block [ 6 + i * 8 ]; <nl> + const unsigned a6 = ( block [ 6 + i * 8 ]>> 1 ) + ( unsigned ) block [ 2 + i * 8 ]; <nl>  <nl> const unsigned b0 = a0 + a6 ; <nl> const unsigned b2 = a2 + a4 ;
static int parse_bintree ( Indeo3DecodeContext * ctx , AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_ERROR , " SkipCell procedure not implemented yet !\ n "); <nl>  <nl> CHECK_CELL <nl> + if (! curr_cell . mv_ptr ) <nl> + return AVERROR_INVALIDDATA ; <nl> copy_cell ( ctx , plane , & curr_cell ); <nl> return 0 ; <nl> }
static inline int set_options ( AVFilterContext * ctx , const char * args ) <nl>  <nl> hue -> hue_expr = NULL ; <nl> hue -> hue_deg_expr = NULL ; <nl> + hue -> saturation_expr = NULL ; <nl>  <nl> if (( ret = av_set_options_string ( hue , args , "=", ":")) < 0 ) <nl> return ret ;
static void fill_buffer ( AVIOContext * s ) <nl> /* make buffer smaller in case it ended up large after probing */ <nl> if ( s -> read_packet && s -> orig_buffer_size && s -> buffer_size > s -> orig_buffer_size ) { <nl> if ( dst == s -> buffer ) { <nl> - ffio_set_buf_size ( s , s -> orig_buffer_size ); <nl> + int ret = ffio_set_buf_size ( s , s -> orig_buffer_size ); <nl> + if ( ret < 0 ) <nl> + av_log ( s , AV_LOG_WARNING , " Failed to decrease buffer size \ n "); <nl>  <nl> s -> checksum_ptr = dst = s -> buffer ; <nl> }
static void process_frame ( AVFilterLink * inlink , AVFilterBufferRef * buf ) <nl> av_log ( ctx , AV_LOG_ERROR , " Frame after EOF on input % s \ n ", <nl> ctx -> input_pads [ in_no ]. name ); <nl> avfilter_unref_buffer ( buf ); <nl> - } if ( in_no >= cat -> cur_idx + ctx -> nb_outputs ) { <nl> + } else if ( in_no >= cat -> cur_idx + ctx -> nb_outputs ) { <nl> ff_bufqueue_add ( ctx , & cat -> in [ in_no ]. queue , buf ); <nl> } else { <nl> push_frame ( ctx , in_no , buf );
static int vp9_raw_reorder_make_output ( AVBSFContext * bsf , <nl> "(%" PRId64 ") from slot % d .\ n ", <nl> frame -> sequence , frame -> pts , s ); <nl>  <nl> - frame -> packet = av_packet_alloc (); <nl> - if (! frame -> packet ) <nl> - return AVERROR ( ENOMEM ); <nl> - <nl> err = av_new_packet ( out , 2 ); <nl> if ( err < 0 ) <nl> return err ;
FF_ENABLE_DEPRECATION_WARNINGS <nl> count ++; <nl> } <nl>  <nl> - // close codecs which were opened in try_decode_frame () <nl> - for ( i = 0 ; i < ic -> nb_streams ; i ++) { <nl> - st = ic -> streams [ i ]; <nl> - avcodec_close ( st -> internal -> avctx ); <nl> - } <nl> for ( i = 0 ; i < ic -> nb_streams ; i ++) { <nl> st = ic -> streams [ i ]; <nl> avctx = st -> internal -> avctx ; <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl>  <nl> find_stream_info_err : <nl> for ( i = 0 ; i < ic -> nb_streams ; i ++) { <nl> + avcodec_close ( ic -> streams [ i ]-> internal -> avctx ); <nl> av_freep (& ic -> streams [ i ]-> info ); <nl> av_bsf_free (& ic -> streams [ i ]-> internal -> extract_extradata . bsf ); <nl> av_packet_free (& ic -> streams [ i ]-> internal -> extract_extradata . pkt );
static int flic_decode_frame_8BPP ( AVCodecContext * avctx , <nl> pixel_ptr = y_ptr ; <nl> CHECK_PIXEL_PTR ( 0 ); <nl> pixel_countdown = s -> avctx -> width ; <nl> - line_packets = buf [ stream_ptr ++]; <nl> - if ( stream_ptr + 2 * line_packets > stream_ptr_after_chunk ) <nl> + if ( stream_ptr + 1 > stream_ptr_after_chunk ) <nl> break ; <nl> + line_packets = buf [ stream_ptr ++]; <nl> if ( line_packets > 0 ) { <nl> for ( i = 0 ; i < line_packets ; i ++) { <nl> /* account for the skip bytes */ <nl> + if ( stream_ptr + 2 > stream_ptr_after_chunk ) <nl> + break ; <nl> pixel_skip = buf [ stream_ptr ++]; <nl> pixel_ptr += pixel_skip ; <nl> pixel_countdown -= pixel_skip ;
int ff_thread_decode_frame ( AVCodecContext * avctx , <nl> * If we ' re still receiving the initial packets , don ' t return a frame . <nl> */ <nl>  <nl> - if ( fctx -> delaying && avpkt -> size ) { <nl> + if ( fctx -> delaying ) { <nl> if ( fctx -> next_decoding >= ( avctx -> thread_count - 1 )) fctx -> delaying = 0 ; <nl>  <nl> * got_picture_ptr = 0 ; <nl> - return avpkt -> size ; <nl> + if ( avpkt -> size ) <nl> + return avpkt -> size ; <nl> } <nl>  <nl> /*
static int read_header ( ShortenContext * s ) <nl> s -> channels = get_uint ( s , CHANSIZE ); <nl> if ( s -> channels <= 0 || s -> channels > MAX_CHANNELS ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + s -> channels = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> s -> avctx -> channels = s -> channels ;
static int decode_frame ( AVCodecContext * avctx , <nl> decoded = loco_decode_plane ( l , p -> data [ 0 ] + p -> linesize [ 0 ]*( avctx -> height - 1 ) + 3 , avctx -> width , avctx -> height , <nl> - p -> linesize [ 0 ], buf , buf_size , 4 ); <nl> break ; <nl> + default : <nl> + av_assert0 ( 0 ); <nl> } <nl>  <nl> if ( decoded < 0 || decoded > buf_size )
static int rv20_decode_picture_header ( MpegEncContext * s ) <nl> if ( s -> avctx -> debug & FF_DEBUG_PICT_INFO ){ <nl> av_log ( s -> avctx , AV_LOG_DEBUG , " F % d /% d \ n ", f , rpr_bits ); <nl> } <nl> - } <nl> + } else if ( av_image_check_size ( s -> width , s -> height , 0 , s -> avctx ) < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl>  <nl> mb_pos = ff_h263_decode_mba ( s ); <nl> 
typedef struct OggVorbisContext { <nl> uint8_t buffer [ BUFFER_SIZE ]; /**< output packet buffer */ <nl> int buffer_index ; /**< current buffer position */ <nl> int eof ; /**< end - of - file flag */ <nl> + int dsp_initialized ; /**< vd has been initialized */ <nl> vorbis_comment vc ; /**< VorbisComment info */ <nl> ogg_packet op ; /**< ogg packet */ <nl> double iblock ; /**< impulse block bias option */ <nl> static av_cold int oggvorbis_encode_close ( AVCodecContext * avctx ) <nl> OggVorbisContext * s = avctx -> priv_data ; <nl>  <nl> /* notify vorbisenc this is EOF */ <nl> - vorbis_analysis_wrote (& s -> vd , 0 ); <nl> + if ( s -> dsp_initialized ) <nl> + vorbis_analysis_wrote (& s -> vd , 0 ); <nl>  <nl> vorbis_block_clear (& s -> vb ); <nl> vorbis_dsp_clear (& s -> vd ); <nl> static av_cold int oggvorbis_encode_init ( AVCodecContext * avctx ) <nl> ret = vorbis_error_to_averror ( ret ); <nl> goto error ; <nl> } <nl> + s -> dsp_initialized = 1 ; <nl> if (( ret = vorbis_block_init (& s -> vd , & s -> vb ))) { <nl> ret = vorbis_error_to_averror ( ret ); <nl> goto error ;
static int ff_asf_parse_packet ( AVFormatContext * s , AVIOContext * pb , AVPacket * pk <nl> // printf (" packet % d % d \ n ", asf_st -> pkt . size , asf -> packet_frag_size ); <nl> asf_st -> pkt . size = 0 ; <nl> asf_st -> pkt . data = 0 ; <nl> + asf_st -> pkt . side_data_elems = 0 ; <nl> + asf_st -> pkt . side_data = NULL ; <nl> break ; // packet completed <nl> } <nl> }
int av_seek_frame_binary ( AVFormatContext * s , int stream_index , int64_t target_ts <nl> int64_t av_uninit ( pos_min ), av_uninit ( pos_max ), pos , pos_limit ; <nl> int64_t ts_min , ts_max , ts ; <nl> int index ; <nl> + int64_t ret ; <nl> AVStream * st ; <nl>  <nl> if ( stream_index < 0 ) <nl> int av_seek_frame_binary ( AVFormatContext * s , int stream_index , int64_t target_ts <nl> return - 1 ; <nl>  <nl> /* do the seek */ <nl> - url_fseek ( s -> pb , pos , SEEK_SET ); <nl> + if (( ret = url_fseek ( s -> pb , pos , SEEK_SET )) < 0 ) <nl> + return ret ; <nl>  <nl> av_update_cur_dts ( s , st , ts ); <nl> 
static av_cold int ulti_decode_init ( AVCodecContext * avctx ) <nl> s -> width = avctx -> width ; <nl> s -> height = avctx -> height ; <nl> s -> blocks = ( s -> width / 8 ) * ( s -> height / 8 ); <nl> + if ( s -> blocks == 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> avctx -> pix_fmt = AV_PIX_FMT_YUV410P ; <nl> s -> ulti_codebook = ulti_codebook ; <nl> 
static int ape_tag_read_field ( AVFormatContext * s ) <nl> { <nl> AVIOContext * pb = s -> pb ; <nl> uint8_t key [ 1024 ], * value ; <nl> - uint32_t size , flags ; <nl> + int64_t size , flags ; <nl> int i , c ; <nl>  <nl> size = avio_rl32 ( pb ); /* field size */
static void do_subtitle_out ( AVFormatContext * s , <nl> else <nl> nb = 1 ; <nl>  <nl> + /* shift timestamp to honor - ss and make check_recording_time () work with - t */ <nl> + pts = av_rescale_q ( pts , ist -> st -> time_base , AV_TIME_BASE_Q ) <nl> + - output_files [ ost -> file_index ]-> start_time ; <nl> for ( i = 0 ; i < nb ; i ++) { <nl> - ost -> sync_opts = av_rescale_q ( pts , ist -> st -> time_base , enc -> time_base ); <nl> + ost -> sync_opts = av_rescale_q ( pts , AV_TIME_BASE_Q , enc -> time_base ); <nl> if (! check_recording_time ( ost )) <nl> return ; <nl>  <nl> - sub -> pts = av_rescale_q ( pts , ist -> st -> time_base , AV_TIME_BASE_Q ); <nl> + sub -> pts = pts ; <nl> // start_display_time is required to be 0 <nl> sub -> pts += av_rescale_q ( sub -> start_display_time , ( AVRational ){ 1 , 1000 }, AV_TIME_BASE_Q ); <nl> sub -> end_display_time -= sub -> start_display_time ;
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl>  <nl> p -> pict_type = AV_PICTURE_TYPE_I ; <nl> + p -> key_frame = 1 ; <nl> * got_frame = 1 ; <nl>  <nl> return buf_size ;
static void idr ( H264Context * h ) <nl> /* forget old pics after a seek */ <nl> static void flush_change ( H264Context * h ) <nl> { <nl> + int i , j ; <nl> + <nl> h -> outputed_poc = h -> next_outputed_poc = INT_MIN ; <nl> h -> prev_interlaced_frame = 1 ; <nl> idr ( h ); <nl> h -> prev_frame_num = - 1 ; <nl> - if ( h -> s . current_picture_ptr ) <nl> + if ( h -> s . current_picture_ptr ) { <nl> h -> s . current_picture_ptr -> f . reference = 0 ; <nl> + for ( j = i = 0 ; h -> delayed_pic [ i ]; i ++) <nl> + if ( h -> delayed_pic [ i ] != h -> s . current_picture_ptr ) <nl> + h -> delayed_pic [ j ++] = h -> delayed_pic [ i ]; <nl> + h -> delayed_pic [ j ] = NULL ; <nl> + } <nl> h -> s . first_field = 0 ; <nl> memset ( h -> ref_list [ 0 ], 0 , sizeof ( h -> ref_list [ 0 ])); <nl> memset ( h -> ref_list [ 1 ], 0 , sizeof ( h -> ref_list [ 1 ]));
static av_cold int initFilter ( int16_t ** outFilter , int32_t ** filterPos , <nl>  <nl> xDstInSrc = xInc - 0x10000 ; <nl> for ( i = 0 ; i < dstW ; i ++) { <nl> - int xx = ( xDstInSrc - (( filterSize - 2 ) << 16 )) / ( 1 << 17 ); <nl> + int xx = ( xDstInSrc - (( int64_t )( filterSize - 2 ) << 16 )) / ( 1 << 17 ); <nl> int j ; <nl> (* filterPos )[ i ] = xx ; <nl> for ( j = 0 ; j < filterSize ; j ++) {
static ResampleContext * resample_init ( ResampleContext * c , int out_rate , int in_r <nl> c -> factor = factor ; <nl> c -> filter_length = FFMAX (( int ) ceil ( filter_size / factor ), 1 ); <nl> c -> filter_alloc = FFALIGN ( c -> filter_length , 8 ); <nl> - c -> filter_bank = av_mallocz ( c -> filter_alloc *( phase_count + 1 )* c -> felem_size ); <nl> + c -> filter_bank = av_calloc ( c -> filter_alloc , ( phase_count + 1 )* c -> felem_size ); <nl> c -> filter_type = filter_type ; <nl> c -> kaiser_beta = kaiser_beta ; <nl> if (! c -> filter_bank )
static int mxf_parse_structural_metadata ( MXFContext * mxf ) <nl> if (( component = mxf_resolve_strong_ref ( mxf , & material_track -> sequence_ref , TimecodeComponent ))) { <nl> mxf_tc = ( MXFTimecodeComponent *) component ; <nl> flags = mxf_tc -> drop_frame == 1 ? AV_TIMECODE_FLAG_DROPFRAME : 0 ; <nl> - if ( av_timecode_init (& tc , mxf_tc -> rate , flags , mxf_tc -> start_frame , mxf ) == 0 ) { <nl> + if ( av_timecode_init (& tc , mxf_tc -> rate , flags , mxf_tc -> start_frame , mxf -> fc ) == 0 ) { <nl> mxf_add_timecode_metadata (& mxf -> fc -> metadata , " timecode ", & tc ); <nl> } <nl> } <nl> static int mxf_parse_structural_metadata ( MXFContext * mxf ) <nl>  <nl> mxf_tc = ( MXFTimecodeComponent *) component ; <nl> flags = mxf_tc -> drop_frame == 1 ? AV_TIMECODE_FLAG_DROPFRAME : 0 ; <nl> - if ( av_timecode_init (& tc , mxf_tc -> rate , flags , mxf_tc -> start_frame , mxf ) == 0 ) { <nl> + if ( av_timecode_init (& tc , mxf_tc -> rate , flags , mxf_tc -> start_frame , mxf -> fc ) == 0 ) { <nl> mxf_add_timecode_metadata (& mxf -> fc -> metadata , " timecode ", & tc ); <nl> break ; <nl> }
int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl> break ; <nl> } <nl>  <nl> - if ( last ){ <nl> + if ( last && s -> current_picture_ptr ){ <nl> if ( r -> loop_filter ) <nl> r -> loop_filter ( r , s -> mb_height - 1 ); <nl> ff_er_frame_end ( s );
static int find_marker ( const uint8_t ** pbuf_ptr , const uint8_t * buf_end ) <nl> int skipped = 0 ; <nl>  <nl> buf_ptr = * pbuf_ptr ; <nl> - while ( buf_ptr < buf_end ) { <nl> + while ( buf_end - buf_ptr > 1 ) { <nl> v = * buf_ptr ++; <nl> v2 = * buf_ptr ; <nl> if (( v == 0xff ) && ( v2 >= 0xc0 ) && ( v2 <= 0xfe ) && buf_ptr < buf_end ) { <nl> static int find_marker ( const uint8_t ** pbuf_ptr , const uint8_t * buf_end ) <nl> } <nl> skipped ++; <nl> } <nl> + buf_ptr = buf_end ; <nl> val = - 1 ; <nl> found : <nl> av_dlog ( NULL , " find_marker skipped % d bytes \ n ", skipped );
retry : <nl> uint8_t * side_data = av_packet_new_side_data ( pkt , <nl> AV_PKT_DATA_METADATA_UPDATE , <nl> os -> new_metadata_size ); <nl> + if ( side_data == NULL ) { <nl> + av_free_packet ( pkt ); <nl> + av_free ( pkt ); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> memcpy ( side_data , os -> new_metadata , os -> new_metadata_size ); <nl> av_freep (& os -> new_metadata ); <nl> os -> new_metadata_size = 0 ;
int main ( int argc , char ** argv ) <nl> while ( count --) { <nl> int burst = 1 + ran () * ( uint64_t ) ( abs ( maxburst ) - 1 ) / UINT32_MAX ; <nl> int pos = ran () * ( uint64_t ) length / UINT32_MAX ; <nl> - fseek ( f , pos , SEEK_SET ); <nl> + if ( fseek ( f , pos , SEEK_SET ) < 0 ) { <nl> + fprintf ( stderr , " seek failed \ n "); <nl> + return 1 ; <nl> + } <nl>  <nl> if ( maxburst < 0 ) <nl> burst = - maxburst ;
static int svq1_decode_frame ( AVCodecContext * avctx , <nl> if ( s -> f_code != 0x20 ) { <nl> uint32_t * src = ( uint32_t *) ( buf + 4 ); <nl>  <nl> + if ( buf_size < 36 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> for ( i = 0 ; i < 4 ; i ++) { <nl> src [ i ] = (( src [ i ] << 16 ) | ( src [ i ] >> 16 )) ^ src [ 7 - i ]; <nl> }
static OutputStream * choose_output ( void ) <nl>  <nl> for ( i = 0 ; i < nb_output_streams ; i ++) { <nl> OutputStream * ost = output_streams [ i ]; <nl> - int64_t opts = av_rescale_q ( ost -> st -> cur_dts , ost -> st -> time_base , <nl> + int64_t opts = ost -> st -> cur_dts == AV_NOPTS_VALUE ? INT64_MIN : <nl> + av_rescale_q ( ost -> st -> cur_dts , ost -> st -> time_base , <nl> AV_TIME_BASE_Q ); <nl> + if ( ost -> st -> cur_dts == AV_NOPTS_VALUE ) <nl> + av_log ( NULL , AV_LOG_DEBUG , " cur_dts is invalid ( this is harmless if it occurs once at the start per stream )\ n "); <nl> + <nl> if (! ost -> finished && opts < opts_min ) { <nl> opts_min = opts ; <nl> ost_min = ost -> unavailable ? NULL : ost ;
static inline void direct_ref_list_init ( H264Context * const h ){ <nl> for ( list = 0 ; list < 2 ; list ++){ <nl> for ( i = 0 ; i < ref1 -> ref_count [ list ]; i ++){ <nl> const int poc = ref1 -> ref_poc [ list ][ i ]; <nl> - h -> map_col_to_list0 [ list ][ i ] = PART_NOT_AVAILABLE ; <nl> + h -> map_col_to_list0 [ list ][ i ] = 0 ; /* bogus ; fills in for missing frames */ <nl> for ( j = 0 ; j < h -> ref_count [ list ]; j ++) <nl> if ( h -> ref_list [ list ][ j ]. poc == poc ){ <nl> h -> map_col_to_list0 [ list ][ i ] = j ; <nl> static inline void mc_dir_part ( H264Context * h , Picture * pic , int n , int square , <nl> const int pic_width = 16 * s -> mb_width ; <nl> const int pic_height = 16 * s -> mb_height ; <nl>  <nl> - assert ( pic -> data [ 0 ]); <nl> + if (! pic -> data [ 0 ]) <nl> + return ; <nl>  <nl> if ( mx & 7 ) extra_width -= 3 ; <nl> if ( my & 7 ) extra_height -= 3 ; <nl> static int decode_frame ( AVCodecContext * avctx , <nl> } <nl>  <nl> out_of_order = ! cross_idr && prev && out -> poc < prev -> poc ; <nl> - if ( prev && pics <= s -> avctx -> has_b_frames ) <nl> + if ( h -> sps . bitstream_restriction_flag && s -> avctx -> has_b_frames >= h -> sps . num_reorder_frames ) <nl> + { } <nl> + else if ( prev && pics <= s -> avctx -> has_b_frames ) <nl> out = prev ; <nl> else if (( out_of_order && pics - 1 == s -> avctx -> has_b_frames && pics < 15 ) <nl> || ( s -> low_delay &&
static int filter_frame ( AVFilterLink * inlink , AVFrame * inpicref ) <nl>  <nl> inpicref -> pts = outlink -> frame_count * sf -> ts_unit ; <nl> ret = ff_filter_frame ( outlink , inpicref ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_frame_free (& second ); <nl> return ret ; <nl> + } <nl>  <nl> second -> pts = outlink -> frame_count * sf -> ts_unit ; <nl> return ff_filter_frame ( outlink , second );
static int decode_pic_hdr ( IVI45DecContext * ctx , AVCodecContext * avctx ) <nl> /* skip picture header extension if any */ <nl> while ( get_bits1 (& ctx -> gb )) { <nl> ff_dlog ( avctx , " Pic hdr extension encountered !\ n "); <nl> + if ( get_bits_left (& ctx -> gb ) < 10 ) <nl> + return AVERROR_INVALIDDATA ; <nl> skip_bits (& ctx -> gb , 8 ); <nl> } <nl> 
static av_cold int vc2_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> int64_t max_frame_bytes , r_bitrate = avctx -> bit_rate >> ( s -> interlaced ); <nl>  <nl> s -> avctx = avctx ; <nl> - s -> size_scaler = 1 ; <nl> + s -> size_scaler = 2 ; <nl> s -> prefix_bytes = 0 ; <nl> s -> last_parse_code = 0 ; <nl> s -> next_parse_offset = 0 ;
static void decode_band_structure ( GetBitContext * gbc , int blk , int eac3 , <nl> uint8_t * band_struct , int * num_subbands , <nl> int * num_bands , uint8_t * band_sizes ) <nl> { <nl> - int subbnd , bnd , n_subbands , n_bands ; <nl> + int subbnd , bnd , n_subbands , n_bands = 0 ; <nl> uint8_t bnd_sz [ 22 ]; <nl>  <nl> n_subbands = end_subband - start_subband ;
static int mov_read_dref ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> dref -> dir = av_malloc ( len + 1 ); <nl> if (! dref -> dir ) <nl> return AVERROR ( ENOMEM ); <nl> - avio_read ( pb , dref -> dir , len ); <nl> + if ( avio_read ( pb , dref -> dir , len ) != len ) <nl> + return AVERROR_INVALIDDATA ; <nl> dref -> dir [ len ] = 0 ; <nl> for ( j = 0 ; j < len ; j ++) <nl> if ( dref -> dir [ j ] == ':')
int ff_jpeg2000_init_component ( Jpeg2000Component * comp , <nl> if (! reslevel -> band ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> + if ( reslevel -> num_precincts_x * ( uint64_t ) reslevel -> num_precincts_y * reslevel -> nbands > avctx -> max_pixels / sizeof (* reslevel -> band -> prec )) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> for ( bandno = 0 ; bandno < reslevel -> nbands ; bandno ++, gbandno ++) { <nl> ret = init_band ( avctx , reslevel , <nl> comp , codsty , qntsty ,
static int mov_write_header ( AVFormatContext * s ) <nl> else if (! TAG_IS_AVCI ( track -> tag )){ <nl> track -> vos_len = st -> codec -> extradata_size ; <nl> track -> vos_data = av_malloc ( track -> vos_len ); <nl> - if (! track -> vos_data ) <nl> + if (! track -> vos_data ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> goto error ; <nl> + } <nl> memcpy ( track -> vos_data , st -> codec -> extradata , track -> vos_len ); <nl> } <nl> }
static int mov_text_decode_frame ( AVCodecContext * avctx , <nl> tsmb_type = AV_RB32 ( tsmb ); <nl> tsmb += 4 ; <nl>  <nl> - if ( tsmb_size == 0 ) { <nl> - return AVERROR_INVALIDDATA ; <nl> - } <nl> - <nl> if ( tsmb_size == 1 ) { <nl> if ( m -> tracksize + 16 > avpkt -> size ) <nl> break ; <nl> static int mov_text_decode_frame ( AVCodecContext * avctx , <nl> m -> size_var = 8 ; <nl> // size_var is equal to 8 or 16 depending on the size of box <nl>  <nl> + if ( tsmb_size == 0 ) { <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( tsmb_size > avpkt -> size - m -> tracksize ) <nl> break ; <nl> 
static int mov_write_tkhd_tag ( AVIOContext * pb , MOVMuxContext * mov , <nl>  <nl> display_matrix = ( uint32_t *) av_stream_get_side_data ( st , AV_PKT_DATA_DISPLAYMATRIX , <nl> & display_matrix_size ); <nl> - if ( display_matrix_size < 9 * sizeof (* display_matrix )) <nl> + if ( display_matrix && display_matrix_size < 9 * sizeof (* display_matrix )) <nl> display_matrix = NULL ; <nl> } <nl> 
static int alac_set_info ( ALACContext * alac ) <nl> bytestream2_skipu (& gb , 12 ); // size : 4 , alac : 4 , version : 4 <nl>  <nl> alac -> max_samples_per_frame = bytestream2_get_be32u (& gb ); <nl> - if (! alac -> max_samples_per_frame || alac -> max_samples_per_frame > INT_MAX ) { <nl> + if (! alac -> max_samples_per_frame || <nl> + alac -> max_samples_per_frame > INT_MAX / sizeof ( int32_t )) { <nl> av_log ( alac -> avctx , AV_LOG_ERROR , " max samples per frame invalid : % u \ n ", <nl> alac -> max_samples_per_frame ); <nl> return AVERROR_INVALIDDATA ;
static void stereo_processing ( PSContext * ps , INTFLOAT (* l )[ 32 ][ 2 ], INTFLOAT (* r ) <nl> h_step [ 1 ][ 3 ] = AAC_MSUB31_V3 ( H22 [ 1 ][ e + 1 ][ b ], h [ 1 ][ 3 ], width ); <nl> } <nl> ps -> dsp . stereo_interpolate [! PS_BASELINE && ps -> enable_ipdopd ]( <nl> - l [ k ] + start + 1 , r [ k ] + start + 1 , <nl> + l [ k ] + 1 + start , r [ k ] + 1 + start , <nl> h , h_step , stop - start ); <nl> } <nl> }
int ff_lzf_uncompress ( GetByteContext * gb , uint8_t ** buf , int64_t * size ) <nl> ret = av_reallocp ( buf , * size ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + p = * buf + len ; <nl> } <nl>  <nl> bytestream2_get_buffer ( gb , p , s ); <nl> int ff_lzf_uncompress ( GetByteContext * gb , uint8_t ** buf , int64_t * size ) <nl> ret = av_reallocp ( buf , * size ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + p = * buf + len ; <nl> } <nl>  <nl> av_memcpy_backptr ( p , off , l );
static av_cold void construct_perm_table ( TwinContext * tctx , enum FrameType ftype ) <nl> { <nl> int block_size ; <nl> const ModeTab * mtab = tctx -> mtab ; <nl> - int size = tctx -> avctx -> channels * mtab -> fmode [ ftype ]. sub ; <nl> + int size ; <nl> int16_t * tmp_perm = ( int16_t *) tctx -> tmp_buf ; <nl>  <nl> if ( ftype == FT_PPC ) { <nl> size = tctx -> avctx -> channels ; <nl> block_size = mtab -> ppc_shape_len ; <nl> - } else <nl> + } else { <nl> + size = tctx -> avctx -> channels * mtab -> fmode [ ftype ]. sub ; <nl> block_size = mtab -> size / mtab -> fmode [ ftype ]. sub ; <nl> + } <nl>  <nl> permutate_in_line ( tmp_perm , tctx -> n_div [ ftype ], size , <nl> block_size , tctx -> length [ ftype ],
static int filter_frame ( AVFilterLink * inlink , AVFrame * picref ) <nl> if (! elemsignature ) <nl> return AVERROR ( ENOMEM ); <nl> sortsignature = av_malloc_array ( elemcat -> elem_count , sizeof ( int64_t )); <nl> - if (! sortsignature ) <nl> + if (! sortsignature ) { <nl> + av_freep (& elemsignature ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl>  <nl> for ( j = 0 ; j < elemcat -> elem_count ; j ++) { <nl> blocksum = 0 ; <nl> static int binary_export ( AVFilterContext * ctx , StreamContext * sc , const char * fi <nl> char buf [ 128 ]; <nl> av_strerror ( err , buf , sizeof ( buf )); <nl> av_log ( ctx , AV_LOG_ERROR , " cannot open file % s : % s \ n ", filename , buf ); <nl> + av_freep (& buffer ); <nl> return err ; <nl> } <nl> init_put_bits (& buf , buffer , len );
static uint64_t calc_rice_params ( RiceContext * rc , <nl> bits [ pmin ] = UINT32_MAX ; <nl> for ( i = pmax ; ; ) { <nl> bits [ i ] = calc_optimal_rice_params (& tmp_rc , i , sums , n , pred_order , kmax , exact ); <nl> - if ( bits [ i ] < bits [ opt_porder ]) { <nl> + if ( bits [ i ] < bits [ opt_porder ] || pmax == pmin ) { <nl> opt_porder = i ; <nl> * rc = tmp_rc ; <nl> }
static int msrle_decode_8_16_24_32 ( AVCodecContext * avctx , AVPicture * pic , int de <nl> int p1 , p2 , line = avctx -> height - 1 , pos = 0 , i ; <nl> uint16_t av_uninit ( pix16 ); <nl> uint32_t av_uninit ( pix32 ); <nl> + unsigned int width = FFABS ( pic -> linesize [ 0 ]) / ( depth >> 3 ); <nl>  <nl> output = pic -> data [ 0 ] + ( avctx -> height - 1 ) * pic -> linesize [ 0 ]; <nl> output_end = pic -> data [ 0 ] + ( avctx -> height ) * pic -> linesize [ 0 ]; <nl> static int msrle_decode_8_16_24_32 ( AVCodecContext * avctx , AVPicture * pic , int de <nl> p1 = * src ++; <nl> p2 = * src ++; <nl> line -= p2 ; <nl> - if ( line < 0 ){ <nl> + pos += p1 ; <nl> + if ( line < 0 || pos >= width ){ <nl> av_log ( avctx , AV_LOG_ERROR , " Skip beyond picture bounds \ n "); <nl> return - 1 ; <nl> } <nl> - pos += p1 ; <nl> output = pic -> data [ 0 ] + line * pic -> linesize [ 0 ] + pos * ( depth >> 3 ); <nl> continue ; <nl> }
static void output_packet ( OutputFile * of , AVPacket * pkt , OutputStream * ost ) <nl> if ( ost -> nb_bitstream_filters ) { <nl> int idx ; <nl>  <nl> + av_packet_split_side_data ( pkt ); <nl> ret = av_bsf_send_packet ( ost -> bsf_ctx [ 0 ], pkt ); <nl> if ( ret < 0 ) <nl> goto finish ;
static void iv_Decode_Chunk ( Indeo3DecodeContext * s , <nl> int * width_tbl , width_tbl_arr [ 10 ]; <nl> const signed char * ref_vectors ; <nl> uint8_t * cur_frm_pos , * ref_frm_pos , * cp , * cp2 ; <nl> + uint8_t * cur_end = cur + width * height + width ; <nl> uint32_t * cur_lp , * ref_lp ; <nl> const uint32_t * correction_lp [ 2 ], * correctionloworder_lp [ 2 ], * correctionhighorder_lp [ 2 ]; <nl> uint8_t * correction_type_sp [ 2 ]; <nl> static void iv_Decode_Chunk ( Indeo3DecodeContext * s , <nl> k = * buf1 ++; <nl> cur_lp = (( uint32_t *) cur_frm_pos ) + width_tbl [ lp2 ]; <nl> ref_lp = (( uint32_t *) ref_frm_pos ) + width_tbl [ lp2 ]; <nl> + if (( uint8_t *) cur_lp >= cur_end - 3 ) <nl> + break ; <nl>  <nl> switch ( correction_type_sp [ 0 ][ k ]) { <nl> case 0 :
static const char * srt_to_ass ( AVCodecContext * avctx , char * out , char * out_end , <nl> for ( j = sptr - 2 ; j >= 0 ; j --) <nl> if ( stack [ j ]. param [ i ][ 0 ]) { <nl> out += snprintf ( out , out_end - out , <nl> - stack [ j ]. param [ i ]); <nl> + "% s ", stack [ j ]. param [ i ]); <nl> break ; <nl> } <nl> } else { <nl> static const char * srt_to_ass ( AVCodecContext * avctx , char * out , char * out_end , <nl> for ( i = 0 ; i < PARAM_NUMBER ; i ++) <nl> if ( stack [ sptr ]. param [ i ][ 0 ]) <nl> out += snprintf ( out , out_end - out , <nl> - stack [ sptr ]. param [ i ]); <nl> + "% s ", stack [ sptr ]. param [ i ]); <nl> } <nl> } else if (! buffer [ 1 ] && strspn ( buffer , " bisu ") == 1 ) { <nl> out += snprintf ( out , out_end - out ,
void avcodec_string ( char * buf , int buf_size , AVCodecContext * enc , int encode ) <nl>  <nl> if ( enc -> sample_aspect_ratio . num ) { <nl> av_reduce (& display_aspect_ratio . num , & display_aspect_ratio . den , <nl> - enc -> width * enc -> sample_aspect_ratio . num , <nl> - enc -> height * enc -> sample_aspect_ratio . den , <nl> + enc -> width * ( int64_t ) enc -> sample_aspect_ratio . num , <nl> + enc -> height * ( int64_t ) enc -> sample_aspect_ratio . den , <nl> 1024 * 1024 ); <nl> snprintf ( buf + strlen ( buf ), buf_size - strlen ( buf ), <nl> " [ SAR % d :% d DAR % d :% d ]",
static av_cold int init ( AVFilterContext * ctx , const char * args , void * opaque ) <nl> aspect -> ratio = ( AVRational ) { 0 , 1 }; <nl>  <nl> if ( args ) { <nl> - if ( av_parse_ratio (& aspect -> ratio , args , 100 , 0 , ctx ) || <nl> + if ( av_parse_ratio (& aspect -> ratio , args , 100 , 0 , ctx ) < 0 || <nl> aspect -> ratio . num < 0 || aspect -> ratio . den <= 0 ) { <nl> av_log ( ctx , AV_LOG_ERROR , <nl> " Invalid string '% s ' for aspect ratio .\ n ", args );
int ff_socket ( int af , int type , int proto ) <nl> # endif <nl> } <nl> # ifdef SO_NOSIGPIPE <nl> - if ( fd != - 1 ) <nl> - setsockopt ( fd , SOL_SOCKET , SO_NOSIGPIPE , &( int ){ 1 }, sizeof ( int )); <nl> + if ( fd != - 1 ) { <nl> + if ( setsockopt ( fd , SOL_SOCKET , SO_NOSIGPIPE , &( int ){ 1 }, sizeof ( int ))) { <nl> + av_log ( NULL , AV_LOG_WARNING , " setsockopt ( SO_NOSIGPIPE ) failed \ n "); <nl> + } <nl> + } <nl> # endif <nl> return fd ; <nl> }
static void RENAME ( decode_rgb_frame )( FFV1Context * s , uint8_t * src [ 3 ], int w , int <nl> } <nl>  <nl> if ( lbd ) <nl> - *(( uint32_t *)( src [ 0 ] + x * 4 + stride [ 0 ]* y )) = b + ( g << 8 ) + ( r << 16 ) + ( a << 24 ); <nl> + *(( uint32_t *)( src [ 0 ] + x * 4 + stride [ 0 ]* y )) = b + (( unsigned ) g << 8 ) + (( unsigned ) r << 16 ) + (( unsigned ) a << 24 ); <nl> else if ( sizeof ( TYPE ) == 4 ) { <nl> *(( uint16_t *)( src [ 0 ] + x * 2 + stride [ 0 ]* y )) = g ; <nl> *(( uint16_t *)( src [ 1 ] + x * 2 + stride [ 1 ]* y )) = b ;
typedef struct { <nl> */ <nl> static int decode_gop_header ( IVI5DecContext * ctx , AVCodecContext * avctx ) <nl> { <nl> - int result , i , p , tile_size , pic_size_indx , mb_size , blk_size ; <nl> + int result , i , p , tile_size , pic_size_indx , mb_size , blk_size , is_scalable ; <nl> int quant_mat , blk_size_changed = 0 ; <nl> IVIBandDesc * band , * band1 , * band2 ; <nl> IVIPicConfig pic_conf ; <nl> static int decode_gop_header ( IVI5DecContext * ctx , AVCodecContext * avctx ) <nl> /* num_levels * 3 + 1 */ <nl> pic_conf . luma_bands = get_bits (& ctx -> gb , 2 ) * 3 + 1 ; <nl> pic_conf . chroma_bands = get_bits1 (& ctx -> gb ) * 3 + 1 ; <nl> - ctx -> is_scalable = pic_conf . luma_bands != 1 || pic_conf . chroma_bands != 1 ; <nl> - if ( ctx -> is_scalable && ( pic_conf . luma_bands != 4 || pic_conf . chroma_bands != 1 )) { <nl> + is_scalable = pic_conf . luma_bands != 1 || pic_conf . chroma_bands != 1 ; <nl> + if ( is_scalable && ( pic_conf . luma_bands != 4 || pic_conf . chroma_bands != 1 )) { <nl> av_log ( avctx , AV_LOG_ERROR , " Scalability : unsupported subdivision ! Luma bands : % d , chroma bands : % d \ n ", <nl> pic_conf . luma_bands , pic_conf . chroma_bands ); <nl> return - 1 ; <nl> static int decode_gop_header ( IVI5DecContext * ctx , AVCodecContext * avctx ) <nl> return - 1 ; <nl> } <nl> ctx -> pic_conf = pic_conf ; <nl> + ctx -> is_scalable = is_scalable ; <nl> blk_size_changed = 1 ; /* force reallocation of the internal structures */ <nl> } <nl> 
static int adts_decode_extradata ( AVFormatContext * s , ADTSContext * adts , const ui <nl> GetBitContext gb ; <nl> PutBitContext pb ; <nl> MPEG4AudioConfig m4ac ; <nl> - int off ; <nl> + int off , ret ; <nl>  <nl> - init_get_bits (& gb , buf , size * 8 ); <nl> + ret = init_get_bits8 (& gb , buf , size ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> off = avpriv_mpeg4audio_get_config2 (& m4ac , buf , size , 1 , s ); <nl> if ( off < 0 ) <nl> return off ;
int audio_resample ( ReSampleContext * s , short * output , short * input , int nb_sampl <nl> /* XXX : move those malloc to resample init code */ <nl> for ( i = 0 ; i < s -> filter_channels ; i ++) { <nl> bufin [ i ] = av_malloc_array (( nb_samples + s -> temp_len ), sizeof ( short )); <nl> + bufout [ i ] = av_malloc_array ( lenout , sizeof ( short )); <nl> + <nl> + if (! bufin [ i ] || ! bufout [ i ]) { <nl> + av_log ( s -> resample_context , AV_LOG_ERROR , " Could not allocate buffer \ n "); <nl> + nb_samples1 = 0 ; <nl> + goto fail ; <nl> + } <nl> + <nl> memcpy ( bufin [ i ], s -> temp [ i ], s -> temp_len * sizeof ( short )); <nl> buftmp2 [ i ] = bufin [ i ] + s -> temp_len ; <nl> - bufout [ i ] = av_malloc_array ( lenout , sizeof ( short )); <nl> } <nl>  <nl> if ( s -> input_channels == 2 && s -> output_channels == 1 ) { <nl> int audio_resample ( ReSampleContext * s , short * output , short * input , int nb_sampl <nl> } <nl> } <nl>  <nl> + fail : <nl> for ( i = 0 ; i < s -> filter_channels ; i ++) { <nl> av_free ( bufin [ i ]); <nl> av_free ( bufout [ i ]);
static int ffserver_parse_config_stream ( FFServerConfig * config , const char * cmd , <nl> ffserver_get_arg ( arg , sizeof ( arg ), p ); <nl> stream -> max_time = atof ( arg ) * 1000 ; <nl> } else if (! av_strcasecmp ( cmd , " AudioBitRate ")) { <nl> - ffserver_get_arg ( arg , sizeof ( arg ), p ); <nl> float f ; <nl> + ffserver_get_arg ( arg , sizeof ( arg ), p ); <nl> ffserver_set_float_param (& f , arg , 1000 , 0 , FLT_MAX , config , line_num , " Invalid % s : % s \ n ", cmd , arg ); <nl> if ( av_dict_set_int (& config -> audio_conf , cmd , lrintf ( f ), 0 ) < 0 ) <nl> goto nomem ;
static int subtitle_disable ; <nl> static const char * wanted_stream_spec [ AVMEDIA_TYPE_NB ] = { 0 }; <nl> static int seek_by_bytes = - 1 ; <nl> static int display_disable ; <nl> + static int borderless ; <nl> static int startup_volume = 100 ; <nl> static int show_status = 1 ; <nl> static int av_sync_type = AV_SYNC_AUDIO_MASTER ; <nl> static int video_open ( VideoState * is ) <nl> window_title = input_filename ; <nl> if ( is_full_screen ) <nl> flags |= SDL_WINDOW_FULLSCREEN_DESKTOP ; <nl> + if ( borderless ) <nl> + flags |= SDL_WINDOW_BORDERLESS ; <nl> window = SDL_CreateWindow ( window_title , SDL_WINDOWPOS_UNDEFINED , SDL_WINDOWPOS_UNDEFINED , w , h , flags ); <nl> SDL_SetHint ( SDL_HINT_RENDER_SCALE_QUALITY , " linear "); <nl> if ( window ) { <nl> static const OptionDef options [] = { <nl> { " t ", HAS_ARG , { . func_arg = opt_duration }, " play \" duration \" seconds of audio / video ", " duration " }, <nl> { " bytes ", OPT_INT | HAS_ARG , { & seek_by_bytes }, " seek by bytes 0 = off 1 = on - 1 = auto ", " val " }, <nl> { " nodisp ", OPT_BOOL , { & display_disable }, " disable graphical display " }, <nl> + { " noborder ", OPT_BOOL , { & borderless }, " borderless window " }, <nl> { " volume ", OPT_INT | HAS_ARG , { & startup_volume }, " set startup volume 0 = min 100 = max ", " volume " }, <nl> { " f ", HAS_ARG , { . func_arg = opt_format }, " force format ", " fmt " }, <nl> { " pix_fmt ", HAS_ARG | OPT_EXPERT | OPT_VIDEO , { . func_arg = opt_frame_pix_fmt }, " set pixel format ", " format " },
static inline int wv_get_value_integer ( WavpackFrameContext * s , uint32_t * crc , in <nl> bit = ((( S + bit ) << s -> shift ) - bit ) << s -> post_shift ; <nl>  <nl> if ( s -> hybrid ) <nl> - bit = av_clip ( bit , - s -> hybrid_maxclip , s -> hybrid_maxclip - 1 ); <nl> + bit = av_clip ( bit , - s -> hybrid_maxclip - 1 , s -> hybrid_maxclip ); <nl>  <nl> return bit ; <nl> } <nl> static int wavpack_decode_block ( AVCodecContext * avctx , int block_no , <nl> s -> joint = s -> frame_flags & WV_JOINT_STEREO ; <nl> s -> hybrid = s -> frame_flags & WV_HYBRID_MODE ; <nl> s -> hybrid_bitrate = s -> frame_flags & WV_HYBRID_BITRATE ; <nl> - s -> hybrid_maxclip = 1 << (((( s -> frame_flags & 0x03 ) + 1 ) << 3 ) - 1 ); <nl> + s -> hybrid_maxclip = ( 1LL << (((( s -> frame_flags & 0x03 ) + 1 ) << 3 ) - 1 )) - 1 ; <nl> s -> post_shift = 8 * ( bpp - 1 -( s -> frame_flags & 0x03 )) + (( s -> frame_flags >> 13 ) & 0x1f ); <nl> s -> CRC = AV_RL32 ( buf ); buf += 4 ; <nl> if ( wc -> mkv_mode )
static inline void xan_wc3_copy_pixel_run ( XanContext * s , <nl>  <nl> palette_plane = s -> current_frame . data [ 0 ]; <nl> prev_palette_plane = s -> last_frame . data [ 0 ]; <nl> + if (! prev_palette_plane ) <nl> + prev_palette_plane = palette_plane ; <nl> stride = s -> current_frame . linesize [ 0 ]; <nl> line_inc = stride - width ; <nl> curframe_index = y * stride + x ;
static const uint8_t map2 [] = <nl>  <nl> int av_base64_decode ( uint8_t * out , const char * in , int out_size ) <nl> { <nl> - int i , v ; <nl> + int i ; <nl> + unsigned v = 0 ; <nl> uint8_t * dst = out ; <nl>  <nl> - v = 0 ; <nl> for ( i = 0 ; in [ i ] && in [ i ] != '='; i ++) { <nl> unsigned int index = in [ i ]- 43 ; <nl> if ( index >= FF_ARRAY_ELEMS ( map2 ) || map2 [ index ] == 0xff )
static int initFilter ( int16_t ** outFilter , int16_t ** filterPos , int * outFilterSi <nl> floatd = d * ( 1 . 0 /( 1 << 30 )); <nl>  <nl> if ( flags & SWS_BICUBIC ) { <nl> +# define SQRT_INT64_MAX 0xb504f333 <nl> int64_t B = ( param [ 0 ] != SWS_PARAM_DEFAULT ? param [ 0 ] : 0 ) * ( 1 << 24 ); <nl> int64_t C = ( param [ 1 ] != SWS_PARAM_DEFAULT ? param [ 1 ] : 0 . 6 ) * ( 1 << 24 ); <nl> - int64_t dd = ( d * d )>> 30 ; <nl> - int64_t ddd = ( dd * d )>> 30 ; <nl> + int64_t dd = d > SQRT_INT64_MAX ? (( d >> 1 ) * d ) >> 29 : ( d * d ) >> 30 ; <nl> + int64_t ddd = d > SQRT_INT64_MAX || dd > SQRT_INT64_MAX ? <nl> + (( dd >> 2 ) * d ) >> 28 : ( dd * d ) >> 30 ; <nl>  <nl> if ( d < 1LL << 30 ) <nl> coeff = ( 12 *( 1 << 24 )- 9 * B - 6 * C )* ddd + (- 18 *( 1 << 24 )+ 12 * B + 6 * C )* dd + ( 6 *( 1 << 24 )- 2 * B )*( 1 << 30 );
static void ipvideo_decode_opcodes ( IpvideoContext * s , AVFrame * frame ) <nl> init_get_bits (& gb , s -> decoding_map , s -> decoding_map_size * 8 ); <nl> for ( y = 0 ; y < s -> avctx -> height ; y += 8 ) { <nl> for ( x = 0 ; x < s -> avctx -> width ; x += 8 ) { <nl> + if ( get_bits_left (& gb ) < 4 ) <nl> + return ; <nl> opcode = get_bits (& gb , 4 ); <nl>  <nl> ff_tlog ( s -> avctx ,
int av_probe_input_buffer ( AVIOContext * pb , AVInputFormat ** fmt , <nl> probe_size = FFMIN ( probe_size << 1 , FFMAX ( max_probe_size , probe_size + 1 ))) { <nl> int score = probe_size < max_probe_size ? AVPROBE_SCORE_MAX / 4 : 0 ; <nl> int buf_offset = ( probe_size == PROBE_BUF_MIN ) ? 0 : probe_size >> 1 ; <nl> + void * buftmp ; <nl>  <nl> if ( probe_size < offset ) { <nl> continue ; <nl> } <nl>  <nl> /* read probe data */ <nl> - buf = av_realloc ( buf , probe_size + AVPROBE_PADDING_SIZE ); <nl> + buftmp = av_realloc ( buf , probe_size + AVPROBE_PADDING_SIZE ); <nl> + if (! buftmp ){ <nl> + av_free ( buf ); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> + buf = buftmp ; <nl> if (( ret = avio_read ( pb , buf + buf_offset , probe_size - buf_offset )) < 0 ) { <nl> /* fail if error was not end of file , otherwise , lower score */ <nl> if ( ret != AVERROR_EOF ) {
static int matroska_read_header ( AVFormatContext * s ) <nl> } else if ( codec_id == CODEC_ID_RA_144 ) { <nl> track -> audio . out_samplerate = 8000 ; <nl> track -> audio . channels = 1 ; <nl> - } else if ( codec_id == CODEC_ID_RA_288 || codec_id == CODEC_ID_COOK || <nl> - codec_id == CODEC_ID_ATRAC3 || codec_id == CODEC_ID_SIPR ) { <nl> + } else if (( codec_id == CODEC_ID_RA_288 || codec_id == CODEC_ID_COOK || <nl> + codec_id == CODEC_ID_ATRAC3 || codec_id == CODEC_ID_SIPR ) <nl> + && track -> codec_priv . data ) { <nl> int flavor ; <nl> + <nl> ffio_init_context (& b , track -> codec_priv . data , track -> codec_priv . size , <nl> 0 , NULL , NULL , NULL , NULL ); <nl> avio_skip (& b , 22 );
static int dxva2_device_create9ex ( AVHWDeviceContext * ctx , UINT adapter ) <nl> if ( FAILED ( hr )) <nl> return AVERROR_UNKNOWN ; <nl>  <nl> - IDirect3D9Ex_GetAdapterDisplayModeEx ( d3d9ex , adapter , & modeex , NULL ); <nl> + hr = IDirect3D9Ex_GetAdapterDisplayModeEx ( d3d9ex , adapter , & modeex , NULL ); <nl> + if ( FAILED ( hr )) { <nl> + IDirect3D9Ex_Release ( d3d9ex ); <nl> + return AVERROR_UNKNOWN ; <nl> + } <nl>  <nl> d3dpp . BackBufferFormat = modeex . Format ; <nl> 
static int rm_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> flags = ( seq ++ == 1 ) ? 2 : 0 ; <nl> } else { <nl> len = sync ( s , & timestamp , & flags , & i , & pos ); <nl> - st = s -> streams [ i ]; <nl> + if ( len > 0 ) <nl> + st = s -> streams [ i ]; <nl> } <nl>  <nl> if ( len < 0 || url_feof ( s -> pb ))
static int process_ipmovie_chunk ( IPMVEContext * s , AVIOContext * pb , <nl>  <nl> case OPCODE_CREATE_TIMER : <nl> av_dlog ( NULL , " create timer \ n "); <nl> - if (( opcode_version > 0 ) || ( opcode_size > 6 )) { <nl> + if (( opcode_version > 0 ) || ( opcode_size != 6 )) { <nl> av_dlog ( NULL , " bad create_timer opcode \ n "); <nl> chunk_type = CHUNK_BAD ; <nl> break ;
int av_find_stream_info ( AVFormatContext * ic ) <nl> int i = st -> parser -> parser -> split ( st -> codec , pkt -> data , pkt -> size ); <nl> if ( i ){ <nl> st -> codec -> extradata_size = i ; <nl> - st -> codec -> extradata = av_malloc ( st -> codec -> extradata_size ); <nl> + st -> codec -> extradata = av_malloc ( st -> codec -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> memcpy ( st -> codec -> extradata , pkt -> data , st -> codec -> extradata_size ); <nl> + memset ( st -> codec -> extradata + i , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> } <nl> } <nl> 
static int parse ( AVCodecParserContext * ctx , <nl> # define case_n ( a , rd ) \ <nl> case a : \ <nl> while ( n_frames --) { \ <nl> - int sz = rd ; \ <nl> + unsigned sz = rd ; \ <nl> idx += a ; \ <nl> if ( sz > size ) { \ <nl> s -> n_frames = 0 ; \ <nl> av_log ( avctx , AV_LOG_ERROR , \ <nl> - " Superframe packet size too big : % d > % d \ n ", \ <nl> + " Superframe packet size too big : % u > % d \ n ", \ <nl> sz , size ); \ <nl> return size ; \ <nl> } \
static int mpeg_decode_mb ( MpegEncContext * s , int16_t block [ 12 ][ 64 ]) <nl>  <nl> cbp = get_vlc2 (& s -> gb , ff_mb_pat_vlc . table , MB_PAT_VLC_BITS , 1 ); <nl> if ( mb_block_count > 6 ) { <nl> - cbp <<= mb_block_count - 6 ; <nl> + cbp *= 1 << mb_block_count - 6 ; <nl> cbp |= get_bits (& s -> gb , mb_block_count - 6 ); <nl> s -> bdsp . clear_blocks ( s -> block [ 6 ]); <nl> }
AVCodec ff_wmv2_decoder = { <nl> wmv2_decode_end , <nl> ff_h263_decode_frame , <nl> CODEC_CAP_DRAW_HORIZ_BAND | CODEC_CAP_DR1 , <nl> - . max_lowres = 3 , <nl> . long_name = NULL_IF_CONFIG_SMALL (" Windows Media Video 8 "), <nl> . pix_fmts = ff_pixfmt_list_420 , <nl> };
static int set_string_binary ( void * obj , const AVOption * o , const char * val , uint <nl> len /= 2 ; <nl>  <nl> ptr = bin = av_malloc ( len ); <nl> + if (! ptr ) <nl> + return AVERROR ( ENOMEM ); <nl> while (* val ) { <nl> int a = hexchar2int (* val ++); <nl> int b = hexchar2int (* val ++);
int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl> slice_count = (* buf ++) + 1 ; <nl> slices_hdr = buf + 4 ; <nl> buf += 8 * slice_count ; <nl> + buf_size -= 1 + 8 * slice_count ; <nl> } else <nl> slice_count = avctx -> slice_count ; <nl>  <nl> int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl> if ( ( avctx -> skip_frame >= AVDISCARD_NONREF && si . type == AV_PICTURE_TYPE_B ) <nl> || ( avctx -> skip_frame >= AVDISCARD_NONKEY && si . type != AV_PICTURE_TYPE_I ) <nl> || avctx -> skip_frame >= AVDISCARD_ALL ) <nl> - return buf_size ; <nl> + return avpkt -> size ; <nl>  <nl> for ( i = 0 ; i < slice_count ; i ++){ <nl> int offset = get_slice_offset ( avctx , slices_hdr , i ); <nl> int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl> } <nl> s -> current_picture_ptr = NULL ; // so we can detect if frame_end wasnt called ( find some nicer solution ...) <nl> } <nl> - return buf_size ; <nl> + return avpkt -> size ; <nl> } <nl>  <nl> av_cold int ff_rv34_decode_end ( AVCodecContext * avctx )
void ff_spatial_idwt_slice2 ( DWTContext * d , int y ); <nl> ( b4 + ((- 2 *( b0 + b8 ) + 10 *( b1 + b7 ) - 25 *( b2 + b6 ) + 81 *( b3 + b5 ) + 128 ) >> 8 )) <nl>  <nl> # define COMPOSE_DAUB97iL1 ( b0 , b1 , b2 )\ <nl> - ( b1 - (( int )( 1817U *( b0 + b2 ) + 2048 ) >> 12 )) <nl> + ( b1 - (( int )( 1817 *( b0 + ( unsigned ) b2 ) + 2048 ) >> 12 )) <nl>  <nl> # define COMPOSE_DAUB97iH1 ( b0 , b1 , b2 )\ <nl> - ( b1 - (( int )( 113U *( b0 + b2 ) + 64 ) >> 7 )) <nl> + ( b1 - (( int )( 113 *( b0 + ( unsigned ) b2 ) + 64 ) >> 7 )) <nl>  <nl> # define COMPOSE_DAUB97iL0 ( b0 , b1 , b2 )\ <nl> - ( b1 + (( int )( 217U *( b0 + b2 ) + 2048 ) >> 12 )) <nl> + ( b1 + (( int )( 217 *( b0 + ( unsigned ) b2 ) + 2048 ) >> 12 )) <nl>  <nl> # define COMPOSE_DAUB97iH0 ( b0 , b1 , b2 )\ <nl> - ( b1 + (( int )( 6497U *( b0 + b2 ) + 2048 ) >> 12 )) <nl> + ( b1 + (( int )( 6497 *( b0 + ( unsigned ) b2 ) + 2048 ) >> 12 )) <nl>  <nl>  <nl> # endif /* AVCODEC_DWT_H */
int ff_lpc_calc_coefs ( LPCContext * s , <nl> LLSModel m [ 2 ]; <nl> double var [ MAX_LPC_ORDER + 1 ], av_uninit ( weight ); <nl>  <nl> + if ( lpc_passes <= 0 ) <nl> + lpc_passes = 2 ; <nl> + <nl> for ( pass = 0 ; pass < lpc_passes ; pass ++){ <nl> av_init_lls (& m [ pass & 1 ], max_order ); <nl> 
static void rv40_loop_filter ( RV34DecContext * r , int row ) <nl> * in addition to the coded ones because because they lie at the edge of <nl> * 8x8 block with different enough motion vectors <nl> */ <nl> - int mvmasks [ 4 ]; <nl> + unsigned mvmasks [ 4 ]; <nl>  <nl> mb_pos = row * s -> mb_stride ; <nl> for ( mb_x = 0 ; mb_x < s -> mb_width ; mb_x ++, mb_pos ++){ <nl> static void rv40_loop_filter ( RV34DecContext * r , int row ) <nl> int c_v_deblock [ 2 ], c_h_deblock [ 2 ]; <nl> int clip_left ; <nl> int avail [ 4 ]; <nl> - int y_to_deblock , c_to_deblock [ 2 ]; <nl> + unsigned y_to_deblock ; <nl> + int c_to_deblock [ 2 ]; <nl>  <nl> q = s -> current_picture_ptr -> f . qscale_table [ mb_pos ]; <nl> alpha = rv40_alpha_tab [ q ];
static int dnxhd_decode_header ( DNXHDContext * ctx , AVFrame * frame , <nl> if ( header_prefix == DNXHD_HEADER_HR2 ) { <nl> ctx -> data_offset = 0x170 + ( ctx -> mb_height << 2 ); <nl> } else { <nl> - if ( ctx -> mb_height > 68 || <nl> - ( ctx -> mb_height << frame -> interlaced_frame ) > ( ctx -> height + 15 ) >> 4 ) { <nl> + if ( ctx -> mb_height > 68 ) { <nl> av_log ( ctx -> avctx , AV_LOG_ERROR , <nl> " mb height too big : % d \ n ", ctx -> mb_height ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> ctx -> data_offset = 0x280 ; <nl> } <nl> + if (( ctx -> mb_height << frame -> interlaced_frame ) > ( ctx -> height + 15 ) >> 4 ) { <nl> + av_log ( ctx -> avctx , AV_LOG_ERROR , <nl> + " mb height too big : % d \ n ", ctx -> mb_height ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> if ( buf_size < ctx -> data_offset ) { <nl> av_log ( ctx -> avctx , AV_LOG_ERROR ,
static int dxtory_decode_v1_410 ( AVCodecContext * avctx , AVFrame * pic , <nl> V = pic -> data [ 2 ]; <nl> for ( h = 0 ; h < avctx -> height ; h += 4 ) { <nl> for ( w = 0 ; w < avctx -> width ; w += 4 ) { <nl> - AV_COPY32 ( Y1 + w , src ); <nl> - AV_COPY32 ( Y2 + w , src + 4 ); <nl> - AV_COPY32 ( Y3 + w , src + 8 ); <nl> - AV_COPY32 ( Y4 + w , src + 12 ); <nl> + AV_COPY32U ( Y1 + w , src ); <nl> + AV_COPY32U ( Y2 + w , src + 4 ); <nl> + AV_COPY32U ( Y3 + w , src + 8 ); <nl> + AV_COPY32U ( Y4 + w , src + 12 ); <nl> U [ w >> 2 ] = src [ 16 ] + 0x80 ; <nl> V [ w >> 2 ] = src [ 17 ] + 0x80 ; <nl> src += 18 ;
void img_resample_close ( ImgReSampleContext * s ) <nl> av_free ( s ); <nl> } <nl>  <nl> + static const AVClass context_class = { " imgresample ", NULL , NULL }; <nl> + <nl> struct SwsContext * sws_getContext ( int srcW , int srcH , int srcFormat , <nl> int dstW , int dstH , int dstFormat , <nl> int flags , SwsFilter * srcFilter , <nl> struct SwsContext * sws_getContext ( int srcW , int srcH , int srcFormat , <nl> struct SwsContext * ctx ; <nl>  <nl> ctx = av_malloc ( sizeof ( struct SwsContext )); <nl> - if ( ctx ) <nl> - ctx -> av_class = av_mallocz ( sizeof ( AVClass )); <nl> - if (! ctx || ! ctx -> av_class ) { <nl> + if (! ctx ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Cannot allocate a resampling context !\ n "); <nl>  <nl> return NULL ; <nl> } <nl> + ctx -> av_class = & context_class ; <nl>  <nl> if (( srcH != dstH ) || ( srcW != dstW )) { <nl> if (( srcFormat != PIX_FMT_YUV420P ) || ( dstFormat != PIX_FMT_YUV420P )) {
static int decode_pic ( AVSContext * h ) <nl> h -> scale_den [ 1 ] = h -> dist [ 1 ] ? 512 / h -> dist [ 1 ] : 0 ; <nl> if ( h -> cur . f -> pict_type == AV_PICTURE_TYPE_B ) { <nl> h -> sym_factor = h -> dist [ 0 ] * h -> scale_den [ 1 ]; <nl> + if ( FFABS ( h -> sym_factor ) > 32768 ) { <nl> + av_log ( h -> avctx , AV_LOG_ERROR , " sym_factor % d too large \ n ", h -> sym_factor ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } else { <nl> h -> direct_den [ 0 ] = h -> dist [ 0 ] ? 16384 / h -> dist [ 0 ] : 0 ; <nl> h -> direct_den [ 1 ] = h -> dist [ 1 ] ? 16384 / h -> dist [ 1 ] : 0 ;
static int filter_frame ( AVFilterLink * inlink , AVFrame * buf ) <nl> } <nl>  <nl> /* now wait for the next timestamp */ <nl> - if ( buf -> pts == AV_NOPTS_VALUE ) { <nl> + if ( buf -> pts == AV_NOPTS_VALUE || av_fifo_size ( s -> fifo ) <= 0 ) { <nl> return write_to_fifo ( s -> fifo , buf ); <nl> } <nl> 
static inline void decode_subband_slice_buffered ( SnowContext * s , SubBand * b , sli <nl> v = b -> x_coeff [ new_index ]. coeff ; <nl> x = b -> x_coeff [ new_index ++]. x ; <nl> while ( x < w ){ <nl> - register int t = ( ( v >> 1 )* qmul + qadd )>> QEXPSHIFT ; <nl> + register int t = ( int )( ( v >> 1 )*( unsigned ) qmul + qadd )>> QEXPSHIFT ; <nl> register int u = -( v & 1 ); <nl> line [ x ] = ( t ^ u ) - u ; <nl> 
resync : <nl> pkt -> data , pkt -> size ); <nl> pkt -> destruct = dstr ; <nl> pkt -> flags |= AV_PKT_FLAG_KEY ; <nl> + if ( size < 0 ) <nl> + av_free_packet ( pkt ); <nl> } else { <nl> /* XXX : How to handle B - frames in AVI ? */ <nl> pkt -> dts = ast -> frame_offset ;
static void rpza_decode_stream ( RpzaContext * s ) <nl> color4 [ 1 ] |= (( 11 * ta + 21 * tb ) >> 5 ); <nl> color4 [ 2 ] |= (( 21 * ta + 11 * tb ) >> 5 ); <nl>  <nl> + if ( s -> size - stream_ptr < n_blocks * 4 ) <nl> + return ; <nl> while ( n_blocks --) { <nl> block_ptr = row_ptr + pixel_ptr ; <nl> for ( pixel_y = 0 ; pixel_y < 4 ; pixel_y ++) { <nl> static void rpza_decode_stream ( RpzaContext * s ) <nl>  <nl> /* Fill block with 16 colors */ <nl> case 0x00 : <nl> + if ( s -> size - stream_ptr < 16 ) <nl> + return ; <nl> block_ptr = row_ptr + pixel_ptr ; <nl> for ( pixel_y = 0 ; pixel_y < 4 ; pixel_y ++) { <nl> for ( pixel_x = 0 ; pixel_x < 4 ; pixel_x ++){
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> if ( st -> nb_index_entries ) <nl> break ; <nl> } <nl> + // DV - in - AVI cannot be non - interleaved , if set this must be <nl> + // a mis - detection . <nl> + if ( avi -> dv_demux ) <nl> + avi -> non_interleaved = 0 ; <nl> if ( i == s -> nb_streams && avi -> non_interleaved ) { <nl> av_log ( s , AV_LOG_WARNING , " non - interleaved AVI without index , switching to interleaved \ n "); <nl> avi -> non_interleaved = 0 ;
static void fill_table ( uint8_t * table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int <nl> } <nl> } <nl>  <nl> - static void fill_gv_table ( int table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int elemsize , const int inc ) <nl> + static void fill_gv_table ( int table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int elemsize , const int64_t inc ) <nl> { <nl> int i ; <nl> int off = -( inc >> 9 );
static int ism_write_header ( AVFormatContext * s ) <nl> goto fail ; <nl> } <nl>  <nl> - c -> streams = av_mallocz ( sizeof (* c -> streams ) * s -> nb_streams ); <nl> + c -> streams = av_mallocz_array ( s -> nb_streams , sizeof (* c -> streams )); <nl> if (! c -> streams ) { <nl> ret = AVERROR ( ENOMEM ); <nl> goto fail ;
void * av_malloc ( size_t size ) <nl> long diff ; <nl> # endif <nl>  <nl> + assert ( size ); <nl> + <nl> /* let ' s disallow possible ambiguous cases */ <nl> - if ( size > ( INT_MAX - 32 ) ) <nl> + if ( size > ( INT_MAX - 32 ) || ! size ) <nl> return NULL ; <nl>  <nl> # if CONFIG_MEMALIGN_HACK
static int vdpau_hevc_start_frame ( AVCodecContext * avctx , <nl> const HEVCFrame * frame = & h -> DPB [ i ]; <nl> if ( frame != h -> ref && ( frame -> flags & ( HEVC_FRAME_FLAG_LONG_REF | <nl> HEVC_FRAME_FLAG_SHORT_REF ))) { <nl> - if ( j > 16 ) { <nl> + if ( j > 15 ) { <nl> av_log ( avctx , AV_LOG_WARNING , <nl> " VDPAU only supports up to 16 references in the DPB . " <nl> " This frame may not be decoded correctly .\ n ");
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl>  <nl> s -> slices [ i ][ j ]. start = offset + header_size ; <nl> - s -> slices [ i ][ j ]. size = avpkt -> size - offset ; <nl> + s -> slices [ i ][ j ]. size = avpkt -> size - s -> slices [ i ][ j ]. start ; <nl> } <nl>  <nl> if ( bytestream2_get_byte (& gb ) != s -> planes )
static int cdxl_decode_frame ( AVCodecContext * avctx , void * data , <nl> c -> padded_bits = aligned_width - c -> avctx -> width ; <nl> if ( c -> video_size < aligned_width * avctx -> height * ( int64_t ) c -> bpp / 8 ) <nl> return AVERROR_INVALIDDATA ; <nl> - if (! encoding && c -> palette_size && c -> bpp <= 8 ) { <nl> + if (! encoding && c -> palette_size && c -> bpp <= 8 && c -> format != CHUNKY ) { <nl> avctx -> pix_fmt = AV_PIX_FMT_PAL8 ; <nl> } else if ( encoding == 1 && ( c -> bpp == 6 || c -> bpp == 8 )) { <nl> if ( c -> palette_size != ( 1 << ( c -> bpp - 1 )))
static int old_codec37 ( SANMVideoContext * ctx , int top , <nl> flags = bytestream2_get_byte (& ctx -> gb ); <nl> bytestream2_skip (& ctx -> gb , 3 ); <nl>  <nl> - if ( decoded_size > height * stride - left - top * stride ) { <nl> - decoded_size = height * stride - left - top * stride ; <nl> + if ( decoded_size > ctx -> height * stride - left - top * stride ) { <nl> + decoded_size = ctx -> height * stride - left - top * stride ; <nl> av_log ( ctx -> avctx , AV_LOG_WARNING , " decoded size is too large \ n "); <nl> } <nl>  <nl> static int old_codec47 ( SANMVideoContext * ctx , int top , <nl> decoded_size = bytestream2_get_le32 (& ctx -> gb ); <nl> bytestream2_skip (& ctx -> gb , 8 ); <nl>  <nl> - if ( decoded_size > height * stride - left - top * stride ) { <nl> - decoded_size = height * stride - left - top * stride ; <nl> + if ( decoded_size > ctx -> height * stride - left - top * stride ) { <nl> + decoded_size = ctx -> height * stride - left - top * stride ; <nl> av_log ( ctx -> avctx , AV_LOG_WARNING , " decoded size is too large \ n "); <nl> } <nl> 
static int fic_decode_frame ( AVCodecContext * avctx , void * data , <nl> av_log ( avctx , AV_LOG_WARNING , " Invalid FIC Header .\ n "); <nl>  <nl> /* Is it a skip frame ? */ <nl> - if ( src [ 17 ]) <nl> + if ( src [ 17 ]) { <nl> + if (! ctx -> final_frame ) { <nl> + av_log ( avctx , AV_LOG_WARNING , " Initial frame is skipped \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> goto skip ; <nl> + } <nl>  <nl> nslices = src [ 13 ]; <nl> if (! nslices ) {
static int decode_frame ( AVCodecContext * avctx , <nl> AVFrame * p = data ; <nl> GetByteContext gb ; <nl> GetBitContext b ; <nl> - int i , j , k ; <nl> + int i , j , k , width , height ; <nl>  <nl> bytestream2_init (& gb , avpkt -> data , avpkt -> size ); <nl> if ( bytestream2_get_le32 (& gb ) != MKTAG (' M ',' A ',' G ',' Y ')) <nl> static int decode_frame ( AVCodecContext * avctx , <nl> s -> interlaced = !!( bytestream2_get_byte (& gb ) & 2 ); <nl> bytestream2_skip (& gb , 3 ); <nl>  <nl> - if (( ret = ff_set_dimensions ( avctx , bytestream2_get_le32 (& gb ), bytestream2_get_le32 (& gb ))) < 0 ) <nl> + width = bytestream2_get_le32 (& gb ); <nl> + height = bytestream2_get_le32 (& gb ); <nl> + if (( ret = ff_set_dimensions ( avctx , width , height )) < 0 ) <nl> return ret ; <nl>  <nl> slice_width = bytestream2_get_le32 (& gb );
static void do_subtitle_out ( AVFormatContext * s , <nl> if ( output_files [ ost -> file_index ]-> start_time != AV_NOPTS_VALUE ) <nl> pts -= output_files [ ost -> file_index ]-> start_time ; <nl> for ( i = 0 ; i < nb ; i ++) { <nl> + unsigned save_num_rects = sub -> num_rects ; <nl> + <nl> ost -> sync_opts = av_rescale_q ( pts , AV_TIME_BASE_Q , enc -> time_base ); <nl> if (! check_recording_time ( ost )) <nl> return ; <nl> static void do_subtitle_out ( AVFormatContext * s , <nl>  <nl> subtitle_out_size = avcodec_encode_subtitle ( enc , subtitle_out , <nl> subtitle_out_max_size , sub ); <nl> + if ( i == 1 ) <nl> + sub -> num_rects = save_num_rects ; <nl> if ( subtitle_out_size < 0 ) { <nl> av_log ( NULL , AV_LOG_FATAL , " Subtitle encoding failed \ n "); <nl> exit_program ( 1 );
static int init_input_threads ( void ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> - if (( ret = pthread_create (& f -> thread , NULL , input_thread , f ))) <nl> + if (( ret = pthread_create (& f -> thread , NULL , input_thread , f ))) { <nl> + av_log ( NULL , AV_LOG_ERROR , " pthread_create failed : % s . Try to increase ` ulimit - v ` or decrease ` ulimit - s `.\ n ", strerror ( ret )); <nl> + av_thread_message_queue_free (& f -> in_thread_queue ); <nl> return AVERROR ( ret ); <nl> + } <nl> } <nl> return 0 ; <nl> }
int64_t av_gen_search ( AVFormatContext * s , int stream_index , int64_t target_ts , i <nl> # ifdef DEBUG_SEEK <nl> av_log ( s , AV_LOG_DEBUG , "%" PRId64 " %" PRId64 " %" PRId64 " / %" PRId64 " %" PRId64 " %" PRId64 " target :%" PRId64 " limit :%" PRId64 " start :%" PRId64 " noc :% d \ n ", pos_min , pos , pos_max , ts_min , ts , ts_max , target_ts , pos_limit , start_pos , no_change ); <nl> # endif <nl> + if ( ts == AV_NOPTS_VALUE ){ <nl> + av_log ( s , AV_LOG_ERROR , " read_timestamp () failed in the middle \ n "); <nl> + return - 1 ; <nl> + } <nl> assert ( ts != AV_NOPTS_VALUE ); <nl> if ( target_ts <= ts ) { <nl> pos_limit = start_pos - 1 ;
static int find_image_range ( int * pfirst_index , int * plast_index , <nl> if ( avio_check ( buf , AVIO_FLAG_READ ) > 0 ) <nl> break ; <nl> } <nl> - if ( first_index == 5 ) <nl> + if ( first_index == start_index + 5 ) <nl> goto fail ; <nl>  <nl> /* find the last image */
static int xbm_decode_frame ( AVCodecContext * avctx , void * data , <nl> int number , len ; <nl>  <nl> ptr += strcspn ( ptr , "#"); <nl> - if ( sscanf ( ptr , "# define % 256s % u ", name , & number ) != 2 ) { <nl> + if ( sscanf ( ptr , "# define % 255s % u ", name , & number ) != 2 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Unexpected preprocessor directive \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static av_cold void uninit ( AVFilterContext * ctx ) <nl>  <nl> av_expr_free ( s -> x_pexpr ); <nl> av_expr_free ( s -> y_pexpr ); <nl> - s -> x_pexpr = s -> y_pexpr = NULL ; <nl> + av_expr_free ( s -> a_pexpr ); <nl> + s -> x_pexpr = s -> y_pexpr = s -> a_pexpr = NULL ; <nl> av_freep (& s -> positions ); <nl> s -> nb_positions = 0 ; <nl>  <nl> static int config_input ( AVFilterLink * inlink ) <nl>  <nl> av_expr_free ( s -> x_pexpr ); <nl> av_expr_free ( s -> y_pexpr ); <nl> - s -> x_pexpr = s -> y_pexpr = NULL ; <nl> + av_expr_free ( s -> a_pexpr ); <nl> + s -> x_pexpr = s -> y_pexpr = s -> a_pexpr = NULL ; <nl>  <nl> if (( ret = av_expr_parse (& s -> x_pexpr , s -> x_expr , var_names , <nl> NULL , NULL , fun2_names , fun2 , 0 , ctx )) < 0 ||
static int decode_ref_pic_marking ( H264Context * h , GetBitContext * gb ){ <nl> } else { <nl> assert ( h -> long_ref_count + h -> short_ref_count <= h -> sps . ref_frame_count ); <nl>  <nl> - if ( h -> long_ref_count + h -> short_ref_count == h -> sps . ref_frame_count && <nl> + if ( h -> short_ref_count && h -> long_ref_count + h -> short_ref_count == h -> sps . ref_frame_count && <nl> !( FIELD_PICTURE && ! s -> first_field && s -> current_picture_ptr -> reference )) { <nl> h -> mmco [ 0 ]. opcode = MMCO_SHORT2UNUSED ; <nl> h -> mmco [ 0 ]. short_pic_num = h -> short_ref [ h -> short_ref_count - 1 ]-> frame_num ;
static int yop_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> s -> low_nibble = NULL ; <nl>  <nl> is_odd_frame = avpkt -> data [ 0 ]; <nl> + if ( is_odd_frame > 1 ){ <nl> + av_log ( avctx , AV_LOG_ERROR , " frame is too odd % d \ n ", is_odd_frame ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> firstcolor = s -> first_color [ is_odd_frame ]; <nl> palette = ( uint32_t *) s -> frame . data [ 1 ]; <nl> 
static int xvid_strip_vol_header ( AVCodecContext * avctx , AVPacket * pkt , <nl> /* We need to store the header , so extract it */ <nl> if (! avctx -> extradata ) { <nl> avctx -> extradata = av_malloc ( vo_len ); <nl> + if (! avctx -> extradata ) <nl> + return AVERROR ( ENOMEM ); <nl> memcpy ( avctx -> extradata , pkt -> data , vo_len ); <nl> avctx -> extradata_size = vo_len ; <nl> } <nl> static av_cold int xvid_encode_init ( AVCodecContext * avctx ) <nl> if ( avctx -> intra_matrix ) { <nl> intra = avctx -> intra_matrix ; <nl> x -> intra_matrix = av_malloc ( sizeof ( unsigned char ) * 64 ); <nl> + if (! x -> intra_matrix ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto fail ; <nl> + } <nl> } else <nl> intra = NULL ; <nl> if ( avctx -> inter_matrix ) { <nl> inter = avctx -> inter_matrix ; <nl> x -> inter_matrix = av_malloc ( sizeof ( unsigned char ) * 64 ); <nl> + if (! x -> inter_matrix ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto fail ; <nl> + } <nl> } else <nl> inter = NULL ; <nl> 
static int crypto_open ( URLContext * h , const char * uri , int flags ) <nl>  <nl> return 0 ; <nl> err : <nl> - av_free ( c -> key ); <nl> - av_free ( c -> iv ); <nl> + av_freep (& c -> key ); <nl> + av_freep (& c -> iv ); <nl> return ret ; <nl> } <nl> 
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> least one frame of codec data , this makes sure the codec initializes <nl> the channel configuration and does not only trust the values from the container . <nl> */ <nl> - try_decode_frame ( st , pkt , ( options && i <= orig_nb_streams )? & options [ i ] : NULL ); <nl> + try_decode_frame ( st , pkt , ( options && i < orig_nb_streams )? & options [ i ] : NULL ); <nl>  <nl> st -> codec_info_nb_frames ++; <nl> count ++;
static int mov_read_default ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> left = a . size - avio_tell ( pb ) + start_pos ; <nl> if ( left > 0 ) /* skip garbage at atom end */ <nl> avio_skip ( pb , left ); <nl> + else if ( left < 0 ) { <nl> + av_log ( c -> fc , AV_LOG_DEBUG , " undoing overread of % d in '%. 4s '\ n ", - left , ( char *)& a . type ); <nl> + avio_seek ( pb , left , SEEK_CUR ); <nl> + } <nl> } <nl>  <nl> total_size += a . size ;
static int g2m_init_buffers ( G2MContext * c ) <nl> aligned_height = FFALIGN ( c -> tile_height , 16 ); <nl> av_free ( c -> synth_tile ); <nl> av_free ( c -> jpeg_tile ); <nl> + av_free ( c -> kempf_buf ); <nl> + av_free ( c -> kempf_flags ); <nl> c -> synth_tile = av_mallocz ( c -> tile_stride * aligned_height ); <nl> c -> jpeg_tile = av_mallocz ( c -> tile_stride * aligned_height ); <nl> c -> kempf_buf = av_mallocz (( c -> tile_width + 1 ) * aligned_height
int configure_filtergraph ( FilterGraph * fg ) <nl> return ret ; <nl> avfilter_inout_free (& inputs ); <nl>  <nl> - for ( cur = outputs , i = 0 ; cur ; cur = cur -> next , i ++) <nl> - configure_output_filter ( fg , fg -> outputs [ i ], cur ); <nl> + for ( cur = outputs , i = 0 ; cur ; cur = cur -> next , i ++) { <nl> + OutputFilter * ofilter = fg -> outputs [ i ]; <nl> + if ( ofilter -> ost ) <nl> + configure_output_filter ( fg , ofilter , cur ); <nl> + } <nl> + <nl> avfilter_inout_free (& outputs ); <nl>  <nl> if (( ret = avfilter_graph_config ( fg -> graph , NULL )) < 0 )
static int decode_frame ( AVCodecContext * avctx , <nl> * pal ++ = ( b << 16 ) | ( g << 8 ) | r ; <nl> } <nl> p -> palette_has_changed = 1 ; <nl> - avctx -> palctrl -> palette_changed = 0 ; <nl> } <nl> } <nl> if (( compr & (~ TGA_RLE )) == TGA_NODATA )
static inline void comp_block ( MadContext * t , int mb_x , int mb_y , <nl> { <nl> MpegEncContext * s = & t -> s ; <nl> if ( j < 4 ) { <nl> + unsigned offset = ( mb_y * 16 + (( j & 2 )<< 2 ) + mv_y )* t -> last_frame . linesize [ 0 ] + mb_x * 16 + (( j & 1 )<< 3 ) + mv_x ; <nl> + if ( offset >= ( s -> height - 7 ) * t -> last_frame . linesize [ 0 ] - 7 ) <nl> + return ; <nl> comp ( t -> frame . data [ 0 ] + ( mb_y * 16 + (( j & 2 )<< 2 ))* t -> frame . linesize [ 0 ] + mb_x * 16 + (( j & 1 )<< 3 ), <nl> t -> frame . linesize [ 0 ], <nl> - t -> last_frame . data [ 0 ] + ( mb_y * 16 + (( j & 2 )<< 2 ) + mv_y )* t -> last_frame . linesize [ 0 ] + mb_x * 16 + (( j & 1 )<< 3 ) + mv_x , <nl> + t -> last_frame . data [ 0 ] + offset , <nl> t -> last_frame . linesize [ 0 ], add ); <nl> } else if (!( s -> avctx -> flags & CODEC_FLAG_GRAY )) { <nl> int index = j - 3 ; <nl> + unsigned offset = ( mb_y * 8 + ( mv_y / 2 ))* t -> last_frame . linesize [ index ] + mb_x * 8 + ( mv_x / 2 ); <nl> + if ( offset >= ( s -> height / 2 - 7 ) * t -> last_frame . linesize [ index ] - 7 ) <nl> + return ; <nl> comp ( t -> frame . data [ index ] + ( mb_y * 8 )* t -> frame . linesize [ index ] + mb_x * 8 , <nl> t -> frame . linesize [ index ], <nl> - t -> last_frame . data [ index ] + ( mb_y * 8 + ( mv_y / 2 ))* t -> last_frame . linesize [ index ] + mb_x * 8 + ( mv_x / 2 ), <nl> + t -> last_frame . data [ index ] + offset , <nl> t -> last_frame . linesize [ index ], add ); <nl> } <nl> }
static int cbs_h264_read_nal_unit ( CodedBitstreamContext * ctx , <nl> err = cbs_h264_read_sei ( ctx , & gbc , sei ); <nl> if ( err < 0 ) { <nl> cbs_h264_free_sei ( sei ); <nl> + av_free ( sei ); <nl> return err ; <nl> } <nl> 
static int decode_iccp_chunk ( PNGDecContext * s , int length , AVFrame * f ) <nl>  <nl> av_bprint_finalize (& bp , ( char **)& data ); <nl>  <nl> - if (!( sd = av_frame_new_side_data ( f , AV_FRAME_DATA_ICC_PROFILE , bp . len ))) <nl> + sd = av_frame_new_side_data ( f , AV_FRAME_DATA_ICC_PROFILE , bp . len ); <nl> + if (! sd ) { <nl> + av_free ( data ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl>  <nl> av_dict_set (& sd -> metadata , " name ", profile_name , 0 ); <nl> memcpy ( sd -> data , data , bp . len );
AVInputFormat ff_ivr_demuxer = { <nl> . read_probe = ivr_probe , <nl> . read_header = ivr_read_header , <nl> . read_packet = ivr_read_packet , <nl> + . read_close = rm_read_close , <nl> . extensions = " ivr ", <nl> };
static int decode_lt_rps ( HEVCContext * s , LongTermRPS * rps , GetBitContext * gb ) <nl> nb_sps = get_ue_golomb_long ( gb ); <nl> nb_sh = get_ue_golomb_long ( gb ); <nl>  <nl> - if ( nb_sh + nb_sps > FF_ARRAY_ELEMS ( rps -> poc )) <nl> + if ( nb_sh + ( uint64_t ) nb_sps > FF_ARRAY_ELEMS ( rps -> poc )) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> rps -> nb_refs = nb_sh + nb_sps ;
static int libschroedinger_decode_frame ( AVCodecContext * avctx , <nl> /* Grab next frame to be returned from the top of the queue . */ <nl> framewithpts = ff_schro_queue_pop (& p_schro_params -> dec_frame_queue ); <nl>  <nl> - if ( framewithpts && framewithpts -> frame ) { <nl> + if ( framewithpts && framewithpts -> frame && framewithpts -> frame -> components [ 0 ]. stride ) { <nl> if ( ff_get_buffer ( avctx , avframe , 0 ) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Unable to allocate buffer \ n "); <nl> return AVERROR ( ENOMEM );
AVInputFormat * av_probe_input_format3 ( AVProbeData * pd , int is_opened , int * score <nl> AVProbeData lpd = * pd ; <nl> AVInputFormat * fmt1 = NULL , * fmt ; <nl> int score , nodat = 0 , score_max = 0 ; <nl> + const static uint8_t zerobuffer [ AVPROBE_PADDING_SIZE ]; <nl> + <nl> + if (! lpd . buf ) <nl> + lpd . buf = zerobuffer ; <nl>  <nl> if ( lpd . buf_size > 10 && ff_id3v2_match ( lpd . buf , ID3v2_DEFAULT_MAGIC )) { <nl> int id3len = ff_id3v2_tag_len ( lpd . buf );
static int decode_5 ( SANMVideoContext * ctx ) <nl> # if HAVE_BIGENDIAN <nl> npixels = ctx -> npixels ; <nl> frm = ctx -> frm0 ; <nl> - while ( npixels --) <nl> - * frm ++ = av_bswap16 (* frm ); <nl> + while ( npixels --) { <nl> + * frm = av_bswap16 (* frm ); <nl> + frm ++; <nl> + } <nl> # endif <nl>  <nl> return 0 ;
static int decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( buf_size < 20 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> + if ( avctx -> width % 16 || avctx -> height % 16 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " Dimensions non - multiple of 16 are invalid .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( buf_size < AV_RL32 ( buf + 4 ) + 8 ) { <nl> av_log ( f -> avctx , AV_LOG_ERROR , " size mismatch % d % d \ n ", <nl> buf_size , AV_RL32 ( buf + 4 ));
static int get_cod ( Jpeg2000DecoderContext * s , Jpeg2000CodingStyle * c , <nl> tmp . nlayers = bytestream2_get_be16u (& s -> g ); <nl> tmp . mct = bytestream2_get_byteu (& s -> g ); // multiple component transformation <nl>  <nl> + if ( tmp . mct && s -> ncomponents < 3 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , <nl> + " MCT % d with too few components (% d )\ n ", <nl> + tmp . mct , s -> ncomponents ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (( ret = get_cox ( s , & tmp )) < 0 ) <nl> return ret ; <nl> 
static void update_stream_timings ( AVFormatContext * ic ) <nl> if ( duration != INT64_MIN && duration > 0 && ic -> duration == AV_NOPTS_VALUE ) { <nl> ic -> duration = duration ; <nl> } <nl> - if ( ic -> pb && ( filesize = avio_size ( ic -> pb )) > 0 && ic -> duration != AV_NOPTS_VALUE ) { <nl> + if ( ic -> pb && ( filesize = avio_size ( ic -> pb )) > 0 && ic -> duration > 0 ) { <nl> /* compute the bitrate */ <nl> double bitrate = ( double ) filesize * 8 . 0 * AV_TIME_BASE / <nl> ( double ) ic -> duration ;
static int decode_frame ( AVCodecContext * avctx , <nl>  <nl> avctx -> pix_fmt = AV_PIX_FMT_PAL8 ; <nl>  <nl> + if ( av_image_check_size ( s -> width , s -> height , 0 , avctx ) < 0 ) <nl> + return - 1 ; <nl> if ( s -> width != avctx -> width && s -> height != avctx -> height ) { <nl> - if ( av_image_check_size ( s -> width , s -> height , 0 , avctx ) < 0 ) <nl> - return - 1 ; <nl> avcodec_set_dimensions ( avctx , s -> width , s -> height ); <nl> } <nl> 
WINDOW_FUNC ( long_start ) <nl> float * out = sce -> ret ; <nl>  <nl> dsp -> vector_fmul ( out , audio , lwindow , 1024 ); <nl> - memcpy ( out + 1024 , audio , sizeof ( out [ 0 ]) * 448 ); <nl> - dsp -> vector_fmul_reverse ( out + 1024 + 448 , audio , swindow , 128 ); <nl> + memcpy ( out + 1024 , audio + 1024 , sizeof ( out [ 0 ]) * 448 ); <nl> + dsp -> vector_fmul_reverse ( out + 1024 + 448 , audio + 1024 + 448 , swindow , 128 ); <nl> memset ( out + 1024 + 576 , 0 , sizeof ( out [ 0 ]) * 448 ); <nl> } <nl> 
static void to_meta_with_crop ( AVCodecContext * avctx , AVFrame * p , int * dest ) <nl> for ( y = blocky ; y < blocky + 8 && y < C64YRES ; y ++) { <nl> for ( x = blockx ; x < blockx + 8 && x < C64XRES ; x += 2 ) { <nl> if ( x < width && y < height ) { <nl> - /* build average over 2 pixels */ <nl> - luma = ( src [( x + 0 + y * p -> linesize [ 0 ])] + <nl> - src [( x + 1 + y * p -> linesize [ 0 ])]) / 2 ; <nl> + if ( x + 1 < width ) { <nl> + /* build average over 2 pixels */ <nl> + luma = ( src [( x + 0 + y * p -> linesize [ 0 ])] + <nl> + src [( x + 1 + y * p -> linesize [ 0 ])]) / 2 ; <nl> + } else { <nl> + luma = src [( x + y * p -> linesize [ 0 ])]; <nl> + } <nl> /* write blocks as linear data now so they are suitable for elbg */ <nl> dest [ 0 ] = luma ; <nl> }
int sws_init_context ( SwsContext * c , SwsFilter * srcFilter , SwsFilter * dstFilter ) <nl> c -> vLumBufSize = c -> vLumFilterSize ; <nl> c -> vChrBufSize = c -> vChrFilterSize ; <nl> for ( i = 0 ; i < dstH ; i ++) { <nl> - int chrI = i * c -> chrDstH / dstH ; <nl> + int chrI = ( int64_t ) i * c -> chrDstH / dstH ; <nl> int nextSlice = FFMAX ( c -> vLumFilterPos [ i ] + c -> vLumFilterSize - 1 , <nl> (( c -> vChrFilterPos [ chrI ] + c -> vChrFilterSize - 1 )<< c -> chrSrcVSubSample )); <nl> 
static int set_sps ( HEVCContext * s , const HEVCSPS * sps ) <nl> enum AVPixelFormat pix_fmts [ HWACCEL_MAX + 2 ], * fmt = pix_fmts ; <nl> int ret ; <nl>  <nl> - export_stream_params ( s -> avctx , & s -> ps , sps ); <nl> - <nl> pic_arrays_free ( s ); <nl> + s -> ps . sps = NULL ; <nl> + s -> ps . vps = NULL ; <nl> + <nl> + if (! sps ) <nl> + return 0 ; <nl> + <nl> ret = pic_arrays_init ( s , sps ); <nl> if ( ret < 0 ) <nl> goto fail ; <nl>  <nl> + export_stream_params ( s -> avctx , & s -> ps , sps ); <nl> + <nl> if ( sps -> pix_fmt == AV_PIX_FMT_YUV420P || sps -> pix_fmt == AV_PIX_FMT_YUVJ420P ) { <nl> # if CONFIG_HEVC_DXVA2_HWACCEL <nl> * fmt ++ = AV_PIX_FMT_DXVA2_VLD ;
int av_seek_frame_binary ( AVFormatContext * s , int stream_index , int64_t target_ts <nl> pos_limit = pos_max ; <nl> } <nl>  <nl> + if ( ts_min > ts_max ){ <nl> + return - 1 ; <nl> + } else if ( ts_min == ts_max ){ <nl> + pos_limit = pos_min ; <nl> + } <nl> + <nl> no_change = 0 ; <nl> while ( pos_min < pos_limit ) { <nl> # ifdef DEBUG_SEEK
static void set_disposition_bits ( AVFormatContext * avf , char * value , int stream_i <nl> static int decode_info_header ( NUTContext * nut ){ <nl> AVFormatContext * s = nut -> avf ; <nl> ByteIOContext * bc = s -> pb ; <nl> - uint64_t tmp ; <nl> - unsigned int stream_id_plus1 , chapter_start , chapter_len , count ; <nl> + uint64_t tmp , chapter_start , chapter_len ; <nl> + unsigned int stream_id_plus1 , count ; <nl> int chapter_id , i ; <nl> int64_t value , end ; <nl> char name [ 256 ], str_value [ 1024 ], type_str [ 256 ];
static void init_input_filter ( FilterGraph * fg , AVFilterInOut * in ) <nl> char * p ; <nl> int file_idx = strtol ( in -> name , & p , 0 ); <nl>  <nl> - if ( file_idx < 0 || file_idx > nb_input_files ) { <nl> + if ( file_idx < 0 || file_idx >= nb_input_files ) { <nl> av_log ( NULL , AV_LOG_FATAL , " Invalid file index % d in filtegraph description % s .\ n ", <nl> file_idx , fg -> graph_desc ); <nl> exit_program ( 1 );
static void set_frame_data ( MIContext * mi_ctx , int alpha , AVFrame * avf_out ) <nl> for ( i = 0 ; i < pixel -> nb ; i ++) { <nl> Frame * frame = & mi_ctx -> frames [ pixel -> refs [ i ]]; <nl> if ( chroma ) { <nl> - x_mv = ( x >> mi_ctx -> chroma_h_shift ) + ( pixel -> mvs [ i ][ 0 ] >> mi_ctx -> chroma_h_shift ); <nl> - y_mv = ( y >> mi_ctx -> chroma_v_shift ) + ( pixel -> mvs [ i ][ 1 ] >> mi_ctx -> chroma_v_shift ); <nl> + x_mv = ( x >> mi_ctx -> chroma_h_shift ) + ( pixel -> mvs [ i ][ 0 ] / ( 1 << mi_ctx -> chroma_h_shift )); <nl> + y_mv = ( y >> mi_ctx -> chroma_v_shift ) + ( pixel -> mvs [ i ][ 1 ] / ( 1 << mi_ctx -> chroma_v_shift )); <nl> } else { <nl> x_mv = x + pixel -> mvs [ i ][ 0 ]; <nl> y_mv = y + pixel -> mvs [ i ][ 1 ];
static void implicit_weight_table ( H264Context * h , int field ){ <nl> static void idr ( H264Context * h ){ <nl> int i ; <nl> ff_h264_remove_all_refs ( h ); <nl> - h -> prev_frame_num = 0 ; <nl> + h -> prev_frame_num = - 1 ; <nl> h -> prev_frame_num_offset = 0 ; <nl> h -> prev_poc_msb = <nl> h -> prev_poc_lsb = 0 ; <nl> static int decode_slice_header ( H264Context * h , H264Context * h0 ){ <nl>  <nl> if ( h0 -> current_slice == 0 ){ <nl> // Shorten frame num gaps so we don ' t have to allocate reference frames just to throw them away <nl> - if ( h -> frame_num != h -> prev_frame_num ) { <nl> + if ( h -> frame_num != h -> prev_frame_num && h -> prev_frame_num >= 0 ) { <nl> int unwrap_prev_frame_num = h -> prev_frame_num , max_frame_num = 1 << h -> sps . log2_max_frame_num ; <nl>  <nl> if ( unwrap_prev_frame_num > h -> frame_num ) unwrap_prev_frame_num -= max_frame_num ; <nl> static int decode_slice_header ( H264Context * h , H264Context * h0 ){ <nl> } <nl> } <nl>  <nl> - while ( h -> frame_num != h -> prev_frame_num && <nl> + while ( h -> frame_num != h -> prev_frame_num && h -> prev_frame_num >= 0 && <nl> h -> frame_num != ( h -> prev_frame_num + 1 )%( 1 << h -> sps . log2_max_frame_num )){ <nl> Picture * prev = h -> short_ref_count ? h -> short_ref [ 0 ] : NULL ; <nl> av_log ( h -> s . avctx , AV_LOG_DEBUG , " Frame num gap % d % d \ n ", h -> frame_num , h -> prev_frame_num );
void avcodec_free_context ( AVCodecContext ** pavctx ) <nl>  <nl> av_freep (& avctx -> extradata ); <nl> av_freep (& avctx -> subtitle_header ); <nl> + av_freep (& avctx -> intra_matrix ); <nl> + av_freep (& avctx -> inter_matrix ); <nl> + av_freep (& avctx -> rc_override ); <nl>  <nl> av_freep ( pavctx ); <nl> }
static int rtmp_open ( URLContext * s , const char * uri , int flags ) <nl> fail : <nl> if ( filename != s -> filename ) <nl> av_freep (& filename ); <nl> + if ( rc ) <nl> + RTMP_Close ( r ); <nl> + <nl> return rc ; <nl> } <nl> 
static int dvbsub_decode ( AVCodecContext * avctx , <nl>  <nl> # endif <nl>  <nl> - if ( buf_size <= 2 ) <nl> + if ( buf_size <= 2 || * buf != 0x0f ) <nl> return - 1 ; <nl>  <nl> p = buf ; <nl> static int dvbsub_decode ( AVCodecContext * avctx , <nl> p += segment_length ; <nl> } <nl>  <nl> - if ( p != p_end ) { <nl> - av_dlog ( avctx , " Junk at end of packet \ n "); <nl> - return - 1 ; <nl> - } <nl> - <nl> - return buf_size ; <nl> + return p - buf ; <nl> } <nl>  <nl> 
static inline int parse_command_line ( AVFormatContext * s , const char * line , <nl> RTSPState * rt = s -> priv_data ; <nl> const char * linept , * searchlinept ; <nl> linept = strchr ( line , ' '); <nl> + <nl> + if (! linept ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( linept - line > methodsize - 1 ) { <nl> av_log ( s , AV_LOG_ERROR , " Method string too long \ n "); <nl> return AVERROR ( EIO );
static int film_read_packet ( AVFormatContext * s , <nl> av_free ( film -> stereo_buffer ); <nl> film -> stereo_buffer_size = sample -> sample_size ; <nl> film -> stereo_buffer = av_malloc ( film -> stereo_buffer_size ); <nl> + if (! film -> stereo_buffer ) { <nl> + film -> stereo_buffer_size = 0 ; <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> } <nl>  <nl> pkt -> pos = avio_tell ( pb );
# define ASSERT_PTHREAD_NORET ( func , ...) do { \ <nl> int ret = func ( __VA_ARGS__ ); \ <nl> if ( ret ) { \ <nl> + char errbuf [ AV_ERROR_MAX_STRING_SIZE ] = ""; \ <nl> av_log ( NULL , AV_LOG_FATAL , AV_STRINGIFY ( func ) \ <nl> - " failed with error : % s \ n ", av_err2str ( AVERROR ( ret ))); \ <nl> + " failed with error : % s \ n ", \ <nl> + av_make_error_string ( errbuf , AV_ERROR_MAX_STRING_SIZE , \ <nl> + AVERROR ( ret ))); \ <nl> abort (); \ <nl> } \ <nl> } while ( 0 )
void avfilter_start_frame ( AVFilterLink * link , AVFilterPicRef * picref ) <nl> link -> cur_pic = avfilter_default_get_video_buffer ( link , dst -> min_perms ); <nl> link -> srcpic = picref ; <nl> link -> cur_pic -> pts = link -> srcpic -> pts ; <nl> + link -> cur_pic -> pixel_aspect = link -> srcpic -> pixel_aspect ; <nl> } <nl> else <nl> link -> cur_pic = picref ;
static int process_ipmovie_chunk ( IPMVEContext * s , AVIOContext * pb , <nl> av_dlog ( NULL , " set palette \ n "); <nl> /* check for the logical maximum palette size <nl> * ( 3 * 256 + 4 bytes ) */ <nl> - if ( opcode_size > 0x304 ) { <nl> - av_dlog ( NULL , " demux_ipmovie : set_palette opcode too large \ n "); <nl> + if ( opcode_size > 0x304 || opcode_size < 4 ) { <nl> + av_dlog ( NULL , " demux_ipmovie : set_palette opcode with invalid size \ n "); <nl> chunk_type = CHUNK_BAD ; <nl> break ; <nl> }
static int mpegts_check_bitstream ( struct AVFormatContext * s , const AVPacket * pkt <nl>  <nl> if ( st -> codecpar -> codec_id == AV_CODEC_ID_H264 ) { <nl> if ( pkt -> size >= 5 && AV_RB32 ( pkt -> data ) != 0x0000001 && <nl> - AV_RB24 ( pkt -> data ) != 0x000001 ) <nl> + ( AV_RB24 ( pkt -> data ) != 0x000001 || <nl> + ( st -> codecpar -> extradata_size > 0 && <nl> + st -> codecpar -> extradata [ 0 ] == 1 ))) <nl> ret = ff_stream_add_bitstream_filter ( st , " h264_mp4toannexb ", NULL ); <nl> } else if ( st -> codecpar -> codec_id == AV_CODEC_ID_HEVC ) { <nl> if ( pkt -> size >= 5 && AV_RB32 ( pkt -> data ) != 0x0000001 && <nl> - AV_RB24 ( pkt -> data ) != 0x000001 ) <nl> + ( AV_RB24 ( pkt -> data ) != 0x000001 || <nl> + ( st -> codecpar -> extradata_size > 0 && <nl> + st -> codecpar -> extradata [ 0 ] == 1 ))) <nl> ret = ff_stream_add_bitstream_filter ( st , " hevc_mp4toannexb ", NULL ); <nl> } <nl> 
void av_log_default_callback ( void * ptr , int level , const char * fmt , va_list vl ) <nl> { <nl> static int print_prefix = 1 ; <nl> static int count ; <nl> - static char line [ 1024 ], prev [ 1024 ]; <nl> + static char prev [ 1024 ]; <nl> + char line [ 1024 ]; <nl> static int is_atty ; <nl> AVClass * avc = ptr ? *( AVClass **) ptr : NULL ; <nl> if ( level > av_log_level ) <nl> void av_log_default_callback ( void * ptr , int level , const char * fmt , va_list vl ) <nl> if (! is_atty ) is_atty = isatty ( 2 ) ? 1 : - 1 ; <nl> # endif <nl>  <nl> - if ( print_prefix && ( flags & AV_LOG_SKIP_REPEATED ) && ! strcmp ( line , prev )){ <nl> + if ( print_prefix && ( flags & AV_LOG_SKIP_REPEATED ) && ! strncmp ( line , prev , sizeof line )){ <nl> count ++; <nl> if ( is_atty == 1 ) <nl> fprintf ( stderr , " Last message repeated % d times \ r ", count ); <nl> void av_log_default_callback ( void * ptr , int level , const char * fmt , va_list vl ) <nl> count = 0 ; <nl> } <nl> colored_fputs ( av_clip ( level >> 3 , 0 , 6 ), line ); <nl> - strcpy ( prev , line ); <nl> + strncpy ( prev , line , sizeof line ); <nl> } <nl>  <nl> static void (* av_log_callback )( void *, int , const char *, va_list ) = av_log_default_callback ;
static int start_frame_overlay ( AVFilterLink * inlink , AVFilterBufferRef * inpicref <nl> OverlayContext * over = ctx -> priv ; <nl>  <nl> inlink -> cur_buf = NULL ; <nl> + avfilter_unref_bufferp (& over -> overpicref ); <nl> over -> overpicref = inpicref ; <nl> over -> overpicref -> pts = av_rescale_q ( inpicref -> pts , ctx -> inputs [ OVERLAY ]-> time_base , <nl> ctx -> outputs [ 0 ]-> time_base );
static int vorbis_parse_setup_hdr_residues ( vorbis_context * vc ) <nl> res_setup -> partition_size = get_bits ( gb , 24 ) + 1 ; <nl> /* Validations to prevent a buffer overflow later . */ <nl> if ( res_setup -> begin > res_setup -> end || <nl> - res_setup -> end > vc -> blocksize [ 1 ] / ( res_setup -> type == 2 ? 1 : 2 ) || <nl> + res_setup -> end > vc -> avccontext -> channels * vc -> blocksize [ 1 ] / ( res_setup -> type == 2 ? 1 : 2 ) || <nl> ( res_setup -> end - res_setup -> begin ) / res_setup -> partition_size > V_MAX_PARTITIONS ) { <nl> av_log ( vc -> avccontext , AV_LOG_ERROR , " partition out of bounds : type , begin , end , size , blocksize : %" PRIdFAST16 ", %" PRIdFAST32 ", %" PRIdFAST32 ", %" PRIdFAST32 ", %" PRIdFAST32 "\ n ", res_setup -> type , res_setup -> begin , res_setup -> end , res_setup -> partition_size , vc -> blocksize [ 1 ] / 2 ); <nl> return - 1 ;
static int vp8_lossy_decode_frame ( AVCodecContext * avctx , AVFrame * p , <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if (!* got_frame ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> update_canvas_size ( avctx , avctx -> width , avctx -> height ); <nl>  <nl> if ( s -> has_alpha ) {
typedef struct { <nl> IVIPicConfig pic_conf ; <nl>  <nl> int gop_invalid ; <nl> + int buf_invalid [ 3 ]; <nl> } IVI5DecContext ; <nl>  <nl>  <nl> static int decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> //{ START_TIMER ; <nl>  <nl> if ( ctx -> frame_type != FRAMETYPE_NULL ) { <nl> + ctx -> buf_invalid [ ctx -> dst_buf ] = 1 ; <nl> for ( p = 0 ; p < 3 ; p ++) { <nl> for ( b = 0 ; b < ctx -> planes [ p ]. num_bands ; b ++) { <nl> result = decode_band ( ctx , p , & ctx -> planes [ p ]. bands [ b ], avctx ); <nl> static int decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> } <nl> } <nl> } <nl> + ctx -> buf_invalid [ ctx -> dst_buf ] = 0 ; <nl> } <nl> + if ( ctx -> buf_invalid [ ctx -> dst_buf ]) <nl> + return - 1 ; <nl>  <nl> // STOP_TIMER (" decode_planes "); } <nl> 
static int vc1_decode_sprites ( VC1Context * v , GetBitContext * gb ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> - if (! s -> current_picture . f -> data [ 0 ]) { <nl> + if (! s -> current_picture . f || ! s -> current_picture . f -> data [ 0 ]) { <nl> av_log ( avctx , AV_LOG_ERROR , " Got no sprites \ n "); <nl> return - 1 ; <nl> }
static void mxf_write_system_item ( AVFormatContext * s ) <nl> avio_w8 ( pb , 0x04 ); // content package rate <nl> avio_w8 ( pb , 0x00 ); // content package type <nl> avio_wb16 ( pb , 0x00 ); // channel handle <nl> - avio_wb16 ( pb , mxf -> tc . start + frame ); // continuity count <nl> + avio_wb16 ( pb , ( mxf -> tc . start + frame ) & 0xFFFF ); // continuity count , supposed to overflow <nl> if ( mxf -> essence_container_count > 1 ) <nl> avio_write ( pb , multiple_desc_ul , 16 ); <nl> else {
static void opt_frame_aspect_ratio ( const char * arg ) <nl> ffmpeg_exit ( 1 ); <nl> } <nl> frame_aspect_ratio = ar ; <nl> - <nl> - x = vfilters ? strlen ( vfilters ) : 0 ; <nl> - vfilters = av_realloc ( vfilters , x + 100 ); <nl> - snprintf ( vfilters + x , x + 100 , "% csetdar =% f \ n ", x ?',':' ', ar ); <nl> } <nl>  <nl> static int opt_metadata ( const char * opt , const char * arg )
static int query_formats ( AVFilterContext * ctx ) <nl> } else { <nl> out_samplerates = ff_all_samplerates (); <nl> } <nl> + if (! out_samplerates ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " Cannot allocate output samplerates .\ n "); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> + <nl> ff_formats_ref ( out_samplerates , & outlink -> in_samplerates ); <nl>  <nl> if ( out_format != AV_SAMPLE_FMT_NONE ) {
static av_cold void uninit ( AVFilterContext * ctx ) <nl> { <nl> MovieContext * movie = ctx -> priv ; <nl>  <nl> - av_free ( movie -> file_name ); <nl> - av_free ( movie -> format_name ); <nl> if ( movie -> codec_ctx ) <nl> avcodec_close ( movie -> codec_ctx ); <nl> if ( movie -> format_ctx )
static int mxg_update_cache ( AVFormatContext * s , unsigned int cache_size ) <nl> MXGContext * mxg = s -> priv_data ; <nl> unsigned int current_pos = mxg -> buffer_ptr - mxg -> buffer ; <nl> unsigned int soi_pos ; <nl> + uint8_t * buffer ; <nl> int ret ; <nl>  <nl> /* reallocate internal buffer */ <nl> if ( current_pos > current_pos + cache_size ) <nl> return AVERROR ( ENOMEM ); <nl> soi_pos = mxg -> soi_ptr - mxg -> buffer ; <nl> - mxg -> buffer = av_fast_realloc ( mxg -> buffer , & mxg -> buffer_size , <nl> - current_pos + cache_size + <nl> - FF_INPUT_BUFFER_PADDING_SIZE ); <nl> - if (! mxg -> buffer ) <nl> + buffer = av_fast_realloc ( mxg -> buffer , & mxg -> buffer_size , <nl> + current_pos + cache_size + <nl> + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! buffer ) <nl> return AVERROR ( ENOMEM ); <nl> + mxg -> buffer = buffer ; <nl> mxg -> buffer_ptr = mxg -> buffer + current_pos ; <nl> if ( mxg -> soi_ptr ) mxg -> soi_ptr = mxg -> buffer + soi_pos ; <nl> 
static int adx_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> AVCodecContext * avctx = s -> streams [ 0 ]-> codec ; <nl> int ret , size ; <nl>  <nl> + if ( avctx -> channels <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " invalid number of channels % d \ n ", avctx -> channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> size = BLOCK_SIZE * avctx -> channels ; <nl>  <nl> pkt -> pos = avio_tell ( s -> pb );
static int parse_bintree ( Indeo3DecodeContext * ctx , AVCodecContext * avctx , <nl> SPLIT_CELL ( ref_cell -> height , curr_cell . height ); <nl> ref_cell -> ypos += curr_cell . height ; <nl> ref_cell -> height -= curr_cell . height ; <nl> + if ( ref_cell -> height <= 0 || curr_cell . height <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> } else if ( code == V_SPLIT ) { <nl> if ( curr_cell . width > strip_width ) { <nl> /* split strip */ <nl> static int parse_bintree ( Indeo3DecodeContext * ctx , AVCodecContext * avctx , <nl> SPLIT_CELL ( ref_cell -> width , curr_cell . width ); <nl> ref_cell -> xpos += curr_cell . width ; <nl> ref_cell -> width -= curr_cell . width ; <nl> + if ( ref_cell -> width <= 0 || curr_cell . width <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> while ( 1 ) { /* loop until return */ <nl> static int decode_frame_headers ( Indeo3DecodeContext * ctx , AVCodecContext * avctx , <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if ( width != ctx -> width || height != ctx -> height ) { <nl> + int res ; <nl> + <nl> av_dlog ( avctx , " Frame dimensions changed !\ n "); <nl>  <nl> ctx -> width = width ; <nl> ctx -> height = height ; <nl>  <nl> free_frame_buffers ( ctx ); <nl> - allocate_frame_buffers ( ctx , avctx ); <nl> + if (( res = allocate_frame_buffers ( ctx , avctx )) < 0 ) <nl> + return res ; <nl> avcodec_set_dimensions ( avctx , width , height ); <nl> } <nl> 
static int h263_probe ( AVProbeData * p ) <nl> } <nl> } <nl> // av_log ( NULL , AV_LOG_ERROR , " h263_probe : psc :% d invalid :% d res_change :% d \ n ", valid_psc , invalid_psc , res_change ); <nl> - if ( valid_psc > 2 * invalid_psc + 2 * res_change + 2 ){ <nl> +// h263_probe : psc : 3 invalid : 0 res_change : 0 ( 1588 / recent_ffmpeg_parses_mpg_incorrectly . mpg ) <nl> + if ( valid_psc > 2 * invalid_psc + 2 * res_change + 3 ){ <nl> return 50 ; <nl> } else if ( valid_psc > 2 * invalid_psc ) <nl> return 25 ;
static av_cold int vp8_init ( AVCodecContext * avctx ) <nl>  <nl> // 0 - 100 ( 0 => CBR , 100 => VBR ) <nl> enccfg . rc_2pass_vbr_bias_pct = round ( avctx -> qcompress * 100 ); <nl> - enccfg . rc_2pass_vbr_minsection_pct = <nl> - avctx -> rc_min_rate * 100LL / avctx -> bit_rate ; <nl> + if ( avctx -> bit_rate ) <nl> + enccfg . rc_2pass_vbr_minsection_pct = <nl> + avctx -> rc_min_rate * 100LL / avctx -> bit_rate ; <nl> if ( avctx -> rc_max_rate ) <nl> enccfg . rc_2pass_vbr_maxsection_pct = <nl> avctx -> rc_max_rate * 100LL / avctx -> bit_rate ;
void ff_dsputil_init_dwt ( DSPContext * c ); <nl> uint8_t la_ ## v [ sizeof ( t s o ) + ( a )]; \ <nl> t (* v ) o = ( void *) FFALIGN (( uintptr_t ) la_ ## v , a ) <nl>  <nl> -# define LOCAL_ALIGNED_D ( a , t , v , s , o , ...) DECLARE_ALIGNED ( a , t , v ) s o <nl> +# define LOCAL_ALIGNED_D ( a , t , v , s , o , ...) \ <nl> + DECLARE_ALIGNED ( a , t , la_ ## v ) s o ; \ <nl> + t (* v ) o = la_ ## v <nl>  <nl> # define LOCAL_ALIGNED ( a , t , v , ...) E ( LOCAL_ALIGNED_A ( a , t , v , __VA_ARGS__ ,,)) <nl> 
static int request_frame ( AVFilterLink * outlink ) <nl> avfilter_start_frame ( outlink , outpicref ); <nl> avfilter_draw_slice ( outlink , 0 , outlink -> h , 1 ); <nl> avfilter_end_frame ( outlink ); <nl> + avfilter_unref_buffer ( movie -> picref ); <nl> + movie -> picref = NULL ; <nl>  <nl> return 0 ; <nl> }
int ff_h264_decode_picture_parameter_set ( GetBitContext * gb , AVCodecContext * avct <nl> pps -> init_qp = get_se_golomb ( gb ) + 26 + qp_bd_offset ; <nl> pps -> init_qs = get_se_golomb ( gb ) + 26 + qp_bd_offset ; <nl> pps -> chroma_qp_index_offset [ 0 ] = get_se_golomb ( gb ); <nl> + if ( pps -> chroma_qp_index_offset [ 0 ] < - 12 || pps -> chroma_qp_index_offset [ 0 ] > 12 ) { <nl> + ret = AVERROR_INVALIDDATA ; <nl> + goto fail ; <nl> + } <nl> + <nl> pps -> deblocking_filter_parameters_present = get_bits1 ( gb ); <nl> pps -> constrained_intra_pred = get_bits1 ( gb ); <nl> pps -> redundant_pic_cnt_present = get_bits1 ( gb ); <nl> int ff_h264_decode_picture_parameter_set ( GetBitContext * gb , AVCodecContext * avct <nl> pps -> scaling_matrix4 , pps -> scaling_matrix8 ); <nl> // second_chroma_qp_index_offset <nl> pps -> chroma_qp_index_offset [ 1 ] = get_se_golomb ( gb ); <nl> + if ( pps -> chroma_qp_index_offset [ 1 ] < - 12 || pps -> chroma_qp_index_offset [ 1 ] > 12 ) { <nl> + ret = AVERROR_INVALIDDATA ; <nl> + goto fail ; <nl> + } <nl> } else { <nl> pps -> chroma_qp_index_offset [ 1 ] = pps -> chroma_qp_index_offset [ 0 ]; <nl> }
static int mov_read_stsd ( MOVContext * c , ByteIOContext * pb , MOV_atom_t atom ) <nl>  <nl> st -> codec -> codec_tag = format ; <nl> id = codec_get_id ( codec_movaudio_tags , format ); <nl> + if ( id <= 0 && ( format & 0xFFFF ) == ' m ' + (' s '<< 8 )) <nl> + id = codec_get_id ( codec_wav_tags , bswap_32 ( format )& 0xFFFF ); <nl> + <nl> if ( st -> codec -> codec_type != CODEC_TYPE_VIDEO && id > 0 ) { <nl> st -> codec -> codec_type = CODEC_TYPE_AUDIO ; <nl> } else if ( st -> codec -> codec_type != CODEC_TYPE_AUDIO && /* do not overwrite codec type */
static inline void copy ( LZOContext * c , int cnt ) <nl> { <nl> register const uint8_t * src = c -> in ; <nl> register uint8_t * dst = c -> out ; <nl> + if ( cnt < 0 ) { <nl> + c -> error |= AV_LZO_ERROR ; <nl> + return ; <nl> + } <nl> if ( cnt > c -> in_end - src ) { <nl> cnt = FFMAX ( c -> in_end - src , 0 ); <nl> c -> error |= AV_LZO_INPUT_DEPLETED ; <nl> static inline void copy ( LZOContext * c , int cnt ) <nl> /** <nl> * @ brief Copies previously decoded bytes to current position . <nl> * @ param back how many bytes back we start <nl> - * @ param cnt number of bytes to copy , must be >= 0 <nl> + * @ param cnt number of bytes to copy , must be > 0 <nl> * <nl> * cnt > back is valid , this will copy the bytes we just copied , <nl> * thus creating a repeating pattern with a period length of back . <nl> static inline void copy ( LZOContext * c , int cnt ) <nl> static inline void copy_backptr ( LZOContext * c , int back , int cnt ) <nl> { <nl> register uint8_t * dst = c -> out ; <nl> + if ( cnt <= 0 ) { <nl> + c -> error |= AV_LZO_ERROR ; <nl> + return ; <nl> + } <nl> if ( dst - c -> out_start < back ) { <nl> c -> error |= AV_LZO_INVALID_BACKPTR ; <nl> return ;
void Process ( void * ctx , AVPicture * picture , enum PixelFormat pix_fmt , int width , <nl> AVPicture picture1 ; <nl> AVPicture picture2 ; <nl> AVPicture * pict = picture ; <nl> - int out_width ; <nl> - int out_height ; <nl> + int av_uninit ( out_width ); <nl> + int av_uninit ( out_height ); <nl> int i ; <nl> uint8_t * ptr = NULL ; <nl> FILE * in = rwpipe_reader ( ci -> rw );
static int decode_slice_header ( H264Context * h , H264Context * h0 ){ <nl> while ( h -> frame_num != h -> prev_frame_num && <nl> h -> frame_num != ( h -> prev_frame_num + 1 )%( 1 << h -> sps . log2_max_frame_num )){ <nl> av_log ( NULL , AV_LOG_DEBUG , " Frame num gap % d % d \ n ", h -> frame_num , h -> prev_frame_num ); <nl> - frame_start ( h ); <nl> + if ( frame_start ( h ) < 0 ) <nl> + return - 1 ; <nl> h -> prev_frame_num ++; <nl> h -> prev_frame_num %= 1 << h -> sps . log2_max_frame_num ; <nl> s -> current_picture_ptr -> frame_num = h -> prev_frame_num ;
static int write_cvid_header ( CinepakEncContext * s , unsigned char * buf , int num_s <nl>  <nl> static int rd_frame ( CinepakEncContext * s , const AVFrame * frame , int isakeyframe , unsigned char * buf , int buf_size ) <nl> { <nl> - int num_strips , strip , i , y , nexty , size , temp_size , best_size ; <nl> + int num_strips , strip , i , y , nexty , size , temp_size ; <nl> AVPicture last_pict , pict , scratch_pict ; <nl> int64_t best_score = 0 , score , score_temp ; <nl> # ifdef CINEPAK_REPORT_SERR <nl> int64_t best_serr = 0 , serr , serr_temp ; <nl> # endif <nl>  <nl> - int best_nstrips ; <nl> + int best_nstrips = - 1 , best_size = - 1 ; // mark as uninitialzed <nl>  <nl> if ( s -> pix_fmt == AV_PIX_FMT_RGB24 ) { <nl> int x ; <nl> static int rd_frame ( CinepakEncContext * s , const AVFrame * frame , int isakeyframe , <nl> break ; <nl> } <nl>  <nl> + av_assert0 ( best_nstrips >= 0 && best_size >= 0 ); <nl> + <nl> // let the number of strips slowly adapt to the changes in the contents , <nl> // compared to full bruteforcing every time this will occasionally lead <nl> // to some r / d performance loss but makes encoding up to several times faster
static int decode_packet ( AVCodecContext * avctx , void * data , <nl> ( frame_size = show_bits ( gb , s -> log2_frame_size )) && <nl> frame_size <= remaining_bits ( s , gb )) { <nl> save_bits ( s , gb , frame_size , 0 ); <nl> - s -> packet_done = ! decode_frame ( s , data , got_frame_ptr ); <nl> + if (! s -> packet_loss ) <nl> + s -> packet_done = ! decode_frame ( s , data , got_frame_ptr ); <nl> } else if (! s -> len_prefix <nl> && s -> num_saved_bits > get_bits_count (& s -> gb )) { <nl> /** when the frames do not have a length prefix , we don ' t know
static inline int get_block ( GetBitContext * gb , DCTELEM * block , const uint8_t * sc <nl>  <nl> // number of non - zero coefficients <nl> coeff = get_bits ( gb , 6 ); <nl> + if ( get_bits_count ( gb ) + ( coeff << 1 ) >= gb -> size_in_bits ) <nl> + return 0 ; <nl> + <nl> // normally we would only need to clear the ( 63 - coeff ) last values , <nl> // but since we do not know where they are we just clear the whole block <nl> memset ( block , 0 , 64 * sizeof ( DCTELEM )); <nl> static inline int get_block ( GetBitContext * gb , DCTELEM * block , const uint8_t * sc <nl>  <nl> // 4 bits per coefficient <nl> ALIGN ( 4 ); <nl> + if ( get_bits_count ( gb ) + ( coeff << 2 ) >= gb -> size_in_bits ) <nl> + return 0 ; <nl> while ( coeff ) { <nl> ac = get_sbits ( gb , 4 ); <nl> if ( ac == - 8 ) <nl> static inline int get_block ( GetBitContext * gb , DCTELEM * block , const uint8_t * sc <nl>  <nl> // 8 bits per coefficient <nl> ALIGN ( 8 ); <nl> + if ( get_bits_count ( gb ) + ( coeff << 3 ) >= gb -> size_in_bits ) <nl> + return 0 ; <nl> while ( coeff ) { <nl> ac = get_sbits ( gb , 8 ); <nl> PUT_COEFF ( ac );
static int build_vlc ( AVCodecContext * avctx , VLC * vlc , const uint32_t * table ) <nl> int new_node = j ; <nl> int first_node = cur_node ; <nl> int second_node = cur_node ; <nl> - int nd , st ; <nl> + unsigned nd , st ; <nl>  <nl> nodes [ cur_node ]. count = - 1 ; <nl>  <nl> static int build_vlc ( AVCodecContext * avctx , VLC * vlc , const uint32_t * table ) <nl> st = nodes [ first_node ]. count ; <nl> nodes [ second_node ]. count = 0 ; <nl> nodes [ first_node ]. count = 0 ; <nl> + if ( nd >= UINT32_MAX - st ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " count overflow \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> nodes [ cur_node ]. count = nd + st ; <nl> nodes [ cur_node ]. sym = - 1 ; <nl> nodes [ cur_node ]. n0 = cur_node ;
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> ret = 0 ; <nl>  <nl> the_end : <nl> - av_free ( crow_base ); <nl> - av_free ( progressive_buf ); <nl> - av_free ( top_buf ); <nl> + av_freep (& crow_base ); <nl> + av_freep (& progressive_buf ); <nl> + av_freep (& top_buf ); <nl> deflateEnd (& s -> zstream ); <nl> return ret ; <nl> fail :
static inline int ape_decode_value_3900 ( APEContext * ctx , APERice * rice ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> x = range_decode_bits ( ctx , tmpk ); <nl> - } else if ( tmpk <= 32 ) { <nl> + } else if ( tmpk <= 31 ) { <nl> x = range_decode_bits ( ctx , 16 ); <nl> x |= ( range_decode_bits ( ctx , tmpk - 16 ) << 16 ); <nl> } else {
static int wavpack_encode_block ( WavPackEncodeContext * s , <nl> uint8_t * out , int out_size ) <nl> { <nl> int block_size , start , end , data_size , tcount , temp , m = 0 ; <nl> - int i , j , ret , got_extra = 0 , nb_samples = s -> block_samples ; <nl> + int i , j , ret = 0 , got_extra = 0 , nb_samples = s -> block_samples ; <nl> uint32_t crc = 0xffffffffu ; <nl> struct Decorr * dpp ; <nl> PutByteContext pb ;
static int mov_read_stts ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> if ( entries >= UINT_MAX / sizeof (* sc -> stts_data )) <nl> return AVERROR ( EINVAL ); <nl>  <nl> + av_free ( sc -> stts_data ); <nl> sc -> stts_data = av_malloc ( entries * sizeof (* sc -> stts_data )); <nl> if (! sc -> stts_data ) <nl> return AVERROR ( ENOMEM );
static int decode_frame ( AVCodecContext * avctx , <nl> if ( p -> data [ 0 ]) <nl> avctx -> release_buffer ( avctx , p ); <nl>  <nl> + if ( buf_size < 16 + avctx -> height + avctx -> width * avctx -> height * 5 / 8 ){ <nl> + av_log ( avctx , AV_LOG_ERROR , " Insufficient input data .\ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> p -> reference = 0 ; <nl> if ( avctx -> get_buffer ( avctx , p ) < 0 ){ <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n ");
void ff_er_frame_end ( MpegEncContext * s ){ <nl> if (! s -> error_recognition || s -> error_count == 0 || s -> avctx -> lowres || <nl> s -> avctx -> hwaccel || <nl> s -> avctx -> codec -> capabilities & CODEC_CAP_HWACCEL_VDPAU || <nl> + s -> picture_structure != PICT_FRAME || // we dont support ER of field pictures yet , though it should not crash if enabled <nl> s -> error_count == 3 * s -> mb_width *( s -> avctx -> skip_top + s -> avctx -> skip_bottom )) return ; <nl>  <nl> if ( s -> current_picture . motion_val [ 0 ] == NULL ){
static void mov_build_index ( MOVContext * mov , AVStream * st ) <nl>  <nl> current_dts -= sc -> dts_shift ; <nl>  <nl> - if (! sc -> sample_count ) <nl> + if (! sc -> sample_count || st -> nb_index_entries ) <nl> return ; <nl> if ( sc -> sample_count >= UINT_MAX / sizeof (* st -> index_entries )) <nl> return ;
static void asf_build_simple_index ( AVFormatContext * s , int stream_index ) <nl> last_pos = pos ; <nl> } <nl> } <nl> - asf -> index_read = 1 ; <nl> + asf -> index_read = ict > 0 ; <nl> } <nl> avio_seek ( s -> pb , current_pos , SEEK_SET ); <nl> }
static int get_siz ( Jpeg2000DecoderContext * s ) <nl> s -> tile_offset_y = bytestream2_get_be32u (& s -> g ); // YT0Siz <nl> ncomponents = bytestream2_get_be16u (& s -> g ); // CSiz <nl>  <nl> + if ( s -> image_offset_x || s -> image_offset_y ) { <nl> + avpriv_request_sample ( s -> avctx , " Support for image offsets "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> + <nl> if ( ncomponents <= 0 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Invalid number of components : % d \ n ", <nl> s -> ncomponents );
static int flush_packet ( AVFormatContext * ctx , int stream_index , <nl>  <nl> if ( stuffing_size < 0 ) <nl> stuffing_size = 0 ; <nl> + <nl> + if ( startcode == PRIVATE_STREAM_1 && id >= 0xa0 ) { <nl> + if ( payload_size < av_fifo_size ( stream -> fifo )) <nl> + stuffing_size += payload_size % stream -> lpcm_align ; <nl> + } <nl> + <nl> if ( stuffing_size > 16 ) { /*<= 16 for MPEG - 1 , <= 32 for MPEG - 2 */ <nl> pad_packet_bytes += stuffing_size ; <nl> packet_size -= stuffing_size ;
FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl>  <nl> if ( s -> oformat -> init && ( ret = s -> oformat -> init ( s )) < 0 ) { <nl> - s -> oformat -> deinit ( s ); <nl> + if ( s -> oformat -> deinit ) <nl> + s -> oformat -> deinit ( s ); <nl> goto fail ; <nl> } <nl> 
static int fourxm_read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> if ( fourcc_tag == std__TAG ) { <nl> + if ( header_size < i + 16 ) { <nl> + av_log ( s , AV_LOG_ERROR , " std TAG truncated \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> fourxm -> fps = av_int2float ( AV_RL32 (& header [ i + 12 ])); <nl> } else if ( fourcc_tag == vtrk_TAG ) { <nl> /* check that there is enough data */
static int query_formats ( AVFilterContext * ctx ) <nl> AVFilterFormats * pix_fmts = NULL ; <nl> int fmt ; <nl>  <nl> - for ( fmt = 0 ; fmt < AV_PIX_FMT_NB ; fmt ++) { <nl> + for ( fmt = 0 ; av_pix_fmt_desc_get ( fmt ); fmt ++) { <nl> const AVPixFmtDescriptor * desc = av_pix_fmt_desc_get ( fmt ); <nl> if (!( desc -> flags & AV_PIX_FMT_FLAG_PAL || <nl> desc -> flags & AV_PIX_FMT_FLAG_HWACCEL ||
static void decode_postinit ( H264Context * h , int setup_finished ){ <nl>  <nl> if ( s -> avctx -> strict_std_compliance >= FF_COMPLIANCE_STRICT <nl> && ! h -> sps . bitstream_restriction_flag ){ <nl> - s -> avctx -> has_b_frames = MAX_DELAYED_PIC_COUNT ; <nl> + s -> avctx -> has_b_frames = MAX_DELAYED_PIC_COUNT - 1 ; <nl> s -> low_delay = 0 ; <nl> } <nl> 
static int decode_frame ( AVCodecContext * avctx , <nl> switch ( buf [ 0 ]) { <nl> case ' 0 ': case ' 1 ': case ' 2 ': case ' 3 ': case ' 4 ': <nl> case ' 5 ': case ' 6 ': case ' 7 ': case ' 8 ': case ' 9 ': <nl> - if ( s -> nb_args < MAX_NB_ARGS ) <nl> + if ( s -> nb_args < MAX_NB_ARGS && s -> args [ s -> nb_args ] < 6553 ) <nl> s -> args [ s -> nb_args ] = FFMAX ( s -> args [ s -> nb_args ], 0 ) * 10 + buf [ 0 ] - ' 0 '; <nl> break ; <nl> case ';':
static int decode_header ( SnowContext * s ){ <nl> s -> mv_scale += get_symbol (& s -> c , s -> header_state , 1 ); <nl> s -> qbias += get_symbol (& s -> c , s -> header_state , 1 ); <nl> s -> block_max_depth += get_symbol (& s -> c , s -> header_state , 1 ); <nl> - if ( s -> block_max_depth > 1 || s -> block_max_depth < 0 ){ <nl> + if ( s -> block_max_depth > 1 || s -> block_max_depth < 0 || s -> mv_scale > 256U ){ <nl> av_log ( s -> avctx , AV_LOG_ERROR , " block_max_depth = % d is too large \ n ", s -> block_max_depth ); <nl> s -> block_max_depth = 0 ; <nl> + s -> mv_scale = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> if ( FFABS ( s -> qbias ) > 127 ) {
static int decode_chunks ( AVCodecContext * avctx , <nl> break ; <nl>  <nl> case PICTURE_START_CODE : <nl> - if ( avctx -> thread_count > 1 && s -> slice_count ) { <nl> + if ( HAVE_THREADS && ( avctx -> active_thread_type & FF_THREAD_SLICE ) && s -> slice_count ) { <nl> int i ; <nl>  <nl> avctx -> execute ( avctx , slice_decode_thread ,
static int mov_read_close ( AVFormatContext * s ) <nl> MOVContext * mov = ( MOVContext *) s -> priv_data ; <nl> for ( i = 0 ; i < mov -> total_streams ; i ++) <nl> mov_free_stream_context ( mov -> streams [ i ]); <nl> - for ( i = 0 ; i < s -> nb_streams ; i ++) <nl> - av_freep (& s -> streams [ i ]); <nl> /* free color tabs */ <nl> for ( i = 0 ; i < mov -> ctab_size ; i ++) <nl> av_freep (& mov -> ctab [ i ]);
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - s -> version_b = avctx -> extradata && avctx -> extradata [ 3 ] == ' b '; <nl> + s -> version_b = avctx -> extradata_size >= 4 && avctx -> extradata [ 3 ] == ' b '; <nl>  <nl> if ( avctx -> codec -> id == CODEC_ID_BINKAUDIO_RDFT ) { <nl> // audio is already interleaved for the RDFT format variant
static int decode_mb_info ( IVI45DecContext * ctx , IVIBandDesc * band , <nl> (( band -> qdelta_present && band -> inherit_qdelta ) || band -> inherit_mv )) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> + if ( tile -> num_MBs != IVI_MBs_PER_TILE ( tile -> width , tile -> height , band -> mb_size )) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Allocated tile size % d mismatches parameters % d \ n ", <nl> + tile -> num_MBs , IVI_MBs_PER_TILE ( tile -> width , tile -> height , band -> mb_size )); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> /* scale factor for motion vectors */ <nl> mv_scale = ( ctx -> planes [ 0 ]. bands [ 0 ]. mb_size >> 3 ) - ( band -> mb_size >> 3 ); <nl> mv_x = mv_y = 0 ;
static AVFilterContext * create_filter_with_args ( const char * filt , void * opaque ) <nl> av_log ( NULL , AV_LOG_ERROR , <nl> " error creating filter \"% s \" with args \"% s \"\ n ", <nl> name , args ? args : "( none )"); <nl> - return NULL ; <nl> } <nl>  <nl> av_free ( filter );
FF_ENABLE_DEPRECATION_WARNINGS <nl> return - 1 ; <nl> } <nl>  <nl> - if ( s -> avctx -> thread_count > 1 ) <nl> - s -> rtp_mode = 1 ; <nl> - <nl> if (! avctx -> time_base . den || ! avctx -> time_base . num ) { <nl> av_log ( avctx , AV_LOG_ERROR , " framerate not set \ n "); <nl> return - 1 ; <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> if (( CONFIG_H263P_ENCODER || CONFIG_RV20_ENCODER ) && s -> modified_quant ) <nl> s -> chroma_qscale_table = ff_h263_chroma_qscale_table ; <nl>  <nl> + if ( s -> slice_context_count > 1 ) { <nl> + s -> rtp_mode = 1 ; <nl> + <nl> + if ( avctx -> codec_id == AV_CODEC_ID_H263 || avctx -> codec_id == AV_CODEC_ID_H263P ) <nl> + s -> h263_slice_structured = 1 ; <nl> + } <nl> + <nl> s -> quant_precision = 5 ; <nl>  <nl> ff_set_cmp (& s -> mecc , s -> mecc . ildct_cmp , s -> avctx -> ildct_cmp );
int inet_aton ( const char * str , struct in_addr * add ) <nl>  <nl> add1 = atoi ( pch ); <nl> pch = strpbrk ( pch ,"."); <nl> - if ( pch == 0 || ++ pch == 0 ) goto done ; <nl> + if ( pch == 0 || ++ pch == 0 ) return 0 ; <nl> add2 = atoi ( pch ); <nl> pch = strpbrk ( pch ,"."); <nl> - if ( pch == 0 || ++ pch == 0 ) goto done ; <nl> + if ( pch == 0 || ++ pch == 0 ) return 0 ; <nl> add3 = atoi ( pch ); <nl> pch = strpbrk ( pch ,"."); <nl> - if ( pch == 0 || ++ pch == 0 ) goto done ; <nl> + if ( pch == 0 || ++ pch == 0 ) return 0 ; <nl> add4 = atoi ( pch ); <nl>  <nl> - done : <nl> + if (! add1 || ( add1 | add2 | add3 | add4 ) > 255 ) return 0 ; <nl> + <nl> add -> s_addr =( add4 << 24 )+( add3 << 16 )+( add2 << 8 )+ add1 ; <nl>  <nl> return 1 ;
static int ffm2_read_header ( AVFormatContext * s ) <nl> } <nl> break ; <nl> case MKBETAG (' S ', ' 2 ', ' V ', ' I '): <nl> - if ( f_stvi ++) { <nl> + if ( f_stvi ++ || ! size ) { <nl> ret = AVERROR ( EINVAL ); <nl> goto fail ; <nl> } <nl> static int ffm2_read_header ( AVFormatContext * s ) <nl> goto fail ; <nl> break ; <nl> case MKBETAG (' S ', ' 2 ', ' A ', ' U '): <nl> - if ( f_stau ++) { <nl> + if ( f_stau ++ || ! size ) { <nl> ret = AVERROR ( EINVAL ); <nl> goto fail ; <nl> }
AVStream * video_st , * audio_st ; <nl> int64_t audio_dts , video_dts ; <nl>  <nl> int bframes ; <nl> - int duration ; <nl> - int audio_duration ; <nl> + int64_t duration ; <nl> + int64_t audio_duration ; <nl> int frames ; <nl> int gop_size ; <nl> int64_t next_p_pts ; <nl> static void init_fps ( int bf , int audio_preroll , int fps ) <nl> frames = 0 ; <nl> gop_size = 30 ; <nl> duration = video_st -> time_base . den / fps ; <nl> - audio_duration = 1024 * audio_st -> time_base . den / audio_st -> codec -> sample_rate ; <nl> + audio_duration = 1024LL * audio_st -> time_base . den / audio_st -> codec -> sample_rate ; <nl> if ( audio_preroll ) <nl> - audio_preroll = 2048 * audio_st -> time_base . den / audio_st -> codec -> sample_rate ; <nl> + audio_preroll = 2048LL * audio_st -> time_base . den / audio_st -> codec -> sample_rate ; <nl>  <nl> bframes = bf ; <nl> video_dts = bframes ? - duration : 0 ;
static int concat_read_packet ( AVFormatContext * avf , AVPacket * pkt ) <nl> ConcatStream * cs ; <nl> AVStream * st ; <nl>  <nl> + if (! cat -> avf ) <nl> + return AVERROR ( EIO ); <nl> + <nl> while ( 1 ) { <nl> ret = av_read_frame ( cat -> avf , pkt ); <nl> if ( ret == AVERROR_EOF ) {
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ) <nl> case H264_NAL_SLICE : <nl> h -> has_slice = 1 ; <nl>  <nl> - if (( err = ff_h264_queue_decode_slice ( h , nal ))) <nl> + if (( err = ff_h264_queue_decode_slice ( h , nal ))) { <nl> + H264SliceContext * sl = h -> slice_ctx + h -> nb_slice_ctx_queued ; <nl> + sl -> ref_count [ 0 ] = sl -> ref_count [ 1 ] = 0 ; <nl> break ; <nl> + } <nl>  <nl> if ( h -> current_slice == 1 ) { <nl> if ( avctx -> active_thread_type & FF_THREAD_FRAME && ! h -> avctx -> hwaccel &&
-/* $ OpenBSD : in_pcb . c , v 1 . 51 2001 / 05 / 21 03 : 02 : 18 angelos Exp $ */ <nl> +/* $ OpenBSD : in_pcb . c , v 1 . 52 2001 / 05 / 27 03 : 54 : 12 angelos Exp $ */ <nl> /* $ NetBSD : in_pcb . c , v 1 . 25 1996 / 02 / 13 23 : 41 : 53 christos Exp $ */ <nl>  <nl> /* <nl> in_pcbdetach ( v ) <nl> ipsp_reffree ( inp -> inp_ipsec_localcred ); <nl> if ( inp -> inp_ipsec_remotecred ) <nl> ipsp_reffree ( inp -> inp_ipsec_remotecred ); <nl> + if ( inp -> inp_ipsec_auth ) <nl> + ipsp_reffree ( inp -> inp_ipsec_auth ); <nl> splx ( s ); <nl> # endif <nl> s = splnet ();
-/* $ OpenBSD : wsconsctl . c , v 1 . 23 2009 / 07 / 15 21 : 38 : 16 martynas Exp $ */ <nl> +/* $ OpenBSD : wsconsctl . c , v 1 . 24 2009 / 07 / 19 15 : 34 : 45 martynas Exp $ */ <nl> /* $ NetBSD : wsconsctl . c , v 1 . 2 1998 / 12 / 29 22 : 40 : 20 hannken Exp $ */ <nl>  <nl> /*- <nl> main ( int argc , char * argv []) <nl> f -> flags |= FLG_SET ; <nl> putval = (* sw -> putval )( sw -> name , sw -> fd ); <nl> f -> flags &= ~ FLG_SET ; <nl> - if ( putval != 0 || f -> flags & FLG_DEAD ) <nl> + if ( putval != 0 || f -> flags & ( FLG_DEAD | FLG_NOAUTO )) <nl> continue ; <nl> if ( f -> flags & FLG_WRONLY ) { <nl> pr_field ( sw -> name , f , setsep );
-/* $ OpenBSD : rnd . c , v 1 . 48 2001 / 06 / 24 20 : 52 : 05 mickey Exp $ */ <nl> +/* $ OpenBSD : rnd . c , v 1 . 49 2001 / 09 / 23 10 : 16 : 27 mickey Exp $ */ <nl>  <nl> /* <nl> * random . c -- A strong random number generator <nl> extract_entropy ( buf , nbytes ) <nl>  <nl> while ( nbytes ) { <nl> register u_char * p = buf ; <nl> - register int i = sizeof ( buffer ); <nl> + register int i = sizeof ( buffer )/ 2 ; <nl>  <nl> if ( i > nbytes ) { <nl> i = nbytes ; <nl> extract_entropy ( buf , nbytes ) <nl> p [ 7 ] ^= p [ 8 ]; <nl>  <nl> /* Modify pool so next hash will produce different results */ <nl> - add_entropy_words (( u_int32_t *) p , sizeof ( buffer )/ 4 ); <nl> + add_entropy_words (( u_int32_t *) p , sizeof ( buffer )/ 8 ); <nl>  <nl> /* Copy data to destination buffer */ <nl> - if ( i < sizeof ( buffer )) <nl> + if ( i < sizeof ( buffer )/ 2 ) <nl> bcopy ( buffer , buf , i ); <nl> nbytes -= i ; <nl> buf += i ;
-/* $ OpenBSD : ibus . c , v 1 . 6 2006 / 07 / 20 19 : 08 : 15 miod Exp $ */ <nl> +/* $ OpenBSD : ibus . c , v 1 . 7 2006 / 07 / 20 19 : 52 : 08 miod Exp $ */ <nl> /* $ NetBSD : ibus . c , v 1 . 7 2001 / 02 / 04 20 : 36 : 32 ragge Exp $ */ <nl> /* <nl> * Copyright ( c ) 1999 Ludd , University of Lule }, Sweden . <nl> ibus_print ( void * aux , const char * name ) <nl> struct bp_conf * bp = aux ; <nl>  <nl> if ( name ) <nl> - printf (" device % s at % s ", bp -> type , name ); <nl> + printf ("% s at % s ", bp -> type , name ); <nl>  <nl> return ( UNCONF ); <nl> }
-/* $ OpenBSD : azalia_codec . c , v 1 . 107 2009 / 01 / 03 19 : 17 : 45 jakemsr Exp $ */ <nl> +/* $ OpenBSD : azalia_codec . c , v 1 . 108 2009 / 01 / 04 22 : 45 : 24 jakemsr Exp $ */ <nl> /* $ NetBSD : azalia_codec . c , v 1 . 8 2006 / 05 / 10 11 : 17 : 27 kent Exp $ */ <nl>  <nl> /*- <nl> azalia_generic_mixer_init ( codec_t * this ) <nl> continue ; <nl>  <nl> /* selector */ <nl> - if ( w -> type != COP_AWTYPE_AUDIO_MIXER && w -> nconnections > 0 && <nl> + if ( w -> nconnections > 0 && <nl> + !( w -> type == COP_AWTYPE_AUDIO_MIXER && <nl> + ( w -> widgetcap & COP_AWCAP_INAMP )) && <nl> !( w -> nconnections == 1 && <nl> azalia_widget_enabled ( this , w -> connections [ 0 ]) && <nl> strcmp ( w -> name , this -> w [ w -> connections [ 0 ]]. name ) == 0 ) &&
* OUT OF THE USE OF THIS SOFTWARE , EVEN IF ADVISED OF THE POSSIBILITY OF <nl> * SUCH DAMAGE . <nl> * <nl> - * $ OpenBSD : bundle . c , v 1 . 37 2000 / 06 / 13 09 : 57 : 50 brian Exp $ <nl> + * $ OpenBSD : bundle . c , v 1 . 38 2000 / 06 / 18 10 : 08 : 59 brian Exp $ <nl> */ <nl>  <nl> # include < sys / param . h > <nl> bundle_ShowStatus ( struct cmdargs const * arg ) <nl> prompt_Printf ( arg -> prompt , ", up time % d :% 02d :% 02d ", secs / 3600 , <nl> ( secs / 60 ) % 60 , secs % 60 ); <nl> } <nl> - prompt_Printf ( arg -> prompt , "\ n Queued : % u of % u \ n ", <nl> + prompt_Printf ( arg -> prompt , "\ n Queued : % lu of % u \ n ", <nl> ip_QueueLen (& arg -> bundle -> ncp . ipcp ), arg -> bundle -> cfg . ifqueue ); <nl>  <nl> prompt_Printf ( arg -> prompt , "\ nDefaults :\ n ");
-/* $ OpenBSD : inetd . c , v 1 . 39 1997 / 08 / 31 18 : 04 : 37 deraadt Exp $ */ <nl> +/* $ OpenBSD : inetd . c , v 1 . 40 1997 / 09 / 19 12 : 21 : 27 deraadt Exp $ */ <nl> /* $ NetBSD : inetd . c , v 1 . 11 1996 / 02 / 22 11 : 14 : 41 mycroft Exp $ */ <nl> /* <nl> * Copyright ( c ) 1983 , 1991 The Regents of the University of California . <nl> char copyright [] = <nl>  <nl> # ifndef lint <nl> /* static char sccsid [] = " from : @(#) inetd . c 5 . 30 ( Berkeley ) 6 / 3 / 91 ";*/ <nl> - static char rcsid [] = "$ OpenBSD : inetd . c , v 1 . 39 1997 / 08 / 31 18 : 04 : 37 deraadt Exp $"; <nl> + static char rcsid [] = "$ OpenBSD : inetd . c , v 1 . 40 1997 / 09 / 19 12 : 21 : 27 deraadt Exp $"; <nl> # endif /* not lint */ <nl>  <nl> /* <nl> main ( argc , argv , envp ) <nl> sigvec ( SIGTERM , & sv , NULL ); <nl> sv . sv_handler = goaway ; <nl> sigvec ( SIGINT , & sv , NULL ); <nl> + sv . sv_handler = SIG_IGN ; <nl> + sigvec ( SIGPIPE , & sv , NULL ); <nl>  <nl> { <nl> /* space for daemons to overwrite environment for ps */
-/* $ OpenBSD : message . c , v 1 . 67 2004 / 03 / 10 11 : 17 : 38 hshoexer Exp $ */ <nl> +/* $ OpenBSD : message . c , v 1 . 68 2004 / 03 / 10 16 : 10 : 57 hshoexer Exp $ */ <nl> /* $ EOM : message . c , v 1 . 156 2000 / 10 / 10 12 : 36 : 39 provos Exp $ */ <nl>  <nl> /* <nl> message_drop ( struct message * msg , int notify , struct proto * proto , <nl> address ? address : "< unknown >", htons ( port ), <nl> constant_name ( isakmp_notify_cst , notify )); <nl>  <nl> + if ( address ) <nl> + free ( address ); <nl> + <nl> /* If specified , return a notification . */ <nl> if ( notify ) <nl> message_send_notification ( msg , msg -> isakmp_sa , notify , proto , incoming );
-/* $ OpenBSD : udp_usrreq . c , v 1 . 99 2004 / 03 / 21 20 : 58 : 10 markus Exp $ */ <nl> +/* $ OpenBSD : udp_usrreq . c , v 1 . 100 2004 / 04 / 14 05 : 34 : 15 itojun Exp $ */ <nl> /* $ NetBSD : udp_usrreq . c , v 1 . 28 1996 / 03 / 16 23 : 54 : 03 christos Exp $ */ <nl>  <nl> /* <nl> udp_input ( struct mbuf * m , ...) <nl> /* <nl> * In IPv6 , the UDP checksum is ALWAYS used . <nl> */ <nl> + if ( uh -> uh_sum == 0 ) { <nl> + udpstat . udps_nosum ++; <nl> + goto bad ; <nl> + } <nl> if (( uh -> uh_sum = in6_cksum ( m , IPPROTO_UDP , iphlen , len ))) { <nl> udpstat . udps_badsum ++; <nl> goto bad ; <nl> udp_input ( struct mbuf * m , ...) <nl> } <nl> # ifdef INET6 <nl> if ( ip6 ) { <nl> + uh -> uh_sum = savesum ; <nl> icmp6_error ( m , ICMP6_DST_UNREACH , <nl> ICMP6_DST_UNREACH_NOPORT , 0 ); <nl> } else
-/* $ OpenBSD : relayctl . c , v 1 . 33 2008 / 01 / 31 12 : 12 : 50 thib Exp $ */ <nl> +/* $ OpenBSD : relayctl . c , v 1 . 34 2008 / 07 / 19 12 : 10 : 07 reyk Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 2006 Pierre - Yves Ritschard < pyr @ openbsd . org > <nl> show_summary_msg ( struct imsg * imsg , int type ) <nl> struct host * host ; <nl> struct relay * rlay ; <nl> struct ctl_stats stats [ RELAY_MAXPROC ]; <nl> + char name [ MAXHOSTNAMELEN ]; <nl>  <nl> switch ( imsg -> hdr . type ) { <nl> case IMSG_CTL_RDR : <nl> show_summary_msg ( struct imsg * imsg , int type ) <nl> if ( type == SHOW_RELAYS || type == SHOW_RDRS ) <nl> break ; <nl> host = imsg -> data ; <nl> + if ( host -> conf . parentid ) <nl> + snprintf ( name , sizeof ( name ), "% s parent % u ", <nl> + host -> conf . name , host -> conf . parentid ); <nl> + else <nl> + strlcpy ( name , host -> conf . name , sizeof ( name )); <nl> printf ("%- 4u \ t %- 8s \ t %- 24s \ t %- 7s \ t % s \ n ", <nl> - host -> conf . id , " host ", host -> conf . name , <nl> + host -> conf . id , " host ", name , <nl> print_availability ( host -> check_cnt , host -> up_cnt ), <nl> print_host_status ( host -> up , host -> flags )); <nl> if ( type == SHOW_HOSTS && host -> check_cnt ) {
-/* $ OpenBSD : kroute . c , v 1 . 168 2009 / 06 / 12 16 : 42 : 53 claudio Exp $ */ <nl> +/* $ OpenBSD : kroute . c , v 1 . 169 2009 / 06 / 25 15 : 54 : 22 claudio Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 2003 , 2004 Henning Brauer < henning @ openbsd . org > <nl> fetchtable ( u_int rtableid , int connected_only ) <nl> lim = buf + len ; <nl> for ( next = buf ; next < lim ; next += rtm -> rtm_msglen ) { <nl> rtm = ( struct rt_msghdr *) next ; <nl> + if ( rtm -> rtm_version != RTM_VERSION ) <nl> + continue ; <nl> sa = ( struct sockaddr *)( next + rtm -> rtm_hdrlen ); <nl> get_rtaddrs ( rtm -> rtm_addrs , sa , rti_info ); <nl>  <nl> dispatch_rtmsg ( void ) <nl> lim = buf + n ; <nl> for ( next = buf ; next < lim ; next += rtm -> rtm_msglen ) { <nl> rtm = ( struct rt_msghdr *) next ; <nl> + if ( rtm -> rtm_version != RTM_VERSION ) <nl> + continue ; <nl>  <nl> switch ( rtm -> rtm_type ) { <nl> case RTM_ADD :
-/* $ OpenBSD : ar5xxx . c , v 1 . 55 2009 / 09 / 23 18 : 03 : 30 damien Exp $ */ <nl> +/* $ OpenBSD : ar5xxx . c , v 1 . 56 2012 / 01 / 28 12 : 45 : 48 stsp Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 2004 , 2005 , 2006 , 2007 Reyk Floeter < reyk @ openbsd . org > <nl> ar5k_rt_copy ( HAL_RATE_TABLE * dst , const HAL_RATE_TABLE * src ) <nl> { <nl> bzero ( dst , sizeof ( HAL_RATE_TABLE )); <nl> dst -> rateCount = src -> rateCount ; <nl> + bcopy ( src -> rateCodeToIndex , dst -> rateCodeToIndex , <nl> + sizeof ( dst -> rateCodeToIndex )); <nl> bcopy ( src -> info , dst -> info , sizeof ( dst -> info )); <nl> } <nl> 
-/* $ OpenBSD : edit . c , v 1 . 21 2007 / 01 / 05 09 : 32 : 49 xsa Exp $ */ <nl> +/* $ OpenBSD : edit . c , v 1 . 22 2007 / 01 / 05 09 : 41 : 30 xsa Exp $ */ <nl> /* <nl> * Copyright ( c ) 2006 , 2007 Xavier Santolaria < xsa @ openbsd . org > <nl> * <nl> cvs_edit_local ( struct cvs_file * cf ) <nl> MAXPATHLEN ) >= MAXPATHLEN ) <nl> fatal (" cvs_edit_local : truncation "); <nl>  <nl> + if ( mkdir ( CVS_PATH_BASEDIR , 0755 ) == - 1 && errno != EEXIST ) <nl> + fatal (" cvs_edit_local : `% s ': % s ", CVS_PATH_BASEDIR , <nl> + strerror ( errno )); <nl> + <nl> /* XXX : copy cf -> file_path to bfpath */ <nl>  <nl> xfree ( bfpath );
-/* $ OpenBSD : if . c , v 1 . 483 2017 / 01 / 31 12 : 16 : 20 mpi Exp $ */ <nl> +/* $ OpenBSD : if . c , v 1 . 484 2017 / 02 / 01 01 : 25 : 19 jsg Exp $ */ <nl> /* $ NetBSD : if . c , v 1 . 35 1996 / 05 / 07 05 : 26 : 04 thorpej Exp $ */ <nl>  <nl> /* <nl> if_attachhead ( struct ifnet * ifp ) <nl> { <nl> int s ; <nl>  <nl> - s = splsoftnet (); <nl> + NET_LOCK ( s ); <nl> if_attach_common ( ifp ); <nl> TAILQ_INSERT_HEAD (& ifnet , ifp , if_list ); <nl> if_attachsetup ( ifp ); <nl> - splx ( s ); <nl> + NET_UNLOCK ( s ); <nl> } <nl>  <nl> void
-/* $ OpenBSD : smi . c , v 1 . 8 2012 / 09 / 17 16 : 43 : 59 reyk Exp $ */ <nl> +/* $ OpenBSD : smi . c , v 1 . 9 2013 / 06 / 21 07 : 07 : 55 gerhard Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 2007 , 2008 Reyk Floeter < reyk @ openbsd . org > <nl> smi_delete ( struct oid * oid ) <nl> { <nl> struct oid key , * value ; <nl>  <nl> + bzero (& key , sizeof ( key )); <nl> bcopy (& oid -> o_id , & key . o_id , sizeof ( struct ber_oid )); <nl> if (( value = RB_FIND ( oidtree , & smi_oidtree , & key )) != NULL && <nl> value == oid ) <nl> smi_insert ( struct oid * oid ) <nl> if (( oid -> o_flags & OID_TABLE ) && oid -> o_get == NULL ) <nl> fatalx (" smi_insert : invalid MIB table "); <nl>  <nl> + bzero (& key , sizeof ( key )); <nl> bcopy (& oid -> o_id , & key . o_id , sizeof ( struct ber_oid )); <nl> value = RB_FIND ( oidtree , & smi_oidtree , & key ); <nl> if ( value != NULL )
*/ <nl>  <nl> # include " includes . h " <nl> - RCSID ("$ OpenBSD : readconf . c , v 1 . 86 2001 / 08 / 01 22 : 03 : 33 markus Exp $"); <nl> + RCSID ("$ OpenBSD : readconf . c , v 1 . 87 2001 / 08 / 28 09 : 51 : 26 markus Exp $"); <nl>  <nl> # include " ssh . h " <nl> # include " xmalloc . h " <nl> parse_int : <nl> if ( fwd_port == 0 ) <nl> fatal ("%. 200s line % d : Badly formatted port number .", <nl> filename , linenum ); <nl> - add_local_forward ( options , fwd_port , " socks4 ", 0 ); <nl> + if (* activep ) <nl> + add_local_forward ( options , fwd_port , " socks4 ", 0 ); <nl> break ; <nl>  <nl> case oHost :
-/* $ OpenBSD : machdep . c , v 1 . 150 2014 / 03 / 13 03 : 52 : 56 dlg Exp $ */ <nl> +/* $ OpenBSD : machdep . c , v 1 . 151 2014 / 03 / 21 03 : 56 : 49 guenther Exp $ */ <nl> /* $ NetBSD : machdep . c , v 1 . 108 2001 / 07 / 24 19 : 30 : 14 eeh Exp $ */ <nl>  <nl> /*- <nl> struct blink_led_softc { <nl> SLIST_HEAD (, blink_led ) bls_head ; <nl> int bls_on ; <nl> struct timeout bls_to ; <nl> -} blink_sc = { SLIST_HEAD_INITIALIZER ( bls_head ), 0 }; <nl> +} blink_sc = { SLIST_HEAD_INITIALIZER ( blink_sc . bls_head ), 0 }; <nl>  <nl> void <nl> blink_led_register ( struct blink_led * l )
-/* $ OpenBSD : hme . c , v 1 . 4 2001 / 08 / 29 05 : 33 : 10 jason Exp $ */ <nl> +/* $ OpenBSD : hme . c , v 1 . 5 2001 / 09 / 20 17 : 58 : 33 jason Exp $ */ <nl> /* $ NetBSD : hme . c , v 1 . 21 2001 / 07 / 07 15 : 59 : 37 thorpej Exp $ */ <nl>  <nl> /*- <nl> # include " bpfilter . h " <nl> # include " vlan . h " <nl>  <nl> -# define HMEDEBUG <nl> +# undef HMEDEBUG <nl>  <nl> # include < sys / param . h > <nl> # include < sys / systm . h > <nl> hme_init ( sc ) <nl> bus_space_write_4 ( t , mac , HME_MACI_FCCNT , 0 ); <nl> bus_space_write_4 ( t , mac , HME_MACI_EXCNT , 0 ); <nl> bus_space_write_4 ( t , mac , HME_MACI_LTCNT , 0 ); <nl> - v = ETHERMTU + <nl> + v = ETHERMTU + sizeof ( struct ether_header ) + <nl> # if NVLAN > 0 <nl> EVL_ENCAPLEN + <nl> # endif <nl> hme_init ( sc ) <nl> bus_space_write_4 ( t , etx , HME_ETXI_RSIZE , sc -> sc_rb . rb_ntbuf ); <nl>  <nl> bus_space_write_4 ( t , erx , HME_ERXI_RING , sc -> sc_rb . rb_rxddma ); <nl> - v = ETHERMTU + <nl> + v = ETHERMTU + sizeof ( struct ether_header ) + <nl> # if NVLAN > 0 <nl> EVL_ENCAPLEN + <nl> # endif <nl> hme_read ( sc , ix , len ) <nl> struct mbuf * m ; <nl>  <nl> if ( len <= sizeof ( struct ether_header ) || <nl> - ( len > ETHERMTU + <nl> + ( len > ETHERMTU + sizeof ( struct ether_header ) + <nl> # if NVLAN > 1 <nl> EVL_ENCAPLEN + <nl> # endif
-/* $ OpenBSD : bfd . c , v 1 . 27 2016 / 09 / 17 07 : 35 : 05 phessler Exp $ */ <nl> +/* $ OpenBSD : bfd . c , v 1 . 28 2016 / 09 / 18 21 : 00 : 55 phessler Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 2016 Peter Hessler < phessler @ openbsd . org > <nl> bfd_send_control ( void * x ) <nl> m -> m_len = m -> m_pkthdr . len = sizeof (* bfd ); <nl> h = mtod ( m , struct bfd_header *); <nl>  <nl> - memset ( bfd , 0xff , sizeof (* h )); /* canary */ <nl> + memset ( h , 0xff , sizeof (* h )); /* canary */ <nl>  <nl> h -> bfd_ver_diag = (( BFD_VERSION << 5 ) | ( bfd -> bc_neighbor -> bn_ldiag )); <nl> h -> bfd_sta_flags = ( bfd -> bc_neighbor -> bn_lstate << 6 );
-/* $ OpenBSD : gzio . c , v 1 . 7 2002 / 03 / 12 00 : 25 : 57 millert Exp $ */ <nl> +/* $ OpenBSD : gzio . c , v 1 . 8 2002 / 07 / 06 00 : 11 : 40 millert Exp $ */ <nl> /* gzio . c -- IO on . gz files <nl> * Copyright ( C ) 1995 - 2002 Jean - loup Gailly . <nl> * For conditions of distribution and use , see copyright notice in zlib . h <nl> z_off_t ZEXPORT gzseek ( file , offset , whence ) <nl> /* At this point , offset is the number of zero bytes to write . */ <nl> if ( s -> inbuf == Z_NULL ) { <nl> s -> inbuf = ( Byte *) ALLOC ( Z_BUFSIZE ); /* for seeking */ <nl> + if ( s -> inbuf == Z_NULL ) return - 1L ; <nl> zmemzero ( s -> inbuf , Z_BUFSIZE ); <nl> } <nl> while ( offset > 0 ) { <nl> z_off_t ZEXPORT gzseek ( file , offset , whence ) <nl>  <nl> if ( offset != 0 && s -> outbuf == Z_NULL ) { <nl> s -> outbuf = ( Byte *) ALLOC ( Z_BUFSIZE ); <nl> + if ( s -> outbuf == Z_NULL ) return - 1L ; <nl> } <nl> while ( offset > 0 ) { <nl> int size = Z_BUFSIZE ; <nl> const char * ZEXPORT gzerror ( file , errnum ) <nl> * errnum = s -> z_err ; <nl> if (* errnum == Z_OK ) return ( const char *)""; <nl>  <nl> - m = ( char *)(* errnum == Z_ERRNO ? zstrerror ( errno ) : s -> stream . msg ); <nl> + m = ( char *)(* errnum == Z_ERRNO ? zstrerror ( errno ) : s -> stream . msg ); <nl>  <nl> if ( m == NULL || * m == '\ 0 ') m = ( char *) ERR_MSG ( s -> z_err ); <nl>  <nl> TRYFREE ( s -> msg ); <nl> s -> msg = ( char *) ALLOC ( strlen ( s -> path ) + strlen ( m ) + 3 ); <nl> + if ( s -> msg == Z_NULL ) return ( const char *) ERR_MSG ( Z_MEM_ERROR ); <nl> strcpy ( s -> msg , s -> path ); <nl> strcat ( s -> msg , ": "); <nl> strcat ( s -> msg , m );
-/* $ OpenBSD : cpu . c , v 1 . 65 2017 / 04 / 20 15 : 42 : 26 visa Exp $ */ <nl> +/* $ OpenBSD : cpu . c , v 1 . 66 2017 / 04 / 22 15 : 43 : 35 visa Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 1997 - 2004 Opsycon AB ( www . opsycon . se ) <nl> cpuattach ( struct device * parent , struct device * dev , void * aux ) <nl> case 0x05 : <nl> printf (" STC Loongson3 % c CPU ", ' A ' + vers_min - 5 ); <nl> break ; <nl> + case 0x08 : <nl> + printf (" STC Loongson3A2000 / 3B2000 CPU "); <nl> + break ; <nl> default : <nl> printf (" Unknown STC Loongson CPU type (% 02x )", <nl> ch -> c0prid & 0xff ); <nl> cpuattach ( struct device * parent , struct device * dev , void * aux ) <nl> case 0x05 : <nl> printf (" STC Loongson3 % c FPU ", ' A ' + vers_min - 5 ); <nl> break ; <nl> + case 0x08 : <nl> + printf (" STC Loongson3A2000 / 3B2000 FPU "); <nl> + break ; <nl> default : <nl> printf (" Unknown STC Loongson FPU type (% 02x )", <nl> ch -> c1prid & 0xff );
-/* $ OpenBSD : powernow - k7 . c , v 1 . 8 2005 / 11 / 26 11 : 22 : 12 tedu Exp $ */ <nl> +/* $ OpenBSD : powernow - k7 . c , v 1 . 9 2005 / 11 / 28 17 : 48 : 02 mickey Exp $ */ <nl> /* <nl> * Copyright ( c ) 2004 Martin Vgiard . <nl> * All rights reserved . <nl> k7_powernow_setperf ( int level ) <nl> wrmsr ( MSR_AMDK7_FIDVID_CTL , ctl | PN7_CTR_FIDC ); <nl> } <nl>  <nl> - if ( cstate -> errata_a0 ); <nl> + if ( cstate -> errata_a0 ) <nl> enable_intr (); <nl>  <nl> pentium_mhz = (( cstate -> state_table [ i ]. freq / 100000 )+ 1 )* 100 ;
-/* $ OpenBSD : config . c , v 1 . 22 2003 / 06 / 18 02 : 26 : 58 itojun Exp $ */ <nl> +/* $ OpenBSD : config . c , v 1 . 23 2006 / 03 / 22 10 : 49 : 17 claudio Exp $ */ <nl> /* $ KAME : config . c , v 1 . 62 2002 / 05 / 29 10 : 13 : 10 itojun Exp $ */ <nl>  <nl> /* <nl> get_prefix ( struct rainfo * rai ) <nl> { <nl> p = ( u_char *)& pp -> prefix ; <nl> ep = ( u_char *)(& pp -> prefix + 1 ); <nl> - while ( m < lim ) <nl> + while ( m < lim && p < ep ) <nl> * p ++ &= * m ++; <nl> while ( p < ep ) <nl> * p ++ = 0x00 ;
-/* $ OpenBSD : uipc_socket . c , v 1 . 103 2012 / 07 / 10 11 : 42 : 53 guenther Exp $ */ <nl> +/* $ OpenBSD : uipc_socket . c , v 1 . 104 2012 / 07 / 22 18 : 11 : 54 guenther Exp $ */ <nl> /* $ NetBSD : uipc_socket . c , v 1 . 21 1996 / 02 / 04 02 : 17 : 52 christos Exp $ */ <nl>  <nl> /* <nl> dontblock : <nl> } else { <nl> sbfree (& so -> so_rcv , m ); <nl> so -> so_rcv . sb_mb = m -> m_next ; <nl> - m -> m_next = 0 ; <nl> + m -> m_nextpkt = m -> m_next = NULL ; <nl> cm = m ; <nl> m = so -> so_rcv . sb_mb ; <nl> sbsync (& so -> so_rcv , nextrecord );
-/* $ OpenBSD : sshd . c , v 1 . 482 2017 / 02 / 06 09 : 22 : 51 djm Exp $ */ <nl> +/* $ OpenBSD : sshd . c , v 1 . 483 2017 / 02 / 24 03 : 16 : 34 djm Exp $ */ <nl> /* <nl> * Author : Tatu Ylonen < ylo @ cs . hut . fi > <nl> * Copyright ( c ) 1995 Tatu Ylonen < ylo @ cs . hut . fi >, Espoo , Finland <nl> server_listen ( void ) <nl> close ( listen_sock ); <nl> continue ; <nl> } <nl> + if ( fcntl ( listen_sock , F_SETFD , FD_CLOEXEC ) == - 1 ) { <nl> + verbose (" socket : CLOEXEC : % s ", strerror ( errno )); <nl> + close ( listen_sock ); <nl> + continue ; <nl> + } <nl> /* <nl> * Set socket options . <nl> * Allow local port reuse in TIME_WAIT .
-/* $ OpenBSD : message . c , v 1 . 19 1999 / 05 / 02 19 : 18 : 48 niklas Exp $ */ <nl> -/* $ EOM : message . c , v 1 . 132 1999 / 05 / 02 12 : 55 : 03 niklas Exp $ */ <nl> +/* $ OpenBSD : message . c , v 1 . 20 1999 / 05 / 03 22 : 45 : 04 niklas Exp $ */ <nl> +/* $ EOM : message . c , v 1 . 133 1999 / 05 / 03 07 : 58 : 37 niklas Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 1998 , 1999 Niklas Hallqvist . All rights reserved . <nl> message_recv ( struct message * msg ) <nl> if (! msg -> exchange ) <nl> { <nl> log_print (" message_recv : phase 1 message after ISAKMP SA is ready "); <nl> + message_free ( msg ); <nl> return - 1 ; <nl> } <nl> else if ( msg -> exchange -> last_sent )
-/* $ OpenBSD : bpf . c , v 1 . 159 2017 / 01 / 24 10 : 08 : 30 krw Exp $ */ <nl> +/* $ OpenBSD : bpf . c , v 1 . 160 2017 / 01 / 24 22 : 40 : 55 mpi Exp $ */ <nl> /* $ NetBSD : bpf . c , v 1 . 33 1997 / 02 / 21 23 : 59 : 35 thorpej Exp $ */ <nl>  <nl> /* <nl> bpfwrite ( dev_t dev , struct uio * uio , int ioflag ) <nl> if ( d -> bd_hdrcmplt && dst . ss_family == AF_UNSPEC ) <nl> dst . ss_family = pseudo_AF_HDRCMPLT ; <nl>  <nl> - s = splsoftnet (); <nl> + NET_LOCK ( s ); <nl> error = ifp -> if_output ( ifp , m , ( struct sockaddr *)& dst , NULL ); <nl> - splx ( s ); <nl> + NET_UNLOCK ( s ); <nl>  <nl> out : <nl> bpf_put ( d );
-/* $ OpenBSD : cmd - set - environment . c , v 1 . 6 2012 / 07 / 11 07 : 10 : 15 nicm Exp $ */ <nl> +/* $ OpenBSD : cmd - set - environment . c , v 1 . 7 2012 / 10 / 31 19 : 11 : 18 okan Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 2009 Nicholas Marriott < nicm @ users . sourceforge . net > <nl> cmd_set_environment_exec ( struct cmd * self , struct cmd_ctx * ctx ) <nl> return ( CMD_RETURN_ERROR ); <nl> } <nl>  <nl> - if ( args -> argc < 1 ) <nl> + if ( args -> argc < 2 ) <nl> value = NULL ; <nl> else <nl> value = args -> argv [ 1 ];
-/* $ OpenBSD : packet . c , v 1 . 215 2015 / 09 / 21 04 : 31 : 00 djm Exp $ */ <nl> +/* $ OpenBSD : packet . c , v 1 . 216 2015 / 10 / 21 11 : 33 : 03 gsoares Exp $ */ <nl> /* <nl> * Author : Tatu Ylonen < ylo @ cs . hut . fi > <nl> * Copyright ( c ) 1995 Tatu Ylonen < ylo @ cs . hut . fi >, Espoo , Finland <nl> ssh_packet_write_wait ( struct ssh * ssh ) <nl> NFDBITS ), sizeof ( fd_mask )); <nl> if ( setp == NULL ) <nl> return SSH_ERR_ALLOC_FAIL ; <nl> - if (( r = ssh_packet_write_poll ( ssh )) != 0 ) <nl> + if (( r = ssh_packet_write_poll ( ssh )) != 0 ) { <nl> + free ( setp ); <nl> return r ; <nl> + } <nl> while ( ssh_packet_have_data_to_write ( ssh )) { <nl> memset ( setp , 0 , howmany ( state -> connection_out + 1 , <nl> NFDBITS ) * sizeof ( fd_mask ));
-/* $ OpenBSD : slaacd . c , v 1 . 23 2018 / 06 / 18 16 : 13 : 45 florian Exp $ */ <nl> +/* $ OpenBSD : slaacd . c , v 1 . 24 2018 / 07 / 07 12 : 08 : 07 sthen Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 2017 Florian Obser < florian @ openbsd . org > <nl> main ( int argc , char * argv []) <nl> log_procinit ( log_procnames [ slaacd_process ]); <nl>  <nl> if (( routesock = socket ( PF_ROUTE , SOCK_RAW | SOCK_CLOEXEC | <nl> - SOCK_NONBLOCK , 0 )) < 0 ) <nl> + SOCK_NONBLOCK , AF_INET6 )) < 0 ) <nl> fatal (" route socket "); <nl> + shutdown ( SHUT_RD , routesock ); <nl>  <nl> event_init (); <nl>  <nl> main ( int argc , char * argv []) <nl> sizeof ( filt )) == - 1 ) <nl> fatal (" ICMP6_FILTER "); <nl>  <nl> - if (( frontend_routesock = socket ( PF_ROUTE , SOCK_RAW | SOCK_CLOEXEC , 0 )) <nl> - < 0 ) <nl> + if (( frontend_routesock = socket ( PF_ROUTE , SOCK_RAW | SOCK_CLOEXEC , <nl> + AF_INET6 )) < 0 ) <nl> fatal (" route socket "); <nl>  <nl> rtfilter = ROUTE_FILTER ( RTM_IFINFO ) | ROUTE_FILTER ( RTM_NEWADDR ) |
-/* $ OpenBSD : ip_output . c , v 1 . 65 2000 / 03 / 17 10 : 25 : 22 angelos Exp $ */ <nl> +/* $ OpenBSD : ip_output . c , v 1 . 66 2000 / 03 / 30 04 : 53 : 36 angelos Exp $ */ <nl> /* $ NetBSD : ip_output . c , v 1 . 28 1996 / 02 / 13 23 : 43 : 07 christos Exp $ */ <nl>  <nl> /* <nl> sendit : <nl> } <nl>  <nl> /* We don ' t need this anymore */ <nl> - if ( re -> re_rt ) <nl> + if ( re -> re_rt ) { <nl> RTFREE ( re -> re_rt ); <nl> + re -> re_rt = NULL ; <nl> + } <nl>  <nl> /* Massage the IP header for use by the IPsec code */ <nl> ip -> ip_len = htons (( u_short ) ip -> ip_len );
-/* $ OpenBSD : rde_spf . c , v 1 . 18 2005 / 05 / 26 22 : 44 : 24 norby Exp $ */ <nl> +/* $ OpenBSD : rde_spf . c , v 1 . 19 2005 / 05 / 26 23 : 11 : 53 norby Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 2005 Esben Norby < norby @ openbsd . org > <nl> spf_calc ( struct area * area ) <nl> } <nl> } <nl>  <nl> - cand_list_dump (); <nl> + /* cand_list_dump (); */ <nl>  <nl> /* get next vertex */ <nl> v = cand_list_pop (); <nl> spf_calc ( struct area * area ) <nl> } <nl> } <nl>  <nl> - spf_dump ( area ); <nl> + /* spf_dump ( area ); */ <nl> log_debug (" spf_calc : calculation ended , area ID % s ", <nl> inet_ntoa ( area -> id )); <nl> 
-/* $ OpenBSD : mem . c , v 1 . 26 2015 / 08 / 28 00 : 03 : 53 deraadt Exp $ */ <nl> +/* $ OpenBSD : mem . c , v 1 . 27 2015 / 08 / 29 12 : 30 : 30 sthen Exp $ */ <nl> /* <nl> * Copyright ( c ) 1988 University of Utah . <nl> * Copyright ( c ) 1982 , 1986 , 1990 , 1993 <nl> mem_ioctl ( dev_t dev , u_long cmd , caddr_t data , int flags , struct proc * p ) <nl> md -> mr_owner [ sizeof ( md -> mr_owner ) - 1 ] = 0 ; <nl> if ( error == 0 ) <nl> error = mem_range_attr_set ( md , & mo -> mo_arg [ 0 ]); <nl> - free ( md , M_MEMDESC , nd * sizeof ( struct mem_range_desc )); <nl> + free ( md , M_MEMDESC , sizeof ( struct mem_range_desc )); <nl> break ; <nl> } <nl> return ( error );
hvn_nvs_cmd ( struct hvn_softc * sc , void * cmd , size_t cmdsize , uint64_t tid , <nl> return ( rv ); <nl> } <nl>  <nl> + if ( timo == 0 ) <nl> + return ( 0 ); <nl> + <nl> do { <nl> if ( cold ) <nl> delay ( 1000 );
-/* $ OpenBSD : i82596 . c , v 1 . 18 2002 / 12 / 19 01 : 25 : 21 mickey Exp $ */ <nl> +/* $ OpenBSD : i82596 . c , v 1 . 19 2003 / 01 / 07 08 : 25 : 23 mickey Exp $ */ <nl> /* $ NetBSD : i82586 . c , v 1 . 18 1998 / 08 / 15 04 : 42 : 42 mycroft Exp $ */ <nl>  <nl> /*- <nl> i82596_cmd_wait ( sc ) <nl> /* spin on i82596 command acknowledge ; wait at most 0 . 9 (!) seconds */ <nl> int i , off ; <nl>  <nl> - for ( i = 0 ; i < 180000 ; i ++) { <nl> + for ( i = 180000 ; i --; DELAY ( 5 )) { <nl> /* Read the command word */ <nl> off = IE_SCB_CMD ( sc -> scb ); <nl> bus_space_barrier ( sc -> bt , sc -> bh , off , 2 , <nl> i82596_cmd_wait ( sc ) <nl> # endif <nl> return ( 0 ); <nl> } <nl> - DELAY ( 5 ); <nl> } <nl>  <nl> printf (" i82596_cmd_wait : timo (% ssync ): scb status : % b \ n ", <nl> i82596_start_cmd ( sc , cmd , iecmdbuf , mask , async ) <nl> * According to the packet driver , the minimum timeout <nl> * should be . 369 seconds . <nl> */ <nl> - for ( i = 0 ; i < 73800 ; i ++) { <nl> + for ( i = 73800 ; i --; DELAY ( 5 )) { <nl> /* Read the command status */ <nl> off = IE_CMD_COMMON_STATUS ( iecmdbuf ); <nl> bus_space_barrier ( sc -> bt , sc -> bh , off , 2 , <nl> - BUS_SPACE_BARRIER_READ ); <nl> + BUS_SPACE_BARRIER_READ ); <nl> status = ( sc -> ie_bus_read16 )( sc , off ); <nl> if ( status & mask ) { <nl> # ifdef I82596_DEBUG <nl> i82596_start_cmd ( sc , cmd , iecmdbuf , mask , async ) <nl> # endif <nl> return ( 0 ); <nl> } <nl> - DELAY ( 5 ); <nl> } <nl>  <nl> } else {
-/* $ Id : mdoc_html . c , v 1 . 18 2010 / 05 / 24 00 : 00 : 10 schwarze Exp $ */ <nl> +/* $ Id : mdoc_html . c , v 1 . 19 2010 / 05 / 24 12 : 33 : 06 schwarze Exp $ */ <nl> /* <nl> * Copyright ( c ) 2008 , 2009 Kristaps Dzonsons < kristaps @ kth . se > <nl> * <nl> mdoc_bl_pre ( MDOC_ARGS ) <nl> return ( 0 ); <nl> if ( MDOC_BLOCK != n -> type ) <nl> return ( 1 ); <nl> - if ( MDOC_Enum != n -> data . list ) <nl> + if ( LIST_enum != n -> data . list ) <nl> return ( 1 ); <nl>  <nl> ord = malloc ( sizeof ( struct ord )); <nl> mdoc_bl_post ( MDOC_ARGS ) <nl>  <nl> if ( MDOC_BLOCK != n -> type ) <nl> return ; <nl> - if ( MDOC_Enum != n -> data . list ) <nl> + if ( LIST_enum != n -> data . list ) <nl> return ; <nl>  <nl> ord = h -> ords . head ;
-/* $ OpenBSD : nfs_socket . c , v 1 . 33 2003 / 06 / 02 23 : 28 : 19 millert Exp $ */ <nl> +/* $ OpenBSD : nfs_socket . c , v 1 . 34 2003 / 07 / 10 22 : 53 : 19 tedu Exp $ */ <nl> /* $ NetBSD : nfs_socket . c , v 1 . 27 1996 / 04 / 15 20 : 20 : 00 thorpej Exp $ */ <nl>  <nl> /* <nl> errout : <nl> error , <nl> rep -> r_nmp -> nm_mountp -> mnt_stat . f_mntfromname ); <nl> error = nfs_sndlock (& rep -> r_nmp -> nm_flag , rep ); <nl> - if (! error ) <nl> + if (! error ) { <nl> error = nfs_reconnect ( rep ); <nl> - if (! error ) <nl> - goto tryagain ; <nl> + if (! error ) <nl> + goto tryagain ; <nl> + nfs_sndunlock (& rep -> r_nmp -> nm_flag ); <nl> + } <nl> } <nl> } else { <nl> if (( so = rep -> r_nmp -> nm_so ) == NULL )
-/* $ OpenBSD : locore . c , v 1 . 16 2001 / 03 / 16 22 : 46 : 26 hugh Exp $ */ <nl> +/* $ OpenBSD : locore . c , v 1 . 17 2001 / 04 / 01 17 : 15 : 22 hugh Exp $ */ <nl> /* $ NetBSD : locore . c , v 1 . 43 2000 / 03 / 26 11 : 39 : 45 ragge Exp $ */ <nl> /* <nl> * Copyright ( c ) 1994 , 1998 Ludd , University of Lule }, Sweden . <nl> start () <nl> case VAX_BTYP_1303 : <nl> dep_call = & ka53_calls ; <nl> switch (( vax_siedata >> 8 ) & 0xFF ) { <nl> + case VAX_STYP_50 : <nl> + strcpy ( cpu_model , " MicroVAX 3100 model 85 or 90 "); <nl> + break ; <nl> case VAX_STYP_51 : <nl> strcpy ( cpu_model , " MicroVAX 3100 model 90 or 95 "); <nl> break ; <nl> start () <nl> case VAX_STYP_53 : <nl> strcpy ( cpu_model , " VAX 4000 105A "); <nl> break ; <nl> - case VAX_STYP_50 : <nl> default : <nl> strcpy ( cpu_model , " VAX - Unknown Cheetah Class "); <nl> }
void ssl_mutex_sem_create ( server_rec * s , pool * p ) <nl> } <nl> semctlbuf . sem_perm . uid = ap_user_id ; <nl> semctlbuf . sem_perm . gid = ap_group_id ; <nl> - semctlbuf . sem_perm . mode = 0660 ; <nl> + semctlbuf . sem_perm . mode = 0600 ; <nl> semctlarg . buf = & semctlbuf ; <nl> if ( semctl ( semid , 0 , IPC_SET , semctlarg ) < 0 ) { <nl> ssl_log ( s , SSL_LOG_ERROR | SSL_ADD_ERRNO ,
-/* $ OpenBSD : softraid . c , v 1 . 376 2016 / 05 / 31 15 : 19 : 12 jsing Exp $ */ <nl> +/* $ OpenBSD : softraid . c , v 1 . 377 2016 / 07 / 20 20 : 45 : 13 krw Exp $ */ <nl> /* <nl> * Copyright ( c ) 2007 , 2008 , 2009 Marco Peereboom < marco @ peereboom . us > <nl> * Copyright ( c ) 2008 Chris Kuethe < ckuethe @ openbsd . org > <nl> sr_ioctl_createraid ( struct sr_softc * sc , struct bioc_createraid * bc , <nl> struct scsi_link * link ; <nl> struct device * dev ; <nl> char * uuid , devname [ 32 ]; <nl> - dev_t * dt ; <nl> + dev_t * dt = NULL ; <nl> int i , no_chunk , rv = EINVAL , target , vol ; <nl> int no_meta ; <nl>  <nl> sr_ioctl_createraid ( struct sr_softc * sc , struct bioc_createraid * bc , <nl>  <nl> sd -> sd_ready = 1 ; <nl>  <nl> + free ( dt , M_DEVBUF , bc -> bc_dev_list_len ); <nl> + <nl> return ( rv ); <nl>  <nl> unwind : <nl> + free ( dt , M_DEVBUF , bc -> bc_dev_list_len ); <nl> + <nl> sr_discipline_shutdown ( sd , 0 ); <nl>  <nl> if ( rv == EAGAIN )
-/* $ OpenBSD : if_bridge . c , v 1 . 307 2018 / 02 / 19 08 : 59 : 52 mpi Exp $ */ <nl> +/* $ OpenBSD : if_bridge . c , v 1 . 308 2018 / 04 / 19 22 : 31 : 25 dlg Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 1999 , 2000 Jason L . Wright ( jason @ thought . net ) <nl> bridge_ioctl ( struct ifnet * ifp , u_long cmd , caddr_t data ) <nl> error = ENOENT ; <nl> break ; <nl> } <nl> + if ( ifs -> if_type != IFT_ETHER && <nl> + ifs -> if_type != IFT_MPLSTUNNEL ) { <nl> + error = EINVAL ; <nl> + break ; <nl> + } <nl> if ( ifs -> if_bridgeport != NULL ) { <nl> error = EBUSY ; <nl> break ;
-/* $ OpenBSD : cmd . c , v 1 . 17 2005 / 05 / 20 05 : 13 : 44 joris Exp $ */ <nl> +/* $ OpenBSD : cmd . c , v 1 . 18 2005 / 05 / 23 17 : 43 : 54 xsa Exp $ */ <nl> /* <nl> * Copyright ( c ) 2005 Joris Vink < joris @ openbsd . org > <nl> * All rights reserved . <nl> cvs_startcmd ( struct cvs_cmd * cmd , int argc , char ** argv ) <nl> if (( root = cvsroot_get (".")) == NULL ) <nl> return ( CVS_EX_BADROOT ); <nl>  <nl> + if ( cvs_trace ) <nl> + cvs_log ( LP_TRACE , " cvs_startcmd () CVSROOT =% s ", root -> cr_str ); <nl> + <nl> if ( root -> cr_method != CVS_METHOD_LOCAL ) { <nl> if ( cvs_connect ( root ) < 0 ) <nl> return ( CVS_EX_PROTO );
-/* $ OpenBSD : brconfig . c , v 1 . 23 2002 / 12 / 18 16 : 10 : 27 markus Exp $ */ <nl> +/* $ OpenBSD : brconfig . c , v 1 . 24 2003 / 05 / 30 21 : 16 : 50 henning Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 1999 , 2000 Jason L . Wright ( jason @ thought . net ) <nl> int <nl> bridge_timeout ( int s , char * brdg , char * arg ) <nl> { <nl> struct ifbrparam bp ; <nl> - u_int32_t newtime ; <nl> + int newtime ; <nl> char * endptr ; <nl>  <nl> errno = 0 ; <nl> - newtime = strtoul ( arg , & endptr , 0 ); <nl> - if ( arg [ 0 ] == '\ 0 ' || endptr [ 0 ] != '\ 0 ' || <nl> + newtime = strtol ( arg , & endptr , 0 ); <nl> + if ( arg [ 0 ] == '\ 0 ' || endptr [ 0 ] != '\ 0 ' || newtime < 0 || <nl> ( errno == ERANGE && newtime == ULONG_MAX )) { <nl> printf (" invalid arg for timeout : % s \ n ", arg ); <nl> return ( EX_USAGE );
-/* $ OpenBSD : systrace . c , v 1 . 58 2012 / 12 / 04 02 : 24 : 47 deraadt Exp $ */ <nl> +/* $ OpenBSD : systrace . c , v 1 . 59 2014 / 08 / 09 22 : 44 : 15 guenther Exp $ */ <nl> /* <nl> * Copyright 2002 Niels Provos < provos @ citi . umich . edu > <nl> * All rights reserved . <nl> main ( int argc , char ** argv ) <nl>  <nl> if ( pidattach == 0 ) { <nl> /* Run a command and attach to it */ <nl> - if (( args = calloc ( argc + 1 , sizeof ( char *))) == NULL ) <nl> + args = reallocarray ( NULL , argc + 1 , sizeof ( char *)); <nl> + if ( args == NULL ) <nl> err ( 1 , " malloc "); <nl>  <nl> for ( i = 0 ; i < argc ; i ++)
-/* $ OpenBSD : trap . c , v 1 . 15 1997 / 09 / 19 17 : 16 : 14 niklas Exp $ */ <nl> +/* $ OpenBSD : trap . c , v 1 . 16 1997 / 10 / 07 22 : 52 : 05 niklas Exp $ */ <nl> /* $ NetBSD : trap . c , v 1 . 56 1997 / 07 / 16 00 : 01 : 47 is Exp $ */ <nl>  <nl> /* <nl> int mmudebug = 0 ; <nl> extern struct pcb * curpcb ; <nl> extern char fubail [], subail []; <nl>  <nl> +/* XXX until we get it from m68k / cpu . h */ <nl> + extern void regdump __P (( struct trapframe *, int )); <nl> + <nl> int _write_back __P (( u_int , u_int , u_int , u_int , vm_map_t )); <nl> void userret __P (( struct proc *, int , u_quad_t )); <nl> void panictrap __P (( int , u_int , u_int , struct frame *));
* IMPLIED WARRANTIES , INCLUDING , WITHOUT LIMITATION , THE IMPLIED <nl> * WARRANTIES OF MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE . <nl> * <nl> - * $ OpenBSD : arp . c , v 1 . 14 2005 / 07 / 09 01 : 44 : 16 brad Exp $ <nl> + * $ OpenBSD : arp . c , v 1 . 15 2008 / 05 / 06 06 : 34 : 10 claudio Exp $ <nl> * <nl> */ <nl>  <nl> arp_EtherAddr ( int s , struct in_addr ipaddr , struct sockaddr_dl * hwaddr , <nl> ifm = ( struct if_msghdr *) ptr ; /* On if_msghdr */ <nl> if ( ifm -> ifm_type != RTM_IFINFO ) <nl> break ; <nl> + ptr += ifm -> ifm_msglen ; <nl> + if ( ifm -> ifm_version != RTM_VERSION ) <nl> + continue ; <nl> dl = ( struct sockaddr_dl *)( ifm + 1 ); /* Single _dl at end */ <nl> skip = ( ifm -> ifm_flags & ( IFF_UP | IFF_BROADCAST | IFF_POINTOPOINT | <nl> IFF_NOARP | IFF_LOOPBACK )) != ( IFF_UP | IFF_BROADCAST ); <nl> - ptr += ifm -> ifm_msglen ; /* First ifa_msghdr */ <nl> while ( ptr < end ) { <nl> ifam = ( struct ifa_msghdr *) ptr ; /* Next ifa_msghdr ( alias ) */ <nl> if ( ifam -> ifam_type != RTM_NEWADDR ) /* finished ? */ <nl> break ; <nl> ptr += ifam -> ifam_msglen ; <nl> + if ( ifam -> ifam_version != RTM_VERSION ) <nl> + continue ; <nl> if ( skip || ( ifam -> ifam_addrs & ( RTA_NETMASK | RTA_IFA )) != <nl> ( RTA_NETMASK | RTA_IFA )) <nl> continue ;
elf32_arm_size_dynamic_sections ( bfd * output_bfd ATTRIBUTE_UNUSED , <nl> if ( elf_hash_table ( info )-> dynamic_sections_created ) <nl> { <nl> /* Set the contents of the . interp section to the interpreter . */ <nl> - if ( info -> executable ) <nl> + if ( info -> executable && ! info -> static_link ) <nl> { <nl> s = bfd_get_section_by_name ( dynobj , ". interp "); <nl> BFD_ASSERT ( s != NULL );
-/* $ OpenBSD : fetch . c , v 1 . 120 2014 / 05 / 19 20 : 05 : 09 jca Exp $ */ <nl> +/* $ OpenBSD : fetch . c , v 1 . 121 2014 / 05 / 19 20 : 09 : 22 jca Exp $ */ <nl> /* $ NetBSD : fetch . c , v 1 . 14 1997 / 08 / 18 10 : 20 : 20 lukem Exp $ */ <nl>  <nl> /*- <nl> again : <nl> } <nl>  <nl> if ( ssl_check_hostname ( cert , host ) != 0 ) { <nl> + X509_free ( cert ); <nl> fprintf ( ttyout , "% s : host `% s ' not present in " <nl> " server certificate \ n ", <nl> getprogname (), host );
-/* $ OpenBSD : uipc_usrreq . c , v 1 . 100 2016 / 07 / 19 05 : 30 : 48 tedu Exp $ */ <nl> +/* $ OpenBSD : uipc_usrreq . c , v 1 . 101 2016 / 08 / 17 13 : 53 : 14 bluhm Exp $ */ <nl> /* $ NetBSD : uipc_usrreq . c , v 1 . 18 1996 / 02 / 09 19 : 00 : 50 christos Exp $ */ <nl>  <nl> /* <nl> uipc_usrreq ( struct socket * so , int req , struct mbuf * m , struct mbuf * nam , <nl> if ( control ) { <nl> if ( sbappendcontrol ( rcv , m , control )) <nl> control = NULL ; <nl> + else { <nl> + error = ENOBUFS ; <nl> + break ; <nl> + } <nl> } else if ( so -> so_type == SOCK_SEQPACKET ) <nl> sbappendrecord ( rcv , m ); <nl> else
-/* $ OpenBSD : ieee80211_output . c , v 1 . 87 2009 / 05 / 24 07 : 46 : 04 damien Exp $ */ <nl> +/* $ OpenBSD : ieee80211_output . c , v 1 . 88 2010 / 07 / 17 16 : 30 : 01 damien Exp $ */ <nl> /* $ NetBSD : ieee80211_output . c , v 1 . 13 2004 / 05 / 31 11 : 02 : 55 dyoung Exp $ */ <nl>  <nl> /*- <nl> struct mbuf * <nl> ieee80211_get_addba_resp ( struct ieee80211com * ic , struct ieee80211_node * ni , <nl> u_int8_t tid , u_int8_t token , u_int16_t status ) <nl> { <nl> - struct ieee80211_tx_ba * ba = & ni -> ni_tx_ba [ tid ]; <nl> + struct ieee80211_rx_ba * ba = & ni -> ni_rx_ba [ tid ]; <nl> struct mbuf * m ; <nl> u_int8_t * frm ; <nl> u_int16_t params ;
-/* $ OpenBSD : isa . c , v 1 . 15 1996 / 08 / 15 05 : 26 : 19 deraadt Exp $ */ <nl> +/* $ OpenBSD : isa . c , v 1 . 16 1996 / 08 / 15 05 : 30 : 45 deraadt Exp $ */ <nl> /* $ NetBSD : isa . c , v 1 . 85 1996 / 05 / 14 00 : 31 : 04 thorpej Exp $ */ <nl>  <nl> /*- <nl> isascan ( parent , match ) <nl> struct isa_attach_args ia ; <nl> struct emap * io_map , * mem_map , * irq_map , * drq_map ; <nl>  <nl> + if ( cf -> cf_loc [ 6 ] == - 1 ) /* pnp device , scanned later */ <nl> + return ; <nl> + <nl> io_map = find_emap (" io "); <nl> mem_map = find_emap (" mem "); <nl> irq_map = find_emap (" irq ");
-/* $ OpenBSD : smtp_session . c , v 1 . 90 2009 / 05 / 19 11 : 42 : 52 jacekm Exp $ */ <nl> +/* $ OpenBSD : smtp_session . c , v 1 . 91 2009 / 05 / 19 12 : 33 : 53 jacekm Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 2008 Gilles Chehade < gilles @ openbsd . org > <nl> session_rfc3207_stls_handler ( struct session * s , char * args ) <nl> return 1 ; <nl> } <nl>  <nl> + if ( s -> s_state != S_HELO ) { <nl> + session_respond ( s , " 503 TLS not allowed at this stage "); <nl> + return 1 ; <nl> + } <nl> + <nl> if ( args != NULL ) { <nl> session_respond ( s , " 501 No parameters allowed "); <nl> return 1 ;
-/* $ OpenBSD : rt2860 . c , v 1 . 50 2010 / 04 / 10 07 : 57 : 21 damien Exp $ */ <nl> +/* $ OpenBSD : rt2860 . c , v 1 . 51 2010 / 04 / 12 18 : 02 : 31 damien Exp $ */ <nl>  <nl> /*- <nl> * Copyright ( c ) 2007 - 2010 Damien Bergamini < damien . bergamini @ free . fr > <nl> rt2860_tx ( struct rt2860_softc * sc , struct mbuf * m , struct ieee80211_node * ni ) <nl> /* setup TX Wireless Information */ <nl> txwi = data -> txwi ; <nl> txwi -> flags = 0 ; <nl> - txwi -> xflags = 0 ; <nl> + /* let HW generate seq numbers for non - QoS frames */ <nl> + txwi -> xflags = hasqos ? 0 : RT2860_TX_NSEQ ; <nl> txwi -> wcid = ( type == IEEE80211_FC0_TYPE_DATA ) ? <nl> RT2860_AID2WCID ( ni -> ni_associd ) : 0xff ; <nl> txwi -> len = htole16 ( m -> m_pkthdr . len ); <nl> rt2860_setup_beacon ( struct rt2860_softc * sc ) <nl> txwi . phy |= htole16 ( RT2860_PHY_OFDM ); <nl> txwi . txop = RT2860_TX_TXOP_HT ; <nl> txwi . flags = RT2860_TX_TS ; <nl> + txwi . xflags = RT2860_TX_NSEQ ; <nl>  <nl> RAL_WRITE_REGION_1 ( sc , RT2860_BCN_BASE ( 0 ), <nl> ( uint8_t *)& txwi , sizeof txwi );
_dl_lookup_symbol ( const char * undef_name , const Elf32_Sym ** ref , <nl> { <nl> Elf32_Addr a ; <nl> const Elf32_Sym * s ; <nl> - } weak_value = { 0 , NULL }; <nl> + } weak_value ; /* = { 0 , NULL }; breaks GCC 2 . 8 due to implicit memset */ <nl> + <nl> + _dl_memset (& weak_value , 0 , sizeof ( weak_value )); <nl>  <nl> /* Search the relevant loaded objects for a definition . */ <nl> for ( map = symbol_scope ; map ; map = map -> next )
-/* $ OpenBSD : kern_malloc . c , v 1 . 31 2001 / 05 / 14 08 : 03 : 13 angelos Exp $ */ <nl> +/* $ OpenBSD : kern_malloc . c , v 1 . 32 2001 / 06 / 21 14 : 27 : 13 niklas Exp $ */ <nl> /* $ NetBSD : kern_malloc . c , v 1 . 15 . 4 . 2 1996 / 06 / 13 17 : 10 : 56 cgd Exp $ */ <nl>  <nl> /* <nl> free ( addr , type ) <nl> return ; <nl> # endif <nl>  <nl> +# ifdef DIAGNOSTIC <nl> + if ( addr < ( void *) kmembase || addr >= ( void *) kmemlimit ) <nl> + panic (" free of non - malloced addr % p type % s ", addr , <nl> + memname [ type ]); <nl> +# endif <nl> + <nl> kup = btokup ( addr ); <nl> size = 1 << kup -> ku_indx ; <nl> kbp = & bucket [ kup -> ku_indx ];
-/* $ OpenBSD : stat_backend . c , v 1 . 8 2013 / 05 / 24 17 : 03 : 14 eric Exp $ */ <nl> +/* $ OpenBSD : stat_backend . c , v 1 . 9 2014 / 07 / 08 10 : 22 : 15 eric Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 2012 Gilles Chehade < gilles @ poolp . org > <nl> stat_increment ( const char * key , size_t count ) <nl> { <nl> struct stat_value * value ; <nl>  <nl> + if ( count == 0 ) <nl> + return ; <nl> + <nl> value = stat_counter ( count ); <nl>  <nl> m_create ( p_control , IMSG_STAT_INCREMENT , 0 , 0 , - 1 ); <nl> stat_decrement ( const char * key , size_t count ) <nl> { <nl> struct stat_value * value ; <nl>  <nl> + if ( count == 0 ) <nl> + return ; <nl> + <nl> value = stat_counter ( count ); <nl>  <nl> m_create ( p_control , IMSG_STAT_DECREMENT , 0 , 0 , - 1 );
-/* $ OpenBSD : agp_i810 . c , v 1 . 57 2009 / 06 / 06 11 : 11 : 10 oga Exp $ */ <nl> +/* $ OpenBSD : agp_i810 . c , v 1 . 58 2009 / 11 / 11 19 : 35 : 34 deraadt Exp $ */ <nl>  <nl> /*- <nl> * Copyright ( c ) 2000 Doug Rabson <nl> agp_i810_alloc_memory ( void * softc , int type , vsize_t size ) <nl> * get their physical address . <nl> */ <nl> if (( mem -> am_dmaseg = malloc ( sizeof (* mem -> am_dmaseg ), M_AGP , <nl> - M_WAITOK | M_CANFAIL )) == NULL ) <nl> + M_WAITOK | M_CANFAIL )) == NULL ) { <nl> + free ( mem , M_AGP ); <nl> return ( NULL ); <nl> + } <nl>  <nl> if (( error = agp_alloc_dmamem ( sc -> sc_dmat , size , <nl> & mem -> am_dmamap , & mem -> am_physical , mem -> am_dmaseg )) != 0 ) { <nl> + free ( mem -> am_dmaseg , M_AGP ); <nl> free ( mem , M_AGP ); <nl> printf (" agp : agp_alloc_dmamem (% d )\ n ", error ); <nl> return ( NULL );
-/* $ OpenBSD : relay_http . c , v 1 . 17 2014 / 04 / 15 22 : 35 : 11 andre Exp $ */ <nl> +/* $ OpenBSD : relay_http . c , v 1 . 18 2014 / 04 / 20 16 : 18 : 32 reyk Exp $ */ <nl>  <nl> /* <nl> * Copyright ( c ) 2006 - 2012 Reyk Floeter < reyk @ openbsd . org > <nl> relay_expand_http ( struct ctl_relay_event * cre , char * val , char * buf , size_t len ) <nl> struct relay * rlay = con -> se_relay ; <nl> char ibuf [ 128 ]; <nl>  <nl> - ( void ) strlcpy ( buf , val , len ); <nl> + if ( strlcpy ( buf , val , len ) >= len ) <nl> + return ( NULL ); <nl>  <nl> if ( strstr ( val , "$ REMOTE_ ") != NULL ) { <nl> if ( strstr ( val , "$ REMOTE_ADDR ") != NULL ) {
-/* $ OpenBSD : azalia_codec . c , v 1 . 163 2014 / 07 / 13 23 : 10 : 23 deraadt Exp $ */ <nl> +/* $ OpenBSD : azalia_codec . c , v 1 . 164 2014 / 11 / 17 16 : 34 : 51 landry Exp $ */ <nl> /* $ NetBSD : azalia_codec . c , v 1 . 8 2006 / 05 / 10 11 : 17 : 27 kent Exp $ */ <nl>  <nl> /*- <nl> azalia_codec_init_vtbl ( codec_t * this ) <nl> this -> name = " Realtek ALC885 "; <nl> this -> qrks |= AZ_QRK_WID_CDIN_1C | AZ_QRK_WID_BEEP_1D ; <nl> if ( this -> subid == 0x00a1106b || /* APPLE_MB3 */ <nl> + this -> subid == 0xcb7910de || /* APPLE_MACMINI3_1 ( line - in + hp ) */ <nl> this -> subid == 0x00a0106b || /* APPLE_MB3_1 */ <nl> this -> subid == 0x00a3106b ) { /* APPLE_MB4 */ <nl> this -> qrks |= AZ_QRK_GPIO_UNMUTE_0 ; <nl> } <nl> if ( this -> subid == 0x00a1106b || <nl> + this -> subid == 0xcb7910de || /* APPLE_MACMINI3_1 ( internal spkr ) */ <nl> this -> subid == 0x00a0106b ) <nl> this -> qrks |= AZ_QRK_WID_OVREF50 ; <nl> break ;
static void mmubooke_dump_mmu ( FILE * f , fprintf_function cpu_fprintf , <nl> mask = ~( entry -> size - 1 ); <nl> ea = entry -> EPN & mask ; <nl> pa = entry -> RPN & mask ; <nl> -# if ( TARGET_PHYS_ADDR_SPACE_BITS >= 36 ) <nl> /* Extend the physical address to 36 bits */ <nl> pa |= ( hwaddr )( entry -> RPN & 0xF ) << 32 ; <nl> -# endif <nl> size /= 1024 ; <nl> if ( size >= 1024 ) { <nl> snprintf ( size_buf , sizeof ( size_buf ), "% 3 " PRId64 " M ", size / 1024 );
static int configuration_post_load ( void * opaque , int version_id ) <nl> const char * current_name = MACHINE_GET_CLASS ( current_machine )-> name ; <nl>  <nl> if ( strncmp ( state -> name , current_name , state -> len ) != 0 ) { <nl> - error_report (" Machine type received is '% s ' and local is '% s '", <nl> - state -> name , current_name ); <nl> + error_report (" Machine type received is '%.* s ' and local is '% s '", <nl> + ( int ) state -> len , state -> name , current_name ); <nl> return - EINVAL ; <nl> } <nl> return 0 ;
static coroutine_fn int qcow2_co_pdiscard ( BlockDriverState * bs , <nl>  <nl> if (! QEMU_IS_ALIGNED ( offset | count , s -> cluster_size )) { <nl> assert ( count < s -> cluster_size ); <nl> - return - ENOTSUP ; <nl> + /* Ignore partial clusters , except for the special case of the <nl> + * complete partial cluster at the end of an unaligned file */ <nl> + if (! QEMU_IS_ALIGNED ( offset , s -> cluster_size ) || <nl> + offset + count != bs -> total_sectors * BDRV_SECTOR_SIZE ) { <nl> + return - ENOTSUP ; <nl> + } <nl> } <nl>  <nl> qemu_co_mutex_lock (& s -> lock );
int64_t bdrv_getlength ( BlockDriverState * bs ) <nl> { <nl> int64_t ret = bdrv_nb_sectors ( bs ); <nl>  <nl> + ret = ret > INT64_MAX / BDRV_SECTOR_SIZE ? - EFBIG : ret ; <nl> return ret < 0 ? ret : ret * BDRV_SECTOR_SIZE ; <nl> } <nl> 
static void qvirtio_scsi_pci_free ( QVirtIOSCSI * vs ) <nl> qvirtqueue_cleanup ( vs -> dev -> bus , vs -> vq [ i ], vs -> qs -> alloc ); <nl> } <nl> qvirtio_pci_device_disable ( container_of ( vs -> dev , QVirtioPCIDevice , vdev )); <nl> - g_free ( vs -> dev ); <nl> + qvirtio_pci_device_free (( QVirtioPCIDevice *) vs -> dev ); <nl> qvirtio_scsi_stop ( vs -> qs ); <nl> g_free ( vs ); <nl> }
static void iscsi_readcapacity_sync ( IscsiLun * iscsilun , Error ** errp ) <nl>  <nl> if ( task == NULL || task -> status != SCSI_STATUS_GOOD ) { <nl> error_setg ( errp , " iSCSI : failed to send readcapacity10 command ."); <nl> + } else if (! iscsilun -> block_size || <nl> + iscsilun -> block_size % BDRV_SECTOR_SIZE ) { <nl> + error_setg ( errp , " iSCSI : the target returned an invalid " <nl> + " block size of % d .", iscsilun -> block_size ); <nl> } <nl> if ( task ) { <nl> scsi_free_scsi_task ( task );
# include " qemu / host - utils . h " <nl> # include " sysemu / qtest . h " <nl> # include " qemu / error - report . h " <nl> +# include " hw / empty_slot . h " <nl>  <nl> //# define DEBUG_BOARD_INIT <nl>  <nl> void mips_malta_init ( QEMUMachineInitArgs * args ) <nl> DeviceState * dev = qdev_create ( NULL , TYPE_MIPS_MALTA ); <nl> MaltaState * s = MIPS_MALTA ( dev ); <nl>  <nl> + /* The whole address space decoded by the GT - 64120A doesn ' t generate <nl> + exception when accessing invalid memory . Create an empty slot to <nl> + emulate this feature . */ <nl> + empty_slot_init ( 0 , 0x20000000 ); <nl> + <nl> qdev_init_nofail ( dev ); <nl>  <nl> /* Make sure the first 3 serial ports are associated with a device . */
static int xen_9pfs_connect ( struct XenDevice * xendev ) <nl> str = g_strdup_printf (" ring - ref % u ", i ); <nl> if ( xenstore_read_fe_int (& xen_9pdev -> xendev , str , <nl> & xen_9pdev -> rings [ i ]. ref ) == - 1 ) { <nl> + g_free ( str ); <nl> goto out ; <nl> } <nl> g_free ( str ); <nl> str = g_strdup_printf (" event - channel -% u ", i ); <nl> if ( xenstore_read_fe_int (& xen_9pdev -> xendev , str , <nl> & xen_9pdev -> rings [ i ]. evtchn ) == - 1 ) { <nl> + g_free ( str ); <nl> goto out ; <nl> } <nl> g_free ( str );
void define_one_arm_cp_reg_with_opaque ( ARMCPU * cpu , <nl> ARMCPRegInfo * r2 = g_memdup ( r , sizeof ( ARMCPRegInfo )); <nl> int is64 = ( r -> type & ARM_CP_64BIT ) ? 1 : 0 ; <nl> * key = ENCODE_CP_REG ( r -> cp , is64 , r -> crn , crm , opc1 , opc2 ); <nl> - r2 -> opaque = opaque ; <nl> + if ( opaque ) { <nl> + r2 -> opaque = opaque ; <nl> + } <nl> /* Make sure reginfo passed to helpers for wildcarded regs <nl> * has the correct crm / opc1 / opc2 for this reg , not CP_ANY : <nl> */
void cpu_interrupt ( CPUState * env , int mask ) <nl> signals are used primarily to interrupt blocking syscalls . */ <nl> # else <nl> if ( use_icount ) { <nl> - env -> icount_decr . u16 . high = 0x8000 ; <nl> + env -> icount_decr . u16 . high = 0xffff ; <nl> # ifndef CONFIG_USER_ONLY <nl> /* CPU_INTERRUPT_EXIT isn ' t a real interrupt . It just means <nl> an async event happened and we need to process it . */
ipmb_checksum ( const unsigned char * data , int size , unsigned char start ) <nl>  <nl> static void continue_send ( IPMIBmcExtern * ibe ) <nl> { <nl> + int ret ; <nl> if ( ibe -> outlen == 0 ) { <nl> goto check_reset ; <nl> } <nl> send : <nl> - ibe -> outpos += qemu_chr_fe_write ( ibe -> chr , ibe -> outbuf + ibe -> outpos , <nl> - ibe -> outlen - ibe -> outpos ); <nl> + ret = qemu_chr_fe_write ( ibe -> chr , ibe -> outbuf + ibe -> outpos , <nl> + ibe -> outlen - ibe -> outpos ); <nl> + if ( ret > 0 ) { <nl> + ibe -> outpos += ret ; <nl> + } <nl> if ( ibe -> outpos < ibe -> outlen ) { <nl> /* Not fully transmitted , try again in a 10ms */ <nl> timer_mod_ns ( ibe -> extern_timer ,
static int iscsi_open ( BlockDriverState * bs , QDict * options , int flags , <nl> QemuOpts * opts ; <nl> Error * local_err = NULL ; <nl> const char * filename ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> if (( BDRV_SECTOR_SIZE % 512 ) != 0 ) { <nl> error_setg ( errp , " iSCSI : Invalid BDRV_SECTOR_SIZE . "
static int ioreq_runio_qemu_aio ( struct ioreq * ioreq ) <nl> break ; <nl> case BLKIF_OP_WRITE : <nl> case BLKIF_OP_WRITE_BARRIER : <nl> - ioreq -> aio_inflight ++; <nl> if (! ioreq -> req . nr_segments ) <nl> break ; <nl> + ioreq -> aio_inflight ++; <nl> bdrv_aio_writev ( blkdev -> bs , ioreq -> start / BLOCK_SIZE , <nl> & ioreq -> v , ioreq -> v . size / BLOCK_SIZE , <nl> qemu_aio_complete , ioreq );
static always_inline void gen_intermediate_code_internal ( CPUState * env , <nl> lj ++; <nl> while ( lj < j ) <nl> gen_opc_instr_start [ lj ++] = 0 ; <nl> - gen_opc_pc [ lj ] = ctx . pc ; <nl> - gen_opc_instr_start [ lj ] = 1 ; <nl> - gen_opc_icount [ lj ] = num_insns ; <nl> } <nl> + gen_opc_pc [ lj ] = ctx . pc ; <nl> + gen_opc_instr_start [ lj ] = 1 ; <nl> + gen_opc_icount [ lj ] = num_insns ; <nl> } <nl> if ( num_insns + 1 == max_insns && ( tb -> cflags & CF_LAST_IO )) <nl> gen_io_start ();
e1000e_write_lgcy_rx_descr ( E1000ECore * core , uint8_t * desc , <nl>  <nl> struct e1000_rx_desc * d = ( struct e1000_rx_desc *) desc ; <nl>  <nl> - memset ( d , 0 , sizeof (* d )); <nl> - <nl> assert (! rss_info -> enabled ); <nl>  <nl> d -> length = cpu_to_le16 ( length ); <nl> + d -> csum = 0 ; <nl>  <nl> e1000e_build_rx_metadata ( core , pkt , pkt != NULL , <nl> rss_info , <nl> e1000e_write_lgcy_rx_descr ( E1000ECore * core , uint8_t * desc , <nl> & d -> special ); <nl> d -> errors = ( uint8_t ) ( le32_to_cpu ( status_flags ) >> 24 ); <nl> d -> status = ( uint8_t ) le32_to_cpu ( status_flags ); <nl> + d -> special = 0 ; <nl> } <nl>  <nl> static inline void <nl> e1000e_write_ext_rx_descr ( E1000ECore * core , uint8_t * desc , <nl> { <nl> union e1000_rx_desc_extended * d = ( union e1000_rx_desc_extended *) desc ; <nl>  <nl> - memset ( d , 0 , sizeof (* d )); <nl> + memset (& d -> wb , 0 , sizeof ( d -> wb )); <nl>  <nl> d -> wb . upper . length = cpu_to_le16 ( length ); <nl>  <nl> e1000e_write_ps_rx_descr ( E1000ECore * core , uint8_t * desc , <nl> union e1000_rx_desc_packet_split * d = <nl> ( union e1000_rx_desc_packet_split *) desc ; <nl>  <nl> - memset ( d , 0 , sizeof (* d )); <nl> + memset (& d -> wb , 0 , sizeof ( d -> wb )); <nl>  <nl> d -> wb . middle . length0 = cpu_to_le16 ((* written )[ 0 ]); <nl> 
static int hda_codec_dev_init ( DeviceState * qdev , DeviceInfo * base ) <nl> if ( dev -> cad == - 1 ) { <nl> dev -> cad = bus -> next_cad ; <nl> } <nl> - if ( dev -> cad > 15 ) <nl> + if ( dev -> cad >= 15 ) { <nl> return - 1 ; <nl> + } <nl> bus -> next_cad = dev -> cad + 1 ; <nl> return info -> init ( dev ); <nl> } <nl> static const struct IntelHDAReg regtab [] = { <nl> [ ICH6_REG_WAKEEN ] = { <nl> . name = " WAKEEN ", <nl> . size = 2 , <nl> - . wmask = 0x3fff , <nl> + . wmask = 0x7fff , <nl> . offset = offsetof ( IntelHDAState , wake_en ), <nl> . whandler = intel_hda_set_wake_en , <nl> }, <nl> [ ICH6_REG_STATESTS ] = { <nl> . name = " STATESTS ", <nl> . size = 2 , <nl> - . wmask = 0x3fff , <nl> - . wclear = 0x3fff , <nl> + . wmask = 0x7fff , <nl> + . wclear = 0x7fff , <nl> . offset = offsetof ( IntelHDAState , state_sts ), <nl> . whandler = intel_hda_set_state_sts , <nl> },
static void readline_hist_add ( ReadLineState * rs , const char * cmdline ) <nl> if ( idx == READLINE_MAX_CMDS ) { <nl> /* Need to get one free slot */ <nl> free ( rs -> history [ 0 ]); <nl> - memcpy ( rs -> history , & rs -> history [ 1 ], <nl> - ( READLINE_MAX_CMDS - 1 ) * sizeof ( char *)); <nl> + memmove ( rs -> history , & rs -> history [ 1 ], <nl> + ( READLINE_MAX_CMDS - 1 ) * sizeof ( char *)); <nl> rs -> history [ READLINE_MAX_CMDS - 1 ] = NULL ; <nl> idx = READLINE_MAX_CMDS - 1 ; <nl> }
static void gen_ldst_pair ( DisasContext * ctx , uint32_t opc , int rd , <nl> const char * opn = " ldst_pair "; <nl> TCGv t0 , t1 ; <nl>  <nl> - if ( ctx -> hflags & MIPS_HFLAG_BMASK || rd == 31 || rd == base ) { <nl> + if ( ctx -> hflags & MIPS_HFLAG_BMASK || rd == 31 ) { <nl> generate_exception ( ctx , EXCP_RI ); <nl> return ; <nl> } <nl> static void gen_ldst_pair ( DisasContext * ctx , uint32_t opc , int rd , <nl>  <nl> switch ( opc ) { <nl> case LWP : <nl> + if ( rd == base ) { <nl> + generate_exception ( ctx , EXCP_RI ); <nl> + return ; <nl> + } <nl> save_cpu_state ( ctx , 0 ); <nl> op_ld_lw ( t1 , t0 , ctx ); <nl> gen_store_gpr ( t1 , rd ); <nl> static void gen_ldst_pair ( DisasContext * ctx , uint32_t opc , int rd , <nl> break ; <nl> # ifdef TARGET_MIPS64 <nl> case LDP : <nl> + if ( rd == base ) { <nl> + generate_exception ( ctx , EXCP_RI ); <nl> + return ; <nl> + } <nl> save_cpu_state ( ctx , 0 ); <nl> op_ld_ld ( t1 , t0 , ctx ); <nl> gen_store_gpr ( t1 , rd );
static int qxl_post_load ( void * opaque , int version ) <nl> qxl_create_guest_primary ( d , 1 , QXL_SYNC ); <nl>  <nl> /* replay surface - create and cursor - set commands */ <nl> - cmds = g_malloc0 ( sizeof ( QXLCommandExt ) * ( d -> ssd . num_surfaces + 1 )); <nl> + cmds = g_new0 ( QXLCommandExt , d -> ssd . num_surfaces + 1 ); <nl> for ( in = 0 , out = 0 ; in < d -> ssd . num_surfaces ; in ++) { <nl> if ( d -> guest_surfaces . cmds [ in ] == 0 ) { <nl> continue ;
bool qemu_log_in_addr_range ( uint64_t addr ) <nl> void qemu_set_dfilter_ranges ( const char * filter_spec ) <nl> { <nl> gchar ** ranges = g_strsplit ( filter_spec , ",", 0 ); <nl> + <nl> + if ( debug_regions ) { <nl> + g_array_unref ( debug_regions ); <nl> + debug_regions = NULL ; <nl> + } <nl> + <nl> if ( ranges ) { <nl> gchar ** next = ranges ; <nl> gchar * r = * next ++; <nl> + <nl> debug_regions = g_array_sized_new ( FALSE , FALSE , <nl> sizeof ( Range ), g_strv_length ( ranges )); <nl> while ( r ) {
static void do_dma_memory_set ( dma_addr_t addr , uint8_t c , dma_addr_t len ) <nl> while ( len > 0 ) { <nl> l = len < FILLBUF_SIZE ? len : FILLBUF_SIZE ; <nl> cpu_physical_memory_rw ( addr , fillbuf , l , true ); <nl> - len -= len ; <nl> - addr += len ; <nl> + len -= l ; <nl> + addr += l ; <nl> } <nl> } <nl> 
void * address_space_map ( AddressSpace * as , <nl> if ( bounce . buffer ) { <nl> return NULL ; <nl> } <nl> - bounce . buffer = qemu_memalign ( TARGET_PAGE_SIZE , TARGET_PAGE_SIZE ); <nl> + /* Avoid unbounded allocations */ <nl> + l = MIN ( l , TARGET_PAGE_SIZE ); <nl> + bounce . buffer = qemu_memalign ( TARGET_PAGE_SIZE , l ); <nl> bounce . addr = addr ; <nl> bounce . len = l ; <nl> 
static int tx_consume ( Rocker * r , DescInfo * info ) <nl> frag_addr = rocker_tlv_get_le64 ( tlvs [ ROCKER_TLV_TX_FRAG_ATTR_ADDR ]); <nl> frag_len = rocker_tlv_get_le16 ( tlvs [ ROCKER_TLV_TX_FRAG_ATTR_LEN ]); <nl>  <nl> + if ( iovcnt >= ROCKER_TX_FRAGS_MAX ) { <nl> + goto err_too_many_frags ; <nl> + } <nl> iov [ iovcnt ]. iov_len = frag_len ; <nl> iov [ iovcnt ]. iov_base = g_malloc ( frag_len ); <nl> if (! iov [ iovcnt ]. iov_base ) { <nl> static int tx_consume ( Rocker * r , DescInfo * info ) <nl> err = - ROCKER_ENXIO ; <nl> goto err_bad_io ; <nl> } <nl> - <nl> - if (++ iovcnt > ROCKER_TX_FRAGS_MAX ) { <nl> - goto err_too_many_frags ; <nl> - } <nl> + iovcnt ++; <nl> } <nl>  <nl> if ( iovcnt ) {
static int load_refcount_block ( BlockDriverState * bs , <nl> static int get_refcount ( BlockDriverState * bs , int64_t cluster_index ) <nl> { <nl> BDRVQcowState * s = bs -> opaque ; <nl> - int refcount_table_index , block_index ; <nl> + uint64_t refcount_table_index , block_index ; <nl> int64_t refcount_block_offset ; <nl> int ret ; <nl> uint16_t * refcount_block ;
static int nfs_file_create ( const char * url , QemuOpts * opts , Error ** errp ) <nl> ret = nfs_ftruncate ( client -> context , client -> fh , total_size ); <nl> nfs_client_close ( client ); <nl> out : <nl> + QDECREF ( options ); <nl> g_free ( client ); <nl> return ret ; <nl> }
void do_compare_and_swap32 ( void * cpu_env , int num ) <nl> uint32_t * value = ( uint32_t *)(( CPUX86State *) cpu_env )-> regs [ R_ECX ]; <nl> DPRINTF (" commpage : compare_and_swap32 (% x , new ,% p )\ n ", old , value ); <nl>  <nl> - if ( value && old == tswap32 (* value )) <nl> + if ( old == tswap32 (* value )) <nl> { <nl> uint32_t new = (( CPUX86State *) cpu_env )-> regs [ R_EDX ]; <nl> * value = tswap32 ( new );
static void init_mbr ( BDRVVVFATState * s ) <nl>  <nl> /* LBA is used when partition is outside the CHS geometry */ <nl> lba = sector2CHS ( s -> bs , & partition -> start_CHS , s -> first_sectors_number - 1 ); <nl> - lba |= sector2CHS ( s -> bs , & partition -> end_CHS , s -> sector_count ); <nl> + lba |= sector2CHS ( s -> bs , & partition -> end_CHS , s -> bs -> total_sectors - 1 ); <nl>  <nl> /* LBA partitions are identified only by start / length_sector_long not by CHS */ <nl> - partition -> start_sector_long = cpu_to_le32 ( s -> first_sectors_number - 1 ); <nl> - partition -> length_sector_long = cpu_to_le32 ( s -> sector_count - s -> first_sectors_number + 1 ); <nl> + partition -> start_sector_long = cpu_to_le32 ( s -> first_sectors_number - 1 ); <nl> + partition -> length_sector_long = cpu_to_le32 ( s -> bs -> total_sectors <nl> + - s -> first_sectors_number + 1 ); <nl>  <nl> /* FAT12 / FAT16 / FAT32 */ <nl> /* DOS uses different types when partition is LBA ,
static int virtio_ccw_set_vqs ( SubchDev * sch , uint64_t addr , uint32_t align , <nl> { <nl> VirtIODevice * vdev = virtio_ccw_get_vdev ( sch ); <nl>  <nl> - if ( index > VIRTIO_PCI_QUEUE_MAX ) { <nl> + if ( index >= VIRTIO_PCI_QUEUE_MAX ) { <nl> return - EINVAL ; <nl> } <nl> 
static void vhost_user_cleanup ( NetClientState * nc ) <nl> vhost_net_cleanup ( s -> vhost_net ); <nl> s -> vhost_net = NULL ; <nl> } <nl> + if ( s -> chr ) { <nl> + qemu_chr_add_handlers ( s -> chr , NULL , NULL , NULL , NULL ); <nl> + qemu_chr_fe_release ( s -> chr ); <nl> + s -> chr = NULL ; <nl> + } <nl>  <nl> qemu_purge_queued_packets ( nc ); <nl> }
char ** breakline ( char * input , int * count ) <nl> int c = 0 ; <nl> char * p ; <nl> char ** rval = calloc ( sizeof ( char *), 1 ); <nl> + char ** tmp ; <nl>  <nl> while ( rval && ( p = qemu_strsep (& input , " ")) != NULL ) { <nl> if (!* p ) { <nl> continue ; <nl> } <nl> c ++; <nl> - rval = realloc ( rval , sizeof (* rval ) * ( c + 1 )); <nl> - if (! rval ) { <nl> + tmp = realloc ( rval , sizeof (* rval ) * ( c + 1 )); <nl> + if (! tmp ) { <nl> + free ( rval ); <nl> + rval = NULL ; <nl> c = 0 ; <nl> break ; <nl> + } else { <nl> + rval = tmp ; <nl> } <nl> rval [ c - 1 ] = p ; <nl> rval [ c ] = NULL ;
static void vscsi_process_login ( VSCSIState * s , vscsi_req * req ) <nl> struct srp_login_rsp * rsp = & iu -> srp . login_rsp ; <nl> uint64_t tag = iu -> srp . rsp . tag ; <nl>  <nl> - trace_spapr_vscsi__process_login (); <nl> + trace_spapr_vscsi_process_login (); <nl>  <nl> /* TODO handle case that requested size is wrong and <nl> * buffer format is wrong
void virtqueue_map_sg ( struct iovec * sg , hwaddr * addr , <nl> unsigned int i ; <nl> hwaddr len ; <nl>  <nl> + if ( num_sg >= VIRTQUEUE_MAX_SIZE ) { <nl> + error_report (" virtio : map attempt out of bounds : % zd > % d ", <nl> + num_sg , VIRTQUEUE_MAX_SIZE ); <nl> + exit ( 1 ); <nl> + } <nl> + <nl> for ( i = 0 ; i < num_sg ; i ++) { <nl> len = sg [ i ]. iov_len ; <nl> sg [ i ]. iov_base = cpu_physical_memory_map ( addr [ i ], & len , is_write );
void tap_fd_set_offload ( int fd , int csum , int tso4 , <nl> { <nl> unsigned int offload = 0 ; <nl>  <nl> + /* Check if our kernel supports TUNSETOFFLOAD */ <nl> + if ( ioctl ( fd , TUNSETOFFLOAD , 0 ) != 0 && errno == EINVAL ) { <nl> + return ; <nl> + } <nl> + <nl> if ( csum ) { <nl> offload |= TUN_F_CSUM ; <nl> if ( tso4 )
static void timer_update_irq ( struct fs_timer_t * t ) <nl> qemu_irq_lower ( t -> irq [ 0 ]); <nl> } <nl>  <nl> - static void timer_hit ( struct fs_timer_t * t ) <nl> + static void timer_hit ( void * opaque ) <nl> { <nl> + struct fs_timer_t * t = opaque ; <nl> t -> r_intr |= 1 ; <nl> timer_update_irq ( t ); <nl> }
net_rx_pkt_pull_data ( struct NetRxPkt * pkt , <nl> pkt -> tot_len = iov_size ( iov , iovcnt ) - ploff + pkt -> ehdr_buf_len ; <nl> pkt -> vec_len = iov_copy ( pkt -> vec + 1 , pkt -> vec_len_total - 1 , <nl> iov , iovcnt , ploff , <nl> - pkt -> tot_len - pkt -> ehdr_buf_len ); <nl> + pkt -> tot_len - pkt -> ehdr_buf_len ) + 1 ; <nl> } else { <nl> net_rx_pkt_iovec_realloc ( pkt , iovcnt ); <nl> 
static int inc_refcounts ( BlockDriverState * bs , <nl> if ( refcount == s -> refcount_max ) { <nl> fprintf ( stderr , " ERROR : overflow cluster offset = 0x %" PRIx64 <nl> "\ n ", cluster_offset ); <nl> + fprintf ( stderr , " Use qemu - img amend to increase the refcount entry " <nl> + " width or qemu - img convert to create a clean copy if the " <nl> + " image cannot be opened for writing \ n "); <nl> res -> corruptions ++; <nl> continue ; <nl> }
static int qcow2_co_flush ( BlockDriverState * bs ) <nl> qemu_co_mutex_lock (& s -> lock ); <nl> ret = qcow2_cache_flush ( bs , s -> l2_table_cache ); <nl> if ( ret < 0 ) { <nl> + qemu_co_mutex_unlock (& s -> lock ); <nl> return ret ; <nl> } <nl>  <nl> ret = qcow2_cache_flush ( bs , s -> refcount_block_cache ); <nl> if ( ret < 0 ) { <nl> + qemu_co_mutex_unlock (& s -> lock ); <nl> return ret ; <nl> } <nl> qemu_co_mutex_unlock (& s -> lock );
int qemu_spice_set_pw_expire ( time_t expires ); <nl> int qemu_spice_migrate_info ( const char * hostname , int port , int tls_port , <nl> const char * subject ); <nl>  <nl> -# define SPICE_NEEDS_SET_MM_TIME \ <nl> - (! defined ( SPICE_SERVER_VERSION ) || ( SPICE_SERVER_VERSION < 0xc06 )) <nl> +# if ! defined ( SPICE_SERVER_VERSION ) || ( SPICE_SERVER_VERSION < 0xc06 ) <nl> +# define SPICE_NEEDS_SET_MM_TIME 1 <nl> +# else <nl> +# define SPICE_NEEDS_SET_MM_TIME 0 <nl> +# endif <nl>  <nl> # if SPICE_SERVER_VERSION >= 0x000c02 <nl> void qemu_spice_register_ports ( void );
static int vdi_create ( const char * filename , QEMUOptionParameter * options ) <nl> static void vdi_close ( BlockDriverState * bs ) <nl> { <nl> BDRVVdiState * s = bs -> opaque ; <nl> + <nl> + g_free ( s -> bmap ); <nl> + <nl> migrate_del_blocker ( s -> migration_blocker ); <nl> error_free ( s -> migration_blocker ); <nl> }
int main ( int argc , char ** argv ) <nl>  <nl> path = g_strdup_printf (" e1000 /% s ", models [ i ]); <nl> qtest_add_data_func ( path , models [ i ], test_device ); <nl> + g_free ( path ); <nl> } <nl>  <nl> return g_test_run ();
static void openrisc_pic_cpu_handler ( void * opaque , int irq , int level ) <nl> { <nl> OpenRISCCPU * cpu = ( OpenRISCCPU *) opaque ; <nl> CPUState * cs = CPU ( cpu ); <nl> - uint32_t irq_bit = 1 << irq ; <nl> + uint32_t irq_bit ; <nl>  <nl> if ( irq > 31 || irq < 0 ) { <nl> return ; <nl> } <nl>  <nl> + irq_bit = 1U << irq ; <nl> + <nl> if ( level ) { <nl> cpu -> env . picsr |= irq_bit ; <nl> } else {
static int vhost_user_call ( struct vhost_dev * dev , unsigned long int request , <nl> for ( i = 0 ; i < dev -> mem -> nregions ; ++ i ) { <nl> struct vhost_memory_region * reg = dev -> mem -> regions + i ; <nl> ram_addr_t ram_addr ; <nl> - qemu_ram_addr_from_host (( void *) reg -> userspace_addr , & ram_addr ); <nl> + <nl> + assert (( uintptr_t ) reg -> userspace_addr == reg -> userspace_addr ); <nl> + qemu_ram_addr_from_host (( void *)( uintptr_t ) reg -> userspace_addr , & ram_addr ); <nl> fd = qemu_get_ram_fd ( ram_addr ); <nl> if ( fd > 0 ) { <nl> msg . memory . regions [ fd_num ]. userspace_addr = reg -> userspace_addr ;
void replay_configure ( QemuOpts * opts ) <nl> rr = qemu_opt_get ( opts , " rr "); <nl> if (! rr ) { <nl> /* Just enabling icount */ <nl> - return ; <nl> + goto out ; <nl> } else if (! strcmp ( rr , " record ")) { <nl> mode = REPLAY_MODE_RECORD ; <nl> } else if (! strcmp ( rr , " replay ")) { <nl> void replay_configure ( QemuOpts * opts ) <nl>  <nl> replay_enable ( fname , mode ); <nl>  <nl> + out : <nl> loc_pop (& loc ); <nl> } <nl> 
void watchdog_perform_action ( void ) <nl> exit ( 0 ); <nl>  <nl> case WDT_PAUSE : /* same as ' stop ' command in monitor */ <nl> + /* In a timer callback , when vm_stop calls qemu_clock_enable <nl> + * you would get a deadlock . Bypass the problem . <nl> + */ <nl> + qemu_system_vmstop_request_prepare (); <nl> qapi_event_send_watchdog ( WATCHDOG_EXPIRATION_ACTION_PAUSE , & error_abort ); <nl> - vm_stop ( RUN_STATE_WATCHDOG ); <nl> + qemu_system_vmstop_request ( RUN_STATE_WATCHDOG ); <nl> break ; <nl>  <nl> case WDT_DEBUG :
static void qemu_rdma_cleanup ( RDMAContext * rdma ) <nl> rdma_destroy_event_channel ( rdma -> channel ); <nl> rdma -> channel = NULL ; <nl> } <nl> + g_free ( rdma -> host ); <nl> + rdma -> host = NULL ; <nl> } <nl>  <nl> 
static PixelFormat sdl_to_qemu_pixelformat ( SDL_PixelFormat * sdl_pf ) <nl> static DisplaySurface * sdl_create_displaysurface ( int width , int height ) <nl> { <nl> DisplaySurface * surface = ( DisplaySurface *) g_malloc0 ( sizeof ( DisplaySurface )); <nl> - if ( surface == NULL ) { <nl> - fprintf ( stderr , " sdl_create_displaysurface : malloc failed \ n "); <nl> - exit ( 1 ); <nl> - } <nl>  <nl> surface -> width = width ; <nl> surface -> height = height ;
static int sd_snapshot_create ( BlockDriverState * bs , QEMUSnapshotInfo * sn_info ) <nl> strncpy ( s -> inode . tag , sn_info -> name , sizeof ( s -> inode . tag )); <nl> /* we don ' t need to update entire object */ <nl> datalen = SD_INODE_SIZE - sizeof ( s -> inode . data_vdi_id ); <nl> + inode = g_malloc ( datalen ); <nl>  <nl> /* refresh inode . */ <nl> fd = connect_to_sdog ( s , & local_err ); <nl> static int sd_snapshot_create ( BlockDriverState * bs , QEMUSnapshotInfo * sn_info ) <nl> goto cleanup ; <nl> } <nl>  <nl> - inode = ( SheepdogInode *) g_malloc ( datalen ); <nl> - <nl> ret = read_object ( fd , ( char *) inode , vid_to_vdi_oid ( new_vid ), <nl> s -> inode . nr_copies , datalen , 0 , s -> cache_flags ); <nl>  <nl> static int sd_snapshot_create ( BlockDriverState * bs , QEMUSnapshotInfo * sn_info ) <nl> s -> inode . name , s -> inode . snap_id , s -> inode . vdi_id ); <nl>  <nl> cleanup : <nl> + g_free ( inode ); <nl> closesocket ( fd ); <nl> return ret ; <nl> }
static int rtl8139_can_receive ( VLANClientState * nc ) <nl> return 1 ; <nl> if (! rtl8139_receiver_enabled ( s )) <nl> return 1 ; <nl> + /* network / host communication happens only in normal mode */ <nl> + if (( s -> Cfg9346 & Chip9346_op_mask ) != Cfg9346_Normal ) <nl> + return 0 ; <nl>  <nl> if ( rtl8139_cp_receiver_enabled ( s )) { <nl> /* ??? Flow control not implemented in c + mode . <nl> static ssize_t rtl8139_do_receive ( VLANClientState * nc , const uint8_t * buf , size_ <nl> return - 1 ; <nl> } <nl>  <nl> + /* check whether we are in normal mode */ <nl> + if (( s -> Cfg9346 & Chip9346_op_mask ) != Cfg9346_Normal ) { <nl> + DPRINTF (" not in normal op mode \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> /* XXX : check this */ <nl> if ( s -> RxConfig & AcceptAllPhys ) { <nl> /* promiscuous : receive all */
void pc_basic_device_init ( ISABus * isa_bus , qemu_irq * gsi , <nl> } <nl> port92 = isa_create_simple ( isa_bus , " port92 "); <nl> port92_init ( port92 , a20_line [ 1 ]); <nl> + g_free ( a20_line ); <nl>  <nl> DMA_init ( isa_bus , 0 ); <nl> 
static void nvic_writel ( NVICState * s , uint32_t offset , uint32_t value , <nl> return ; <nl> } <nl> cpu -> env . sau . ctrl = value & 3 ; <nl> + break ; <nl> case 0xdd4 : /* SAU_TYPE */ <nl> if (! arm_feature (& cpu -> env , ARM_FEATURE_V8 )) { <nl> goto bad_offset ;
static void free_test_data ( test_data * data ) <nl> g_free ( temp -> asl_file ); <nl> } <nl>  <nl> - g_array_free ( data -> tables , false ); <nl> + g_array_free ( data -> tables , true ); <nl> } <nl>  <nl> static uint8_t acpi_checksum ( const uint8_t * data , int len )
e1000_can_receive ( void * opaque ) <nl> { <nl> E1000State * s = opaque ; <nl>  <nl> - return (!( s -> mac_reg [ RCTL ] & E1000_RCTL_EN ) || <nl> - s -> mac_reg [ RDH ] != s -> mac_reg [ RDT ]); <nl> + return ( s -> mac_reg [ RCTL ] & E1000_RCTL_EN ); <nl> } <nl>  <nl> static void
long do_rt_sigreturn ( CPUM68KState * env ) <nl> { <nl> struct target_rt_sigframe * frame ; <nl> abi_ulong frame_addr = env -> aregs [ 7 ] - 4 ; <nl> - target_sigset_t target_set ; <nl> sigset_t set ; <nl>  <nl> trace_user_do_rt_sigreturn ( env , frame_addr ); <nl> if (! lock_user_struct ( VERIFY_READ , frame , frame_addr , 1 )) <nl> goto badframe ; <nl>  <nl> - target_to_host_sigset_internal (& set , & target_set ); <nl> + target_to_host_sigset (& set , & frame -> uc . tuc_sigmask ); <nl> set_sigmask (& set ); <nl>  <nl> /* restore registers */
static inline bool memory_access_is_direct ( MemoryRegion * mr , bool is_write ) <nl>  <nl> static int memory_access_size ( MemoryRegion * mr , unsigned l , hwaddr addr ) <nl> { <nl> - unsigned access_size_min = mr -> ops -> impl . min_access_size ; <nl> - unsigned access_size_max = mr -> ops -> impl . max_access_size ; <nl> + unsigned access_size_max = mr -> ops -> valid . max_access_size ; <nl>  <nl> /* Regions are assumed to support 1 - 4 byte accesses unless <nl> otherwise specified . */ <nl> - if ( access_size_min == 0 ) { <nl> - access_size_min = 1 ; <nl> - } <nl> if ( access_size_max == 0 ) { <nl> access_size_max = 4 ; <nl> } <nl> static int memory_access_size ( MemoryRegion * mr , unsigned l , hwaddr addr ) <nl> if ( l > access_size_max ) { <nl> l = access_size_max ; <nl> } <nl> - /* ??? The users of this function are wrong , not supporting minimums larger <nl> - than the remaining length . C . f . memory . c : access_with_adjusted_size . */ <nl> - assert ( l >= access_size_min ); <nl>  <nl> return l ; <nl> }
typedef uint64_t vaddr ; <nl>  <nl> # define TYPE_CPU " cpu " <nl>  <nl> -# define CPU ( obj ) OBJECT_CHECK ( CPUState , ( obj ), TYPE_CPU ) <nl> +/* Since this macro is used a lot in hot code paths and in conjunction with <nl> + * FooCPU * foo_env_get_cpu (), we deviate from usual QOM practice by using <nl> + * an unchecked cast . <nl> + */ <nl> +# define CPU ( obj ) (( CPUState *)( obj )) <nl> + <nl> # define CPU_CLASS ( class ) OBJECT_CLASS_CHECK ( CPUClass , ( class ), TYPE_CPU ) <nl> # define CPU_GET_CLASS ( obj ) OBJECT_GET_CLASS ( CPUClass , ( obj ), TYPE_CPU ) <nl> 
static int pc_dimm_slot2bitmap ( Object * obj , void * opaque ) <nl>  <nl> int pc_dimm_get_free_slot ( const int * hint , int max_slots , Error ** errp ) <nl> { <nl> - unsigned long * bitmap = bitmap_new ( max_slots ); <nl> + unsigned long * bitmap ; <nl> int slot = 0 ; <nl>  <nl> + if ( max_slots <= 0 ) { <nl> + error_setg ( errp , " no slots where allocated , please specify " <nl> + " the ' slots ' option "); <nl> + return slot ; <nl> + } <nl> + <nl> + bitmap = bitmap_new ( max_slots ); <nl> object_child_foreach ( qdev_get_machine (), pc_dimm_slot2bitmap , bitmap ); <nl>  <nl> /* check if requested slot is not occupied */
static inline uint32_t vmsvga_fifo_read ( struct vmsvga_state_s * s ) <nl> static void vmsvga_fifo_run ( struct vmsvga_state_s * s ) <nl> { <nl> uint32_t cmd , colour ; <nl> - int args , len ; <nl> + int args , len , maxloop = 1024 ; <nl> int x , y , dx , dy , width , height ; <nl> struct vmsvga_cursor_definition_s cursor ; <nl> uint32_t cmd_start ; <nl>  <nl> len = vmsvga_fifo_length ( s ); <nl> - while ( len > 0 ) { <nl> + while ( len > 0 && -- maxloop > 0 ) { <nl> /* May need to go back to the start of the command if incomplete */ <nl> cmd_start = s -> fifo_stop ; <nl> 
static void gic_cpu_write ( gic_state * s , int cpu , int offset , uint32_t value ) <nl> switch ( offset ) { <nl> case 0x00 : /* Control */ <nl> s -> cpu_enabled [ cpu ] = ( value & 1 ); <nl> - DPRINTF (" CPU % d % sabled \ n ", cpu , s -> cpu_enabled ? " En " : " Dis "); <nl> + DPRINTF (" CPU % d % sabled \ n ", cpu , s -> cpu_enabled [ cpu ] ? " En " : " Dis "); <nl> break ; <nl> case 0x04 : /* Priority mask */ <nl> s -> priority_mask [ cpu ] = ( value & 0xff );
static void handle_ti ( ESPState * s ) <nl> { <nl> uint32_t dmalen , minlen ; <nl>  <nl> + if ( s -> dma && ! s -> dma_enabled ) { <nl> + s -> dma_cb = handle_ti ; <nl> + return ; <nl> + } <nl> + <nl> dmalen = s -> rregs [ ESP_TCLO ] | ( s -> rregs [ ESP_TCMID ] << 8 ); <nl> if ( dmalen == 0 ) { <nl> dmalen = 0x10000 ;
static uint32_t nvic_readl ( NVICState * s , uint32_t offset , MemTxAttrs attrs ) <nl> return (( s -> num_irq - NVIC_FIRST_IRQ ) / 32 ) - 1 ; <nl> case 0x380 ... 0x3bf : /* NVIC_ITNS < n > */ <nl> { <nl> - int startvec = 32 * ( offset - 0x380 ) + NVIC_FIRST_IRQ ; <nl> + int startvec = 8 * ( offset - 0x380 ) + NVIC_FIRST_IRQ ; <nl> int i ; <nl>  <nl> if (! arm_feature (& cpu -> env , ARM_FEATURE_V8 )) { <nl> static void nvic_writel ( NVICState * s , uint32_t offset , uint32_t value , <nl> switch ( offset ) { <nl> case 0x380 ... 0x3bf : /* NVIC_ITNS < n > */ <nl> { <nl> - int startvec = 32 * ( offset - 0x380 ) + NVIC_FIRST_IRQ ; <nl> + int startvec = 8 * ( offset - 0x380 ) + NVIC_FIRST_IRQ ; <nl> int i ; <nl>  <nl> if (! arm_feature (& cpu -> env , ARM_FEATURE_V8 )) {
uint16_t pvpanic_port ( void ) <nl> if (! o ) { <nl> return 0 ; <nl> } <nl> - return object_property_get_int ( o , PVPANIC_IOPORT_PROP , NULL ); <nl> + return object_property_get_uint ( o , PVPANIC_IOPORT_PROP , NULL ); <nl> } <nl>  <nl> static Property pvpanic_isa_properties [] = {
void virtio_scsi_common_realize ( DeviceState * dev , Error ** errp , <nl> error_setg ( errp , " Invalid number of queues (= %" PRId32 "), " <nl> " must be a positive integer less than % d .", <nl> s -> conf . num_queues , VIRTIO_PCI_QUEUE_MAX ); <nl> + virtio_cleanup ( vdev ); <nl> return ; <nl> } <nl> s -> cmd_vqs = g_malloc0 ( s -> conf . num_queues * sizeof ( VirtQueue *));
static void virtio_blk_handle_write ( VirtIOBlockReq * req , MultiReqBuffer * mrb ) <nl> virtio_blk_rw_complete ( req , - EIO ); <nl> return ; <nl> } <nl> + if ( req -> qiov . size % req -> dev -> conf -> logical_block_size ) { <nl> + virtio_blk_rw_complete ( req , - EIO ); <nl> + return ; <nl> + } <nl>  <nl> if ( mrb -> num_writes == 32 ) { <nl> virtio_submit_multiwrite ( req -> dev -> bs , mrb ); <nl> static void virtio_blk_handle_read ( VirtIOBlockReq * req ) <nl> virtio_blk_rw_complete ( req , - EIO ); <nl> return ; <nl> } <nl> + if ( req -> qiov . size % req -> dev -> conf -> logical_block_size ) { <nl> + virtio_blk_rw_complete ( req , - EIO ); <nl> + return ; <nl> + } <nl>  <nl> acb = bdrv_aio_readv ( req -> dev -> bs , sector , & req -> qiov , <nl> req -> qiov . size / BDRV_SECTOR_SIZE ,
void * virtqueue_pop ( VirtQueue * vq , size_t sz ) <nl> int64_t len ; <nl> VirtIODevice * vdev = vq -> vdev ; <nl> VirtQueueElement * elem = NULL ; <nl> - unsigned out_num , in_num ; <nl> + unsigned out_num , in_num , elem_entries ; <nl> hwaddr addr [ VIRTQUEUE_MAX_SIZE ]; <nl> struct iovec iov [ VIRTQUEUE_MAX_SIZE ]; <nl> VRingDesc desc ; <nl> void * virtqueue_pop ( VirtQueue * vq , size_t sz ) <nl> smp_rmb (); <nl>  <nl> /* When we start there are none of either input nor output . */ <nl> - out_num = in_num = 0 ; <nl> + out_num = in_num = elem_entries = 0 ; <nl>  <nl> max = vq -> vring . num ; <nl>  <nl> void * virtqueue_pop ( VirtQueue * vq , size_t sz ) <nl> } <nl>  <nl> /* If we ' ve got too many , that implies a descriptor loop . */ <nl> - if (( in_num + out_num ) > max ) { <nl> + if (++ elem_entries > max ) { <nl> virtio_error ( vdev , " Looped descriptor "); <nl> goto err_undo_map ; <nl> }
static void pvpanic_isa_realizefn ( DeviceState * dev , Error ** errp ) <nl> static void pvpanic_fw_cfg ( ISADevice * dev , FWCfgState * fw_cfg ) <nl> { <nl> PVPanicState * s = ISA_PVPANIC_DEVICE ( dev ); <nl> + uint16_t * pvpanic_port = g_malloc ( sizeof (* pvpanic_port )); <nl> + * pvpanic_port = cpu_to_le16 ( s -> ioport ); <nl>  <nl> - fw_cfg_add_file ( fw_cfg , " etc / pvpanic - port ", <nl> - g_memdup (& s -> ioport , sizeof ( s -> ioport )), <nl> - sizeof ( s -> ioport )); <nl> + fw_cfg_add_file ( fw_cfg , " etc / pvpanic - port ", pvpanic_port , <nl> + sizeof (* pvpanic_port )); <nl> } <nl>  <nl> void pvpanic_init ( ISABus * bus )
void serial_realize_core ( SerialState * s , Error ** errp ) <nl> void serial_exit_core ( SerialState * s ) <nl> { <nl> qemu_chr_fe_deinit (& s -> chr ); <nl> + <nl> + timer_del ( s -> modem_status_poll ); <nl> + timer_free ( s -> modem_status_poll ); <nl> + <nl> + timer_del ( s -> fifo_timeout_timer ); <nl> + timer_free ( s -> fifo_timeout_timer ); <nl> + <nl> + fifo8_destroy (& s -> recv_fifo ); <nl> + fifo8_destroy (& s -> xmit_fifo ); <nl> + <nl> qemu_unregister_reset ( serial_reset , s ); <nl> } <nl> 
static int32_t scsi_disk_dma_command ( SCSIRequest * req , uint8_t * buf ) <nl> DPRINTF (" Write % s ( sector %" PRId64 ", count % u )\ n ", <nl> ( command & 0xe ) == 0xe ? " And Verify " : "", <nl> r -> req . cmd . lba , len ); <nl> + case VERIFY_10 : <nl> + case VERIFY_12 : <nl> + case VERIFY_16 : <nl> + /* We get here only for BYTCHK == 0x01 and only for scsi - block . <nl> + * As far as DMA is concerned , we can treat it the same as a write ; <nl> + * scsi_block_do_sgio will send VERIFY commands . <nl> + */ <nl> if ( r -> req . cmd . buf [ 1 ] & 0xe0 ) { <nl> goto illegal_request ; <nl> } <nl> static bool scsi_block_is_passthrough ( SCSIDiskState * s , uint8_t * buf ) <nl> case WRITE_VERIFY_16 : <nl> /* MMC writing cannot be done via DMA helpers , because it sometimes <nl> * involves writing beyond the maximum LBA or to negative LBA ( lead - in ). <nl> - * We might use scsi_disk_dma_reqops as long as no writing commands are <nl> + * We might use scsi_block_dma_reqops as long as no writing commands are <nl> * seen , but performance usually isn ' t paramount on optical media . So , <nl> * just make scsi - block operate the same as scsi - generic for them . <nl> */
static PCIDevice * qemu_pci_hot_add_storage ( Monitor * mon , <nl>  <nl> switch ( type ) { <nl> case IF_SCSI : <nl> - if (! dinfo ) { <nl> - monitor_printf ( mon , " scsi requires a backing file / device .\ n "); <nl> - return NULL ; <nl> - } <nl> dev = pci_create ( bus , devfn , " lsi53c895a "); <nl> if ( qdev_init (& dev -> qdev ) < 0 ) <nl> dev = NULL ; <nl> - if ( dev ) { <nl> + if ( dev && dinfo ) { <nl> if ( scsi_hot_add (& dev -> qdev , dinfo , 0 ) != 0 ) { <nl> qdev_unplug (& dev -> qdev ); <nl> dev = NULL ;
static bool is_zero_sectors ( BlockDriverState * bs , int64_t start , <nl> BlockDriverState * file ; <nl> int64_t res ; <nl>  <nl> + if ( start + count > bs -> total_sectors ) { <nl> + count = bs -> total_sectors - start ; <nl> + } <nl> + <nl> if (! count ) { <nl> return true ; <nl> } <nl> static coroutine_fn int qcow2_co_pwrite_zeroes ( BlockDriverState * bs , <nl> uint32_t tail = ( offset + count ) % s -> cluster_size ; <nl>  <nl> trace_qcow2_pwrite_zeroes_start_req ( qemu_coroutine_self (), offset , count ); <nl> + if ( offset + count == bs -> total_sectors * BDRV_SECTOR_SIZE ) { <nl> + tail = 0 ; <nl> + } <nl>  <nl> if ( head || tail ) { <nl> int64_t cl_start = ( offset - head ) >> BDRV_SECTOR_BITS ;
void coroutine_fn qemu_coroutine_yield ( void ) <nl> } <nl>  <nl> self -> caller = NULL ; <nl> - coroutine_swap ( self , to ); <nl> + qemu_coroutine_switch ( self , to , COROUTINE_YIELD ); <nl> }
static inline void bitmap_directory_to_be ( uint8_t * dir , size_t size ) <nl>  <nl> static void bitmap_free ( Qcow2Bitmap * bm ) <nl> { <nl> + if ( bm == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> g_free ( bm -> name ); <nl> g_free ( bm ); <nl> }
static int parse_drive ( DeviceState * dev , Property * prop , const char * str ) <nl> static int print_drive ( DeviceState * dev , Property * prop , char * dest , size_t len ) <nl> { <nl> DriveInfo ** ptr = qdev_get_prop_ptr ( dev , prop ); <nl> - return snprintf ( dest , len , "% s ", (* ptr )-> id ); <nl> + return snprintf ( dest , len , "% s ", (* ptr ) ? (* ptr )-> id : "< null >"); <nl> } <nl>  <nl> PropertyInfo qdev_prop_drive = {
static bool blit_region_is_unsafe ( struct CirrusVGAState * s , <nl> { <nl> if ( pitch < 0 ) { <nl> int64_t min = addr <nl> - + (( int64_t ) s -> cirrus_blt_height - 1 ) * pitch ; <nl> - int32_t max = addr <nl> - + s -> cirrus_blt_width ; <nl> - if ( min < 0 || max > s -> vga . vram_size ) { <nl> + + (( int64_t ) s -> cirrus_blt_height - 1 ) * pitch <nl> + - s -> cirrus_blt_width ; <nl> + if ( min < - 1 || addr >= s -> vga . vram_size ) { <nl> return true ; <nl> } <nl> } else {
static int ram_save_complete ( QEMUFile * f , void * opaque ) <nl>  <nl> flush_compressed_data ( f ); <nl> ram_control_after_iterate ( f , RAM_CONTROL_FINISH ); <nl> - migration_end (); <nl>  <nl> rcu_read_unlock (); <nl> + <nl> + migration_end (); <nl> qemu_put_be64 ( f , RAM_SAVE_FLAG_EOS ); <nl>  <nl> return 0 ;
static int64_t seek_to_sector ( BlockDriverState * bs , int64_t sector_num ) <nl> offset = sector_num % s -> tracks ; <nl>  <nl> /* not allocated */ <nl> - if (( index > s -> catalog_size ) || ( s -> catalog_bitmap [ index ] == 0 )) <nl> + if (( index >= s -> catalog_size ) || ( s -> catalog_bitmap [ index ] == 0 )) <nl> return - 1 ; <nl> return <nl> (( uint64_t ) s -> catalog_bitmap [ index ] * s -> off_multiplier + offset ) * 512 ;
iscsi_co_generic_cb ( struct iscsi_context * iscsi , int status , <nl>  <nl> if ( iTask -> retries -- > 0 && status == SCSI_STATUS_CHECK_CONDITION <nl> && task -> sense . key == SCSI_SENSE_UNIT_ATTENTION ) { <nl> + error_report (" iSCSI CheckCondition : % s ", iscsi_get_error ( iscsi )); <nl> iTask -> do_retry = 1 ; <nl> goto out ; <nl> } <nl>  <nl> if ( status != SCSI_STATUS_GOOD ) { <nl> - error_report (" iSCSI : Failure . % s ", iscsi_get_error ( iscsi )); <nl> + error_report (" iSCSI Failure : % s ", iscsi_get_error ( iscsi )); <nl> } <nl>  <nl> out : <nl> retry : <nl> } <nl>  <nl> if ( iTask . do_retry ) { <nl> + iTask . complete = 0 ; <nl> goto retry ; <nl> } <nl>  <nl> retry : <nl> } <nl>  <nl> if ( iTask . do_retry ) { <nl> + iTask . complete = 0 ; <nl> goto retry ; <nl> } <nl>  <nl> retry : <nl> } <nl>  <nl> if ( iTask . do_retry ) { <nl> + iTask . complete = 0 ; <nl> goto retry ; <nl> } <nl>  <nl> retry : <nl> scsi_free_scsi_task ( iTask . task ); <nl> iTask . task = NULL ; <nl> } <nl> + iTask . complete = 0 ; <nl> goto retry ; <nl> } <nl>  <nl> retry : <nl> } <nl>  <nl> if ( iTask . do_retry ) { <nl> + iTask . complete = 0 ; <nl> goto retry ; <nl> } <nl>  <nl> retry : <nl> } <nl>  <nl> if ( iTask . do_retry ) { <nl> + iTask . complete = 0 ; <nl> goto retry ; <nl> } <nl> 
static int32_t scsi_send_command ( SCSIDevice * d , uint32_t tag , <nl> uint8_t * cmd , int lun ) <nl> { <nl> SCSIDeviceState * s = d -> state ; <nl> - uint32_t len ; <nl> - int cmdlen ; <nl> + uint32_t len = 0 ; <nl> + int cmdlen = 0 ; <nl> SCSIRequest * r ; <nl> int ret ; <nl> 
int qdev_device_help ( QemuOpts * opts ) <nl> return 1 ; <nl> } <nl>  <nl> - if (! qemu_opt_get ( opts , "?")) { <nl> + if (! driver || ! qemu_opt_get ( opts , "?")) { <nl> return 0 ; <nl> } <nl> 
ssize_t v9fs_get_xattr ( FsContext * ctx , const char * path , <nl> if ( xops ) { <nl> return xops -> getxattr ( ctx , path , name , value , size ); <nl> } <nl> - errno = - EOPNOTSUPP ; <nl> + errno = EOPNOTSUPP ; <nl> return - 1 ; <nl> } <nl>  <nl> int v9fs_set_xattr ( FsContext * ctx , const char * path , const char * name , <nl> if ( xops ) { <nl> return xops -> setxattr ( ctx , path , name , value , size , flags ); <nl> } <nl> - errno = - EOPNOTSUPP ; <nl> + errno = EOPNOTSUPP ; <nl> return - 1 ; <nl>  <nl> } <nl> int v9fs_remove_xattr ( FsContext * ctx , <nl> if ( xops ) { <nl> return xops -> removexattr ( ctx , path , name ); <nl> } <nl> - errno = - EOPNOTSUPP ; <nl> + errno = EOPNOTSUPP ; <nl> return - 1 ; <nl>  <nl> }
int kvm_init ( void ) <nl> fprintf ( stderr , " Please add the ' switch_amode ' kernel parameter to " <nl> " your host kernel command line \ n "); <nl> # endif <nl> + ret = s -> vmfd ; <nl> goto err ; <nl> } <nl>  <nl> int kvm_init ( void ) <nl>  <nl> err : <nl> if ( s ) { <nl> - if ( s -> vmfd != - 1 ) { <nl> + if ( s -> vmfd >= 0 ) { <nl> close ( s -> vmfd ); <nl> } <nl> if ( s -> fd != - 1 ) {
static int megasas_dcmd_ld_get_info ( MegasasState * s , MegasasCmd * cmd ) <nl>  <nl> static int megasas_dcmd_cfg_read ( MegasasState * s , MegasasCmd * cmd ) <nl> { <nl> - uint8_t data [ 4096 ]; <nl> + uint8_t data [ 4096 ] = { 0 }; <nl> struct mfi_config_data * info ; <nl> int num_pd_disks = 0 , array_offset , ld_offset ; <nl> BusChild * kid ;
static int nbd_negotiate_handle_info ( NBDClient * client , uint32_t length , <nl> msg = " name length is incorrect "; <nl> goto invalid ; <nl> } <nl> + if ( namelen >= sizeof ( name )) { <nl> + msg = " name too long for qemu "; <nl> + goto invalid ; <nl> + } <nl> if ( nbd_read ( client -> ioc , name , namelen , errp ) < 0 ) { <nl> return - EIO ; <nl> }
static void omap2_gpio_module_write ( void * opaque , target_phys_addr_t addr , <nl>  <nl> static uint32_t omap2_gpio_module_readp ( void * opaque , target_phys_addr_t addr ) <nl> { <nl> - return omap2_gpio_module_readp ( opaque , addr ) >> (( addr & 3 ) << 3 ); <nl> + return omap2_gpio_module_read ( opaque , addr & ~ 3 ) >> (( addr & 3 ) << 3 ); <nl> } <nl>  <nl> static void omap2_gpio_module_writep ( void * opaque , target_phys_addr_t addr ,
static void tmu2_start ( MilkymistTMU2State * s ) <nl> glColor4f ( m , m , m , ( float )( s -> regs [ R_ALPHA ] + 1 ) / 64 . 0f ); <nl>  <nl> /* Read the QEMU dest . framebuffer into the OpenGL framebuffer */ <nl> - fb_len = 2 * s -> regs [ R_DSTHRES ] * s -> regs [ R_DSTVRES ]; <nl> + fb_len = 2ULL * s -> regs [ R_DSTHRES ] * s -> regs [ R_DSTVRES ]; <nl> fb = cpu_physical_memory_map ( s -> regs [ R_DSTFBUF ], & fb_len , 0 ); <nl> if ( fb == NULL ) { <nl> glDeleteTextures ( 1 , & texture );
void qbus_free ( BusState * bus ) <nl> QLIST_REMOVE ( bus , sibling ); <nl> bus -> parent -> num_child_bus --; <nl> } <nl> + qemu_free (( void *) bus -> name ); <nl> if ( bus -> qdev_allocated ) { <nl> qemu_free ( bus ); <nl> }
int64_t throttle_compute_wait ( LeakyBucket * bkt ) <nl> /* If the main bucket is not full yet we still have to check the <nl> * burst bucket in order to enforce the burst limit */ <nl> if ( bkt -> burst_length > 1 ) { <nl> + assert ( bkt -> max > 0 ); /* see throttle_is_valid () */ <nl> extra = bkt -> burst_level - burst_bucket_size ; <nl> if ( extra > 0 ) { <nl> return throttle_do_compute_wait ( bkt -> max , extra );
static int coroutine_fn bdrv_co_do_copy_on_readv ( BdrvChild * child , <nl> size_t skip_bytes ; <nl> int ret ; <nl>  <nl> - assert ( child -> perm & ( BLK_PERM_WRITE_UNCHANGED | BLK_PERM_WRITE )); <nl> + /* FIXME We cannot require callers to have write permissions when all they <nl> + * are doing is a read request . If we did things right , write permissions <nl> + * would be obtained anyway , but internally by the copy - on - read code . As <nl> + * long as it is implemented here rather than in a separat filter driver , <nl> + * the copy - on - read code doesn ' t have its own BdrvChild , however , for which <nl> + * it could request permissions . Therefore we have to bypass the permission <nl> + * system for the moment . */ <nl> + // assert ( child -> perm & ( BLK_PERM_WRITE_UNCHANGED | BLK_PERM_WRITE )); <nl>  <nl> /* Cover entire cluster so no additional backing file I / O is required when <nl> * allocating cluster in the image file .
static void test_acpi_piix4_tcg_cphp ( void ) <nl> memset (& data , 0 , sizeof ( data )); <nl> data . machine = MACHINE_PC ; <nl> data . variant = ". cphp "; <nl> - test_acpi_one ("- smp 2 , cores = 3 , sockets = 2 , maxcpus = 6 ", <nl> + test_acpi_one ("- smp 2 , cores = 3 , sockets = 2 , maxcpus = 6 " <nl> + " - numa node - numa node ", <nl> & data ); <nl> free_test_data (& data ); <nl> } <nl> static void test_acpi_q35_tcg_cphp ( void ) <nl> memset (& data , 0 , sizeof ( data )); <nl> data . machine = MACHINE_Q35 ; <nl> data . variant = ". cphp "; <nl> - test_acpi_one (" - smp 2 , cores = 3 , sockets = 2 , maxcpus = 6 ", <nl> + test_acpi_one (" - smp 2 , cores = 3 , sockets = 2 , maxcpus = 6 " <nl> + " - numa node - numa node ", <nl> & data ); <nl> free_test_data (& data ); <nl> }
static ExitStatus gen_fbcond ( DisasContext * ctx , TCGCond cond , int ra , <nl> int32_t disp ) <nl> { <nl> TCGv cmp_tmp = tcg_temp_new (); <nl> + ExitStatus ret ; <nl> + <nl> gen_fold_mzero ( cond , cmp_tmp , load_fpr ( ctx , ra )); <nl> - return gen_bcond_internal ( ctx , cond , cmp_tmp , disp ); <nl> + ret = gen_bcond_internal ( ctx , cond , cmp_tmp , disp ); <nl> + tcg_temp_free ( cmp_tmp ); <nl> + return ret ; <nl> } <nl>  <nl> static void gen_fcmov ( DisasContext * ctx , TCGCond cond , int ra , int rb , int rc )
Coroutine * qemu_coroutine_new ( void ) <nl> } <nl>  <nl> # ifdef CONFIG_VALGRIND_H <nl> +# ifdef CONFIG_PRAGMA_DISABLE_UNUSED_BUT_SET <nl> /* Work around an unused variable in the valgrind . h macro ... */ <nl> # pragma GCC diagnostic ignored "- Wunused - but - set - variable " <nl> +# endif <nl> static inline void valgrind_stack_deregister ( CoroutineUContext * co ) <nl> { <nl> VALGRIND_STACK_DEREGISTER ( co -> valgrind_stack_id ); <nl> } <nl> +# ifdef CONFIG_PRAGMA_DISABLE_UNUSED_BUT_SET <nl> # pragma GCC diagnostic error "- Wunused - but - set - variable " <nl> # endif <nl> +# endif <nl>  <nl> void qemu_coroutine_delete ( Coroutine * co_ ) <nl> {
static void pc_fw_add_pflash_drv ( void ) <nl> filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl>  <nl> opts = drive_add ( IF_PFLASH , - 1 , filename , " readonly = on "); <nl> + <nl> + g_free ( filename ); <nl> + <nl> if ( opts == NULL ) { <nl> return ; <nl> }
static void test_io_channel_command_fifo ( bool async ) <nl> # define TEST_FIFO " tests / test - io - channel - command . fifo " <nl> QIOChannel * src , * dst ; <nl> QIOChannelTest * test ; <nl> - char * srcfifo = g_strdup_printf (" PIPE :% s , wronly ", TEST_FIFO ); <nl> - char * dstfifo = g_strdup_printf (" PIPE :% s , rdonly ", TEST_FIFO ); <nl> + const char * srcfifo = " PIPE :" TEST_FIFO ", wronly "; <nl> + const char * dstfifo = " PIPE :" TEST_FIFO ", rdonly "; <nl> const char * srcargv [] = { <nl> "/ bin / socat ", "-", srcfifo , NULL , <nl> }; <nl> static void test_io_channel_command_fifo ( bool async ) <nl> object_unref ( OBJECT ( src )); <nl> object_unref ( OBJECT ( dst )); <nl>  <nl> - g_free ( srcfifo ); <nl> - g_free ( dstfifo ); <nl> unlink ( TEST_FIFO ); <nl> } <nl> 
static ssize_t mp_dacl_listxattr ( FsContext * ctx , const char * path , <nl> } <nl>  <nl> /* len includes the trailing NUL */ <nl> - memcpy ( value , ACL_ACCESS , len ); <nl> + memcpy ( value , ACL_DEFAULT , len ); <nl> return 0 ; <nl> } <nl> 
static void test_dma_fragmented ( void ) <nl> ahci_command_commit ( ahci , cmd , px ); <nl> ahci_command_issue ( ahci , cmd ); <nl> ahci_command_verify ( ahci , cmd ); <nl> - g_free ( cmd ); <nl> + ahci_command_free ( cmd ); <nl>  <nl> cmd = ahci_command_create ( CMD_READ_DMA ); <nl> ahci_command_adjust ( cmd , 0 , ptr , bufsize , 32 ); <nl> ahci_command_commit ( ahci , cmd , px ); <nl> ahci_command_issue ( ahci , cmd ); <nl> ahci_command_verify ( ahci , cmd ); <nl> - g_free ( cmd ); <nl> + ahci_command_free ( cmd ); <nl>  <nl> /* Read back the guest ' s receive buffer into local memory */ <nl> bufread ( ptr , rx , bufsize );
int main ( int argc , char ** argv ) <nl> unsigned long ifargs [ 4 ]; <nl> # endif <nl> int ifindex ; <nl> - int fd , ctlfd , unixfd = - 1 ; <nl> + int fd = - 1 , ctlfd = - 1 , unixfd = - 1 ; <nl> int use_vnet = 0 ; <nl> int mtu ; <nl> const char * bridge = NULL ; <nl> int main ( int argc , char ** argv ) <nl> /* profit ! */ <nl>  <nl> cleanup : <nl> - <nl> + if ( fd >= 0 ) { <nl> + close ( fd ); <nl> + } <nl> + if ( ctlfd >= 0 ) { <nl> + close ( ctlfd ); <nl> + } <nl> while (( acl_rule = QSIMPLEQ_FIRST (& acl_list )) != NULL ) { <nl> QSIMPLEQ_REMOVE_HEAD (& acl_list , entry ); <nl> g_free ( acl_rule );
static ssize_t handle_aiocb_rw ( RawPosixAIOData * aiocb ) <nl> * Ok , we have to do it the hard way , copy all segments into <nl> * a single aligned buffer . <nl> */ <nl> - buf = qemu_blockalign ( aiocb -> bs , aiocb -> aio_nbytes ); <nl> + buf = qemu_try_blockalign ( aiocb -> bs , aiocb -> aio_nbytes ); <nl> + if ( buf == NULL ) { <nl> + return - ENOMEM ; <nl> + } <nl> + <nl> if ( aiocb -> aio_type & QEMU_AIO_WRITE ) { <nl> char * p = buf ; <nl> int i ;
static char * SocketAddress_to_str ( const char * prefix , SocketAddress * addr , <nl> return g_strdup_printf ("% sfd :% s % s ", prefix , addr -> u . fd . data -> str , <nl> is_listen ? ", server " : ""); <nl> break ; <nl> + case SOCKET_ADDRESS_KIND_VSOCK : <nl> + return g_strdup_printf ("% svsock :% s :% s ", prefix , <nl> + addr -> u . vsock . data -> cid , <nl> + addr -> u . vsock . data -> port ); <nl> default : <nl> abort (); <nl> }
static VirtIOSCSIVring * virtio_scsi_vring_init ( VirtIOSCSI * s , <nl> { <nl> BusState * qbus = BUS ( qdev_get_parent_bus ( DEVICE ( s ))); <nl> VirtioBusClass * k = VIRTIO_BUS_GET_CLASS ( qbus ); <nl> - VirtIOSCSIVring * r = g_slice_new ( VirtIOSCSIVring ); <nl> + VirtIOSCSIVring * r ; <nl> int rc ; <nl>  <nl> /* Set up virtqueue notify */ <nl> static VirtIOSCSIVring * virtio_scsi_vring_init ( VirtIOSCSI * s , <nl> s -> dataplane_fenced = true ; <nl> return NULL ; <nl> } <nl> + <nl> + r = g_slice_new ( VirtIOSCSIVring ); <nl> r -> host_notifier = * virtio_queue_get_host_notifier ( vq ); <nl> r -> guest_notifier = * virtio_queue_get_guest_notifier ( vq ); <nl> aio_set_event_notifier ( s -> ctx , & r -> host_notifier , handler );
static void qapi_dealloc_type_number ( Visitor * v , double * obj , const char * name , <nl> { <nl> } <nl>  <nl> + static void qapi_dealloc_type_size ( Visitor * v , size_t * obj , const char * name , <nl> + Error ** errp ) <nl> +{ <nl> +} <nl> + <nl> static void qapi_dealloc_type_enum ( Visitor * v , int * obj , const char * strings [], <nl> const char * kind , const char * name , <nl> Error ** errp ) <nl> QapiDeallocVisitor * qapi_dealloc_visitor_new ( void ) <nl> v -> visitor . type_bool = qapi_dealloc_type_bool ; <nl> v -> visitor . type_str = qapi_dealloc_type_str ; <nl> v -> visitor . type_number = qapi_dealloc_type_number ; <nl> + v -> visitor . type_size = qapi_dealloc_type_size ; <nl>  <nl> QTAILQ_INIT (& v -> stack ); <nl> 
static int disas_vfp_insn ( CPUARMState * env , DisasContext * s , uint32_t insn ) <nl> } <nl> } else { <nl> /* arm -> vfp */ <nl> - tmp = load_reg ( s , rd ); <nl> if ( insn & ( 1 << 21 )) { <nl> rn >>= 1 ; <nl> /* system register */ <nl> static int disas_vfp_insn ( CPUARMState * env , DisasContext * s , uint32_t insn ) <nl> /* Writes are ignored . */ <nl> break ; <nl> case ARM_VFP_FPSCR : <nl> + tmp = load_reg ( s , rd ); <nl> gen_helper_vfp_set_fpscr ( cpu_env , tmp ); <nl> tcg_temp_free_i32 ( tmp ); <nl> gen_lookup_tb ( s ); <nl> static int disas_vfp_insn ( CPUARMState * env , DisasContext * s , uint32_t insn ) <nl> return 1 ; <nl> /* TODO : VFP subarchitecture support . <nl> * For now , keep the EN bit only */ <nl> + tmp = load_reg ( s , rd ); <nl> tcg_gen_andi_i32 ( tmp , tmp , 1 << 30 ); <nl> store_cpu_field ( tmp , vfp . xregs [ rn ]); <nl> gen_lookup_tb ( s ); <nl> break ; <nl> case ARM_VFP_FPINST : <nl> case ARM_VFP_FPINST2 : <nl> + tmp = load_reg ( s , rd ); <nl> store_cpu_field ( tmp , vfp . xregs [ rn ]); <nl> break ; <nl> default : <nl> return 1 ; <nl> } <nl> } else { <nl> + tmp = load_reg ( s , rd ); <nl> gen_vfp_msr ( tmp ); <nl> gen_mov_vreg_F0 ( 0 , rn ); <nl> }
static void htab_save_first_pass ( QEMUFile * f , sPAPRMachineState * spapr , <nl> /* Consume invalid HPTEs */ <nl> while (( index < htabslots ) <nl> && ! HPTE_VALID ( HPTE ( spapr -> htab , index ))) { <nl> - index ++; <nl> CLEAN_HPTE ( HPTE ( spapr -> htab , index )); <nl> + index ++; <nl> } <nl>  <nl> /* Consume valid HPTEs */ <nl> chunkstart = index ; <nl> while (( index < htabslots ) && ( index - chunkstart < USHRT_MAX ) <nl> && HPTE_VALID ( HPTE ( spapr -> htab , index ))) { <nl> - index ++; <nl> CLEAN_HPTE ( HPTE ( spapr -> htab , index )); <nl> + index ++; <nl> } <nl>  <nl> if ( index > chunkstart ) {
static int mmu_translate_region ( CPUS390XState * env , target_ulong vaddr , <nl> __func__ , origin , offs , new_entry ); <nl>  <nl> if (( new_entry & _REGION_ENTRY_INV ) != 0 ) { <nl> - /* XXX different regions have different faults */ <nl> DPRINTF ("% s : invalid region \ n ", __func__ ); <nl> - trigger_page_fault ( env , vaddr , PGM_SEGMENT_TRANS , asc , rw , exc ); <nl> + trigger_page_fault ( env , vaddr , pchks [ level / 4 ], asc , rw , exc ); <nl> return - 1 ; <nl> } <nl> 
typedef struct { <nl>  <nl> struct VirtIOBlockDataPlane { <nl> bool started ; <nl> + bool starting ; <nl> bool stopping ; <nl> QEMUBH * start_bh ; <nl> QemuThread thread ; <nl> void virtio_blk_data_plane_start ( VirtIOBlockDataPlane * s ) <nl> return ; <nl> } <nl>  <nl> + if ( s -> starting ) { <nl> + return ; <nl> + } <nl> + <nl> + s -> starting = true ; <nl> + <nl> vq = virtio_get_queue ( s -> vdev , 0 ); <nl> if (! vring_setup (& s -> vring , s -> vdev , 0 )) { <nl> + s -> starting = false ; <nl> return ; <nl> } <nl>  <nl> void virtio_blk_data_plane_start ( VirtIOBlockDataPlane * s ) <nl> s -> io_notifier = * ioq_get_notifier (& s -> ioqueue ); <nl> aio_set_event_notifier ( s -> ctx , & s -> io_notifier , handle_io ); <nl>  <nl> + s -> starting = false ; <nl> s -> started = true ; <nl> trace_virtio_blk_data_plane_start ( s ); <nl> 
static int proxy_init ( FsContext * ctx ) <nl> sock_id = atoi ( ctx -> fs_root ); <nl> if ( sock_id < 0 ) { <nl> fprintf ( stderr , " socket descriptor not initialized \ n "); <nl> + g_free ( proxy ); <nl> return - 1 ; <nl> } <nl> } <nl> g_free ( ctx -> fs_root ); <nl> + ctx -> fs_root = NULL ; <nl>  <nl> proxy -> in_iovec . iov_base = g_malloc ( PROXY_MAX_IO_SZ + PROXY_HDR_SZ ); <nl> proxy -> in_iovec . iov_len = PROXY_MAX_IO_SZ + PROXY_HDR_SZ ;
static void test_aio_external_client ( void ) <nl> aio_enable_external ( ctx ); <nl> } <nl> assert ( aio_poll ( ctx , false )); <nl> + set_event_notifier ( ctx , & data . e , NULL ); <nl> event_notifier_cleanup (& data . e ); <nl> } <nl> }
int do_snapshot_blkdev ( Monitor * mon , const QDict * qdict , QObject ** ret_data ) <nl> int ret = 0 ; <nl> int flags ; <nl>  <nl> + if (! filename ) { <nl> + qerror_report ( QERR_MISSING_PARAMETER , " snapshot_file "); <nl> + ret = - 1 ; <nl> + goto out ; <nl> + } <nl> + <nl> bs = bdrv_find ( device ); <nl> if (! bs ) { <nl> qerror_report ( QERR_DEVICE_NOT_FOUND , device );
static void set_chr ( Object * obj , Visitor * v , const char * name , void * opaque , <nl> } <nl>  <nl> s = qemu_chr_find ( str ); <nl> - g_free ( str ); <nl> if ( s == NULL ) { <nl> error_setg ( errp , " Property '% s .% s ' can ' t find value '% s '", <nl> object_get_typename ( obj ), prop -> name , str ); <nl> - return ; <nl> - } <nl> - <nl> - if (! qemu_chr_fe_init ( be , s , errp )) { <nl> + } else if (! qemu_chr_fe_init ( be , s , errp )) { <nl> error_prepend ( errp , " Property '% s .% s ' can ' t take value '% s ': ", <nl> object_get_typename ( obj ), prop -> name , str ); <nl> - return ; <nl> } <nl> + g_free ( str ); <nl> } <nl>  <nl> static void release_chr ( Object * obj , const char * name , void * opaque )
target_ulong helper_srad ( CPUPPCState * env , target_ulong value , <nl> if ( likely (( uint64_t ) shift != 0 )) { <nl> shift &= 0x3f ; <nl> ret = ( int64_t ) value >> shift ; <nl> - if ( likely ( ret >= 0 || ( value & (( 1 << shift ) - 1 )) == 0 )) { <nl> + if ( likely ( ret >= 0 || ( value & (( 1ULL << shift ) - 1 )) == 0 )) { <nl> env -> ca = 0 ; <nl> } else { <nl> env -> ca = 1 ;
static int img_create ( int argc , char ** argv ) <nl> /* Get image size , if specified */ <nl> if ( optind < argc ) { <nl> int64_t sval ; <nl> - sval = strtosz_suffix ( argv [ optind ++], NULL , STRTOSZ_DEFSUFFIX_B ); <nl> - if ( sval < 0 ) { <nl> + char * end ; <nl> + sval = strtosz_suffix ( argv [ optind ++], & end , STRTOSZ_DEFSUFFIX_B ); <nl> + if ( sval < 0 || * end ) { <nl> error_report (" Invalid image size specified ! You may use k , M , G or " <nl> " T suffixes for "); <nl> error_report (" kilobytes , megabytes , gigabytes and terabytes ."); <nl> static int img_convert ( int argc , char ** argv ) <nl> case ' S ': <nl> { <nl> int64_t sval ; <nl> - sval = strtosz_suffix ( optarg , NULL , STRTOSZ_DEFSUFFIX_B ); <nl> - if ( sval < 0 ) { <nl> + char * end ; <nl> + sval = strtosz_suffix ( optarg , & end , STRTOSZ_DEFSUFFIX_B ); <nl> + if ( sval < 0 || * end ) { <nl> error_report (" Invalid minimum zero buffer size for sparse output specified "); <nl> return 1 ; <nl> }
typedef struct { <nl> } __siginfo_t ; <nl>  <nl> typedef struct { <nl> - unsigned long si_float_regs [ 32 ]; <nl> + abi_ulong si_float_regs [ 32 ]; <nl> unsigned long si_fsr ; <nl> unsigned long si_fpqdepth ; <nl> struct { <nl> restore_fpu_state ( CPUSPARCState * env , qemu_siginfo_fpu_t * fpu ) <nl> return - EFAULT ; <nl> # endif <nl>  <nl> -# if 0 <nl> /* XXX : incorrect */ <nl> - err = __copy_from_user (& env -> fpr [ 0 ], & fpu -> si_float_regs [ 0 ], <nl> - ( sizeof ( unsigned long ) * 32 )); <nl> -# endif <nl> + err = copy_from_user (& env -> fpr [ 0 ], fpu -> si_float_regs [ 0 ], <nl> + ( sizeof ( abi_ulong ) * 32 )); <nl> err |= __get_user ( env -> fsr , & fpu -> si_fsr ); <nl> # if 0 <nl> err |= __get_user ( current -> thread . fpqdepth , & fpu -> si_fpqdepth );
static ExitStatus op_ex ( DisasContext * s , DisasOps * o ) <nl> TCGv_i64 tmp ; <nl>  <nl> update_psw_addr ( s ); <nl> - update_cc_op ( s ); <nl> + gen_op_calc_cc ( s ); <nl>  <nl> tmp = tcg_const_i64 ( s -> next_pc ); <nl> gen_helper_ex ( cc_op , cpu_env , cc_op , o -> in1 , o -> in2 , tmp ); <nl> tcg_temp_free_i64 ( tmp ); <nl>  <nl> - set_cc_static ( s ); <nl> return NO_EXIT ; <nl> } <nl> 
static ssize_t drop_sync ( QIOChannel * ioc , size_t size ) <nl> char small [ 1024 ]; <nl> char * buffer ; <nl>  <nl> - buffer = sizeof ( small ) < size ? small : g_malloc ( MIN ( 65536 , size )); <nl> + buffer = sizeof ( small ) >= size ? small : g_malloc ( MIN ( 65536 , size )); <nl> while ( size > 0 ) { <nl> ssize_t count = read_sync ( ioc , buffer , MIN ( 65536 , size )); <nl> 
create_iovec ( QEMUIOVector * qiov , char ** argv , int nr_iov , int pattern ) <nl> { <nl> size_t * sizes = calloc ( nr_iov , sizeof ( size_t )); <nl> size_t count = 0 ; <nl> - void * buf , * p ; <nl> + void * buf = NULL ; <nl> + void * p ; <nl> int i ; <nl>  <nl> for ( i = 0 ; i < nr_iov ; i ++) { <nl> create_iovec ( QEMUIOVector * qiov , char ** argv , int nr_iov , int pattern ) <nl> len = cvtnum ( arg ); <nl> if ( len < 0 ) { <nl> printf (" non - numeric length argument -- % s \ n ", arg ); <nl> - return NULL ; <nl> + goto fail ; <nl> } <nl>  <nl> /* should be SIZE_T_MAX , but that doesn ' t exist */ <nl> if ( len > UINT_MAX ) { <nl> printf (" too large length argument -- % s \ n ", arg ); <nl> - return NULL ; <nl> + goto fail ; <nl> } <nl>  <nl> if ( len & 0x1ff ) { <nl> printf (" length argument % lld is not sector aligned \ n ", <nl> len ); <nl> - return NULL ; <nl> + goto fail ; <nl> } <nl>  <nl> sizes [ i ] = len ; <nl> create_iovec ( QEMUIOVector * qiov , char ** argv , int nr_iov , int pattern ) <nl> p += sizes [ i ]; <nl> } <nl>  <nl> + fail : <nl> free ( sizes ); <nl> return buf ; <nl> }
e1000e_set_ics ( E1000ECore * core , int index , uint32_t val ) <nl> static void <nl> e1000e_set_icr ( E1000ECore * core , int index , uint32_t val ) <nl> { <nl> + uint32_t icr = 0 ; <nl> if (( core -> mac [ ICR ] & E1000_ICR_ASSERTED ) && <nl> ( core -> mac [ CTRL_EXT ] & E1000_CTRL_EXT_IAME )) { <nl> trace_e1000e_irq_icr_process_iame (); <nl> e1000e_clear_ims_bits ( core , core -> mac [ IAM ]); <nl> } <nl>  <nl> - trace_e1000e_irq_icr_write ( val , core -> mac [ ICR ], core -> mac [ ICR ] & ~ val ); <nl> - core -> mac [ ICR ] &= ~ val ; <nl> + icr = core -> mac [ ICR ] & ~ val ; <nl> + /* Windows driver expects that the " receive overrun " bit and other <nl> + * ones to be cleared when the " Other " bit (# 24 ) is cleared . <nl> + */ <nl> + icr = ( val & E1000_ICR_OTHER ) ? ( icr & ~ E1000_ICR_OTHER_CAUSES ) : icr ; <nl> + trace_e1000e_irq_icr_write ( val , core -> mac [ ICR ], icr ); <nl> + core -> mac [ ICR ] = icr ; <nl> e1000e_update_interrupt_state ( core ); <nl> } <nl> 
DriveInfo * drive_init ( QemuOpts * opts , void * opaque , <nl> } <nl> ( void ) bdrv_set_read_only ( dinfo -> bdrv , 1 ); <nl> } <nl> + /* <nl> + * cdrom is read - only . Set it now , after above interface checking <nl> + * since readonly attribute not explicitly required , so no error . <nl> + */ <nl> + if ( media == MEDIA_CDROM ) { <nl> + ( void ) bdrv_set_read_only ( dinfo -> bdrv , 1 ); <nl> + } <nl>  <nl> if ( bdrv_open2 ( dinfo -> bdrv , file , bdrv_flags , drv ) < 0 ) { <nl> fprintf ( stderr , " qemu : could not open disk image % s : % s \ n ",
ISADevice * isa_ide_init ( ISABus * bus , int iobase , int iobase2 , int isairq , <nl> qdev_prop_set_uint32 ( dev , " iobase ", iobase ); <nl> qdev_prop_set_uint32 ( dev , " iobase2 ", iobase2 ); <nl> qdev_prop_set_uint32 ( dev , " irq ", isairq ); <nl> - if ( qdev_init ( dev ) < 0 ) { <nl> - return NULL ; <nl> - } <nl> + qdev_init_nofail ( dev ); <nl>  <nl> s = ISA_IDE ( dev ); <nl> if ( hd0 ) {
static inline int kvmppc_remove_spapr_tce ( void * table , int pfd , <nl>  <nl> static inline int kvmppc_reset_htab ( int shift_hint ) <nl> { <nl> - return - 1 ; <nl> + return 0 ; <nl> } <nl>  <nl> static inline uint64_t kvmppc_rma_size ( uint64_t current_size ,
int kvm_arch_init ( MachineState * ms , KVMState * s ) <nl> cap_interrupt_level = kvm_check_extension ( s , KVM_CAP_PPC_IRQ_LEVEL ); <nl> cap_segstate = kvm_check_extension ( s , KVM_CAP_PPC_SEGSTATE ); <nl> cap_booke_sregs = kvm_check_extension ( s , KVM_CAP_PPC_BOOKE_SREGS ); <nl> - cap_ppc_smt_possible = kvm_check_extension ( s , KVM_CAP_PPC_SMT_POSSIBLE ); <nl> + cap_ppc_smt_possible = kvm_vm_check_extension ( s , KVM_CAP_PPC_SMT_POSSIBLE ); <nl> cap_ppc_rma = kvm_check_extension ( s , KVM_CAP_PPC_RMA ); <nl> cap_spapr_tce = kvm_check_extension ( s , KVM_CAP_SPAPR_TCE ); <nl> cap_spapr_tce_64 = kvm_check_extension ( s , KVM_CAP_SPAPR_TCE_64 ); <nl> int kvm_arch_init ( MachineState * ms , KVMState * s ) <nl> cap_ppc_watchdog = kvm_check_extension ( s , KVM_CAP_PPC_BOOKE_WATCHDOG ); <nl> /* Note : we don ' t set cap_papr here , because this capability is <nl> * only activated after this by kvmppc_set_papr () */ <nl> - cap_htab_fd = kvm_check_extension ( s , KVM_CAP_PPC_HTAB_FD ); <nl> + cap_htab_fd = kvm_vm_check_extension ( s , KVM_CAP_PPC_HTAB_FD ); <nl> cap_fixup_hcalls = kvm_check_extension ( s , KVM_CAP_PPC_FIXUP_HCALL ); <nl> cap_ppc_smt = kvm_vm_check_extension ( s , KVM_CAP_PPC_SMT ); <nl> cap_htm = kvm_vm_check_extension ( s , KVM_CAP_PPC_HTM ); <nl> int kvmppc_reset_htab ( int shift_hint ) <nl> /* Full emulation , tell caller to allocate htab itself */ <nl> return 0 ; <nl> } <nl> - if ( kvm_check_extension ( kvm_state , KVM_CAP_PPC_ALLOC_HTAB )) { <nl> + if ( kvm_vm_check_extension ( kvm_state , KVM_CAP_PPC_ALLOC_HTAB )) { <nl> int ret ; <nl> ret = kvm_vm_ioctl ( kvm_state , KVM_PPC_ALLOCATE_HTAB , & shift ); <nl> if ( ret == - ENOTTY ) {
int qcow2_pre_write_overlap_check ( BlockDriverState * bs , int ign , int64_t offset , <nl> offset , <nl> true , <nl> size , <nl> + true , <nl> & error_abort ); <nl> g_free ( message ); <nl> 
again : <nl> fail : <nl> qcow2_cache_put ( bs , s -> l2_table_cache , ( void **) & l2_table ); <nl> fail_put : <nl> - if ( nb_clusters > 0 ) { <nl> + if ( m -> nb_clusters > 0 ) { <nl> QLIST_REMOVE ( m , next_in_flight ); <nl> } <nl> return ret ;
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl> # endif <nl> # ifdef TARGET_NR_pause /* not on alpha */ <nl> case TARGET_NR_pause : <nl> - ret = get_errno ( pause ()); <nl> + if (! block_signals ()) { <nl> + sigsuspend (&(( TaskState *) cpu -> opaque )-> signal_mask ); <nl> + } <nl> + ret = - TARGET_EINTR ; <nl> break ; <nl> # endif <nl> # ifdef TARGET_NR_utime
static int nbd_co_flush ( BlockDriverState * bs ) <nl> return nbd_client_session_co_flush (& s -> client ); <nl> } <nl>  <nl> + static void nbd_refresh_limits ( BlockDriverState * bs , Error ** errp ) <nl> +{ <nl> + bs -> bl . max_discard = UINT32_MAX >> BDRV_SECTOR_BITS ; <nl> + bs -> bl . max_transfer_length = UINT32_MAX >> BDRV_SECTOR_BITS ; <nl> +} <nl> + <nl> static int nbd_co_discard ( BlockDriverState * bs , int64_t sector_num , <nl> int nb_sectors ) <nl> { <nl> static BlockDriver bdrv_nbd = { <nl> . bdrv_close = nbd_close , <nl> . bdrv_co_flush_to_os = nbd_co_flush , <nl> . bdrv_co_discard = nbd_co_discard , <nl> + . bdrv_refresh_limits = nbd_refresh_limits , <nl> . bdrv_getlength = nbd_getlength , <nl> . bdrv_detach_aio_context = nbd_detach_aio_context , <nl> . bdrv_attach_aio_context = nbd_attach_aio_context , <nl> static BlockDriver bdrv_nbd_tcp = { <nl> . bdrv_close = nbd_close , <nl> . bdrv_co_flush_to_os = nbd_co_flush , <nl> . bdrv_co_discard = nbd_co_discard , <nl> + . bdrv_refresh_limits = nbd_refresh_limits , <nl> . bdrv_getlength = nbd_getlength , <nl> . bdrv_detach_aio_context = nbd_detach_aio_context , <nl> . bdrv_attach_aio_context = nbd_attach_aio_context , <nl> static BlockDriver bdrv_nbd_unix = { <nl> . bdrv_close = nbd_close , <nl> . bdrv_co_flush_to_os = nbd_co_flush , <nl> . bdrv_co_discard = nbd_co_discard , <nl> + . bdrv_refresh_limits = nbd_refresh_limits , <nl> . bdrv_getlength = nbd_getlength , <nl> . bdrv_detach_aio_context = nbd_detach_aio_context , <nl> . bdrv_attach_aio_context = nbd_attach_aio_context ,
static void n8x0_init ( MachineState * machine , <nl>  <nl> if ( option_rom [ 0 ]. name && <nl> ( machine -> boot_order [ 0 ] == ' n ' || ! machine -> kernel_filename )) { <nl> - uint8_t nolo_tags [ 0x10000 ]; <nl> + uint8_t * nolo_tags = g_new ( uint8_t , 0x10000 ); <nl> /* No , wait , better start at the ROM . */ <nl> s -> mpu -> cpu -> env . regs [ 15 ] = OMAP2_Q2_BASE + 0x400000 ; <nl>  <nl> static void n8x0_init ( MachineState * machine , <nl>  <nl> n800_setup_nolo_tags ( nolo_tags ); <nl> cpu_physical_memory_write ( OMAP2_SRAM_BASE , nolo_tags , 0x10000 ); <nl> + g_free ( nolo_tags ); <nl> } <nl> } <nl> 
void vhost_dev_stop ( struct vhost_dev * hdev , VirtIODevice * vdev ) <nl>  <nl> hdev -> started = false ; <nl> qemu_free ( hdev -> log ); <nl> + hdev -> log = NULL ; <nl> hdev -> log_size = 0 ; <nl> }
vgafb_write ( void * opaque , target_phys_addr_t addr , uint32_t value ) <nl> addr >>= 2 ; <nl> switch ( addr ) { <nl> case R_CTRL : <nl> + s -> regs [ addr ] = value ; <nl> + vgafb_resize ( s ); <nl> + break ; <nl> case R_HSYNC_START : <nl> case R_HSYNC_END : <nl> case R_HSCAN :
static TCGArg * tcg_constant_folding ( TCGContext * s , uint16_t * tcg_opc_ptr , <nl> switch ( op ) { <nl> CASE_OP_32_64 ( or ): <nl> CASE_OP_32_64 ( and ): <nl> - if ( args [ 1 ] == args [ 2 ]) { <nl> + if ( temps_are_copies ( args [ 1 ], args [ 2 ])) { <nl> if ( temps_are_copies ( args [ 0 ], args [ 1 ])) { <nl> gen_opc_buf [ op_index ] = INDEX_op_nop ; <nl> } else {
void trace3 ( TraceEventID event , uint64_t x1 , uint64_t x2 , uint64_t x3 ); <nl> void trace4 ( TraceEventID event , uint64_t x1 , uint64_t x2 , uint64_t x3 , uint64_t x4 ); <nl> void trace5 ( TraceEventID event , uint64_t x1 , uint64_t x2 , uint64_t x3 , uint64_t x4 , uint64_t x5 ); <nl> void trace6 ( TraceEventID event , uint64_t x1 , uint64_t x2 , uint64_t x3 , uint64_t x4 , uint64_t x5 , uint64_t x6 ); <nl> - void st_print_trace ( FILE * stream , int (* stream_printf )( FILE * stream , const char * fmt , ...)); <nl> - void st_print_trace_events ( FILE * stream , int (* stream_printf )( FILE * stream , const char * fmt , ...)); <nl> + void st_print_trace ( FILE * stream , fprintf_function stream_printf ); <nl> + void st_print_trace_events ( FILE * stream , fprintf_function stream_printf ); <nl> bool st_change_trace_event_state ( const char * tname , bool tstate ); <nl> - void st_print_trace_file_status ( FILE * stream , int (* stream_printf )( FILE * stream , const char * fmt , ...)); <nl> + void st_print_trace_file_status ( FILE * stream , fprintf_function stream_printf ); <nl> void st_set_trace_file_enabled ( bool enable ); <nl> bool st_set_trace_file ( const char * file ); <nl> void st_flush_trace_buffer ( void );
# define MAX_TOKEN_COUNT ( 2ULL << 20 ) <nl> # define MAX_NESTING ( 1ULL << 10 ) <nl>  <nl> + static void json_message_free_token ( void * token , void * opaque ) <nl> +{ <nl> + g_free ( token ); <nl> +} <nl> + <nl> static void json_message_free_tokens ( JSONMessageParser * parser ) <nl> { <nl> if ( parser -> tokens ) { <nl> + g_queue_foreach ( parser -> tokens , json_message_free_token , NULL ); <nl> g_queue_free ( parser -> tokens ); <nl> parser -> tokens = NULL ; <nl> }
out : <nl> g_free ( dummy ); <nl> if ( err ) { <nl> qerror_report_err ( err ); <nl> + error_free ( err ); <nl> return - 1 ; <nl> } <nl> return 0 ;
static void vga_putcharxy ( DisplayState * ds , int x , int y , int ch , <nl> font_data = * font_ptr ++; <nl> if ( t_attrib -> uline <nl> && (( i == FONT_HEIGHT - 2 ) || ( i == FONT_HEIGHT - 3 ))) { <nl> - font_data = 0xFFFF ; <nl> + font_data = 0xFF ; <nl> } <nl> (( uint32_t *) d )[ 0 ] = ( dmask16 [( font_data >> 4 )] & xorcol ) ^ bgcol ; <nl> (( uint32_t *) d )[ 1 ] = ( dmask16 [( font_data >> 0 ) & 0xf ] & xorcol ) ^ bgcol ; <nl> static void vga_putcharxy ( DisplayState * ds , int x , int y , int ch , <nl> font_data = * font_ptr ++; <nl> if ( t_attrib -> uline <nl> && (( i == FONT_HEIGHT - 2 ) || ( i == FONT_HEIGHT - 3 ))) { <nl> - font_data = 0xFFFF ; <nl> + font_data = 0xFF ; <nl> } <nl> (( uint32_t *) d )[ 0 ] = ( dmask4 [( font_data >> 6 )] & xorcol ) ^ bgcol ; <nl> (( uint32_t *) d )[ 1 ] = ( dmask4 [( font_data >> 4 ) & 3 ] & xorcol ) ^ bgcol ; <nl> static void vga_putcharxy ( DisplayState * ds , int x , int y , int ch , <nl> for ( i = 0 ; i < FONT_HEIGHT ; i ++) { <nl> font_data = * font_ptr ++; <nl> if ( t_attrib -> uline && (( i == FONT_HEIGHT - 2 ) || ( i == FONT_HEIGHT - 3 ))) { <nl> - font_data = 0xFFFF ; <nl> + font_data = 0xFF ; <nl> } <nl> (( uint32_t *) d )[ 0 ] = (-(( font_data >> 7 )) & xorcol ) ^ bgcol ; <nl> (( uint32_t *) d )[ 1 ] = (-(( font_data >> 6 ) & 1 ) & xorcol ) ^ bgcol ;
qio_channel_socket_accept ( QIOChannelSocket * ioc , <nl> cioc -> fd = qemu_accept ( ioc -> fd , ( struct sockaddr *)& cioc -> remoteAddr , <nl> & cioc -> remoteAddrLen ); <nl> if ( cioc -> fd < 0 ) { <nl> - trace_qio_channel_socket_accept_fail ( ioc ); <nl> if ( errno == EINTR ) { <nl> goto retry ; <nl> } <nl> + error_setg_errno ( errp , errno , " Unable to accept connection "); <nl> + trace_qio_channel_socket_accept_fail ( ioc ); <nl> goto error ; <nl> } <nl> 
fail : <nl> static inline int seek_to_sector ( BlockDriverState * bs , int64_t sector_num ) <nl> { <nl> BDRVParallelsState * s = bs -> opaque ; <nl> - uint32_t index , offset , position ; <nl> + uint32_t index , offset ; <nl> + uint64_t position ; <nl>  <nl> index = sector_num / s -> tracks ; <nl> offset = sector_num % s -> tracks ; <nl> static inline int seek_to_sector ( BlockDriverState * bs , int64_t sector_num ) <nl> if (( index > s -> catalog_size ) || ( s -> catalog_bitmap [ index ] == 0 )) <nl> return - 1 ; <nl>  <nl> - position = ( s -> catalog_bitmap [ index ] + offset ) * 512 ; <nl> + position = ( uint64_t )( s -> catalog_bitmap [ index ] + offset ) * 512 ; <nl>  <nl> // fprintf ( stderr , " sector : % llx index =% x offset =% x pointer =% x position =% x \ n ", <nl> // sector_num , index , offset , s -> catalog_bitmap [ index ], position );
void memory_region_init_reservation ( MemoryRegion * mr , <nl> void memory_region_destroy ( MemoryRegion * mr ) <nl> { <nl> assert ( QTAILQ_EMPTY (& mr -> subregions )); <nl> + assert ( memory_region_transaction_depth == 0 ); <nl> mr -> destructor ( mr ); <nl> memory_region_clear_coalescing ( mr ); <nl> g_free (( char *) mr -> name );
static DriveInfo * blockdev_init ( QemuOpts * all_opts , <nl>  <nl> drv = bdrv_find_whitelisted_format ( buf , ro ); <nl> if (! drv ) { <nl> - error_report ("'% s ' invalid format ", buf ); <nl> + if (! ro && bdrv_find_whitelisted_format ( buf , ! ro )) { <nl> + error_report ("'% s ' can be only used as read - only device .", buf ); <nl> + } else { <nl> + error_report ("'% s ' invalid format ", buf ); <nl> + } <nl> return NULL ; <nl> } <nl> }
static inline void gen_intermediate_code_internal ( X86CPU * cpu , <nl> if ( bp -> pc == pc_ptr && <nl> !(( bp -> flags & BP_CPU ) && ( tb -> flags & HF_RF_MASK ))) { <nl> gen_debug ( dc , pc_ptr - dc -> cs_base ); <nl> - break ; <nl> + goto done_generating ; <nl> } <nl> } <nl> } <nl> static inline void gen_intermediate_code_internal ( X86CPU * cpu , <nl> } <nl> if ( tb -> cflags & CF_LAST_IO ) <nl> gen_io_end (); <nl> + done_generating : <nl> gen_tb_end ( tb , num_insns ); <nl> * tcg_ctx . gen_opc_ptr = INDEX_op_end ; <nl> /* we don ' t forget to fill the last values */
void migration_incoming_state_destroy ( void ) <nl> { <nl> struct MigrationIncomingState * mis = migration_incoming_get_current (); <nl>  <nl> + if ( mis -> to_src_file ) { <nl> + qemu_fclose ( mis -> to_src_file ); <nl> + mis -> to_src_file = NULL ; <nl> + } <nl> + <nl> qemu_event_destroy (& mis -> main_thread_load_event ); <nl> } <nl> 
static int get_physical_address ( CPUState * env , target_ulong * physical , <nl> # if defined ( TARGET_MIPS64 ) <nl> } else if ( address < 0x4000000000000000ULL ) { <nl> /* xuseg */ <nl> - if ( UX && address < ( 0x3FFFFFFFFFFFFFFFULL & env -> SEGMask )) { <nl> + if ( UX && address <= ( 0x3FFFFFFFFFFFFFFFULL & env -> SEGMask )) { <nl> ret = env -> tlb -> map_address ( env , physical , prot , address , rw , access_type ); <nl> } else { <nl> ret = TLBRET_BADADDR ; <nl> static int get_physical_address ( CPUState * env , target_ulong * physical , <nl> } else if ( address < 0x8000000000000000ULL ) { <nl> /* xsseg */ <nl> if (( supervisor_mode || kernel_mode ) && <nl> - SX && address < ( 0x7FFFFFFFFFFFFFFFULL & env -> SEGMask )) { <nl> + SX && address <= ( 0x7FFFFFFFFFFFFFFFULL & env -> SEGMask )) { <nl> ret = env -> tlb -> map_address ( env , physical , prot , address , rw , access_type ); <nl> } else { <nl> ret = TLBRET_BADADDR ; <nl> static int get_physical_address ( CPUState * env , target_ulong * physical , <nl> /* xkphys */ <nl> /* XXX : Assumes PABITS = 36 ( correct for MIPS64R1 ) */ <nl> if ( kernel_mode && KX && <nl> - ( address & 0x07FFFFFFFFFFFFFFULL ) < 0x0000000FFFFFFFFFULL ) { <nl> + ( address & 0x07FFFFFFFFFFFFFFULL ) <= 0x0000000FFFFFFFFFULL ) { <nl> * physical = address & 0x0000000FFFFFFFFFULL ; <nl> * prot = PAGE_READ | PAGE_WRITE ; <nl> } else { <nl> static int get_physical_address ( CPUState * env , target_ulong * physical , <nl> } else if ( address < 0xFFFFFFFF80000000ULL ) { <nl> /* xkseg */ <nl> if ( kernel_mode && KX && <nl> - address < ( 0xFFFFFFFF7FFFFFFFULL & env -> SEGMask )) { <nl> + address <= ( 0xFFFFFFFF7FFFFFFFULL & env -> SEGMask )) { <nl> ret = env -> tlb -> map_address ( env , physical , prot , address , rw , access_type ); <nl> } else { <nl> ret = TLBRET_BADADDR ;
static void handle_qmp_command ( JSONMessageParser * parser , QList * tokens ) <nl> obj = qdict_get ( input , " arguments "); <nl> if (! obj ) { <nl> args = qdict_new (); <nl> + } else if ( qobject_type ( obj ) != QTYPE_QDICT ) { <nl> + qerror_report ( QERR_QMP_BAD_INPUT_OBJECT_MEMBER , " arguments ", " object "); <nl> + goto err_input ; <nl> } else { <nl> args = qobject_to_qdict ( obj ); <nl> QINCREF ( args );
static void coroutine_fn v9fs_xattrcreate ( void * opaque ) <nl> xattr_fidp -> fs . xattr . flags = flags ; <nl> v9fs_string_init (& xattr_fidp -> fs . xattr . name ); <nl> v9fs_string_copy (& xattr_fidp -> fs . xattr . name , & name ); <nl> + g_free ( xattr_fidp -> fs . xattr . value ); <nl> xattr_fidp -> fs . xattr . value = g_malloc0 ( size ); <nl> err = offset ; <nl> put_fid ( pdu , file_fidp );
static int coroutine_fn nfs_co_writev ( BlockDriverState * bs , <nl>  <nl> nfs_co_init_task ( client , & task ); <nl>  <nl> - buf = g_malloc ( nb_sectors * BDRV_SECTOR_SIZE ); <nl> + buf = g_try_malloc ( nb_sectors * BDRV_SECTOR_SIZE ); <nl> + if ( nb_sectors && buf == NULL ) { <nl> + return - ENOMEM ; <nl> + } <nl> + <nl> qemu_iovec_to_buf ( iov , 0 , buf , nb_sectors * BDRV_SECTOR_SIZE ); <nl>  <nl> if ( nfs_pwrite_async ( client -> context , client -> fh ,
static int stdio_put_buffer ( void * opaque , const uint8_t * buf , int64_t pos , <nl> int size ) <nl> { <nl> QEMUFileStdio * s = opaque ; <nl> - return fwrite ( buf , 1 , size , s -> stdio_file ); <nl> + int res ; <nl> + <nl> + res = fwrite ( buf , 1 , size , s -> stdio_file ); <nl> + <nl> + if ( res != size ) { <nl> + return - EIO ; /* fake errno value */ <nl> + } <nl> + return res ; <nl> } <nl>  <nl> static int stdio_get_buffer ( void * opaque , uint8_t * buf , int64_t pos , int size )
vpc_co_pwritev ( BlockDriverState * bs , uint64_t offset , uint64_t bytes , <nl> int64_t image_offset ; <nl> int64_t n_bytes ; <nl> int64_t bytes_done = 0 ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> VHDFooter * footer = ( VHDFooter *) s -> footer_buf ; <nl> QEMUIOVector local_qiov ; <nl> 
static void assign_failed_examine ( AssignedDevice * dev ) <nl> goto fail ; <nl> } <nl>  <nl> + driver [ r ] = 0 ; <nl> ns = strrchr ( driver , '/'); <nl> if (! ns ) { <nl> goto fail ;
static size_t curl_read_cb ( void * ptr , size_t size , size_t nmemb , void * opaque ) <nl> if (! s || ! s -> orig_buf ) <nl> goto read_end ; <nl>  <nl> + if ( s -> buf_off >= s -> buf_len ) { <nl> + /* buffer full , read nothing */ <nl> + return 0 ; <nl> + } <nl> + realsize = MIN ( realsize , s -> buf_len - s -> buf_off ); <nl> memcpy ( s -> orig_buf + s -> buf_off , ptr , realsize ); <nl> s -> buf_off += realsize ; <nl> 
int cpu_exec ( CPUArchState * env ) <nl> TranslationBlock * tb ; <nl> uint8_t * tc_ptr ; <nl> uintptr_t next_tb ; <nl> + /* This must be volatile so it is not trashed by longjmp () */ <nl> + volatile bool have_tb_lock = false ; <nl>  <nl> if ( cpu -> halted ) { <nl> if (! cpu_has_work ( cpu )) { <nl> int cpu_exec ( CPUArchState * env ) <nl> cpu_loop_exit ( cpu ); <nl> } <nl> spin_lock (& tcg_ctx . tb_ctx . tb_lock ); <nl> + have_tb_lock = true ; <nl> tb = tb_find_fast ( env ); <nl> /* Note : we do it here to avoid a gcc bug on Mac OS X when <nl> doing it in tb_find_slow */ <nl> int cpu_exec ( CPUArchState * env ) <nl> tb_add_jump (( TranslationBlock *)( next_tb & ~ TB_EXIT_MASK ), <nl> next_tb & TB_EXIT_MASK , tb ); <nl> } <nl> + have_tb_lock = false ; <nl> spin_unlock (& tcg_ctx . tb_ctx . tb_lock ); <nl>  <nl> /* cpu_interrupt might be called while translating the <nl> int cpu_exec ( CPUArchState * env ) <nl> # ifdef TARGET_I386 <nl> x86_cpu = X86_CPU ( cpu ); <nl> # endif <nl> + if ( have_tb_lock ) { <nl> + spin_unlock (& tcg_ctx . tb_ctx . tb_lock ); <nl> + have_tb_lock = false ; <nl> + } <nl> } <nl> } /* for (;;) */ <nl> 
int kvm_init ( int smp_cpus ) <nl> int ret ; <nl> int i ; <nl>  <nl> - if ( smp_cpus > 1 ) <nl> + if ( smp_cpus > 1 ) { <nl> + fprintf ( stderr , " No SMP KVM support , use '- smp 1 '\ n "); <nl> return - EINVAL ; <nl> + } <nl>  <nl> s = qemu_mallocz ( sizeof ( KVMState )); <nl> 
static void set_encodings ( VncState * vs , int32_t * encodings , size_t n_encodings ) <nl> vs -> tight . compression = ( enc & 0x0F ); <nl> break ; <nl> case VNC_ENCODING_QUALITYLEVEL0 ... VNC_ENCODING_QUALITYLEVEL0 + 9 : <nl> - vs -> tight . quality = ( enc & 0x0F ); <nl> + if ( vs -> vd -> lossy ) { <nl> + vs -> tight . quality = ( enc & 0x0F ); <nl> + } <nl> break ; <nl> default : <nl> VNC_DEBUG (" Unknown encoding : % d ( 0x %. 8x ): % d \ n ", i , enc , enc );
void error_vprepend ( Error ** errp , const char * fmt , va_list ap ) <nl> newmsg = g_string_new ( NULL ); <nl> g_string_vprintf ( newmsg , fmt , ap ); <nl> g_string_append ( newmsg , (* errp )-> msg ); <nl> + g_free ((* errp )-> msg ); <nl> (* errp )-> msg = g_string_free ( newmsg , 0 ); <nl> } <nl> 
static int parse_netdev ( DeviceState * dev , const char * str , void ** ptr ) <nl> goto err ; <nl> } <nl>  <nl> + if ( ncs [ i ]) { <nl> + ret = - EINVAL ; <nl> + goto err ; <nl> + } <nl> + <nl> ncs [ i ] = peers [ i ]; <nl> ncs [ i ]-> queue_index = i ; <nl> } <nl> static void set_vlan ( Object * obj , Visitor * v , void * opaque , <nl> * ptr = NULL ; <nl> return ; <nl> } <nl> + if (* ptr ) { <nl> + error_set_from_qdev_prop_error ( errp , - EINVAL , dev , prop , name ); <nl> + return ; <nl> + } <nl>  <nl> hubport = net_hub_port_find ( id ); <nl> if (! hubport ) {
static void spapr_dr_connector_class_init ( ObjectClass * k , void * data ) <nl> drck -> attach = attach ; <nl> drck -> detach = detach ; <nl> drck -> release_pending = release_pending ; <nl> + /* <nl> + * Reason : it crashes FIXME find and document the real reason <nl> + */ <nl> + dk -> cannot_instantiate_with_device_add_yet = true ; <nl> } <nl>  <nl> static const TypeInfo spapr_dr_connector_info = {
static void do_key_event ( VncState * vs , int down , int keycode , int sym ) <nl> break ; <nl> } <nl>  <nl> + /* Turn off the lock state sync logic if the client support the led <nl> + state extension . <nl> + */ <nl> if ( down && vs -> vd -> lock_key_sync && <nl> + ! vnc_has_feature ( vs , VNC_FEATURE_LED_STATE ) && <nl> keycode_is_keypad ( vs -> vd -> kbd_layout , keycode )) { <nl> /* If the numlock state needs to change then simulate an additional <nl> keypress before sending this one . This will happen if the user <nl> static void do_key_event ( VncState * vs , int down , int keycode , int sym ) <nl> } <nl>  <nl> if ( down && vs -> vd -> lock_key_sync && <nl> + ! vnc_has_feature ( vs , VNC_FEATURE_LED_STATE ) && <nl> (( sym >= ' A ' && sym <= ' Z ') || ( sym >= ' a ' && sym <= ' z '))) { <nl> /* If the capslock state needs to change then simulate an additional <nl> keypress before sending this one . This will happen if the user
extern int loglevel ; <nl> # define PREFIX_LOCK 0x04 <nl> # define PREFIX_DATA 0x08 <nl> # define PREFIX_ADR 0x10 <nl> -# define PREFIX_FWAIT 0x20 <nl>  <nl> typedef struct DisasContext { <nl> /* current insn context */ <nl> long disas_insn ( DisasContext * s , uint8_t * pc_start ) <nl> case 0x67 : <nl> prefixes |= PREFIX_ADR ; <nl> goto next_byte ; <nl> - case 0x9b : <nl> - prefixes |= PREFIX_FWAIT ; <nl> - goto next_byte ; <nl> } <nl>  <nl> if ( prefixes & PREFIX_DATA ) <nl> long disas_insn ( DisasContext * s , uint8_t * pc_start ) <nl> /* misc */ <nl> case 0x90 : /* nop */ <nl> break ; <nl> + case 0x9b : /* fwait */ <nl> + break ; <nl> case 0xcc : /* int3 */ <nl> gen_exception ( s , EXCP03_INT3 , s -> pc - s -> cs_base ); <nl> break ;
static inline int thunk_type_size ( const argtype * type_ptr , int is_host ) <nl> defined ( HOST_PARISC ) || defined ( HOST_SPARC64 ) <nl> return 4 ; <nl> # elif defined ( HOST_PPC ) <nl> - return TARGET_ABI_BITS / 8 ; <nl> + return sizeof ( void *); <nl> # else <nl> return 2 ; <nl> # endif
struct ohci_hcca { <nl> uint32_t done ; <nl> }; <nl>  <nl> + static void ohci_bus_stop ( OHCIState * ohci ); <nl> + <nl> /* Bitfields for the first word of an Endpoint Desciptor . */ <nl> # define OHCI_ED_FA_SHIFT 0 <nl> # define OHCI_ED_FA_MASK ( 0x7f << OHCI_ED_FA_SHIFT ) <nl> static void ohci_attach ( USBPort * port1 , USBDevice * dev ) <nl> } <nl>  <nl> /* Reset the controller */ <nl> - static void ohci_reset ( OHCIState * ohci ) <nl> + static void ohci_reset ( void * opaque ) <nl> { <nl> + OHCIState * ohci = opaque ; <nl> OHCIPort * port ; <nl> int i ; <nl>  <nl> + ohci_bus_stop ( ohci ); <nl> ohci -> ctl = 0 ; <nl> ohci -> old_ctl = 0 ; <nl> ohci -> status = 0 ; <nl> static void ohci_bus_stop ( OHCIState * ohci ) <nl> { <nl> if ( ohci -> eof_timer ) <nl> qemu_del_timer ( ohci -> eof_timer ); <nl> + ohci -> eof_timer = NULL ; <nl> } <nl>  <nl> /* Sets a flag in a port status register but only set it if the port is <nl> static void ohci_set_ctl ( OHCIState * ohci , uint32_t val ) <nl> dprintf (" usb - ohci : % s : USB Resume \ n ", ohci -> name ); <nl> break ; <nl> case OHCI_USB_RESET : <nl> + ohci_reset ( ohci ); <nl> dprintf (" usb - ohci : % s : USB Reset \ n ", ohci -> name ); <nl> break ; <nl> } <nl> static void usb_ohci_init ( OHCIState * ohci , int num_ports , int devfn , <nl> } <nl>  <nl> ohci -> async_td = 0 ; <nl> + qemu_register_reset ( ohci_reset , ohci ); <nl> ohci_reset ( ohci ); <nl> } <nl> 
static int mmu_translate_asc ( CPUS390XState * env , target_ulong vaddr , <nl> if ( vaddr & 0xffe0000000000000ULL ) { <nl> DPRINTF ("% s : vaddr doesn ' t fit 0x % 16 " PRIx64 <nl> " 0xffe0000000000000ULL \ n ", __func__ , vaddr ); <nl> - trigger_page_fault ( env , vaddr , PGM_TRANS_SPEC , asc , rw , exc ); <nl> + trigger_page_fault ( env , vaddr , PGM_ASCE_TYPE , asc , rw , exc ); <nl> return - 1 ; <nl> } <nl> if (( vaddr >> 51 & 3 ) > ( asce & _ASCE_TABLE_LENGTH )) { <nl> static int mmu_translate_asc ( CPUS390XState * env , target_ulong vaddr , <nl> if ( vaddr & 0xfffffc0000000000ULL ) { <nl> DPRINTF ("% s : vaddr doesn ' t fit 0x % 16 " PRIx64 <nl> " 0xfffffc0000000000ULL \ n ", __func__ , vaddr ); <nl> - trigger_page_fault ( env , vaddr , PGM_TRANS_SPEC , asc , rw , exc ); <nl> + trigger_page_fault ( env , vaddr , PGM_ASCE_TYPE , asc , rw , exc ); <nl> return - 1 ; <nl> } <nl> if (( vaddr >> 40 & 3 ) > ( asce & _ASCE_TABLE_LENGTH )) { <nl> static int mmu_translate_asc ( CPUS390XState * env , target_ulong vaddr , <nl> if ( vaddr & 0xffffffff80000000ULL ) { <nl> DPRINTF ("% s : vaddr doesn ' t fit 0x % 16 " PRIx64 <nl> " 0xffffffff80000000ULL \ n ", __func__ , vaddr ); <nl> - trigger_page_fault ( env , vaddr , PGM_TRANS_SPEC , asc , rw , exc ); <nl> + trigger_page_fault ( env , vaddr , PGM_ASCE_TYPE , asc , rw , exc ); <nl> return - 1 ; <nl> } <nl> if (( vaddr >> 29 & 3 ) > ( asce & _ASCE_TABLE_LENGTH )) {
static GSList * gd_vc_gfx_init ( GtkDisplayState * s , VirtualConsole * vc , <nl> QemuConsole * con , int idx , <nl> GSList * group , GtkWidget * view_menu ) <nl> { <nl> - Error * local_err = NULL ; <nl> Object * obj ; <nl>  <nl> - obj = object_property_get_link ( OBJECT ( con ), " device ", & local_err ); <nl> + obj = object_property_get_link ( OBJECT ( con ), " device ", NULL ); <nl> if ( obj ) { <nl> vc -> label = g_strdup_printf ("% s ", object_get_typename ( obj )); <nl> } else {
static target_ulong disas_insn ( CPUX86State * env , DisasContext * s , <nl> s -> vex_l = 0 ; <nl> s -> vex_v = 0 ; <nl> next_byte : <nl> + /* x86 has an upper limit of 15 bytes for an instruction . Since we <nl> + * do not want to decode and generate IR for an illegal <nl> + * instruction , the following check limits the instruction size to <nl> + * 25 bytes : 14 prefix + 1 opc + 6 ( modrm + sib + ofs ) + 4 imm */ <nl> + if ( s -> pc - pc_start > 14 ) { <nl> + goto illegal_op ; <nl> + } <nl> b = cpu_ldub_code ( env , s -> pc ); <nl> s -> pc ++; <nl> /* Collect prefixes . */
static void vhost_iommu_region_add ( MemoryListener * listener , <nl> struct vhost_iommu * iommu ; <nl> Int128 end ; <nl> int iommu_idx ; <nl> - IOMMUMemoryRegion * iommu_mr = IOMMU_MEMORY_REGION ( section -> mr ); <nl> + IOMMUMemoryRegion * iommu_mr ; <nl>  <nl> if (! memory_region_is_iommu ( section -> mr )) { <nl> return ; <nl> } <nl>  <nl> + iommu_mr = IOMMU_MEMORY_REGION ( section -> mr ); <nl> + <nl> iommu = g_malloc0 ( sizeof (* iommu )); <nl> end = int128_add ( int128_make64 ( section -> offset_within_region ), <nl> section -> size );
start_xmit ( E1000State * s ) <nl> * bogus values to TDT / TDLEN . <nl> * there ' s nothing too intelligent we could do about this . <nl> */ <nl> - if ( s -> mac_reg [ TDH ] == tdh_start ) { <nl> + if ( s -> mac_reg [ TDH ] == tdh_start || <nl> + tdh_start >= s -> mac_reg [ TDLEN ] / sizeof ( desc )) { <nl> DBGOUT ( TXERR , " TDH wraparound @% x , TDT % x , TDLEN % x \ n ", <nl> tdh_start , s -> mac_reg [ TDT ], s -> mac_reg [ TDLEN ]); <nl> break ; <nl> e1000_receive_iov ( NetClientState * nc , const struct iovec * iov , int iovcnt ) <nl> if (++ s -> mac_reg [ RDH ] * sizeof ( desc ) >= s -> mac_reg [ RDLEN ]) <nl> s -> mac_reg [ RDH ] = 0 ; <nl> /* see comment in start_xmit ; same here */ <nl> - if ( s -> mac_reg [ RDH ] == rdh_start ) { <nl> + if ( s -> mac_reg [ RDH ] == rdh_start || <nl> + rdh_start >= s -> mac_reg [ RDLEN ] / sizeof ( desc )) { <nl> DBGOUT ( RXERR , " RDH wraparound @% x , RDT % x , RDLEN % x \ n ", <nl> rdh_start , s -> mac_reg [ RDT ], s -> mac_reg [ RDLEN ]); <nl> set_ics ( s , 0 , E1000_ICS_RXO );
static void iothread_instance_finalize ( Object * obj ) <nl> iothread_stop ( obj , NULL ); <nl> qemu_cond_destroy (& iothread -> init_done_cond ); <nl> qemu_mutex_destroy (& iothread -> init_done_lock ); <nl> + if (! iothread -> ctx ) { <nl> + return ; <nl> + } <nl> aio_context_unref ( iothread -> ctx ); <nl> } <nl> 
static int img_convert ( int argc , char ** argv ) <nl> } <nl>  <nl> if ( sn_opts ) { <nl> - ret = bdrv_snapshot_load_tmp ( bs [ 0 ], <nl> - qemu_opt_get ( sn_opts , SNAPSHOT_OPT_ID ), <nl> - qemu_opt_get ( sn_opts , SNAPSHOT_OPT_NAME ), <nl> - & local_err ); <nl> + bdrv_snapshot_load_tmp ( bs [ 0 ], <nl> + qemu_opt_get ( sn_opts , SNAPSHOT_OPT_ID ), <nl> + qemu_opt_get ( sn_opts , SNAPSHOT_OPT_NAME ), <nl> + & local_err ); <nl> } else if ( snapshot_name != NULL ) { <nl> if ( bs_n > 1 ) { <nl> error_report (" No support for concatenating multiple snapshot ");
DeviceState * qdev_try_create ( BusState * bus , const char * name ) <nl> { <nl> DeviceState * dev ; <nl>  <nl> + if ( object_class_by_name ( name ) == NULL ) { <nl> + return NULL ; <nl> + } <nl> dev = DEVICE ( object_new ( name )); <nl> if (! dev ) { <nl> return NULL ;
int main ( int argc , char ** argv , char ** envp ) <nl> /* init remote displays */ <nl> qemu_opts_foreach ( qemu_find_opts (" vnc "), vnc_init_func , NULL , 0 ); <nl> if ( show_vnc_port ) { <nl> - printf (" VNC server running on `% s '\ n ", <nl> - vnc_display_local_addr (" default ")); <nl> + char * ret = vnc_display_local_addr (" default "); <nl> + printf (" VNC server running on `% s '\ n ", ret ); <nl> + g_free ( ret ); <nl> } <nl> # endif <nl> # ifdef CONFIG_SPICE
qcrypto_tls_session_check_certificate ( QCryptoTLSSession * session , <nl>  <nl> allow = qemu_acl_party_is_allowed ( acl , session -> peername ); <nl>  <nl> - error_setg ( errp , " TLS x509 ACL check for % s is % s ", <nl> - session -> peername , allow ? " allowed " : " denied "); <nl> if (! allow ) { <nl> + error_setg ( errp , " TLS x509 ACL check for % s is denied ", <nl> + session -> peername ); <nl> goto error ; <nl> } <nl> }
static BlockBackend * img_open_opts ( const char * optstr , <nl> if ( qdict_haskey ( options , BDRV_OPT_FORCE_SHARE ) <nl> && ! qdict_get_bool ( options , BDRV_OPT_FORCE_SHARE )) { <nl> error_report ("-- force - share /- U conflicts with image options "); <nl> + QDECREF ( options ); <nl> return NULL ; <nl> } <nl> qdict_put ( options , BDRV_OPT_FORCE_SHARE , qbool_from_bool ( true ));
static int bdrv_open_common ( BlockDriverState * bs , BlockDriverState * file , <nl> ret = - EINVAL ; <nl> goto free_and_fail ; <nl> } <nl> - assert ( file != NULL ); <nl> bs -> file = file ; <nl> ret = drv -> bdrv_open ( bs , options , open_flags ); <nl> }
 <nl> void qdist_init ( struct qdist * dist ) <nl> { <nl> - dist -> entries = g_malloc ( sizeof (* dist -> entries )); <nl> + dist -> entries = g_new ( struct qdist_entry , 1 ); <nl> dist -> size = 1 ; <nl> dist -> n = 0 ; <nl> } <nl> void qdist_add ( struct qdist * dist , double x , long count ) <nl>  <nl> if ( unlikely ( dist -> n == dist -> size )) { <nl> dist -> size *= 2 ; <nl> - dist -> entries = g_realloc ( dist -> entries , <nl> - sizeof (* dist -> entries ) * ( dist -> size )); <nl> + dist -> entries = g_renew ( struct qdist_entry , dist -> entries , dist -> size ); <nl> } <nl> dist -> n ++; <nl> entry = & dist -> entries [ dist -> n - 1 ]; <nl> void qdist_bin__internal ( struct qdist * to , const struct qdist * from , size_t n ) <nl> } <nl> } <nl> /* they ' re equally spaced , so copy the dist and bail out */ <nl> - to -> entries = g_realloc_n ( to -> entries , n , sizeof (* to -> entries )); <nl> + to -> entries = g_renew ( struct qdist_entry , to -> entries , n ); <nl> to -> n = from -> n ; <nl> memcpy ( to -> entries , from -> entries , sizeof (* to -> entries ) * to -> n ); <nl> return ;
static void r2d_init ( ram_addr_t ram_size , int vga_ram_size , <nl> serial_hds [ 2 ]); <nl>  <nl> /* onboard CF ( True IDE mode , Master only ). */ <nl> - mmio_ide_init ( 0x14001000 , 0x1400080c , irq [ CF_IDE ], 1 , <nl> - drives_table [ drive_get_index ( IF_IDE , 0 , 0 )]. bdrv , NULL ); <nl> + if (( i = drive_get_index ( IF_IDE , 0 , 0 )) != - 1 ) <nl> + mmio_ide_init ( 0x14001000 , 0x1400080c , irq [ CF_IDE ], 1 , <nl> + drives_table [ i ]. bdrv , NULL ); <nl>  <nl> /* NIC : rtl8139 on - board , and 2 slots . */ <nl> - pci_nic_init ( pci , & nd_table [ 0 ], 2 << 3 , " rtl8139 "); <nl> - for ( i = 1 ; i < nb_nics ; i ++) <nl> - pci_nic_init ( pci , & nd_table [ i ], - 1 , " ne2k_pci "); <nl> + for ( i = 0 ; i < nb_nics ; i ++) <nl> + pci_nic_init ( pci , & nd_table [ i ], ( i == 0 )? 2 << 3 : - 1 , " rtl8139 "); <nl>  <nl> /* Todo : register on board registers */ <nl> {
static void virtio_pci_device_plugged ( DeviceState * d ) <nl>  <nl> static void virtio_pci_device_unplugged ( DeviceState * d ) <nl> { <nl> - PCIDevice * pci_dev = PCI_DEVICE ( d ); <nl> VirtIOPCIProxy * proxy = VIRTIO_PCI ( d ); <nl>  <nl> virtio_pci_stop_ioeventfd ( proxy ); <nl> - msix_uninit_exclusive_bar ( pci_dev ); <nl> } <nl>  <nl> static int virtio_pci_init ( PCIDevice * pci_dev ) <nl> static int virtio_pci_init ( PCIDevice * pci_dev ) <nl> static void virtio_pci_exit ( PCIDevice * pci_dev ) <nl> { <nl> VirtIOPCIProxy * proxy = VIRTIO_PCI ( pci_dev ); <nl> + <nl> + msix_uninit_exclusive_bar ( pci_dev ); <nl> memory_region_destroy (& proxy -> bar ); <nl> } <nl> 
void gen_intermediate_code ( CPUAlphaState * env , struct TranslationBlock * tb ) <nl> } <nl>  <nl> gen_tb_start ( tb ); <nl> + tcg_clear_temp_count (); <nl> + <nl> do { <nl> tcg_gen_insn_start ( ctx . pc ); <nl> num_insns ++; <nl> void gen_intermediate_code ( CPUAlphaState * env , struct TranslationBlock * tb ) <nl> ret = translate_one ( ctxp , insn ); <nl> free_context_temps ( ctxp ); <nl>  <nl> + if ( tcg_check_temp_count ()) { <nl> + qemu_log (" TCG temporary leak before " TARGET_FMT_lx "\ n ", ctx . pc ); <nl> + } <nl> + <nl> /* If we reach a page boundary , are single stepping , <nl> or exhaust instruction count , stop generation . */ <nl> if ( ret == NO_EXIT
do { printf (" usb - serial : " fmt , ## args ); } while ( 0 ) <nl> # endif <nl>  <nl> # define RECV_BUF 384 <nl> -# define SEND_BUF 128 // Not used for now <nl>  <nl> /* Commands */ <nl> # define FTDI_RESET 0 <nl> typedef struct { <nl> uint16_t vendorid ; <nl> uint16_t productid ; <nl> uint8_t recv_buf [ RECV_BUF ]; <nl> - uint8_t recv_ptr ; <nl> - uint8_t recv_used ; <nl> - uint8_t send_buf [ SEND_BUF ]; <nl> + uint16_t recv_ptr ; <nl> + uint16_t recv_used ; <nl> uint8_t event_chr ; <nl> uint8_t error_chr ; <nl> uint8_t event_trigger ;
static void pci_bios_init_device ( PCIDevice * d ) <nl> } <nl> break ; <nl> case 0x0300 : <nl> + if ( vendor_id != 0x1234 ) <nl> + goto default_map ; <nl> /* VGA : map frame buffer to default Bochs VBE address */ <nl> pci_set_io_region_addr ( d , 0 , 0xE0000000 ); <nl> break ; <nl> static void pci_bios_init_device ( PCIDevice * d ) <nl> } <nl> break ; <nl> default : <nl> + default_map : <nl> /* default memory mappings */ <nl> for ( i = 0 ; i < PCI_NUM_REGIONS ; i ++) { <nl> r = & d -> io_regions [ i ];
static int32_t scsi_disk_dma_command ( SCSIRequest * req , uint8_t * buf ) <nl> case READ_16 : <nl> len = r -> req . cmd . xfer / s -> qdev . blocksize ; <nl> DPRINTF (" Read ( sector %" PRId64 ", count % d )\ n ", r -> req . cmd . lba , len ); <nl> + if ( r -> req . cmd . buf [ 1 ] & 0xe0 ) { <nl> + goto illegal_request ; <nl> + } <nl> if ( r -> req . cmd . lba > s -> qdev . max_lba ) { <nl> goto illegal_lba ; <nl> } <nl> static int32_t scsi_disk_dma_command ( SCSIRequest * req , uint8_t * buf ) <nl> DPRINTF (" Write % s ( sector %" PRId64 ", count % d )\ n ", <nl> ( command & 0xe ) == 0xe ? " And Verify " : "", <nl> r -> req . cmd . lba , len ); <nl> + if ( r -> req . cmd . buf [ 1 ] & 0xe0 ) { <nl> + goto illegal_request ; <nl> + } <nl> if ( r -> req . cmd . lba > s -> qdev . max_lba ) { <nl> goto illegal_lba ; <nl> } <nl> static int32_t scsi_disk_dma_command ( SCSIRequest * req , uint8_t * buf ) <nl> break ; <nl> default : <nl> abort (); <nl> + illegal_request : <nl> + scsi_check_condition ( r , SENSE_CODE ( INVALID_FIELD )); <nl> + return 0 ; <nl> illegal_lba : <nl> scsi_check_condition ( r , SENSE_CODE ( LBA_OUT_OF_RANGE )); <nl> return 0 ;
static int readv_f ( int argc , char ** argv ) <nl> print_report (" read ", & t2 , offset , qiov . size , total , cnt , Cflag ); <nl>  <nl> out : <nl> + qemu_iovec_destroy (& qiov ); <nl> qemu_io_free ( buf ); <nl> return 0 ; <nl> } <nl> static int writev_f ( int argc , char ** argv ) <nl> t2 = tsub ( t2 , t1 ); <nl> print_report (" wrote ", & t2 , offset , qiov . size , total , cnt , Cflag ); <nl> out : <nl> + qemu_iovec_destroy (& qiov ); <nl> qemu_io_free ( buf ); <nl> return 0 ; <nl> } <nl> static void aio_write_done ( void * opaque , int ret ) <nl> ctx -> qiov . size , 1 , ctx -> Cflag ); <nl> out : <nl> qemu_io_free ( ctx -> buf ); <nl> + qemu_iovec_destroy (& ctx -> qiov ); <nl> g_free ( ctx ); <nl> } <nl>  <nl> static void aio_read_done ( void * opaque , int ret ) <nl> ctx -> qiov . size , 1 , ctx -> Cflag ); <nl> out : <nl> qemu_io_free ( ctx -> buf ); <nl> + qemu_iovec_destroy (& ctx -> qiov ); <nl> g_free ( ctx ); <nl> } <nl> 
GEN_HANDLER ( dcbtst , 0x1F , 0x16 , 0x07 , 0x03E00001 , PPC_CACHE ) <nl> # define op_dcbz () (* gen_op_dcbz [ ctx -> mem_idx ])() <nl> static GenOpFunc * gen_op_dcbz [] = { <nl> & gen_op_dcbz_user , <nl> + & gen_op_dcbz_user , <nl> + & gen_op_dcbz_kernel , <nl> & gen_op_dcbz_kernel , <nl> }; <nl> # endif
void * qemu_memalign ( size_t alignment , size_t size ) <nl> void * qemu_anon_ram_alloc ( size_t size , uint64_t * alignment ) <nl> { <nl> size_t align = QEMU_VMALLOC_ALIGN ; <nl> - size_t total = size + align - getpagesize (); <nl> + size_t total = size + align ; <nl> void * ptr = mmap ( 0 , total , PROT_NONE , MAP_ANONYMOUS | MAP_PRIVATE , - 1 , 0 ); <nl> size_t offset = QEMU_ALIGN_UP (( uintptr_t ) ptr , align ) - ( uintptr_t ) ptr ; <nl> void * ptr1 ; <nl> void * qemu_anon_ram_alloc ( size_t size , uint64_t * alignment ) <nl> if ( offset > 0 ) { <nl> munmap ( ptr - offset , offset ); <nl> } <nl> - if ( total > size ) { <nl> - munmap ( ptr + size , total - size ); <nl> + if ( total > size + getpagesize ()) { <nl> + munmap ( ptr + size + getpagesize (), total - size - getpagesize ()); <nl> } <nl>  <nl> trace_qemu_anon_ram_alloc ( size , ptr ); <nl> void qemu_anon_ram_free ( void * ptr , size_t size ) <nl> { <nl> trace_qemu_anon_ram_free ( ptr , size ); <nl> if ( ptr ) { <nl> - munmap ( ptr , size ); <nl> + munmap ( ptr , size + getpagesize ()); <nl> } <nl> } <nl> 
int bdrv_open ( BlockDriverState * bs , const char * filename , QDict * options , <nl> if ( drvname ) { <nl> drv = bdrv_find_format ( drvname ); <nl> qdict_del ( options , " driver "); <nl> + if (! drv ) { <nl> + error_setg ( errp , " Invalid driver : '% s '", drvname ); <nl> + ret = - EINVAL ; <nl> + goto unlink_and_fail ; <nl> + } <nl> } <nl>  <nl> if (! drv ) {
static void do_flush_queued_data ( VirtIOSerialPort * port , VirtQueue * vq , <nl> port -> elem -> out_sg [ i ]. iov_base <nl> + port -> iov_offset , <nl> buf_size ); <nl> + if (! port -> elem ) { /* bail if we got disconnected */ <nl> + return ; <nl> + } <nl> if ( port -> throttled ) { <nl> port -> iov_idx = i ; <nl> if ( ret > 0 ) {
static int bdrv_inactivate_recurse ( BlockDriverState * bs , <nl> } <nl> } <nl>  <nl> - if ( setting_flag ) { <nl> + if ( setting_flag && !( bs -> open_flags & BDRV_O_INACTIVE )) { <nl> uint64_t perm , shared_perm ; <nl>  <nl> - bs -> open_flags |= BDRV_O_INACTIVE ; <nl> - <nl> QLIST_FOREACH ( parent , & bs -> parents , next_parent ) { <nl> if ( parent -> role -> inactivate ) { <nl> ret = parent -> role -> inactivate ( parent ); <nl> if ( ret < 0 ) { <nl> - bs -> open_flags &= ~ BDRV_O_INACTIVE ; <nl> return ret ; <nl> } <nl> } <nl> } <nl>  <nl> + bs -> open_flags |= BDRV_O_INACTIVE ; <nl> + <nl> /* Update permissions , they may differ for inactive nodes */ <nl> bdrv_get_cumulative_perm ( bs , & perm , & shared_perm ); <nl> bdrv_check_perm ( bs , perm , shared_perm , NULL , & error_abort );
static int ioreq_map ( struct ioreq * ioreq ) <nl> xen_be_printf (& ioreq -> blkdev -> xendev , 0 , <nl> " can ' t map grant ref % d (% s , % d maps )\ n ", <nl> refs [ i ], strerror ( errno ), ioreq -> blkdev -> cnt_map ); <nl> + ioreq -> mapped = 1 ; <nl> ioreq_unmap ( ioreq ); <nl> return - 1 ; <nl> }
char * object_property_get_str ( Object * obj , const char * name , <nl> void object_property_set_link ( Object * obj , Object * value , <nl> const char * name , Error ** errp ) <nl> { <nl> - object_property_set_str ( obj , object_get_canonical_path ( value ), <nl> - name , errp ); <nl> + gchar * path = object_get_canonical_path ( value ); <nl> + object_property_set_str ( obj , path , name , errp ); <nl> + g_free ( path ); <nl> } <nl>  <nl> Object * object_property_get_link ( Object * obj , const char * name ,
static void test_i440fx_defaults ( gconstpointer opaque ) <nl> /* 3 . 2 . 26 */ <nl> g_assert_cmpint ( qpci_config_readb ( dev , 0x93 ), ==, 0x00 ); /* TRC */ <nl>  <nl> + g_free ( dev ); <nl> + qpci_free_pc ( bus ); <nl> qtest_end (); <nl> } <nl>  <nl> static void test_i440fx_pam ( gconstpointer opaque ) <nl> /* Verify the area is not our new mask */ <nl> g_assert (! verify_area ( pam_area [ i ]. start , pam_area [ i ]. end , 0x82 )); <nl> } <nl> + <nl> + g_free ( dev ); <nl> + qpci_free_pc ( bus ); <nl> qtest_end (); <nl> } <nl> 
QemuOpts * qemu_chr_parse_compat ( const char * label , const char * filename ) <nl> if ( strstart ( filename , " vc ", & p )) { <nl> qemu_opt_set ( opts , " backend ", " vc "); <nl> if (* p == ':') { <nl> - if ( sscanf ( p + 1 , "% 8 [ 0 - 9 ] x % 8 [ 0 - 9 ]", width , height ) == 2 ) { <nl> + if ( sscanf ( p + 1 , "% 7 [ 0 - 9 ] x % 7 [ 0 - 9 ]", width , height ) == 2 ) { <nl> /* pixels */ <nl> qemu_opt_set ( opts , " width ", width ); <nl> qemu_opt_set ( opts , " height ", height ); <nl> - } else if ( sscanf ( p + 1 , "% 8 [ 0 - 9 ] Cx % 8 [ 0 - 9 ] C ", width , height ) == 2 ) { <nl> + } else if ( sscanf ( p + 1 , "% 7 [ 0 - 9 ] Cx % 7 [ 0 - 9 ] C ", width , height ) == 2 ) { <nl> /* chars */ <nl> qemu_opt_set ( opts , " cols ", width ); <nl> qemu_opt_set ( opts , " rows ", height );
# include " kvm_i386 . h " <nl> # include " hw / sysbus . h " <nl> # include " hw / kvm / clock . h " <nl> +# include " qapi / error . h " <nl>  <nl> # include < linux / kvm . h > <nl> # include < linux / kvm_para . h > <nl> static void kvmclock_realize ( DeviceState * dev , Error ** errp ) <nl> { <nl> KVMClockState * s = KVM_CLOCK ( dev ); <nl>  <nl> + if (! kvm_enabled ()) { <nl> + error_setg ( errp , " kvmclock device requires KVM "); <nl> + return ; <nl> + } <nl> + <nl> kvm_update_clock ( s ); <nl>  <nl> qemu_add_vm_change_state_handler ( kvmclock_vm_state_change , s );
static int get_bits_from_size ( size_t size ) <nl> static int preallocate ( BlockDriverState * bs ) <nl> { <nl> BDRVQcowState * s = bs -> opaque ; <nl> - uint64_t cluster_offset ; <nl> + uint64_t cluster_offset = 0 ; <nl> uint64_t nb_sectors ; <nl> uint64_t offset ; <nl> int num ;
int qemu_opts_foreach ( QemuOptsList * list , qemu_opts_loopfunc func , void * opaque , <nl> int rc = 0 ; <nl>  <nl> QTAILQ_FOREACH ( opts , & list -> head , next ) { <nl> - rc = func ( opts , opaque ); <nl> + rc |= func ( opts , opaque ); <nl> if ( abort_on_failure && rc != 0 ) <nl> break ; <nl> }
static const VMStateDescription * vmstate_get_subsection ( const VMStateSubsection <nl> static int vmstate_subsection_load ( QEMUFile * f , const VMStateDescription * vmsd , <nl> void * opaque ) <nl> { <nl> - const VMStateSubsection * sub = vmsd -> subsections ; <nl> - <nl> - if (! sub || ! sub -> needed ) { <nl> - return 0 ; <nl> - } <nl> - <nl> while ( qemu_peek_byte ( f , 0 ) == QEMU_VM_SUBSECTION ) { <nl> char idstr [ 256 ]; <nl> int ret ; <nl> static int vmstate_subsection_load ( QEMUFile * f , const VMStateDescription * vmsd , <nl> /* it don ' t have a valid subsection name */ <nl> return 0 ; <nl> } <nl> - sub_vmsd = vmstate_get_subsection ( sub , idstr ); <nl> + sub_vmsd = vmstate_get_subsection ( vmsd -> subsections , idstr ); <nl> if ( sub_vmsd == NULL ) { <nl> return - ENOENT ; <nl> } <nl> static int vmstate_subsection_load ( QEMUFile * f , const VMStateDescription * vmsd , <nl> qemu_file_skip ( f , len ); /* idstr */ <nl> version_id = qemu_get_be32 ( f ); <nl>  <nl> - assert (! sub_vmsd -> subsections ); <nl> ret = vmstate_load_state ( f , sub_vmsd , opaque , version_id ); <nl> if ( ret ) { <nl> return ret ; <nl> static void vmstate_subsection_save ( QEMUFile * f , const VMStateDescription * vmsd , <nl> qemu_put_byte ( f , len ); <nl> qemu_put_buffer ( f , ( uint8_t *) vmsd -> name , len ); <nl> qemu_put_be32 ( f , vmsd -> version_id ); <nl> - assert (! vmsd -> subsections ); <nl> vmstate_save_state ( f , vmsd , opaque ); <nl> } <nl> sub ++;
static void test_io_channel_tls ( const void * opaque ) <nl> mainloop = g_main_context_default (); <nl> do { <nl> g_main_context_iteration ( mainloop , TRUE ); <nl> - } while (! clientHandshake . finished && <nl> + } while (! clientHandshake . finished || <nl> ! serverHandshake . finished ); <nl>  <nl> g_assert ( clientHandshake . failed == data -> expectClientFail );
 <nl> int event_notifier_init ( EventNotifier * e , int active ) <nl> { <nl> - e -> event = CreateEvent ( NULL , FALSE , FALSE , NULL ); <nl> + e -> event = CreateEvent ( NULL , TRUE , FALSE , NULL ); <nl> assert ( e -> event ); <nl> return 0 ; <nl> }
static CharDriverState * gd_vc_handler ( ChardevVC * vc , Error ** errp ) <nl> chr -> chr_set_echo = gd_vc_chr_set_echo ; <nl>  <nl> /* Temporary , until gd_vc_vte_init runs . */ <nl> - chr -> opaque = g_new ( VirtualConsole , 1 ); <nl> + chr -> opaque = g_new0 ( VirtualConsole , 1 ); <nl>  <nl> /* defer OPENED events until our vc is fully initialized */ <nl> chr -> explicit_be_open = true ;
static void win32_aio_process_completion ( QEMUWin32AIOState * s , <nl> memcpy ( qiov -> iov [ i ]. iov_base , p , qiov -> iov [ i ]. iov_len ); <nl> p += qiov -> iov [ i ]. iov_len ; <nl> } <nl> - qemu_vfree ( waiocb -> buf ); <nl> } <nl> + qemu_vfree ( waiocb -> buf ); <nl> } <nl>  <nl> 
void vfio_region_write ( void * opaque , hwaddr addr , <nl> case 4 : <nl> buf . dword = cpu_to_le32 ( data ); <nl> break ; <nl> + case 8 : <nl> + buf . qword = cpu_to_le64 ( data ); <nl> + break ; <nl> default : <nl> hw_error (" vfio : unsupported write size , % d bytes ", size ); <nl> break ; <nl> uint64_t vfio_region_read ( void * opaque , <nl> case 4 : <nl> data = le32_to_cpu ( buf . dword ); <nl> break ; <nl> + case 8 : <nl> + data = le64_to_cpu ( buf . qword ); <nl> + break ; <nl> default : <nl> hw_error (" vfio : unsupported read size , % d bytes ", size ); <nl> break ; <nl> const MemoryRegionOps vfio_region_ops = { <nl> . min_access_size = 1 , <nl> . max_access_size = 8 , <nl> }, <nl> + . impl = { <nl> + . min_access_size = 1 , <nl> + . max_access_size = 8 , <nl> + }, <nl> }; <nl>  <nl> /*
static QemuOptsList parallels_runtime_opts = { <nl> . name = PARALLELS_OPT_PREALLOC_SIZE , <nl> . type = QEMU_OPT_SIZE , <nl> . help = " Preallocation size on image expansion ", <nl> - . def_value_str = " 128MiB ", <nl> + . def_value_str = " 128M ", <nl> }, <nl> { <nl> . name = PARALLELS_OPT_PREALLOC_MODE ,
static target_ulong h_random ( PowerPCCPU * cpu , sPAPRMachineState * spapr , <nl> hrdata . val . v64 = 0 ; <nl> hrdata . received = 0 ; <nl>  <nl> - qemu_mutex_unlock_iothread (); <nl> while ( hrdata . received < 8 ) { <nl> rng_backend_request_entropy ( rngstate -> backend , 8 - hrdata . received , <nl> random_recv , & hrdata ); <nl> + qemu_mutex_unlock_iothread (); <nl> qemu_sem_wait (& hrdata . sem ); <nl> + qemu_mutex_lock_iothread (); <nl> } <nl> - qemu_mutex_lock_iothread (); <nl>  <nl> qemu_sem_destroy (& hrdata . sem ); <nl> args [ 0 ] = hrdata . val . v64 ;
static int sd_create ( const char * filename , QemuOpts * opts , <nl> bdrv_unref ( bs ); <nl> } <nl>  <nl> + s -> aio_context = qemu_get_aio_context (); <nl> ret = do_sd_create ( s , & vid , 0 , errp ); <nl> if ( ret ) { <nl> goto out ;
static QemuOptsList machine_opts = { <nl> . name = " dea - key - wrap ", <nl> . type = QEMU_OPT_BOOL , <nl> . help = " enable / disable DEA key wrapping using the CPACF wrapping key ", <nl> + },{ <nl> + . name = " loadparm ", <nl> + . type = QEMU_OPT_STRING , <nl> + . help = " Up to 8 chars in set of [ A - Za - z0 - 9 . ]( lower case chars " <nl> + " converted to upper case ) to pass to machine " <nl> + " loader , boot manager , and guest kernel ", <nl> }, <nl> { /* End of list */ } <nl> }
static void virtio_scsi_reset ( VirtIODevice * vdev ) <nl>  <nl> s -> sense_size = VIRTIO_SCSI_SENSE_SIZE ; <nl> s -> cdb_size = VIRTIO_SCSI_CDB_SIZE ; <nl> + s -> events_dropped = false ; <nl> } <nl>  <nl> /* The device does not have anything to save beyond the virtio data .
static void disas_xtensa_insn ( CPUXtensaState * env , DisasContext * dc ) <nl> switch ( OP2 ) { <nl> case 0 : /* L32E */ <nl> HAS_OPTION ( XTENSA_OPTION_WINDOWED_REGISTER ); <nl> - if ( gen_check_privilege ( dc )) { <nl> + if ( gen_check_privilege ( dc ) && <nl> + gen_window_check2 ( dc , RRR_S , RRR_T )) { <nl> TCGv_i32 addr = tcg_temp_new_i32 (); <nl> tcg_gen_addi_i32 ( addr , cpu_R [ RRR_S ], <nl> ( 0xffffffc0 | ( RRR_R << 2 ))); <nl> static void disas_xtensa_insn ( CPUXtensaState * env , DisasContext * dc ) <nl>  <nl> case 4 : /* S32E */ <nl> HAS_OPTION ( XTENSA_OPTION_WINDOWED_REGISTER ); <nl> - if ( gen_check_privilege ( dc )) { <nl> + if ( gen_check_privilege ( dc ) && <nl> + gen_window_check2 ( dc , RRR_S , RRR_T )) { <nl> TCGv_i32 addr = tcg_temp_new_i32 (); <nl> tcg_gen_addi_i32 ( addr , cpu_R [ RRR_S ], <nl> ( 0xffffffc0 | ( RRR_R << 2 )));
static int sd_snapshot_create ( BlockDriverState * bs , QEMUSnapshotInfo * sn_info ) <nl> if ( ret < 0 ) { <nl> error_report (" failed to create inode for snapshot : % s ", <nl> error_get_pretty ( local_err )); <nl> + error_free ( local_err ); <nl> goto cleanup ; <nl> } <nl> 
int do_drive_del ( Monitor * mon , const QDict * qdict , QObject ** ret_data ) <nl> { <nl> const char * id = qdict_get_str ( qdict , " id "); <nl> BlockDriverState * bs ; <nl> + DriveInfo * dinfo ; <nl> AioContext * aio_context ; <nl> Error * local_err = NULL ; <nl>  <nl> int do_drive_del ( Monitor * mon , const QDict * qdict , QObject ** ret_data ) <nl> return - 1 ; <nl> } <nl>  <nl> + dinfo = drive_get_by_blockdev ( bs ); <nl> + if ( dinfo && ! dinfo -> enable_auto_del ) { <nl> + error_report (" Deleting device added with blockdev - add " <nl> + " is not supported "); <nl> + return - 1 ; <nl> + } <nl> + <nl> aio_context = bdrv_get_aio_context ( bs ); <nl> aio_context_acquire ( aio_context ); <nl>  <nl> int do_drive_del ( Monitor * mon , const QDict * qdict , QObject ** ret_data ) <nl> bdrv_set_on_error ( bs , BLOCKDEV_ON_ERROR_REPORT , <nl> BLOCKDEV_ON_ERROR_REPORT ); <nl> } else { <nl> - drive_del ( drive_get_by_blockdev ( bs )); <nl> + drive_del ( dinfo ); <nl> } <nl>  <nl> aio_context_release ( aio_context );
static int vpc_open ( BlockDriverState * bs , QDict * options , int flags , <nl> uint64_t pagetable_size ; <nl> int disk_type = VHD_DYNAMIC ; <nl> int ret ; <nl> + int64_t bs_size ; <nl>  <nl> bs -> file = bdrv_open_child ( NULL , options , " file ", bs , & child_file , <nl> false , errp ); <nl> static int vpc_open ( BlockDriverState * bs , QDict * options , int flags , <nl> } <nl> } <nl>  <nl> - if ( s -> free_data_block_offset > bdrv_getlength ( bs -> file -> bs )) { <nl> + bs_size = bdrv_getlength ( bs -> file -> bs ); <nl> + if ( bs_size < 0 ) { <nl> + error_setg_errno ( errp , - bs_size , " Unable to learn image size "); <nl> + ret = bs_size ; <nl> + goto fail ; <nl> + } <nl> + if ( s -> free_data_block_offset > bs_size ) { <nl> error_setg ( errp , " block - vpc : free_data_block_offset points after " <nl> " the end of file . The image has been truncated ."); <nl> ret = - EINVAL ;
static bool ga_open_pidfile ( const char * pidfile ) <nl> goto fail ; <nl> } <nl>  <nl> + /* keep pidfile open & locked forever */ <nl> return true ; <nl>  <nl> fail : <nl> unlink ( pidfile ); <nl> + close ( pidfd ); <nl> return false ; <nl> } <nl> # else /* _WIN32 */
static int con_init ( struct XenDevice * xendev ) <nl> { <nl> struct XenConsole * con = container_of ( xendev , struct XenConsole , xendev ); <nl> char * type , * dom ; <nl> + int ret = 0 ; <nl>  <nl> /* setup */ <nl> dom = xs_get_domain_path ( xenstore , con -> xendev . dom ); <nl> static int con_init ( struct XenDevice * xendev ) <nl> type = xenstore_read_str ( con -> console , " type "); <nl> if (! type || strcmp ( type , " ioemu ") != 0 ) { <nl> xen_be_printf ( xendev , 1 , " not for me ( type =% s )\ n ", type ); <nl> - return - 1 ; <nl> + ret = - 1 ; <nl> + goto out ; <nl> } <nl>  <nl> if (! serial_hds [ con -> xendev . dev ]) <nl> static int con_init ( struct XenDevice * xendev ) <nl> else <nl> con -> chr = serial_hds [ con -> xendev . dev ]; <nl>  <nl> - return 0 ; <nl> + out : <nl> + qemu_free ( type ); <nl> + return ret ; <nl> } <nl>  <nl> static int con_connect ( struct XenDevice * xendev )
char * socket_address_to_string ( struct SocketAddress * addr , Error ** errp ) <nl> { <nl> char * buf ; <nl> InetSocketAddress * inet ; <nl> - char host_port [ INET6_ADDRSTRLEN + 5 + 4 ]; <nl>  <nl> switch ( addr -> type ) { <nl> case SOCKET_ADDRESS_KIND_INET : <nl> inet = addr -> u . inet . data ; <nl> if ( strchr ( inet -> host , ':') == NULL ) { <nl> - snprintf ( host_port , sizeof ( host_port ), "% s :% s ", inet -> host , <nl> - inet -> port ); <nl> - buf = g_strdup ( host_port ); <nl> + buf = g_strdup_printf ("% s :% s ", inet -> host , inet -> port ); <nl> } else { <nl> - snprintf ( host_port , sizeof ( host_port ), "[% s ]:% s ", inet -> host , <nl> - inet -> port ); <nl> - buf = g_strdup ( host_port ); <nl> + buf = g_strdup_printf ("[% s ]:% s ", inet -> host , inet -> port ); <nl> } <nl> break ; <nl> 
void dpy_gfx_replace_surface ( QemuConsole * con , <nl> DisplaySurface * old_surface = con -> surface ; <nl> DisplayChangeListener * dcl ; <nl>  <nl> + assert ( old_surface != surface ); <nl> + <nl> con -> surface = surface ; <nl> QLIST_FOREACH ( dcl , & s -> listeners , next ) { <nl> if ( con != ( dcl -> con ? dcl -> con : active_console )) {
exit_err : <nl> void qmp_netdev_del ( const char * id , Error ** errp ) <nl> { <nl> NetClientState * nc ; <nl> + QemuOpts * opts ; <nl>  <nl> nc = qemu_find_netdev ( id ); <nl> if (! nc ) { <nl> void qmp_netdev_del ( const char * id , Error ** errp ) <nl> return ; <nl> } <nl>  <nl> + opts = qemu_opts_find ( qemu_find_opts_err (" netdev ", NULL ), id ); <nl> + if (! opts ) { <nl> + error_setg ( errp , " Device '% s ' is not a netdev ", id ); <nl> + return ; <nl> + } <nl> + <nl> qemu_del_net_client ( nc ); <nl> - qemu_opts_del ( qemu_opts_find ( qemu_find_opts_err (" netdev ", errp ), id )); <nl> + qemu_opts_del ( opts ); <nl> } <nl>  <nl> void print_net_client ( Monitor * mon , NetClientState * nc )
static void test_visitor_in_fuzz ( TestInputVisitorData * data , <nl>  <nl> v = visitor_input_test_init ( data , buf ); <nl> visit_type_intList ( v , NULL , & ilres , NULL ); <nl> + qapi_free_intList ( ilres ); <nl> visitor_input_teardown ( data , NULL ); <nl>  <nl> v = visitor_input_test_init ( data , buf );
MigrationCapabilityStatusList * qmp_query_migrate_capabilities ( Error ** errp ) <nl> MigrationState * s = migrate_get_current (); <nl> int i ; <nl>  <nl> + caps = NULL ; /* silence compiler warning */ <nl> for ( i = 0 ; i < MIGRATION_CAPABILITY_MAX ; i ++) { <nl> if ( head == NULL ) { <nl> head = g_malloc0 ( sizeof (* caps ));
static const char * stream_video_names [] = { <nl> [ SPICE_STREAM_VIDEO_FILTER ] = " filter ", <nl> }; <nl> # define parse_stream_video ( _name ) \ <nl> - name2enum ( _name , stream_video_names , ARRAY_SIZE ( stream_video_names )) <nl> + parse_name ( _name , " stream video control ", \ <nl> + stream_video_names , ARRAY_SIZE ( stream_video_names )) <nl>  <nl> static const char * compression_names [] = { <nl> [ SPICE_IMAGE_COMPRESS_OFF ] = " off ",
int coroutine_fn bdrv_co_discard ( BlockDriverState * bs , int64_t sector_num , <nl>  <nl> tracked_request_begin (& req , bs , sector_num << BDRV_SECTOR_BITS , <nl> nb_sectors << BDRV_SECTOR_BITS , BDRV_TRACKED_DISCARD ); <nl> - bdrv_set_dirty ( bs , sector_num , nb_sectors ); <nl>  <nl> max_discard = MIN_NON_ZERO ( bs -> bl . max_discard , BDRV_REQUEST_MAX_SECTORS ); <nl> while ( nb_sectors > 0 ) { <nl> int coroutine_fn bdrv_co_discard ( BlockDriverState * bs , int64_t sector_num , <nl> } <nl> ret = 0 ; <nl> out : <nl> + bdrv_set_dirty ( bs , req . offset >> BDRV_SECTOR_BITS , <nl> + req . bytes >> BDRV_SECTOR_BITS ); <nl> tracked_request_end (& req ); <nl> return ret ; <nl> }
static uint32_t timer_enabled ( HPETTimer * t ) <nl>  <nl> static uint32_t hpet_time_after ( uint64_t a , uint64_t b ) <nl> { <nl> - return (( int32_t )( b ) - ( int32_t )( a ) < 0 ); <nl> + return (( int32_t )( b - a ) < 0 ); <nl> } <nl>  <nl> static uint32_t hpet_time_after64 ( uint64_t a , uint64_t b ) <nl> { <nl> - return (( int64_t )( b ) - ( int64_t )( a ) < 0 ); <nl> + return (( int64_t )( b - a ) < 0 ); <nl> } <nl>  <nl> static uint64_t ticks_to_ns ( uint64_t value )
static CharDriverState * qemu_chr_open_pipe ( QemuOpts * opts ) <nl> close ( fd_in ); <nl> if ( fd_out >= 0 ) <nl> close ( fd_out ); <nl> - TFR ( fd_in = fd_out = open ( filename , O_RDWR | O_BINARY )); <nl> + TFR ( fd_in = fd_out = qemu_open ( filename , O_RDWR | O_BINARY )); <nl> if ( fd_in < 0 ) <nl> return NULL ; <nl> } <nl> static CharDriverState * qemu_chr_open_tty ( QemuOpts * opts ) <nl> CharDriverState * chr ; <nl> int fd ; <nl>  <nl> - TFR ( fd = open ( filename , O_RDWR | O_NONBLOCK )); <nl> + TFR ( fd = qemu_open ( filename , O_RDWR | O_NONBLOCK )); <nl> if ( fd < 0 ) { <nl> return NULL ; <nl> } <nl> static CharDriverState * qemu_chr_open_pp ( QemuOpts * opts ) <nl> ParallelCharDriver * drv ; <nl> int fd ; <nl>  <nl> - TFR ( fd = open ( filename , O_RDWR )); <nl> + TFR ( fd = qemu_open ( filename , O_RDWR )); <nl> if ( fd < 0 ) <nl> return NULL ; <nl>  <nl> static CharDriverState * qemu_chr_open_pp ( QemuOpts * opts ) <nl> CharDriverState * chr ; <nl> int fd ; <nl>  <nl> - fd = open ( filename , O_RDWR ); <nl> + fd = qemu_open ( filename , O_RDWR ); <nl> if ( fd < 0 ) <nl> return NULL ; <nl> 
static void acquire_privilege ( const char * name , Error ** errp ) <nl> TOKEN_PRIVILEGES priv ; <nl> Error * local_err = NULL ; <nl>  <nl> - if ( error_is_set ( errp )) { <nl> - return ; <nl> - } <nl> - <nl> if ( OpenProcessToken ( GetCurrentProcess (), <nl> TOKEN_ADJUST_PRIVILEGES | TOKEN_QUERY , & token )) <nl> { <nl> static void execute_async ( DWORD WINAPI (* func )( LPVOID ), LPVOID opaque , <nl> { <nl> Error * local_err = NULL ; <nl>  <nl> - if ( error_is_set ( errp )) { <nl> - return ; <nl> - } <nl> HANDLE thread = CreateThread ( NULL , 0 , func , opaque , 0 , NULL ); <nl> if (! thread ) { <nl> error_set (& local_err , QERR_QGA_COMMAND_FAILED , <nl> static void check_suspend_mode ( GuestSuspendMode mode , Error ** errp ) <nl> SYSTEM_POWER_CAPABILITIES sys_pwr_caps ; <nl> Error * local_err = NULL ; <nl>  <nl> - if ( error_is_set ( errp )) { <nl> - return ; <nl> - } <nl> ZeroMemory (& sys_pwr_caps , sizeof ( sys_pwr_caps )); <nl> if (! GetPwrCapabilities (& sys_pwr_caps )) { <nl> error_set (& local_err , QERR_QGA_COMMAND_FAILED ,
static void run_block_job ( BlockJob * job , Error ** errp ) <nl>  <nl> do { <nl> aio_poll ( aio_context , true ); <nl> - qemu_progress_print (( float ) job -> offset / job -> len * 100 . f , 0 ); <nl> + qemu_progress_print ( job -> len ? <nl> + (( float ) job -> offset / job -> len * 100 . f ) : 0 . 0f , 0 ); <nl> } while (! job -> ready ); <nl>  <nl> block_job_complete_sync ( job , errp );
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl>  <nl> if (! lock_user_struct ( VERIFY_WRITE , target_st , arg2 , 0 )) <nl> goto efault ; <nl> + memset ( target_st , 0 , sizeof (* target_st )); <nl> __put_user ( st . st_dev , & target_st -> st_dev ); <nl> __put_user ( st . st_ino , & target_st -> st_ino ); <nl> __put_user ( st . st_mode , & target_st -> st_mode );
static USBPort * xhci_lookup_uport ( XHCIState * xhci , uint32_t * slot_ctx ) <nl> int i , pos , port ; <nl>  <nl> port = ( slot_ctx [ 1 ]>> 16 ) & 0xFF ; <nl> + if ( port < 1 || port > xhci -> numports ) { <nl> + return NULL ; <nl> + } <nl> port = xhci -> ports [ port - 1 ]. uport -> index + 1 ; <nl> pos = snprintf ( path , sizeof ( path ), "% d ", port ); <nl> for ( i = 0 ; i < 5 ; i ++) { <nl> static int usb_xhci_post_load ( void * opaque , int version_id ) <nl> xhci_mask64 ( ldq_le_pci_dma ( pci_dev , dcbaap + 8 * slotid )); <nl> xhci_dma_read_u32s ( xhci , slot -> ctx , slot_ctx , sizeof ( slot_ctx )); <nl> slot -> uport = xhci_lookup_uport ( xhci , slot_ctx ); <nl> + if (! slot -> uport ) { <nl> + /* should not happen , but may trigger on guest bugs */ <nl> + slot -> enabled = 0 ; <nl> + slot -> addressed = 0 ; <nl> + continue ; <nl> + } <nl> assert ( slot -> uport && slot -> uport -> dev ); <nl>  <nl> for ( epid = 1 ; epid <= 31 ; epid ++) {
void slirp_input ( const uint8_t * pkt , int pkt_len ) <nl> if (! m ) <nl> return ; <nl> /* Note : we add to align the IP header */ <nl> + if ( M_FREEROOM ( m ) < pkt_len + 2 ) { <nl> + m_inc ( m , pkt_len + 2 ); <nl> + } <nl> m -> m_len = pkt_len + 2 ; <nl> memcpy ( m -> m_data + 2 , pkt , pkt_len ); <nl> 
static int ehci_state_writeback ( EHCIQueue * q ) <nl> static void ehci_advance_state ( EHCIState * ehci , int async ) <nl> { <nl> EHCIQueue * q = NULL ; <nl> + int itd_count = 0 ; <nl> int again ; <nl>  <nl> do { <nl> static void ehci_advance_state ( EHCIState * ehci , int async ) <nl>  <nl> case EST_FETCHITD : <nl> again = ehci_state_fetchitd ( ehci , async ); <nl> + itd_count ++; <nl> break ; <nl>  <nl> case EST_FETCHSITD : <nl> again = ehci_state_fetchsitd ( ehci , async ); <nl> + itd_count ++; <nl> break ; <nl>  <nl> case EST_ADVANCEQUEUE : <nl> static void ehci_advance_state ( EHCIState * ehci , int async ) <nl> break ; <nl> } <nl>  <nl> - if ( again < 0 ) { <nl> + if ( again < 0 || itd_count > 16 ) { <nl> + /* TODO : notify guest ( raise HSE irq ?) */ <nl> fprintf ( stderr , " processing error - resetting ehci HC \ n "); <nl> ehci_reset ( ehci ); <nl> again = 0 ;
static int virtio_net_handle_mac ( VirtIONet * n , uint8_t cmd , <nl> goto error ; <nl> } <nl>  <nl> - if ( in_use + mac_data . entries <= MAC_TABLE_ENTRIES ) { <nl> + if ( mac_data . entries <= MAC_TABLE_ENTRIES - in_use ) { <nl> s = iov_to_buf ( iov , iov_cnt , 0 , & macs [ in_use * ETH_ALEN ], <nl> mac_data . entries * ETH_ALEN ); <nl> if ( s != mac_data . entries * ETH_ALEN ) {
static void vfio_ccw_register_io_notifier ( VFIOCCWDevice * vcdev , Error ** errp ) <nl> return ; <nl> } <nl>  <nl> - argsz = sizeof (* irq_set ); <nl> + argsz = sizeof (* irq_info ); <nl> irq_info = g_malloc0 ( argsz ); <nl> irq_info -> index = VFIO_CCW_IO_IRQ_INDEX ; <nl> irq_info -> argsz = argsz ;
void configure_icount ( const char * option ) <nl>  <nl> void qemu_run_all_timers ( void ) <nl> { <nl> + alarm_timer -> pending = 0 ; <nl> + <nl> /* rearm timer , if not periodic */ <nl> if ( alarm_timer -> expired ) { <nl> alarm_timer -> expired = 0 ; <nl> qemu_rearm_alarm_timer ( alarm_timer ); <nl> } <nl>  <nl> - alarm_timer -> pending = 0 ; <nl> - <nl> /* vm time timers */ <nl> if ( vm_running ) { <nl> qemu_run_timers ( vm_clock );
int cpu_exec ( CPUState * env1 ) <nl> TB , but before it is linked into a potentially <nl> infinite loop and becomes env -> current_tb . Avoid <nl> starting execution if there is a pending interrupt . */ <nl> - if (! unlikely ( env -> exit_request )) { <nl> - env -> current_tb = tb ; <nl> + env -> current_tb = tb ; <nl> + barrier (); <nl> + if ( likely (! env -> exit_request )) { <nl> tc_ptr = tb -> tc_ptr ; <nl> /* execute the generated code */ <nl> # if defined ( __sparc__ ) && ! defined ( CONFIG_SOLARIS ) <nl> int cpu_exec ( CPUState * env1 ) <nl> # define env cpu_single_env <nl> # endif <nl> next_tb = tcg_qemu_tb_exec ( tc_ptr ); <nl> - env -> current_tb = NULL ; <nl> if (( next_tb & 3 ) == 2 ) { <nl> /* Instruction counter expired . */ <nl> int insns_left ; <nl> int cpu_exec ( CPUState * env1 ) <nl> } <nl> } <nl> } <nl> + env -> current_tb = NULL ; <nl> /* reset soft MMU for next block ( it can currently <nl> only be set by a memory fault ) */ <nl> } /* for (;;) */
static void migration_bitmap_sync_range ( ram_addr_t start , ram_addr_t length ) <nl> static int64_t start_time ; <nl> static int64_t bytes_xfer_prev ; <nl> static int64_t num_dirty_pages_period ; <nl> + static uint64_t xbzrle_cache_miss_prev ; <nl> + static uint64_t iterations_prev ; <nl>  <nl> static void migration_bitmap_sync_init ( void ) <nl> { <nl> start_time = 0 ; <nl> bytes_xfer_prev = 0 ; <nl> num_dirty_pages_period = 0 ; <nl> + xbzrle_cache_miss_prev = 0 ; <nl> + iterations_prev = 0 ; <nl> } <nl>  <nl> /* Called with iothread lock held , to protect ram_list . dirty_memory [] */ <nl> static void migration_bitmap_sync ( void ) <nl> MigrationState * s = migrate_get_current (); <nl> int64_t end_time ; <nl> int64_t bytes_xfer_now ; <nl> - static uint64_t xbzrle_cache_miss_prev ; <nl> - static uint64_t iterations_prev ; <nl>  <nl> bitmap_sync_count ++; <nl>  <nl> static void migration_bitmap_sync ( void ) <nl> mig_throttle_on = false ; <nl> } <nl> if ( migrate_use_xbzrle ()) { <nl> - if ( iterations_prev != 0 ) { <nl> + if ( iterations_prev != acct_info . iterations ) { <nl> acct_info . xbzrle_cache_miss_rate = <nl> ( double )( acct_info . xbzrle_cache_miss - <nl> xbzrle_cache_miss_prev ) /
static void format_string ( StringOutputVisitor * sov , Range * r , bool next , <nl> { <nl> if ( r -> end - r -> begin > 1 ) { <nl> if ( human ) { <nl> - g_string_append_printf ( sov -> string , " 0x %" PRIx64 "-%" PRIx64 , <nl> + g_string_append_printf ( sov -> string , " 0x %" PRIx64 "- 0x %" PRIx64 , <nl> r -> begin , r -> end - 1 ); <nl>  <nl> } else {
const VMStateDescription vmstate_ISAIPMIKCSDevice = { <nl> VMSTATE_BOOL ( kcs . use_irq , ISAIPMIKCSDevice ), <nl> VMSTATE_BOOL ( kcs . irqs_enabled , ISAIPMIKCSDevice ), <nl> VMSTATE_UINT32 ( kcs . outpos , ISAIPMIKCSDevice ), <nl> - VMSTATE_VBUFFER_UINT32 ( kcs . outmsg , ISAIPMIKCSDevice , 1 , NULL , 0 , <nl> - kcs . outlen ), <nl> - VMSTATE_VBUFFER_UINT32 ( kcs . inmsg , ISAIPMIKCSDevice , 1 , NULL , 0 , <nl> - kcs . inlen ), <nl> + VMSTATE_UINT8_ARRAY ( kcs . outmsg , ISAIPMIKCSDevice , MAX_IPMI_MSG_SIZE ), <nl> + VMSTATE_UINT8_ARRAY ( kcs . inmsg , ISAIPMIKCSDevice , MAX_IPMI_MSG_SIZE ), <nl> VMSTATE_BOOL ( kcs . write_end , ISAIPMIKCSDevice ), <nl> VMSTATE_UINT8 ( kcs . status_reg , ISAIPMIKCSDevice ), <nl> VMSTATE_UINT8 ( kcs . data_out_reg , ISAIPMIKCSDevice ),
void op_addo ( void ) <nl>  <nl> tmp = T0 ; <nl> T0 += T1 ; <nl> - if (( T0 >> 31 ) ^ ( T1 >> 31 ) ^ ( tmp >> 31 )) { <nl> + if ((( tmp ^ T1 ^ (- 1 )) & ( T0 ^ T1 )) >> 31 ) { <nl> + /* operands of same sign , result different sign */ <nl> CALL_FROM_TB1 ( do_raise_exception_direct , EXCP_OVERFLOW ); <nl> } <nl> RETURN (); <nl> void op_subo ( void ) <nl>  <nl> tmp = T0 ; <nl> T0 = ( int32_t ) T0 - ( int32_t ) T1 ; <nl> - if (!(( T0 >> 31 ) ^ ( T1 >> 31 ) ^ ( tmp >> 31 ))) { <nl> + if ((( tmp ^ T1 ) & ( tmp ^ T0 )) >> 31 ) { <nl> + /* operands of different sign , first operand and result different sign */ <nl> CALL_FROM_TB1 ( do_raise_exception_direct , EXCP_OVERFLOW ); <nl> } <nl> RETURN ();
static uint64_t get_migration_pass ( void ) <nl> } else { <nl> rsp_ram = qdict_get_qdict ( rsp_return , " ram "); <nl> result = qdict_get_try_int ( rsp_ram , " dirty - sync - count ", 0 ); <nl> - QDECREF ( rsp ); <nl> } <nl> + QDECREF ( rsp ); <nl> return result ; <nl> } <nl> 
static void close_guest_eventfds ( IVShmemState * s , int posn ) <nl> { <nl> int i , guest_curr_max ; <nl>  <nl> + if (! ivshmem_has_feature ( s , IVSHMEM_IOEVENTFD )) { <nl> + return ; <nl> + } <nl> + <nl> guest_curr_max = s -> peers [ posn ]. nb_eventfds ; <nl>  <nl> memory_region_transaction_begin ();
static CharDriverState * qemu_chr_open_tty ( QemuOpts * opts ) <nl> } <nl> tty_serial_init ( fd , 115200 , ' N ', 8 , 1 ); <nl> chr = qemu_chr_open_fd ( fd , fd ); <nl> - if (! chr ) { <nl> - close ( fd ); <nl> - return NULL ; <nl> - } <nl> chr -> chr_ioctl = tty_serial_ioctl ; <nl> chr -> chr_close = qemu_chr_close_tty ; <nl> return chr ;
static void coroutine_fn mirror_run ( void * opaque ) <nl>  <nl> s -> common . len = bdrv_getlength ( bs ); <nl> if ( s -> common . len <= 0 ) { <nl> - block_job_completed (& s -> common , s -> common . len ); <nl> - return ; <nl> + ret = s -> common . len ; <nl> + goto immediate_exit ; <nl> } <nl>  <nl> length = DIV_ROUND_UP ( s -> common . len , s -> granularity );
static int wm8750_tx ( I2CSlave * i2c , uint8_t data ) <nl> uint16_t value ; <nl>  <nl> if ( s -> i2c_len >= 2 ) { <nl> - printf ("% s : long message (% i bytes )\ n ", __FUNCTION__ , s -> i2c_len ); <nl> # ifdef VERBOSE <nl> - return 1 ; <nl> + printf ("% s : long message (% i bytes )\ n ", __func__ , s -> i2c_len ); <nl> # endif <nl> + return 1 ; <nl> } <nl> s -> i2c_data [ s -> i2c_len ++] = data ; <nl> if ( s -> i2c_len != 2 )
static void gic_dist_writeb ( void * opaque , hwaddr offset , <nl> if ( irq < GIC_INTERNAL ) <nl> value |= 0xaa ; <nl> for ( i = 0 ; i < 4 ; i ++) { <nl> - if ( value & ( 1 << ( i * 2 ))) { <nl> - GIC_SET_MODEL ( irq + i ); <nl> - } else { <nl> - GIC_CLEAR_MODEL ( irq + i ); <nl> + if ( s -> revision == REV_11MPCORE || s -> revision == REV_NVIC ) { <nl> + if ( value & ( 1 << ( i * 2 ))) { <nl> + GIC_SET_MODEL ( irq + i ); <nl> + } else { <nl> + GIC_CLEAR_MODEL ( irq + i ); <nl> + } <nl> } <nl> if ( value & ( 2 << ( i * 2 ))) { <nl> GIC_SET_EDGE_TRIGGER ( irq + i );
static int slirp_guestfwd ( SlirpState * s , const char * config_str , <nl> } <nl>  <nl> fwd = qemu_malloc ( sizeof ( struct GuestFwd )); <nl> - snprintf ( buf , sizeof ( buf ), " guestfwd . tcp :% d ", port ); <nl> + snprintf ( buf , sizeof ( buf ), " guestfwd . tcp .% d ", port ); <nl> fwd -> hd = qemu_chr_open ( buf , p , NULL ); <nl> if (! fwd -> hd ) { <nl> error_report (" could not open guest forwarding device '% s '", buf );
static void wav_capture_destroy ( void * opaque ) <nl> WAVState * wav = opaque ; <nl>  <nl> AUD_del_capture ( wav -> cap , wav ); <nl> + g_free ( wav ); <nl> } <nl>  <nl> static void wav_capture_info ( void * opaque )
static void eepro100_write_port ( EEPRO100State * s , uint32_t val ) <nl>  <nl> static uint8_t eepro100_read1 ( EEPRO100State * s , uint32_t addr ) <nl> { <nl> - uint8_t val ; <nl> + uint8_t val = 0 ; <nl> if ( addr <= sizeof ( s -> mem ) - sizeof ( val )) { <nl> memcpy (& val , & s -> mem [ addr ], sizeof ( val )); <nl> } <nl> static uint8_t eepro100_read1 ( EEPRO100State * s , uint32_t addr ) <nl>  <nl> static uint16_t eepro100_read2 ( EEPRO100State * s , uint32_t addr ) <nl> { <nl> - uint16_t val ; <nl> + uint16_t val = 0 ; <nl> if ( addr <= sizeof ( s -> mem ) - sizeof ( val )) { <nl> memcpy (& val , & s -> mem [ addr ], sizeof ( val )); <nl> } <nl> static uint16_t eepro100_read2 ( EEPRO100State * s , uint32_t addr ) <nl>  <nl> static uint32_t eepro100_read4 ( EEPRO100State * s , uint32_t addr ) <nl> { <nl> - uint32_t val ; <nl> + uint32_t val = 0 ; <nl> if ( addr <= sizeof ( s -> mem ) - sizeof ( val )) { <nl> memcpy (& val , & s -> mem [ addr ], sizeof ( val )); <nl> }
 <nl> # define DRC_CONTAINER_PATH "/ dr - connector " <nl> # define DRC_INDEX_TYPE_SHIFT 28 <nl> -# define DRC_INDEX_ID_MASK (~(~ 0 << DRC_INDEX_TYPE_SHIFT )) <nl> +# define DRC_INDEX_ID_MASK (( 1ULL << DRC_INDEX_TYPE_SHIFT ) - 1 ) <nl>  <nl> static sPAPRDRConnectorTypeShift get_type_shift ( sPAPRDRConnectorType type ) <nl> {
static void usb_uas_handle_destroy ( USBDevice * dev ) <nl> static void usb_uas_realize ( USBDevice * dev , Error ** errp ) <nl> { <nl> UASDevice * uas = USB_UAS ( dev ); <nl> + DeviceState * d = DEVICE ( dev ); <nl>  <nl> usb_desc_create_serial ( dev ); <nl> usb_desc_init ( dev ); <nl> + if ( d -> hotplugged ) { <nl> + uas -> dev . auto_attach = 0 ; <nl> + } <nl>  <nl> QTAILQ_INIT (& uas -> results ); <nl> QTAILQ_INIT (& uas -> requests ); <nl> static void usb_uas_class_initfn ( ObjectClass * klass , void * data ) <nl> uc -> handle_control = usb_uas_handle_control ; <nl> uc -> handle_data = usb_uas_handle_data ; <nl> uc -> handle_destroy = usb_uas_handle_destroy ; <nl> + uc -> attached_settable = true ; <nl> set_bit ( DEVICE_CATEGORY_STORAGE , dc -> categories ); <nl> dc -> fw_name = " storage "; <nl> dc -> vmsd = & vmstate_usb_uas ;
static void check_cmd ( AHCIState * s , int port ) <nl>  <nl> if (( pr -> cmd & PORT_CMD_START ) && pr -> cmd_issue ) { <nl> for ( slot = 0 ; ( slot < 32 ) && pr -> cmd_issue ; slot ++) { <nl> - if (( pr -> cmd_issue & ( 1 << slot )) && <nl> + if (( pr -> cmd_issue & ( 1U << slot )) && <nl> ! handle_cmd ( s , port , slot )) { <nl> - pr -> cmd_issue &= ~( 1 << slot ); <nl> + pr -> cmd_issue &= ~( 1U << slot ); <nl> } <nl> } <nl> }
static void ehci_advance_state ( EHCIState * ehci , <nl> fprintf ( stderr , " processing error - resetting ehci HC \ n "); <nl> ehci_reset ( ehci ); <nl> again = 0 ; <nl> - assert ( 0 ); <nl> } <nl> } <nl> while ( again );
int qemu_acl_insert ( qemu_acl * acl , <nl> const char * match , <nl> int index ) <nl> { <nl> - qemu_acl_entry * entry ; <nl> qemu_acl_entry * tmp ; <nl> int i = 0 ; <nl>  <nl> int qemu_acl_insert ( qemu_acl * acl , <nl> return qemu_acl_append ( acl , deny , match ); <nl> } <nl>  <nl> - entry = g_malloc ( sizeof (* entry )); <nl> - entry -> match = g_strdup ( match ); <nl> - entry -> deny = deny ; <nl> - <nl> QTAILQ_FOREACH ( tmp , & acl -> entries , next ) { <nl> i ++; <nl> if ( i == index ) { <nl> + qemu_acl_entry * entry ; <nl> + entry = g_malloc ( sizeof (* entry )); <nl> + entry -> match = g_strdup ( match ); <nl> + entry -> deny = deny ; <nl> + <nl> QTAILQ_INSERT_BEFORE ( tmp , entry , next ); <nl> acl -> nentries ++; <nl> break ;
static TCGv gen_lea_indexed ( CPUM68KState * env , DisasContext * s , TCGv base ) <nl> if (( ext & 0x800 ) == 0 && ! m68k_feature ( s -> env , M68K_FEATURE_WORD_INDEX )) <nl> return NULL_QREG ; <nl>  <nl> + if ( m68k_feature ( s -> env , M68K_FEATURE_M68000 ) && <nl> + ! m68k_feature ( s -> env , M68K_FEATURE_SCALED_INDEX )) { <nl> + ext &= ~( 3 << 9 ); <nl> + } <nl> + <nl> if ( ext & 0x100 ) { <nl> /* full extension word format */ <nl> if (! m68k_feature ( s -> env , M68K_FEATURE_EXT_FULL ))
static MTPData * usb_mtp_get_object ( MTPState * s , MTPControl * c , <nl>  <nl> d -> fd = open ( o -> path , O_RDONLY ); <nl> if ( d -> fd == - 1 ) { <nl> + usb_mtp_data_free ( d ); <nl> return NULL ; <nl> } <nl> d -> length = o -> stat . st_size ; <nl> static MTPData * usb_mtp_get_partial_object ( MTPState * s , MTPControl * c , <nl>  <nl> d -> fd = open ( o -> path , O_RDONLY ); <nl> if ( d -> fd == - 1 ) { <nl> + usb_mtp_data_free ( d ); <nl> return NULL ; <nl> } <nl> 
static uint64_t cg3_reg_read ( void * opaque , hwaddr addr , unsigned size ) <nl> /* monitor ID 6 , board type = 1 ( color ) */ <nl> val = s -> regs [ 1 ] | CG3_SR_1152_900_76_B | CG3_SR_ID_COLOR ; <nl> break ; <nl> - case CG3_REG_FBC_CURSTART ... CG3_REG_SIZE : <nl> + case CG3_REG_FBC_CURSTART ... CG3_REG_SIZE - 1 : <nl> val = s -> regs [ addr - 0x10 ]; <nl> break ; <nl> default : <nl> static void cg3_reg_write ( void * opaque , hwaddr addr , uint64_t val , <nl> qemu_irq_lower ( s -> irq ); <nl> } <nl> break ; <nl> - case CG3_REG_FBC_CURSTART ... CG3_REG_SIZE : <nl> + case CG3_REG_FBC_CURSTART ... CG3_REG_SIZE - 1 : <nl> s -> regs [ addr - 0x10 ] = val ; <nl> break ; <nl> default :
int qcow2_update_header ( BlockDriverState * bs ) <nl> ret = sizeof (* header ); <nl> break ; <nl> default : <nl> - return - EINVAL ; <nl> + ret = - EINVAL ; <nl> + goto fail ; <nl> } <nl>  <nl> buf += ret ;
static int htab_save_complete ( QEMUFile * f , void * opaque ) <nl> if ( rc < 0 ) { <nl> return rc ; <nl> } <nl> - close_htab_fd ( spapr ); <nl> } else { <nl> if ( spapr -> htab_first_pass ) { <nl> htab_save_first_pass ( f , spapr , - 1 ); <nl> static int htab_load ( QEMUFile * f , void * opaque , int version_id ) <nl> return 0 ; <nl> } <nl>  <nl> + static void htab_cleanup ( void * opaque ) <nl> +{ <nl> + sPAPRMachineState * spapr = opaque ; <nl> + <nl> + close_htab_fd ( spapr ); <nl> +} <nl> + <nl> static SaveVMHandlers savevm_htab_handlers = { <nl> . save_live_setup = htab_save_setup , <nl> . save_live_iterate = htab_save_iterate , <nl> . save_live_complete_precopy = htab_save_complete , <nl> + . cleanup = htab_cleanup , <nl> . load_state = htab_load , <nl> }; <nl> 
int trace_record_start ( TraceBufferRecord * rec , TraceEventID event , size_t datasi <nl> rec -> next_tbuf_idx = new_idx % TRACE_BUF_LEN ; <nl>  <nl> rec_off = idx ; <nl> - rec_off = write_to_buffer ( rec_off , ( uint8_t *)& event , sizeof ( event )); <nl> - rec_off = write_to_buffer ( rec_off , ( uint8_t *)& timestamp_ns , sizeof ( timestamp_ns )); <nl> - rec_off = write_to_buffer ( rec_off , ( uint8_t *)& rec_len , sizeof ( rec_len )); <nl> + rec_off = write_to_buffer ( rec_off , & event , sizeof ( event )); <nl> + rec_off = write_to_buffer ( rec_off , & timestamp_ns , sizeof ( timestamp_ns )); <nl> + rec_off = write_to_buffer ( rec_off , & rec_len , sizeof ( rec_len )); <nl>  <nl> rec -> tbuf_idx = idx ; <nl> rec -> rec_off = ( idx + sizeof ( TraceRecord )) % TRACE_BUF_LEN ;
static void gen_mullwo ( DisasContext * ctx ) <nl> tcg_gen_sari_i32 ( t0 , t0 , 31 ); <nl> tcg_gen_setcond_i32 ( TCG_COND_NE , t0 , t0 , t1 ); <nl> tcg_gen_extu_i32_tl ( cpu_ov , t0 ); <nl> + if ( is_isa300 ( ctx )) { <nl> + tcg_gen_mov_tl ( cpu_ov32 , cpu_ov ); <nl> + } <nl> tcg_gen_or_tl ( cpu_so , cpu_so , cpu_ov ); <nl>  <nl> tcg_temp_free_i32 ( t0 ); <nl> static void gen_mulldo ( DisasContext * ctx ) <nl>  <nl> tcg_gen_sari_i64 ( t0 , t0 , 63 ); <nl> tcg_gen_setcond_i64 ( TCG_COND_NE , cpu_ov , t0 , t1 ); <nl> + if ( is_isa300 ( ctx )) { <nl> + tcg_gen_mov_tl ( cpu_ov32 , cpu_ov ); <nl> + } <nl> tcg_gen_or_tl ( cpu_so , cpu_so , cpu_ov ); <nl>  <nl> tcg_temp_free_i64 ( t0 );
static void test_uuid_unparse_strdup ( void ) <nl> } <nl> out = qemu_uuid_unparse_strdup (& uuid_test_data [ i ]. uuid ); <nl> g_assert_cmpstr ( uuid_test_data [ i ]. uuidstr , ==, out ); <nl> + g_free ( out ); <nl> } <nl> } <nl> 
static void fsl_imx6_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = fsl_imx6_realize ; <nl> - <nl> dc -> desc = " i . MX6 SOC "; <nl> + /* Reason : Uses serial_hds [] in the realize () function */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo fsl_imx6_type_info = {
static inline void cpu_x86_load_seg_cache ( CPUX86State * env , <nl> } <nl>  <nl> static inline void cpu_x86_load_seg_cache_sipi ( X86CPU * cpu , <nl> - int sipi_vector ) <nl> + uint8_t sipi_vector ) <nl> { <nl> CPUState * cs = CPU ( cpu ); <nl> CPUX86State * env = & cpu -> env ;
static int64_t try_fiemap ( BlockDriverState * bs , off_t start , off_t * data , <nl>  <nl> f . fm . fm_start = start ; <nl> f . fm . fm_length = ( int64_t ) nb_sectors * BDRV_SECTOR_SIZE ; <nl> - f . fm . fm_flags = 0 ; <nl> + f . fm . fm_flags = FIEMAP_FLAG_SYNC ; <nl> f . fm . fm_extent_count = 1 ; <nl> f . fm . fm_reserved = 0 ; <nl> if ( ioctl ( s -> fd , FS_IOC_FIEMAP , & f ) == - 1 ) {
static TRBCCode xhci_disable_ep ( XHCIState * xhci , unsigned int slotid , <nl> usb_packet_cleanup (& epctx -> transfers [ i ]. packet ); <nl> } <nl>  <nl> - xhci_set_ep_state ( xhci , epctx , NULL , EP_DISABLED ); <nl> + /* only touch guest RAM if we ' re not resetting the HC */ <nl> + if ( xhci -> dcbaap_low || xhci -> dcbaap_high ) { <nl> + xhci_set_ep_state ( xhci , epctx , NULL , EP_DISABLED ); <nl> + } <nl>  <nl> timer_free ( epctx -> kick_timer ); <nl> g_free ( epctx );
static int client_migrate_info ( Monitor * mon , const QDict * qdict , <nl> const char * subject = qdict_get_try_str ( qdict , " cert - subject "); <nl> int port = qdict_get_try_int ( qdict , " port ", - 1 ); <nl> int tls_port = qdict_get_try_int ( qdict , " tls - port ", - 1 ); <nl> - Error * err ; <nl> + Error * err = NULL ; <nl> int ret ; <nl>  <nl> if ( strcmp ( protocol , " spice ") == 0 ) { <nl> if (! qemu_using_spice (& err )) { <nl> qerror_report_err ( err ); <nl> + error_free ( err ); <nl> return - 1 ; <nl> } <nl> 
static int raw_read_options ( QDict * options , BlockDriverState * bs , <nl>  <nl> /* Make sure size is multiple of BDRV_SECTOR_SIZE to prevent rounding <nl> * up and leaking out of the specified area . */ <nl> - if (! QEMU_IS_ALIGNED ( s -> size , BDRV_SECTOR_SIZE )) { <nl> + if ( s -> has_size && ! QEMU_IS_ALIGNED ( s -> size , BDRV_SECTOR_SIZE )) { <nl> error_setg ( errp , " Specified size is not multiple of % llu ", <nl> BDRV_SECTOR_SIZE ); <nl> ret = - EINVAL ;
static void qemu_gluster_close ( BlockDriverState * bs ) <nl> glfs_fini ( s -> glfs ); <nl> } <nl>  <nl> + static int qemu_gluster_has_zero_init ( BlockDriverState * bs ) <nl> +{ <nl> + /* GlusterFS volume could be backed by a block device */ <nl> + return 0 ; <nl> +} <nl> + <nl> static QEMUOptionParameter qemu_gluster_create_options [] = { <nl> { <nl> . name = BLOCK_OPT_SIZE , <nl> static BlockDriver bdrv_gluster = { <nl> . bdrv_aio_readv = qemu_gluster_aio_readv , <nl> . bdrv_aio_writev = qemu_gluster_aio_writev , <nl> . bdrv_aio_flush = qemu_gluster_aio_flush , <nl> + . bdrv_has_zero_init = qemu_gluster_has_zero_init , <nl> . create_options = qemu_gluster_create_options , <nl> }; <nl>  <nl> static BlockDriver bdrv_gluster_tcp = { <nl> . bdrv_aio_readv = qemu_gluster_aio_readv , <nl> . bdrv_aio_writev = qemu_gluster_aio_writev , <nl> . bdrv_aio_flush = qemu_gluster_aio_flush , <nl> + . bdrv_has_zero_init = qemu_gluster_has_zero_init , <nl> . create_options = qemu_gluster_create_options , <nl> }; <nl>  <nl> static BlockDriver bdrv_gluster_unix = { <nl> . bdrv_aio_readv = qemu_gluster_aio_readv , <nl> . bdrv_aio_writev = qemu_gluster_aio_writev , <nl> . bdrv_aio_flush = qemu_gluster_aio_flush , <nl> + . bdrv_has_zero_init = qemu_gluster_has_zero_init , <nl> . create_options = qemu_gluster_create_options , <nl> }; <nl>  <nl> static BlockDriver bdrv_gluster_rdma = { <nl> . bdrv_aio_readv = qemu_gluster_aio_readv , <nl> . bdrv_aio_writev = qemu_gluster_aio_writev , <nl> . bdrv_aio_flush = qemu_gluster_aio_flush , <nl> + . bdrv_has_zero_init = qemu_gluster_has_zero_init , <nl> . create_options = qemu_gluster_create_options , <nl> }; <nl> 
int main ( int argc , char ** argv , char ** envp ) <nl> break ; <nl> case QEMU_OPTION_m : { <nl> int64_t value ; <nl> + char * end ; <nl>  <nl> - value = strtosz ( optarg , NULL ); <nl> - if ( value < 0 ) { <nl> + value = strtosz ( optarg , & end ); <nl> + if ( value < 0 || * end ) { <nl> fprintf ( stderr , " qemu : invalid ram size : % s \ n ", optarg ); <nl> exit ( 1 ); <nl> }
sPAPRTCETable * spapr_tce_new_table ( DeviceState * owner , uint32_t liobn , <nl> return tcet ; <nl> } <nl>  <nl> - static void spapr_tce_table_finalize ( Object * obj ) <nl> + static void spapr_tce_table_unrealize ( DeviceState * dev , Error ** errp ) <nl> { <nl> - sPAPRTCETable * tcet = SPAPR_TCE_TABLE ( obj ); <nl> + sPAPRTCETable * tcet = SPAPR_TCE_TABLE ( dev ); <nl>  <nl> QLIST_REMOVE ( tcet , list ); <nl>  <nl> static void spapr_tce_table_class_init ( ObjectClass * klass , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( klass ); <nl> dc -> init = spapr_tce_table_realize ; <nl> dc -> reset = spapr_tce_reset ; <nl> + dc -> unrealize = spapr_tce_table_unrealize ; <nl>  <nl> QLIST_INIT (& spapr_tce_tables ); <nl>  <nl> static TypeInfo spapr_tce_table_info = { <nl> . parent = TYPE_DEVICE , <nl> . instance_size = sizeof ( sPAPRTCETable ), <nl> . class_init = spapr_tce_table_class_init , <nl> - . instance_finalize = spapr_tce_table_finalize , <nl> }; <nl>  <nl> static void register_types ( void )
static void vnc_init_basic_info_from_server_addr ( QIOChannelSocket * ioc , <nl> { <nl> SocketAddress * addr = NULL ; <nl>  <nl> + if (! ioc ) { <nl> + error_setg ( errp , " No listener socket available "); <nl> + return ; <nl> + } <nl> + <nl> addr = qio_channel_socket_get_local_address ( ioc , errp ); <nl> if (! addr ) { <nl> return ;
static int raw_create ( const char * filename , QemuOpts * opts , Error ** errp ) <nl> goto out ; <nl> } <nl>  <nl> - fd = qemu_open ( filename , O_WRONLY | O_CREAT | O_TRUNC | O_BINARY , <nl> + fd = qemu_open ( filename , O_RDWR | O_CREAT | O_TRUNC | O_BINARY , <nl> 0644 ); <nl> if ( fd < 0 ) { <nl> result = - errno ;
udp_input ( register struct mbuf * m , int iphlen ) <nl> * Locate pcb for datagram . <nl> */ <nl> so = slirp -> udp_last_so ; <nl> - if ( so -> so_lport != uh -> uh_sport || <nl> + if ( so == & slirp -> udb || so -> so_lport != uh -> uh_sport || <nl> so -> so_laddr . s_addr != ip -> ip_src . s_addr ) { <nl> struct socket * tmp ; <nl> 
static struct pathelem * new_entry ( const char * root , <nl> struct pathelem * parent , <nl> const char * name ) <nl> { <nl> - struct pathelem * new = malloc ( sizeof (* new )); <nl> - new -> name = strdup ( name ); <nl> + struct pathelem * new = g_malloc ( sizeof (* new )); <nl> + new -> name = g_strdup ( name ); <nl> new -> pathname = g_strdup_printf ("% s /% s ", root , name ); <nl> new -> num_entries = 0 ; <nl> return new ; <nl> static struct pathelem * add_entry ( struct pathelem * root , const char * name , <nl>  <nl> root -> num_entries ++; <nl>  <nl> - root = realloc ( root , sizeof (* root ) <nl> + root = g_realloc ( root , sizeof (* root ) <nl> + sizeof ( root -> entries [ 0 ])* root -> num_entries ); <nl> e = & root -> entries [ root -> num_entries - 1 ]; <nl>  <nl> void init_paths ( const char * prefix ) <nl> base = add_dir_maybe ( base ); <nl> if ( base -> num_entries == 0 ) { <nl> g_free ( base -> pathname ); <nl> - free ( base -> name ); <nl> - free ( base ); <nl> + g_free ( base -> name ); <nl> + g_free ( base ); <nl> base = NULL ; <nl> } else { <nl> set_parents ( base , base );
struct { \ <nl> } \ <nl> } while (/* CONSTCOND */ 0 ) <nl>  <nl> +# define QSIMPLEQ_PREPEND ( head1 , head2 ) do { \ <nl> + if (! QSIMPLEQ_EMPTY (( head2 ))) { \ <nl> + *( head2 )-> sqh_last = ( head1 )-> sqh_first ; \ <nl> + ( head1 )-> sqh_first = ( head2 )-> sqh_first ; \ <nl> + QSIMPLEQ_INIT (( head2 )); \ <nl> + } \ <nl> +} while (/* CONSTCOND */ 0 ) <nl> + <nl> # define QSIMPLEQ_LAST ( head , type , field ) \ <nl> ( QSIMPLEQ_EMPTY (( head )) ? \ <nl> NULL : \
void qmp_migrate ( const char * uri , bool has_blk , bool blk , <nl> # endif <nl> } else { <nl> error_set ( errp , QERR_INVALID_PARAMETER_VALUE , " uri ", " a valid migration protocol "); <nl> + s -> state = MIG_STATE_ERROR ; <nl> return ; <nl> } <nl> 
fail : <nl> /* refcount checking functions */ <nl>  <nl>  <nl> - static size_t refcount_array_byte_size ( BDRVQcow2State * s , uint64_t entries ) <nl> + static uint64_t refcount_array_byte_size ( BDRVQcow2State * s , uint64_t entries ) <nl> { <nl> /* This assertion holds because there is no way we can address more than <nl> * 2 ^( 64 - 9 ) clusters at once ( with cluster size 512 = 2 ^ 9 , and because
static ssize_t eth_rx ( NetClientState * nc , const uint8_t * buf , size_t size ) <nl> } <nl>  <nl> D ( qemu_log ("% s % zd rxbase =% x \ n ", __func__ , size , rxbase )); <nl> + if ( size > ( R_MAX - R_RX_BUF0 - rxbase ) * 4 ) { <nl> + D ( qemu_log (" ethlite packet is too big , size =% x \ n ", size )); <nl> + return - 1 ; <nl> + } <nl> memcpy (& s -> regs [ rxbase + R_RX_BUF0 ], buf , size ); <nl>  <nl> s -> regs [ rxbase + R_RX_CTRL0 ] |= CTRL_S ;
static int scsi_req_length ( SCSICommand * cmd , SCSIDevice * dev , uint8_t * buf ) <nl> case VERIFY_16 : <nl> if (( buf [ 1 ] & 2 ) == 0 ) { <nl> cmd -> xfer = 0 ; <nl> - } else if (( buf [ 1 ] & 4 ) == 1 ) { <nl> + } else if (( buf [ 1 ] & 4 ) != 0 ) { <nl> cmd -> xfer = 1 ; <nl> } <nl> cmd -> xfer *= dev -> blocksize ;
int bdrv_open ( BlockDriverState * bs , const char * filename , int flags , <nl> bdrv_delete ( bs1 ); <nl> return ret ; <nl> } <nl> - total_size = bdrv_getlength ( bs1 ) >> BDRV_SECTOR_BITS ; <nl> + total_size = bdrv_getlength ( bs1 ) & BDRV_SECTOR_MASK ; <nl>  <nl> if ( bs1 -> drv && bs1 -> drv -> protocol_name ) <nl> is_protocol = 1 ; <nl> int bdrv_open ( BlockDriverState * bs , const char * filename , int flags , <nl> bdrv_qcow2 = bdrv_find_format (" qcow2 "); <nl> options = parse_option_parameters ("", bdrv_qcow2 -> create_options , NULL ); <nl>  <nl> - set_option_parameter_int ( options , BLOCK_OPT_SIZE , total_size * 512 ); <nl> + set_option_parameter_int ( options , BLOCK_OPT_SIZE , total_size ); <nl> set_option_parameter ( options , BLOCK_OPT_BACKING_FILE , backing_filename ); <nl> if ( drv ) { <nl> set_option_parameter ( options , BLOCK_OPT_BACKING_FMT ,
static uint64_t imx_serial_read ( void * opaque , hwaddr offset , <nl> s -> usr2 &= ~ USR2_RDR ; <nl> s -> uts1 |= UTS1_RXEMPTY ; <nl> imx_update ( s ); <nl> - qemu_chr_accept_input ( s -> chr ); <nl> + if ( s -> chr ) { <nl> + qemu_chr_accept_input ( s -> chr ); <nl> + } <nl> } <nl> return c ; <nl>  <nl> static void imx_serial_write ( void * opaque , hwaddr offset , <nl> } <nl> if ( value & UCR2_RXEN ) { <nl> if (!( s -> ucr2 & UCR2_RXEN )) { <nl> - qemu_chr_accept_input ( s -> chr ); <nl> + if ( s -> chr ) { <nl> + qemu_chr_accept_input ( s -> chr ); <nl> + } <nl> } <nl> } <nl> s -> ucr2 = value & 0xffff ;
# include " qemu - timer . h " <nl> # include " sysemu . h " <nl> # include " acpi . h " <nl> +# include " kvm . h " <nl>  <nl> # include " ich9 . h " <nl>  <nl> static void pm_reset ( void * opaque ) <nl> acpi_pm_tmr_reset (& pm -> acpi_regs ); <nl> acpi_gpe_reset (& pm -> acpi_regs ); <nl>  <nl> + if ( kvm_enabled ()) { <nl> + /* Mark SMM as already inited to prevent SMM from running . KVM does not <nl> + * support SMM mode . */ <nl> + pm -> smi_en |= ICH9_PMIO_SMI_EN_APMC_EN ; <nl> + } <nl> + <nl> pm_update_sci ( pm ); <nl> } <nl> 
void virtio_queue_set_notification ( VirtQueue * vq , int enable ) <nl> } else { <nl> vring_used_flags_set_bit ( vq , VRING_USED_F_NO_NOTIFY ); <nl> } <nl> + if ( enable ) { <nl> + /* Expose avail event / used flags before caller checks the avail idx . */ <nl> + smp_mb (); <nl> + } <nl> } <nl>  <nl> int virtio_queue_ready ( VirtQueue * vq )
static void htab_save_first_pass ( QEMUFile * f , sPAPREnvironment * spapr , <nl>  <nl> /* Consume valid HPTEs */ <nl> chunkstart = index ; <nl> - while (( index < htabslots ) <nl> + while (( index < htabslots ) && ( index - chunkstart < USHRT_MAX ) <nl> && HPTE_VALID ( HPTE ( spapr -> htab , index ))) { <nl> index ++; <nl> CLEAN_HPTE ( HPTE ( spapr -> htab , index )); <nl> static int htab_save_later_pass ( QEMUFile * f , sPAPREnvironment * spapr , <nl>  <nl> chunkstart = index ; <nl> /* Consume valid dirty HPTEs */ <nl> - while (( index < htabslots ) <nl> + while (( index < htabslots ) && ( index - chunkstart < USHRT_MAX ) <nl> && HPTE_DIRTY ( HPTE ( spapr -> htab , index )) <nl> && HPTE_VALID ( HPTE ( spapr -> htab , index ))) { <nl> CLEAN_HPTE ( HPTE ( spapr -> htab , index )); <nl> static int htab_save_later_pass ( QEMUFile * f , sPAPREnvironment * spapr , <nl>  <nl> invalidstart = index ; <nl> /* Consume invalid dirty HPTEs */ <nl> - while (( index < htabslots ) <nl> + while (( index < htabslots ) && ( index - invalidstart < USHRT_MAX ) <nl> && HPTE_DIRTY ( HPTE ( spapr -> htab , index )) <nl> && ! HPTE_VALID ( HPTE ( spapr -> htab , index ))) { <nl> CLEAN_HPTE ( HPTE ( spapr -> htab , index ));
static int usb_host_open ( USBHostDevice * dev , int bus_num , <nl>  <nl> dev -> bus_num = bus_num ; <nl> dev -> addr = addr ; <nl> - strcpy ( dev -> port , port ); <nl> + pstrcpy ( dev -> port , sizeof ( dev -> port ), port ); <nl> dev -> fd = fd ; <nl>  <nl> /* read the device description */
static int qcow2_open ( BlockDriverState * bs , QDict * options , int flags , <nl> s -> incompatible_features & <nl> ~ QCOW2_INCOMPAT_MASK ); <nl> ret = - ENOTSUP ; <nl> + g_free ( feature_table ); <nl> goto fail ; <nl> } <nl>  <nl> static int qcow2_open ( BlockDriverState * bs , QDict * options , int flags , <nl> if ( s -> l2_table_cache ) { <nl> qcow2_cache_destroy ( bs , s -> l2_table_cache ); <nl> } <nl> + if ( s -> refcount_block_cache ) { <nl> + qcow2_cache_destroy ( bs , s -> refcount_block_cache ); <nl> + } <nl> g_free ( s -> cluster_cache ); <nl> qemu_vfree ( s -> cluster_data ); <nl> return ret ;
static void update_irq ( struct xlx_pic * p ) <nl>  <nl> /* Update the vector register . */ <nl> for ( i = 0 ; i < 32 ; i ++) { <nl> - if ( p -> regs [ R_IPR ] & ( 1 << i )) <nl> + if ( p -> regs [ R_IPR ] & ( 1U << i )) { <nl> break ; <nl> + } <nl> } <nl> if ( i == 32 ) <nl> i = ~ 0 ;
void cpu_x86_cpuid ( CPUX86State * env , uint32_t index , uint32_t count , <nl> index = env -> cpuid_xlevel ; <nl> } <nl> } else { <nl> - index = env -> cpuid_xlevel ; <nl> + /* Intel documentation states that invalid EAX input will <nl> + * return the same information as EAX = cpuid_level <nl> + * ( Intel SDM Vol . 2A - Instruction Set Reference - CPUID ) <nl> + */ <nl> + index = env -> cpuid_level ; <nl> } <nl> } <nl> } else {
static void qemu_laio_completion_bh ( void * opaque ) <nl> if (! s -> io_q . plugged && ! QSIMPLEQ_EMPTY (& s -> io_q . pending )) { <nl> ioq_submit ( s ); <nl> } <nl> + <nl> + qemu_bh_cancel ( s -> completion_bh ); <nl> } <nl>  <nl> static void qemu_laio_completion_cb ( EventNotifier * e ) <nl> static void qemu_laio_completion_cb ( EventNotifier * e ) <nl> LinuxAioState * s = container_of ( e , LinuxAioState , e ); <nl>  <nl> if ( event_notifier_test_and_clear (& s -> e )) { <nl> - qemu_bh_schedule ( s -> completion_bh ); <nl> + qemu_laio_completion_bh ( s ); <nl> } <nl> } <nl> 
int page_check_range ( target_ulong start , target_ulong len , int flags ) <nl> return - 1 ; <nl> } <nl> } <nl> - return 0 ; <nl> } <nl> } <nl> return 0 ;
void net_client_uninit ( NICInfo * nd ) <nl> { <nl> nd -> vlan -> nb_guest_devs --; <nl> nb_nics --; <nl> - nd -> used = 0 ; <nl> - free (( void *) nd -> model ); <nl> + <nl> + qemu_free (( void *) nd -> model ); <nl> + qemu_free (( void *) nd -> name ); <nl> + qemu_free (( void *) nd -> devaddr ); <nl> + qemu_free (( void *) nd -> id ); <nl> + <nl> + memset ( nd , 0 , sizeof (* nd )); <nl> } <nl>  <nl> static int net_host_check_device ( const char * device )
abi_long target_mmap ( abi_ulong start , abi_ulong len , int prot , <nl> may need to truncate file maps at EOF and add extra anonymous pages <nl> up to the targets page boundary . */ <nl>  <nl> - if (( qemu_real_host_page_size < TARGET_PAGE_SIZE ) <nl> - && !( flags & MAP_ANONYMOUS )) { <nl> - struct stat sb ; <nl> + if (( qemu_real_host_page_size < qemu_host_page_size ) && <nl> + !( flags & MAP_ANONYMOUS )) { <nl> + struct stat sb ; <nl>  <nl> if ( fstat ( fd , & sb ) == - 1 ) <nl> goto fail ;
int spapr_populate_pci_devices ( sPAPRPHBState * phb , <nl> reg [ 0 ]. size = 0 ; <nl>  <nl> n = 0 ; <nl> - for ( i = 0 ; i < PCI_NUM_REGIONS ; ++ i ) { <nl> + for ( i = 0 ; i < ARRAY_SIZE ( bars ); ++ i ) { <nl> if ( 0 == dev -> io_regions [ i ]. size ) { <nl> continue ; <nl> }
void socket_listen_cleanup ( int fd , Error ** errp ) <nl> SocketAddress * addr ; <nl>  <nl> addr = socket_local_address ( fd , errp ); <nl> + if (! addr ) { <nl> + return ; <nl> + } <nl>  <nl> if ( addr -> type == SOCKET_ADDRESS_TYPE_UNIX <nl> && addr -> u . q_unix . path ) {
static int sd_create ( const char * filename , QemuOpts * opts , <nl> if ( s -> inode . block_size_shift == 0 ) { <nl> SheepdogVdiReq hdr ; <nl> SheepdogClusterRsp * rsp = ( SheepdogClusterRsp *)& hdr ; <nl> - Error * local_err = NULL ; <nl> int fd ; <nl> unsigned int wlen = 0 , rlen = 0 ; <nl>  <nl> - fd = connect_to_sdog ( s , & local_err ); <nl> + fd = connect_to_sdog ( s , errp ); <nl> if ( fd < 0 ) { <nl> - error_report_err ( local_err ); <nl> - ret = - EIO ; <nl> + ret = fd ; <nl> goto out ; <nl> } <nl> 
void ppc_tlb_invalidate_one ( CPUPPCState * env , target_ulong addr ) <nl> /* XXX : this case should be optimized , <nl> * giving a mask to tlb_flush_page <nl> */ <nl> + /* This is broken , some CPUs invalidate a whole congruence <nl> + * class on an even smaller subset of bits and some OSes take <nl> + * advantage of this . Just blow the whole thing away . <nl> + */ <nl> +# if 0 <nl> tlb_flush_page ( cs , addr | ( 0x0 << 28 )); <nl> tlb_flush_page ( cs , addr | ( 0x1 << 28 )); <nl> tlb_flush_page ( cs , addr | ( 0x2 << 28 )); <nl> void ppc_tlb_invalidate_one ( CPUPPCState * env , target_ulong addr ) <nl> tlb_flush_page ( cs , addr | ( 0xD << 28 )); <nl> tlb_flush_page ( cs , addr | ( 0xE << 28 )); <nl> tlb_flush_page ( cs , addr | ( 0xF << 28 )); <nl> +# else <nl> + tlb_flush ( cs , 1 ); <nl> +# endif <nl> break ; <nl> # if defined ( TARGET_PPC64 ) <nl> case POWERPC_MMU_64B :
static void disas_arm_insn ( DisasContext * s , unsigned int insn ) <nl> ARCH ( 6T2 ); <nl> shift = ( insn >> 7 ) & 0x1f ; <nl> i = ( insn >> 16 ) & 0x1f ; <nl> + if ( i < shift ) { <nl> + /* UNPREDICTABLE ; we choose to UNDEF */ <nl> + goto illegal_op ; <nl> + } <nl> i = i + 1 - shift ; <nl> if ( rm == 15 ) { <nl> tmp = tcg_temp_new_i32 ();
static void virtqueue_map_desc ( unsigned int * p_num_sg , hwaddr * addr , struct iove <nl> } <nl>  <nl> iov [ num_sg ]. iov_base = cpu_physical_memory_map ( pa , & len , is_write ); <nl> + if (! iov [ num_sg ]. iov_base ) { <nl> + error_report (" virtio : bogus descriptor or out of resources "); <nl> + exit ( 1 ); <nl> + } <nl> + <nl> iov [ num_sg ]. iov_len = len ; <nl> addr [ num_sg ] = pa ; <nl> 
static void virtio_net_handle_ctrl ( VirtIODevice * vdev , VirtQueue * vq ) <nl> virtio_net_ctrl_ack status = VIRTIO_NET_ERR ; <nl> VirtQueueElement elem ; <nl> size_t s ; <nl> - struct iovec * iov ; <nl> + struct iovec * iov , * iov2 ; <nl> unsigned int iov_cnt ; <nl>  <nl> while ( virtqueue_pop ( vq , & elem )) { <nl> static void virtio_net_handle_ctrl ( VirtIODevice * vdev , VirtQueue * vq ) <nl> exit ( 1 ); <nl> } <nl>  <nl> - iov = elem . out_sg ; <nl> iov_cnt = elem . out_num ; <nl> + iov2 = iov = g_memdup ( elem . out_sg , sizeof ( struct iovec ) * elem . out_num ); <nl> s = iov_to_buf ( iov , iov_cnt , 0 , & ctrl , sizeof ( ctrl )); <nl> iov_discard_front (& iov , & iov_cnt , sizeof ( ctrl )); <nl> if ( s != sizeof ( ctrl )) { <nl> static void virtio_net_handle_ctrl ( VirtIODevice * vdev , VirtQueue * vq ) <nl>  <nl> virtqueue_push ( vq , & elem , sizeof ( status )); <nl> virtio_notify ( vdev , vq ); <nl> + g_free ( iov2 ); <nl> } <nl> } <nl> 
static int blk_mig_save_bulked_block ( Monitor * mon , QEMUFile * f ) <nl> } <nl> } <nl>  <nl> - progress = completed_sector_sum * 100 / block_mig_state . total_sector_sum ; <nl> + if ( block_mig_state . total_sector_sum != 0 ) { <nl> + progress = completed_sector_sum * 100 / <nl> + block_mig_state . total_sector_sum ; <nl> + } else { <nl> + progress = 100 ; <nl> + } <nl> if ( progress != block_mig_state . prev_progress ) { <nl> block_mig_state . prev_progress = progress ; <nl> qemu_put_be64 ( f , ( progress << BDRV_SECTOR_BITS )
int virtqueue_pop ( VirtQueue * vq , VirtQueueElement * elem ) <nl> struct iovec * sg ; <nl>  <nl> if ( vring_desc_flags ( desc_pa , i ) & VRING_DESC_F_WRITE ) { <nl> + if ( elem -> in_num >= ARRAY_SIZE ( elem -> in_sg )) { <nl> + error_report (" Too many write descriptors in indirect table "); <nl> + exit ( 1 ); <nl> + } <nl> elem -> in_addr [ elem -> in_num ] = vring_desc_addr ( desc_pa , i ); <nl> sg = & elem -> in_sg [ elem -> in_num ++]; <nl> } else { <nl> + if ( elem -> out_num >= ARRAY_SIZE ( elem -> out_sg )) { <nl> + error_report (" Too many read descriptors in indirect table "); <nl> + exit ( 1 ); <nl> + } <nl> elem -> out_addr [ elem -> out_num ] = vring_desc_addr ( desc_pa , i ); <nl> sg = & elem -> out_sg [ elem -> out_num ++]; <nl> }
static void vga_draw_graphic ( VGACommonState * s , int full_update ) <nl> y1 = 0 ; <nl>  <nl> if (! full_update ) { <nl> + ram_addr_t region_start = addr1 ; <nl> + ram_addr_t region_end = addr1 + line_offset * height ; <nl> vga_sync_dirty_bitmap ( s ); <nl> - snap = memory_region_snapshot_and_clear_dirty (& s -> vram , addr1 , <nl> - line_offset * height , <nl> + if ( s -> line_compare < height ) { <nl> + /* split screen mode */ <nl> + region_start = 0 ; <nl> + } <nl> + snap = memory_region_snapshot_and_clear_dirty (& s -> vram , region_start , <nl> + region_end - region_start , <nl> DIRTY_MEMORY_VGA ); <nl> } <nl> 
typedef struct VmdkExtent { <nl> uint32_t l2_cache_offsets [ L2_CACHE_SIZE ]; <nl> uint32_t l2_cache_counts [ L2_CACHE_SIZE ]; <nl>  <nl> - unsigned int cluster_sectors ; <nl> + int64_t cluster_sectors ; <nl> } VmdkExtent ; <nl>  <nl> typedef struct BDRVVmdkState { <nl> static int vmdk_add_extent ( BlockDriverState * bs , <nl> extent -> l1_size = l1_size ; <nl> extent -> l1_entry_sectors = l2_size * cluster_sectors ; <nl> extent -> l2_size = l2_size ; <nl> - extent -> cluster_sectors = cluster_sectors ; <nl> + extent -> cluster_sectors = flat ? sectors : cluster_sectors ; <nl>  <nl> if ( s -> num_extents > 1 ) { <nl> extent -> end_sector = (*( extent - 1 )). end_sector + extent -> sectors ; <nl> static int vmdk_parse_extents ( const char * desc , BlockDriverState * bs , <nl> VmdkExtent * extent ; <nl>  <nl> ret = vmdk_add_extent ( bs , extent_file , true , sectors , <nl> - 0 , 0 , 0 , 0 , sectors , & extent ); <nl> + 0 , 0 , 0 , 0 , 0 , & extent ); <nl> if ( ret < 0 ) { <nl> return ret ; <nl> }
static int qcow2_write_compressed ( BlockDriverState * bs , int64_t sector_num , <nl>  <nl> if ( ret != Z_STREAM_END || out_len >= s -> cluster_size ) { <nl> /* could not compress : write normal cluster */ <nl> - <nl> - ret = qcow2_pre_write_overlap_check ( bs , QCOW2_OL_DEFAULT , <nl> - sector_num * BDRV_SECTOR_SIZE , <nl> - s -> cluster_sectors * BDRV_SECTOR_SIZE ); <nl> - if ( ret < 0 ) { <nl> - goto fail ; <nl> - } <nl> - <nl> ret = bdrv_write ( bs , sector_num , buf , s -> cluster_sectors ); <nl> if ( ret < 0 ) { <nl> goto fail ;
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl> goto efault ; <nl> ret = get_errno ( sys_uname ( buf )); <nl> if (! is_error ( ret )) { <nl> - /* Overrite the native machine name with whatever is being <nl> + /* Overwrite the native machine name with whatever is being <nl> emulated . */ <nl> strcpy ( buf -> machine , cpu_to_uname_machine ( cpu_env )); <nl> /* Allow the user to override the reported release . */ <nl> - if ( qemu_uname_release && * qemu_uname_release ) <nl> - strcpy ( buf -> release , qemu_uname_release ); <nl> + if ( qemu_uname_release && * qemu_uname_release ) { <nl> + g_strlcpy ( buf -> release , qemu_uname_release , <nl> + sizeof ( buf -> release )); <nl> + } <nl> } <nl> unlock_user_struct ( buf , arg1 , 1 ); <nl> }
static TRBCCode xhci_address_slot ( XHCIState * xhci , unsigned int slotid , <nl> slot_ctx [ 3 ] = SLOT_DEFAULT << SLOT_STATE_SHIFT ; <nl> } else { <nl> USBPacket p ; <nl> + uint8_t buf [ 1 ]; <nl> + <nl> slot_ctx [ 3 ] = ( SLOT_ADDRESSED << SLOT_STATE_SHIFT ) | slotid ; <nl> usb_device_reset ( dev ); <nl> + memset (& p , 0 , sizeof ( p )); <nl> + usb_packet_addbuf (& p , buf , sizeof ( buf )); <nl> usb_packet_setup (& p , USB_TOKEN_OUT , <nl> usb_ep_get ( dev , USB_TOKEN_OUT , 0 ), 0 , <nl> 0 , false , false );
int nbd_receive_reply ( QIOChannel * ioc , NBDReply * reply , Error ** errp ) <nl> if ( ret < 0 ) { <nl> break ; <nl> } <nl> - <nl> trace_nbd_receive_simple_reply ( reply -> simple . error , <nl> nbd_err_lookup ( reply -> simple . error ), <nl> reply -> handle ); <nl> - if ( reply -> simple . error == NBD_ESHUTDOWN ) { <nl> - /* This works even on mingw which lacks a native ESHUTDOWN */ <nl> - error_setg ( errp , " server shutting down "); <nl> - return - EINVAL ; <nl> - } <nl> break ; <nl> case NBD_STRUCTURED_REPLY_MAGIC : <nl> ret = nbd_receive_structured_reply_chunk ( ioc , & reply -> structured , errp );
static int qcow2_write_snapshots ( BlockDriverState * bs ) <nl>  <nl> id_str_size = strlen ( sn -> id_str ); <nl> name_size = strlen ( sn -> name ); <nl> + assert ( id_str_size <= UINT16_MAX && name_size <= UINT16_MAX ); <nl> h . id_str_size = cpu_to_be16 ( id_str_size ); <nl> h . name_size = cpu_to_be16 ( name_size ); <nl> offset = align_offset ( offset , 8 );
static int qio_channel_buffer_close ( QIOChannel * ioc , <nl> QIOChannelBuffer * bioc = QIO_CHANNEL_BUFFER ( ioc ); <nl>  <nl> g_free ( bioc -> data ); <nl> + bioc -> data = NULL ; <nl> bioc -> capacity = bioc -> usage = bioc -> offset = 0 ; <nl>  <nl> return 0 ;
static void netfilter_finalize ( Object * obj ) <nl> if ( nf -> netdev && ! QTAILQ_EMPTY (& nf -> netdev -> filters )) { <nl> QTAILQ_REMOVE (& nf -> netdev -> filters , nf , next ); <nl> } <nl> + g_free ( nf -> netdev_id ); <nl> } <nl>  <nl> static void netfilter_class_init ( ObjectClass * oc , void * data )
void cpu_dump_state ( CPUPPCState * env , FILE * f , fprintf_function cpu_fprintf , <nl>  <nl> int i ; <nl>  <nl> + cpu_synchronize_state ( env ); <nl> + <nl> cpu_fprintf ( f , " NIP " TARGET_FMT_lx " LR " TARGET_FMT_lx " CTR " <nl> TARGET_FMT_lx " XER " TARGET_FMT_lx "\ n ", <nl> env -> nip , env -> lr , env -> ctr , env -> xer );
VIOsPAPRDevice * vty_lookup ( sPAPRMachineState * spapr , target_ulong reg ) <nl> return spapr_vty_get_default ( spapr -> vio_bus ); <nl> } <nl>  <nl> + if (! object_dynamic_cast ( OBJECT ( sdev ), TYPE_VIO_SPAPR_VTY_DEVICE )) { <nl> + return NULL ; <nl> + } <nl> + <nl> return sdev ; <nl> } <nl> 
int ppc_compat_max_threads ( PowerPCCPU * cpu ); <nl> # define SPR_601_UDECR ( 0x006 ) <nl> # define SPR_LR ( 0x008 ) <nl> # define SPR_CTR ( 0x009 ) <nl> -# define SPR_UAMR ( 0x00C ) <nl> +# define SPR_UAMR ( 0x00D ) <nl> # define SPR_DSCR ( 0x011 ) <nl> # define SPR_DSISR ( 0x012 ) <nl> # define SPR_DAR ( 0x013 ) /* DAE for PowerPC 601 */
void qio_channel_test_run_reader ( QIOChannelTest * test , <nl>  <nl> void qio_channel_test_validate ( QIOChannelTest * test ) <nl> { <nl> + g_assert ( test -> readerr == NULL ); <nl> + g_assert ( test -> writeerr == NULL ); <nl> g_assert_cmpint ( memcmp ( test -> input , <nl> test -> output , <nl> test -> len ), ==, 0 ); <nl> - g_assert ( test -> readerr == NULL ); <nl> - g_assert ( test -> writeerr == NULL ); <nl>  <nl> g_free ( test -> inputv ); <nl> g_free ( test -> outputv );
qcow2_co_pwritev_compressed ( BlockDriverState * bs , uint64_t offset , <nl> z_stream strm ; <nl> int ret , out_len ; <nl> uint8_t * buf , * out_buf ; <nl> - uint64_t cluster_offset ; <nl> + int64_t cluster_offset ; <nl>  <nl> if ( bytes == 0 ) { <nl> /* align end of file to a sector boundary to ease reading with <nl> sector based I / Os */ <nl> cluster_offset = bdrv_getlength ( bs -> file -> bs ); <nl> + if ( cluster_offset < 0 ) { <nl> + return cluster_offset ; <nl> + } <nl> return bdrv_truncate ( bs -> file , cluster_offset , PREALLOC_MODE_OFF , NULL ); <nl> } <nl> 
static void virgl_resource_attach_backing ( VirtIOGPU * g , <nl> return ; <nl> } <nl>  <nl> - virgl_renderer_resource_attach_iov ( att_rb . resource_id , <nl> - res_iovs , att_rb . nr_entries ); <nl> + ret = virgl_renderer_resource_attach_iov ( att_rb . resource_id , <nl> + res_iovs , att_rb . nr_entries ); <nl> + <nl> + if ( ret != 0 ) <nl> + virtio_gpu_cleanup_mapping_iov ( res_iovs , att_rb . nr_entries ); <nl> } <nl>  <nl> static void virgl_resource_detach_backing ( VirtIOGPU * g ,
static uint32_t apic_mem_readl ( void * opaque , target_phys_addr_t addr ) <nl> /* ppr */ <nl> val = apic_get_ppr ( s ); <nl> break ; <nl> + case 0x0b : <nl> + val = 0 ; <nl> + break ; <nl> case 0x0d : <nl> val = s -> log_dest << 24 ; <nl> break ;
static void sch_handle_start_func_virtual ( SubchDev * sch ) <nl> SCSW_STCTL_ALERT | SCSW_STCTL_STATUS_PEND ; <nl> s -> cpa = sch -> channel_prog + 8 ; <nl> break ; <nl> - case - EFAULT : <nl> - /* memory problem , generate channel data check */ <nl> - s -> ctrl &= ~ SCSW_ACTL_START_PEND ; <nl> - s -> cstat = SCSW_CSTAT_DATA_CHECK ; <nl> - s -> ctrl &= ~ SCSW_CTRL_MASK_STCTL ; <nl> - s -> ctrl |= SCSW_STCTL_PRIMARY | SCSW_STCTL_SECONDARY | <nl> - SCSW_STCTL_ALERT | SCSW_STCTL_STATUS_PEND ; <nl> - s -> cpa = sch -> channel_prog + 8 ; <nl> - break ; <nl> case - EBUSY : <nl> /* subchannel busy , generate deferred cc 1 */ <nl> s -> flags &= ~ SCSW_FLAGS_MASK_CC ;
static void apic_timer_update ( APICState * s , int64_t current_time ) <nl> d = ( current_time - s -> initial_count_load_time ) >> <nl> s -> count_shift ; <nl> if ( s -> lvt [ APIC_LVT_TIMER ] & APIC_LVT_TIMER_PERIODIC ) { <nl> + if (! s -> initial_count ) <nl> + goto no_timer ; <nl> d = (( d / (( uint64_t ) s -> initial_count + 1 )) + 1 ) * (( uint64_t ) s -> initial_count + 1 ); <nl> } else { <nl> if ( d >= s -> initial_count )
BlockInfoList * qmp_query_block ( Error ** errp ) <nl> BlockBackend * blk ; <nl> Error * local_err = NULL ; <nl>  <nl> - for ( blk = blk_next ( NULL ); blk ; blk = blk_next ( blk )) { <nl> - BlockInfoList * info = g_malloc0 ( sizeof (* info )); <nl> + for ( blk = blk_all_next ( NULL ); blk ; blk = blk_all_next ( blk )) { <nl> + BlockInfoList * info ; <nl> + <nl> + if (!* blk_name ( blk )) { <nl> + continue ; <nl> + } <nl> + <nl> + info = g_malloc0 ( sizeof (* info )); <nl> bdrv_query_info ( blk , & info -> value , & local_err ); <nl> if ( local_err ) { <nl> error_propagate ( errp , local_err );
PCIBus * pci_pmac_init ( qemu_irq * pic ) <nl> pci_unin_set_irq , pci_unin_map_irq , <nl> pic , 11 << 3 , 4 ); <nl>  <nl> +# if 0 <nl> pci_create_simple ( d -> host_state . bus , 11 << 3 , " Uni - north main "); <nl> +# endif <nl>  <nl> sysbus_mmio_map ( s , 0 , 0xf2800000 ); <nl> sysbus_mmio_map ( s , 1 , 0xf2c00000 ); <nl> PCIBus * pci_pmac_init ( qemu_irq * pic ) <nl> # endif <nl>  <nl> /* Uninorth AGP bus */ <nl> - pci_create_simple ( d -> host_state . bus , 13 << 3 , " Uni - north AGP "); <nl> + pci_create_simple ( d -> host_state . bus , 11 << 3 , " Uni - north AGP "); <nl>  <nl> /* Uninorth internal bus */ <nl> # if 0
tcp_sockclosed ( struct tcpcb * tp ) <nl> DEBUG_CALL (" tcp_sockclosed "); <nl> DEBUG_ARG (" tp = % p ", tp ); <nl>  <nl> + if (! tp ) { <nl> + return ; <nl> + } <nl> + <nl> switch ( tp -> t_state ) { <nl>  <nl> case TCPS_CLOSED : <nl> tcp_sockclosed ( struct tcpcb * tp ) <nl> tp -> t_state = TCPS_LAST_ACK ; <nl> break ; <nl> } <nl> - if ( tp ) <nl> - tcp_output ( tp ); <nl> + tcp_output ( tp ); <nl> } <nl>  <nl> /*
static int grow_refcount_table ( BlockDriverState * bs , int min_size ) <nl> qcow2_free_clusters ( bs , old_table_offset , old_table_size * sizeof ( uint64_t )); <nl> return 0 ; <nl> fail : <nl> - qcow2_free_clusters ( bs , table_offset , new_table_size2 ); <nl> qemu_free ( new_table ); <nl> return - EIO ; <nl> }
static int img_amend ( int argc , char ** argv ) <nl> goto out ; <nl> } <nl>  <nl> + if (! bs -> drv -> create_opts ) { <nl> + error_report (" Format driver '% s ' does not support any options to amend ", <nl> + fmt ); <nl> + ret = - 1 ; <nl> + goto out ; <nl> + } <nl> + <nl> create_opts = qemu_opts_append ( create_opts , bs -> drv -> create_opts ); <nl> opts = qemu_opts_create ( create_opts , NULL , 0 , & error_abort ); <nl> if ( options && qemu_opts_do_parse ( opts , options , NULL )) {
static void host_cpuid ( uint32_t function , uint32_t count , <nl> # else <nl> asm volatile (" pusha \ n \ t " <nl> " cpuid \ n \ t " <nl> - " mov %% eax , 0 (% 1 ) \ n \ t " <nl> - " mov %% ebx , 4 (% 1 ) \ n \ t " <nl> - " mov %% ecx , 8 (% 1 ) \ n \ t " <nl> - " mov %% edx , 12 (% 1 ) \ n \ t " <nl> + " mov %% eax , 0 (% 2 ) \ n \ t " <nl> + " mov %% ebx , 4 (% 2 ) \ n \ t " <nl> + " mov %% ecx , 8 (% 2 ) \ n \ t " <nl> + " mov %% edx , 12 (% 2 ) \ n \ t " <nl> " popa " <nl> : : " a "( function ), " c "( count ), " S "( vec ) <nl> : " memory ", " cc ");
static void dec_barrel ( DisasContext * dc ) <nl> tcg_gen_shr_tl ( cpu_R [ dc -> rd ], cpu_R [ dc -> ra ], t0 ); <nl> } <nl> } <nl> + tcg_temp_free ( t0 ); <nl> } <nl>  <nl> static void dec_bit ( DisasContext * dc )
static int ahci_dma_set_inactive ( IDEDMA * dma ) <nl>  <nl> ad -> dma_cb = NULL ; <nl>  <nl> - /* maybe we still have something to process , check later */ <nl> - ad -> check_bh = qemu_bh_new ( ahci_check_cmd_bh , ad ); <nl> - qemu_bh_schedule ( ad -> check_bh ); <nl> + if (! ad -> check_bh ) { <nl> + /* maybe we still have something to process , check later */ <nl> + ad -> check_bh = qemu_bh_new ( ahci_check_cmd_bh , ad ); <nl> + qemu_bh_schedule ( ad -> check_bh ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static void destroy_all_mappings ( AddressSpaceDispatch * d ) <nl>  <nl> static uint16_t phys_section_add ( MemoryRegionSection * section ) <nl> { <nl> + /* The physical section number is ORed with a page - aligned <nl> + * pointer to produce the iotlb entries . Thus it should <nl> + * never overflow into the page - aligned value . <nl> + */ <nl> + assert ( phys_sections_nb < TARGET_PAGE_SIZE ); <nl> + <nl> if ( phys_sections_nb == phys_sections_nb_alloc ) { <nl> phys_sections_nb_alloc = MAX ( phys_sections_nb_alloc * 2 , 16 ); <nl> phys_sections = g_renew ( MemoryRegionSection , phys_sections ,
static void sd_reset ( SDState * sd , BlockDriverState * bdrv ) <nl> } else { <nl> sect = 0 ; <nl> } <nl> - sect <<= 9 ; <nl> - <nl> - size = sect + 1 ; <nl> + size = sect << 9 ; <nl>  <nl> sect = ( size >> ( HWBLOCK_SHIFT + SECTOR_SHIFT + WPGROUP_SHIFT )) + 1 ; <nl> 
static int qxl_destroy_primary ( PCIQXLDevice * d , qxl_async_io async ) <nl> return 1 ; <nl> } <nl>  <nl> - static void qxl_set_mode ( PCIQXLDevice * d , int modenr , int loadvm ) <nl> + static void qxl_set_mode ( PCIQXLDevice * d , unsigned int modenr , int loadvm ) <nl> { <nl> pcibus_t start = d -> pci . io_regions [ QXL_RAM_RANGE_INDEX ]. addr ; <nl> pcibus_t end = d -> pci . io_regions [ QXL_RAM_RANGE_INDEX ]. size + start ; <nl> static void qxl_set_mode ( PCIQXLDevice * d , int modenr , int loadvm ) <nl> . mem_start = start , <nl> . mem_end = end <nl> }; <nl> + <nl> + if ( modenr >= d -> modes -> n_modes ) { <nl> + qxl_set_guest_bug ( d , " mode number out of range "); <nl> + return ; <nl> + } <nl> + <nl> QXLSurfaceCreate surface = { <nl> . width = mode -> x_res , <nl> . height = mode -> y_res ,
enum RSState { <nl> typedef struct GDBState { <nl> CPUArchState * c_cpu ; /* current CPU for step / continue ops */ <nl> CPUArchState * g_cpu ; /* current CPU for other ops */ <nl> - CPUArchState * query_cpu ; /* for q { f | s } ThreadInfo */ <nl> + CPUState * query_cpu ; /* for q { f | s } ThreadInfo */ <nl> enum RSState state ; /* parsing state */ <nl> char line_buf [ MAX_PACKET_LENGTH ]; <nl> int line_buf_index ; <nl> static int gdb_handle_packet ( GDBState * s , const char * line_buf ) <nl> put_packet ( s , " QC1 "); <nl> break ; <nl> } else if ( strcmp ( p ," fThreadInfo ") == 0 ) { <nl> - s -> query_cpu = first_cpu -> env_ptr ; <nl> + s -> query_cpu = first_cpu ; <nl> goto report_cpuinfo ; <nl> } else if ( strcmp ( p ," sThreadInfo ") == 0 ) { <nl> report_cpuinfo : <nl> if ( s -> query_cpu ) { <nl> - snprintf ( buf , sizeof ( buf ), " m % x ", <nl> - cpu_index ( ENV_GET_CPU ( s -> query_cpu ))); <nl> + snprintf ( buf , sizeof ( buf ), " m % x ", cpu_index ( s -> query_cpu )); <nl> put_packet ( s , buf ); <nl> - s -> query_cpu = ENV_GET_CPU ( s -> query_cpu )-> next_cpu -> env_ptr ; <nl> + s -> query_cpu = s -> query_cpu -> next_cpu ; <nl> } else <nl> put_packet ( s , " l "); <nl> break ;
static int parallels_open ( BlockDriverState * bs , QDict * options , int flags , <nl> goto fail_options ; <nl> } <nl>  <nl> - if (!( flags & BDRV_O_RESIZE ) || ! bdrv_has_zero_init ( bs -> file -> bs ) || <nl> - bdrv_truncate ( bs -> file , bdrv_getlength ( bs -> file -> bs ), <nl> - PREALLOC_MODE_OFF , NULL ) != 0 ) { <nl> + if (! bdrv_has_zero_init ( bs -> file -> bs )) { <nl> s -> prealloc_mode = PRL_PREALLOC_MODE_FALLOCATE ; <nl> } <nl> 
GEN_HANDLER ( tlbiva , 0x1F , 0x12 , 0x18 , 0x03FFF801 , PPC_TLBIVA ) <nl> GEN_EXCP_PRIVOPC ( ctx ); <nl> return ; <nl> } <nl> + t0 = tcg_temp_new (); <nl> gen_addr_reg_index ( t0 , ctx ); <nl> # if defined ( TARGET_PPC64 ) <nl> if (! ctx -> sf_mode )
static int use_stdin () { <nl> int n = read ( 0 , buf + l , sizeof ( buf )- l ); <nl> if ( n < 1 ) break ; <nl> l += n ; <nl> - if ( buf [ l ]== 0 ) { <nl> + if ( buf [ l - 1 ]== 0 ) { <nl> l --; <nl> continue ; <nl> }
R_API RCoreFile * r_core_file_open ( RCore * r , const char * file , int flags , ut64 lo <nl> if (! flags ) { <nl> flags = R_IO_READ ; <nl> } <nl> + if ( strstr ( file , " malloc ://")) { <nl> + flags = R_IO_READ | R_IO_WRITE ; <nl> + } <nl> r -> io -> bits = r -> assembler -> bits ; // TODO : we need an api for this <nl> fd = r_io_open_nomap ( r -> io , file , flags , 0644 ); <nl> if (! fd && openmany > 2 ) {
R_API int r_io_pread_at ( RIO * io , ut64 paddr , ut8 * buf , int len ) { <nl> return 0 ; <nl> } <nl> if ( io -> ff ) { <nl> - memset ( buf , 0xff , len ); <nl> + memset ( buf , io -> Oxff , len ); <nl> } <nl> return r_io_desc_read_at ( io -> desc , paddr , buf , len ); <nl> } <nl> R_API bool r_io_vread_at ( RIO * io , ut64 vaddr , ut8 * buf , int len ) { <nl> return false ; <nl> } <nl> if ( io -> ff ) { <nl> - memset ( buf , 0xff , len ); <nl> + memset ( buf , io -> Oxff , len ); <nl> } <nl> r_io_map_cleanup ( io ); <nl> if (! io -> maps ) { <nl> R_API RIOAccessLog * r_io_al_vread_at ( RIO * io , ut64 vaddr , ut8 * buf , int len ) { <nl> return NULL ; <nl> } <nl> if ( io -> ff ) { <nl> - memset ( buf , 0xff , len ); <nl> + memset ( buf , io -> Oxff , len ); <nl> } <nl> log -> buf = buf ; <nl> onIterMap ( io -> maps -> tail , io , vaddr , buf , len , R_IO_READ , al_fd_read_at_wrap , log ); <nl> R_API RIOAccessLog * r_io_al_read_at ( RIO * io , ut64 addr , ut8 * buf , int len ) { <nl> } <nl> log -> buf = buf ; <nl> if ( io -> ff ) { <nl> - memset ( buf , 0xff , len ); <nl> + memset ( buf , io -> Oxff , len ); <nl> } <nl> rlen = r_io_pread_at ( io , addr , buf , len ); <nl> if ( io -> cached & R_IO_READ ) {
static bool cps2_use ( const char * algo ) { <nl>  <nl> static int update ( RCrypto * cry , const ut8 * buf , int len ) { <nl> ut8 * output = calloc ( 1 , len ); <nl> + /* TODO : control decryption errors */ <nl> cps2_decrypt (( const ut16 *) buf , ( ut16 *) output , len , cps2key , UPPER_LIMIT ); <nl> + r_crypto_append ( cry , output , len ); <nl> + free ( output ); <nl> return true ; <nl> } <nl> 
static void rcc_context ( REgg * egg , int delta ) { <nl> REggEmit * emit = egg -> remit ; <nl> char str [ 64 ]; <nl>  <nl> - nestedi [ CTX - 1 ]++; <nl> + <nl> + if ( CTX > 31 || CTX < 0 ) <nl> + return ; <nl> + <nl> + nestedi [ CTX ]++; <nl> if ( callname && CTX > 0 ) {// && delta > 0 ) { <nl> // set_nested ( callname ); <nl> // eprintf (" - - - - - - - set nested d =% d c =% d (% s )\ n ", delta , context - 1 , callname );
static int analop ( RAnal * a , RAnalOp * op , ut64 addr , const ut8 * buf , int len ) { <nl> esilprintf ( op , "% d ,% s ,-=,% s ,% s ,=[% d ]", rs , sp , dst , sp , rs ); <nl> free ( dst ); <nl> } <nl> - op -> type = R_ANAL_OP_TYPE_PUSH ; <nl> switch ( INSOP ( 0 ). type ) { <nl> case X86_OP_IMM : <nl> op -> ptr = INSOP ( 0 ). imm ; <nl> + op -> type = R_ANAL_OP_TYPE_PUSH ; <nl> default : <nl> + op -> type = R_ANAL_OP_TYPE_UPUSH ; <nl> break ; <nl> } <nl> op -> stackop = R_ANAL_STACK_INC ;
struct r_bin_elf_reloc_t * Elf_ ( r_bin_elf_get_relocs )( struct Elf_ ( r_bin_elf_obj_t <nl> perror (" malloc ( reloc )"); <nl> return NULL ; <nl> } <nl> - if ( sym ) <nl> - for ( j = 0 ; j < nrel ; j ++) { <nl> + j = 0 ; <nl> + if ( sym ) for (; j < nrel ; j ++) { <nl> idx = ELF_R_SYM ( rel [ j ]. r_info ); <nl> if ( idx < nsym ) { <nl> if ( sym [ idx ]. st_name > bin -> strtab_section -> sh_size ) {
R_API RList * r_bin_file_get_strings ( RBinFile * a , int min , int dump , int raw ) { <nl> int i ; <nl> // XXX do not walk if bin . strings == 0 <nl> ut8 * p ; <nl> + if ( section -> size > a -> size ) { <nl> + continue ; <nl> + } <nl> for ( i = 0 ; i < section -> size ; i += cfstr_size ) { <nl> ut8 buf [ 32 ]; <nl> if (! r_buf_read_at (
eprintf ("-- % s \ n ", buf ); <nl> } <nl> case ' C ': /* comment */ <nl> if ( input [ 1 ] == '+') { <nl> - const char * text , * newcomment = input + 2 ; <nl> + const char * newcomment = input + 2 ; <nl> + char * text ; <nl> while (* newcomment ==' ') newcomment ++; <nl> char * comment = r_meta_get_string ( <nl> core -> anal , R_META_TYPE_COMMENT , addr );
static int cmd_print ( void * data , const char * input ) { <nl> if ( len > core -> blocksize ) <nl> len = core -> blocksize ; <nl>  <nl> - if ( input [ 0 ] != ' d ' && input [ 0 ] != ' m ' && input [ 0 ]!=' a ') { <nl> + if ( input [ 0 ] != ' d ' && input [ 0 ] != ' m ' && input [ 0 ]!=' a ' && input [ 0 ] != ' f ') { <nl> n = core -> blocksize_max ; <nl> i = ( int ) n ; <nl> if ( i != n ) i = 0 ;
R_API int r_anal_xrefs_set ( RAnal * anal , const RAnalRefType type , ut64 from , ut6 <nl> if (! anal || ! DB ) { <nl> return false ; <nl> } <nl> + if (! anal -> iob . is_valid_offset ( anal -> iob . io , to , 0 )) { <nl> + return false ; <nl> + } <nl> // unknown refs should not be stored . seems wrong <nl> if ( type == R_ANAL_REF_TYPE_NULL ) { <nl> return false ;
static int r_debug_native_continue_syscall ( RDebug * dbg , int pid , int num ) { <nl> # if __linux__ <nl> return ptrace ( PTRACE_SYSCALL , pid , 0 , 0 ); <nl> # elif __BSD__ <nl> - ut64 pc = r_debug_reg_get ( dbg , " pc "); <nl> + ut64 pc = r_debug_reg_get ( dbg , " PC "); <nl> return ptrace ( PTRACE_SYSCALL , pid , ( void *)( size_t ) pc , 0 ); <nl> # else <nl> eprintf (" TODO : continue syscall not implemented yet \ n "); <nl> static int r_debug_native_continue ( RDebug * dbg , int pid , int tid , int sig ) { <nl> return tid ; <nl> # elif __BSD__ <nl> void * data = ( void *)( size_t )(( sig != - 1 ) ? sig : dbg -> reason . signum ); <nl> - ut64 pc = r_debug_reg_get ( dbg , " pc "); <nl> + ut64 pc = r_debug_reg_get ( dbg , " PC "); <nl> return ptrace ( PTRACE_CONT , pid , ( void *)( size_t ) pc , ( int )( size_t ) data ) == 0 ; <nl> # elif __CYGWIN__ <nl> # warning " r_debug_native_continue not supported on this platform "
R_API RBinJavaAttrInfo * r_bin_java_stack_map_table_attr_new ( ut8 * buffer , ut64 sz <nl> ut64 offset = 0 ; <nl> RBinJavaStackMapFrame * stack_frame = NULL , * new_stack_frame = NULL ; <nl> RBinJavaAttrInfo * attr = r_bin_java_default_attr_new ( buffer , sz , buf_offset ); <nl> + if ( sz < 10 ) { <nl> + return NULL ; <nl> + } <nl> offset += 6 ; <nl> IFDBG eprintf (" r_bin_java_stack_map_table_attr_new : New stack map allocated .\ n "); <nl> if (! attr ) {
R_API int r_core_visual_graph ( RCore * core , RAGraph * g , RAnalFunction * _fcn , int <nl> // handle end key <nl> const RGraphNode * gn = find_near_of ( g , NULL , false ); <nl> g -> update_seek_on = get_anode ( gn ); <nl> + } else { <nl> + r_core_visual_trackflags ( core ); <nl> } <nl> break ; <nl> case ' H ': <nl> R_API int r_core_visual_graph ( RCore * core , RAGraph * g , RAnalFunction * _fcn , int <nl> case ' w ': <nl> agraph_toggle_speed ( g , core ); <nl> break ; <nl> + case ' _ ': <nl> + r_core_visual_hudstuff ( core ); <nl> + break ; <nl> case - 1 : // EOF <nl> case ' ': <nl> case ' q ':
static void ds_build_op_str ( RDisasmState * ds ) { <nl> core -> parser -> relsub_addr = num ; <nl> } <nl> } <nl> - r_parse_filter ( core -> parser , core -> flags , ds -> opstr , ds -> str , sizeof ( ds -> str ), core -> print -> big_endian ); <nl> + r_parse_filter ( core -> parser , core -> flags , asm_str , ds -> str , sizeof ( ds -> str ), core -> print -> big_endian ); <nl> core -> parser -> flagspace = ofs ; <nl> free ( ds -> opstr ); <nl> ds -> opstr = strdup ( ds -> str );
static void store_versioninfo_gnu_versym ( struct Elf_ ( r_bin_elf_obj_t ) * bin , Elf_ <nl> const char * link_section_name = ""; <nl> int num_entries = shdr -> sh_size / sizeof ( Elf_ ( Versym )); <nl> ut8 * data = calloc ( num_entries , sizeof ( short )); <nl> - if ( shdr -> sh_link > bin -> ehdr . e_shnum ) <nl> + if ( shdr -> sh_link > bin -> ehdr . e_shnum ) { <nl> + free ( data ); <nl> return ; <nl> + } <nl> link_shdr = & bin -> shdr [ shdr -> sh_link ]; <nl>  <nl> if ( bin -> shstrtab && shdr -> sh_name < bin -> shstrtab_size ) {
R_API RAnalData * r_anal_data_new ( ut64 addr , int type , ut64 n , const ut8 * buf , i <nl> int l = R_MIN ( len , 8 ); <nl> ad -> buf = ( ut8 *) &( ad -> sbuf ); <nl> memset ( ad -> buf , 0 , 8 ); <nl> - if ( l < 1 ) <nl> - free ( ad ); <nl> + if ( l < 1 ) { <nl> + r_anal_data_free ( ad ); <nl> return NULL ; <nl> + } <nl> if ( buf ) { <nl> memcpy ( ad -> buf , buf , l ); <nl> } <nl> R_API RAnalData * r_anal_data_new ( ut64 addr , int type , ut64 n , const ut8 * buf , i <nl> } <nl>  <nl> R_API void r_anal_data_free ( RAnalData * d ) { <nl> - if ( d ){ <nl> + if ( d ) { <nl> if ( d -> buf != ( ut8 *)&( d -> sbuf )) free ( d -> buf ); <nl> - free ( d -> str ); <nl> + if ( d -> str != NULL ) free ( d -> str ); <nl> free ( d ); <nl> } <nl> }
SETL / SETNGE <nl> esilprintf ( op , "% s ,% s =,$ o , of ,=,$ s , sf ,=,$ z , zf ,=,$ p , pf ,=", src , dst ); <nl> } <nl> free ( src ); <nl> + free ( dst ); <nl> } <nl> break ; <nl> case X86_INS_DEC :
static void * load_bytes ( RBinFile * arch , const ut8 * buf , ut64 sz , ut64 loaddr , Sd <nl> return NULL ; <nl> } <nl> RBuffer * tbuf = r_buf_new (); <nl> + if (! tbuf ) { <nl> + return NULL ; <nl> + } <nl> r_buf_set_bytes ( tbuf , buf , sz ); <nl> struct r_bin_bflt_obj * res = r_bin_bflt_new_buf ( tbuf ); <nl> r_buf_free ( tbuf ); <nl> static void __patch_reloc ( RBuffer * buf , ut32 addr_to_patch , ut32 data_offset ) { <nl> r_buf_write_at ( buf , addr_to_patch , ( void *) val , sizeof ( val )); <nl> } <nl>  <nl> - static int search_old_relocation ( struct reloc_struct_t * reloc_table , ut32 addr_to_patch , int n_reloc ) { <nl> + static int search_old_relocation ( struct reloc_struct_t * reloc_table , <nl> + ut32 addr_to_patch , int n_reloc ) { <nl> int i ; <nl> for ( i = 0 ; i < n_reloc ; i ++) { <nl> if ( addr_to_patch == reloc_table [ i ]. data_offset ) { <nl> static RList * relocs ( RBinFile * arch ) { <nl> if ( amount < n_got || amount > UT32_MAX ) { <nl> goto out_error ; <nl> } <nl> - struct reloc_struct_t * got_table = calloc ( 1 , n_got * sizeof ( ut32 )); <nl> + struct reloc_struct_t * got_table = calloc ( <nl> + 1 , n_got * sizeof ( struct reloc_struct_t )); <nl> if ( got_table ) { <nl> ut32 offset = 0 ; <nl> for ( i = 0 ; i < n_got ; offset += 4 , i ++) {
OPCODE_DESC opcodes [] = { <nl>  <nl> static OPCODE_DESC * avr_op_analyze ( RAnal * anal , RAnalOp * op , ut64 addr , const ut8 * buf , int len , CPU_MODEL * cpu ) { <nl> OPCODE_DESC * opcode_desc ; <nl> + if ( len < 2 ) { <nl> + return NULL ; <nl> + } <nl> ut16 ins = ( buf [ 1 ] << 8 ) | buf [ 0 ]; <nl> int fail ; <nl> char * t ;
static int handleMidFlags ( RCore * core , RDisasmState * ds , bool print ) { <nl> for ( i = 1 ; i < ds -> oplen ; i ++) { <nl> fi = r_flag_get_i ( core -> flags , ds -> at + i ); <nl> if ( fi ) { <nl> - if ( ds -> midflags == 2 && ( fi -> name [ 0 ] == '$' || fi -> realname [ 0 ] == '$')) { <nl> + if ( ds -> midflags == 2 && (( fi -> name && fi -> name [ 0 ] == '$') || ( fi -> realname && fi -> realname [ 0 ] == '$'))) { <nl> i = 0 ; <nl> } else if (! strncmp ( fi -> name , " hit .", 4 )) { // use search . prefix ? <nl> i = 0 ;
int x86_udis86_op ( RAnal * anal , RAnalOp * op , ut64 addr , const ut8 * data , int len ) <nl> case UD_Isyscall : <nl> op -> type = R_ANAL_OP_TYPE_SWI ; <nl> break ; <nl> + case UD_Inop : <nl> + op -> type = R_ANAL_OP_TYPE_NOP ; <nl> + break ; <nl> default : <nl> break ; <nl> }
read_tmo ( int fd , char * ptr , unsigned len , double tmo ) <nl> pfd . events = POLLIN ; <nl> for ( j = 0 ; len > 0 ; ) { <nl> i = poll (& pfd , 1 , to ); <nl> + if ( i < 0 ) { <nl> + errno = EINTR ; <nl> + return (- 1 ); <nl> + } <nl> if ( i == 0 ) { <nl> errno = ETIMEDOUT ; <nl> return (- 1 );
static char * addenv ( struct cgi_env_block * block , const char * fmt , ...) <nl> return NULL ; <nl>  <nl> /* Calculate how much space is left in the buffer */ <nl> - space = sizeof ( block -> buf ) - block -> len - 2 ; <nl> + space = ( int )( sizeof ( block -> buf ) - block -> len ) - 2 ; <nl> /* assert ( space >= 0 ); */ <nl> if ( space < 0 ) <nl> return NULL ;
static char * uwsgi_scheme_section ( char * url , size_t * size , int add_zero ) { <nl> } <nl>  <nl> struct uwsgi_string_list * uwsgi_register_scheme ( char * name , char * (* func )( char *, size_t *, int )) { <nl> - struct uwsgi_string_list * usl = uwsgi_string_new_list (& uwsgi . schemes , name ); <nl> + struct uwsgi_string_list * usl = NULL ; <nl> + uwsgi_foreach ( usl , uwsgi . schemes ) { <nl> + if (! strcmp ( usl -> value , name )) goto found ; <nl> + } <nl> + <nl> + usl = uwsgi_string_new_list (& uwsgi . schemes , name ); <nl> + <nl> + found : <nl> usl -> custom_ptr = func ; <nl> return usl ; <nl> }
static void uwsgi_alarm_thread_loop ( struct uwsgi_thread * ut ) { <nl> long ptr = 0 ; <nl> memcpy (& ptr , buf , sizeof ( long )); <nl> struct uwsgi_alarm_instance * uai = ( struct uwsgi_alarm_instance *) ptr ; <nl> - if (! uai ) return ; <nl> + if (! uai ) <nl> + break ; <nl> uwsgi_alarm_run ( uai , msg , msg_size ); <nl> } <nl> } <nl> } <nl> + free ( buf ); <nl> } <nl>  <nl> // initialize alarms , instances and log regexps
int uwsgi_mount ( char * fs , char * what , char * where , char * flags , char * data ) { <nl> char * mflags = uwsgi_str ( flags ); <nl> char * p , * ctx = NULL ; <nl> uwsgi_foreach_token ( mflags , ",", p , ctx ) { <nl> + if ( strcmp ( p , " defaults ") == 0 ) <nl> + continue ; <nl> unsigned long flag = ( unsigned long ) uwsgi_mount_flag ( p ); <nl> if (! flag ) { <nl> uwsgi_log (" unknown mount flag \"% s \"\ n ", p ); <nl> int uwsgi_mount ( char * fs , char * what , char * where , char * flags , char * data ) { <nl> } <nl> mountflags |= flag ; <nl> } <nl> + if (!* fs ) fs = NULL ; <nl> free ( mflags ); <nl> parsed : <nl> # ifdef __linux__
static struct uwsgi_option rawrouter_options [] = { <nl> {" rawrouter - ss ", required_argument , 0 , " run the rawrouter stats server ", uwsgi_opt_set_str , & urr . cr . stats_server , 0 }, <nl> {" rawrouter - harakiri ", required_argument , 0 , " enable rawrouter harakiri ", uwsgi_opt_set_int , & urr . cr . harakiri , 0 }, <nl>  <nl> - {" rawrouter - xclient ", no_argument , 0 , " use the xclient protocol to pass the client addres ", uwsgi_opt_true , & urr . xclient , 0 }, <nl> + {" rawrouter - xclient ", no_argument , 0 , " use the xclient protocol to pass the client address ", uwsgi_opt_true , & urr . xclient , 0 }, <nl>  <nl> {" rawrouter - buffer - size ", required_argument , 0 , " set internal buffer size ( default : page size )", uwsgi_opt_set_64bit , & urr . cr . buffer_size , 0 }, <nl> 
static v8 :: Handle < v8 :: Value > uwsgi_v8_commonjs_require ( const v8 :: Arguments & <nl> free ( tmp_filename ); <nl> return ret ; <nl> } <nl> + free ( tmp_filename ); <nl> } <nl> - free ( tmp_filename ); <nl> usl = usl -> next ; <nl> } <nl> }
error : <nl> if ( urcc -> key ) free ( urcc -> key ); <nl> if ( urcc -> name ) free ( urcc -> name ); <nl> if ( urcc -> expires_str ) free ( urcc -> expires_str ); <nl> + free ( urcc ); <nl> return - 1 ; <nl> } <nl> 
# include " controller_base . hpp " <nl> # include " dialogs . hpp " <nl> # include " mouse_handler_base . hpp " <nl> +# include " foreach . hpp " <nl>  <nl> controller_base :: controller_base ( <nl> int ticks , const config & game_config , CVideo & /* video */) : <nl> bool controller_base :: handle_scroll ( CKey & key , int mousex , int mousey , int mouse <nl> bool scrolling = false ; <nl> bool mouse_in_window = ( SDL_GetAppState () & SDL_APPMOUSEFOCUS ) <nl> || utils :: string_bool ( preferences :: get (" scroll_when_mouse_outside "), true ); <nl> - const int scroll_threshold = ( preferences :: mouse_scroll_enabled ()) <nl> + int scroll_threshold = ( preferences :: mouse_scroll_enabled ()) <nl> ? preferences :: mouse_scroll_threshold () <nl> : 0 ; <nl> - <nl> + foreach ( const theme :: menu & m , get_display (). get_theme (). menus ()) { <nl> + if ( point_in_rect ( mousex , mousey , m . get_location ())) { <nl> + scroll_threshold = 0 ; <nl> + } <nl> + } <nl> if (( key [ SDLK_UP ] && have_keyboard_focus ()) <nl> || ( mousey < scroll_threshold && mouse_in_window )) { <nl> get_display (). scroll ( 0 ,- preferences :: scroll_speed ());
config connect :: side :: get_config () const <nl> } <nl> { <nl> res [" id "] = res [" save_id "]; <nl> - const config & ai_cfg = ai :: configuration :: get_ai_config_for ( ai_algorithm_ ); <nl> - res . add_child (" ai ", ai_cfg ); <nl> utils :: string_map symbols ; <nl> - symbols [" playername "] = ai_cfg [" description "]; <nl> + if ( allow_player_ ) { <nl> + const config & ai_cfg = ai :: configuration :: get_ai_config_for ( ai_algorithm_ ); <nl> + res . add_child (" ai ", ai_cfg ); <nl> + symbols [" playername "] = ai_cfg [" description "]; <nl> + } else { // do not import default ai cfg here - all is set by scenario config <nl> + symbols [" playername "] = _ (" Computer Player "); <nl> + } <nl> symbols [" side "] = res [" side "]. str (); <nl> description = vgettext ("$ playername $ side ", symbols ); <nl> }
void draw ( surface screen ) <nl> if ( use_colour_cursors () == false ) { <nl> return ; <nl> } <nl> - <nl> + <nl> + if ( current_cursor == NUM_CURSORS ) { <nl> + return ; <nl> + } <nl> + <nl> if (! colour_ready ) { <nl> // display start to draw cursor <nl> // so it can now display colour cursor <nl> void draw ( surface screen ) <nl> set ( current_cursor ); <nl> } <nl>  <nl> - if ( current_cursor == NUM_CURSORS ) { <nl> - return ; <nl> - } <nl> - <nl> if ( have_focus == false ) { <nl> cursor_buf = NULL ; <nl> return ;
bool gamemap :: filter_location ( const gamemap :: location & loc , const config & /* con * <nl> void gamemap :: write_terrain ( const gamemap :: location & loc , config & cfg ) const <nl> { <nl> // will need to be updated for multi - letter terrain -- Sapient <nl> - char * loc_terrain = " "; <nl> + char loc_terrain [] = " "; <nl> * loc_terrain = get_terrain ( loc ); <nl> cfg [" terrain "] = loc_terrain ; <nl> }
LEVEL_RESULT play_level ( game_data & gameinfo , const config & game_config , <nl> if ( first_human_team != - 1 ) { <nl> clear_shroud ( gui , status , map , gameinfo , units , teams , first_human_team ); <nl> LOG_NG << " b " << ( SDL_GetTicks () - ticks ) << "\ n "; <nl> - gui . scroll_to_tile ( map . starting_position ( first_human_team + 1 ). x , map . starting_position ( first_human_team + 1 ). y , display :: WARP ); <nl> + gui . scroll_to_tile ( map . starting_position ( first_human_team + 1 ). x , <nl> + map . starting_position ( first_human_team + 1 ). y , display :: WARP ); <nl> LOG_NG << " c " << ( SDL_GetTicks () - ticks ) << "\ n "; <nl> } <nl>  <nl> LEVEL_RESULT play_level ( game_data & gameinfo , const config & game_config , <nl> events :: raise_draw_event (); <nl> if (! loading_game ) { <nl> game_events :: fire (" start "); <nl> + game_events :: set_variable (" turn_number ", " 1 "); <nl> } <nl>  <nl> gui . draw ();
LEVEL_RESULT play_game ( game_display & disp , saved_game & gamestate , <nl> if ( is_unit_test ) { <nl> return res ; <nl> } <nl> + // in this case we might have skipped state . set_snapshot which means wew cannot do gamestate . convert_to_start_save (); <nl> + if ( res == QUIT ) <nl> + { <nl> + return res ; <nl> + } <nl>  <nl> // Save - management options fire on game end . <nl> // This means : ( a ) we have a victory , or <nl> LEVEL_RESULT play_game ( game_display & disp , saved_game & gamestate , <nl> // On DEFEAT , QUIT , or OBSERVER_END , we ' re done now <nl>  <nl> // If there is no next scenario we ' re done now . <nl> - if ( res == QUIT || ! end_level . proceed_to_next_level || gamestate . carryover_sides_start [" next_scenario "]. empty ()) <nl> + if (! end_level . proceed_to_next_level || gamestate . carryover_sides_start [" next_scenario "]. empty ()) <nl> { <nl> return res ; <nl> }
bool loadgame :: load_multiplayer_game () <nl> return false ; <nl> } <nl>  <nl> + if ( is_replay_save ( summary_ )) { <nl> + gui2 :: show_transient_message ( video_ , _ (" Load Game "), _ (" Replays are not supported in multiplayer mode .")); <nl> + return false ; <nl> + } <nl> + <nl> if ( gamestate_ . classification (). campaign_type != game_classification :: CAMPAIGN_TYPE :: MULTIPLAYER ) { <nl> gui2 :: show_transient_error_message ( video_ , _ (" This is not a multiplayer save .")); <nl> return false ;
void play_multiplayer ( display & disp , game_data & units_data , config cfg , <nl> } else if ( result < int ( choices . size ()/ 3 )* 2 ) { <nl> controller = " ai "; <nl> result -= choices . size ()/ 3 ; <nl> + sides [ res ]-> values [" description "] = ""; <nl> } else { <nl> controller = " network "; <nl> result -= ( choices . size ()/ 3 )* 2 ;
void encounter_start_units ( unit_map & units ){ <nl> void encounter_recallable_units ( std :: vector < team >& teams ){ <nl> for ( std :: vector < team >:: iterator help_team_it = teams . begin (); <nl> help_team_it != teams . end (); ++ help_team_it ) { <nl> - for ( std :: vector < unit >:: iterator help_recall_it = help_team_it -> recall_list (). begin (); help_recall_it != help_team_it -> recall_list (). end (); help_recall_it ++) { <nl> + for ( std :: vector < unit >:: iterator help_recall_it = help_team_it -> recall_list (). begin (); help_recall_it != help_team_it -> recall_list (). end (); ++ help_recall_it ) { <nl> encountered_units_set . insert ( help_recall_it -> type_id ()); <nl> } <nl> } <nl> } <nl>  <nl> void encounter_map_terrain ( gamemap & map ){ <nl> - for ( int map_x = 0 ; map_x < map . w (); map_x ++) { <nl> - for ( int map_y = 0 ; map_y < map . h (); map_y ++) { <nl> + for ( int map_x = 0 ; map_x < map . w (); ++ map_x ) { <nl> + for ( int map_y = 0 ; map_y < map . h (); ++ map_y ) { <nl> const t_translation :: t_terrain t = map . get_terrain ( map_location ( map_x , map_y )); <nl> preferences :: encountered_terrains (). insert ( t ); <nl> const t_translation :: t_list & underlaying_list = map . underlying_union_terrain ( map_location ( map_x , map_y )); <nl> - for ( std :: vector < t_translation :: t_terrain >:: const_iterator ut = underlaying_list . begin (); ut != underlaying_list . end (); ut ++) { <nl> + for ( std :: vector < t_translation :: t_terrain >:: const_iterator ut = underlaying_list . begin (); ut != underlaying_list . end (); ++ ut ) { <nl> preferences :: encountered_terrains (). insert (* ut ); <nl> }; <nl> }
void positional_source :: write_config ( config & cfg ) const <nl> cfg [" delay "] = str_cast < unsigned int >( this -> min_delay_ ); <nl> cfg [" chance "] = str_cast < unsigned int >( this -> chance_ ); <nl> cfg [" check_fogged "] = this -> check_fogged_ ? " yes " : " no "; <nl> - cfg [" check_shrouded "] = this -> check_fogged_ ? " yes " : " no "; <nl> + cfg [" check_shrouded "] = this -> check_shrouded_ ? " yes " : " no "; <nl>  <nl> cfg [" x "] = cfg [" y "] = ""; <nl> bool first_loc = true ;
public : <nl> static void set_sunset ( const unsigned interval ) <nl> { sunset_ = interval ? interval : 5 ; } <nl>  <nl> + bool get_need_layout () const { return need_layout_ ; } <nl> + <nl> private : <nl>  <nl> /** Needed so we can change what ' s drawn on the screen . */
std :: vector < std :: string > get_text () { <nl> " _ " N_ ("+ Catalan Translation "), <nl> "- Carles Company ( brrr )", <nl> "- Dan Rosàs Garcia ( focks )", <nl> + "- Jonatan Alamà ( tin )", <nl> "- Jordà Polo ( ettin )", <nl> "- Mark Recasens ", <nl> "- Pau Rul · lan Ferragut ",
LEVEL_RESULT play_game ( display & disp , game_state & gamestate , const config & game_ <nl> gamestate . set_variables (* gamestate . snapshot . child (" variables ")); <nl> } <nl> // Replace game label with that from snapshot <nl> - if (! state . snapshot [" label "]. empty ()){ <nl> - state . label = state . snapshot [" label "]; <nl> + if (! gamestate . snapshot [" label "]. empty ()){ <nl> + gamestate . label = gamestate . snapshot [" label "]; <nl> } <nl> // get the current gold values of players so they don ' t start with the amount <nl> // they had at the start of the scenario
void saved_game :: expand_random_scenario () <nl> LOG_NG << " randomly generating scenario ...\ n "; <nl> const cursor :: setter cursor_setter ( cursor :: WAIT ); <nl>  <nl> - starting_pos_ = random_generate_scenario ( starting_pos_ [" scenario_generation "], <nl> + config scenario_new = random_generate_scenario ( starting_pos_ [" scenario_generation "], <nl> starting_pos_ . child (" generator ")); <nl> + // Preserve " story " form the scenario toplevel . <nl> + BOOST_FOREACH ( config & story , starting_pos_ . child_range (" story ")) <nl> + { <nl> + scenario_new . add_child (" story ", story ); <nl> + } <nl> + starting_pos_ = scenario_new ; <nl> } <nl> // it looks like we support a map = where map = filename equals more or less map_data ={ filename } <nl> if ( starting_pos_ [" map_data "]. empty () && starting_pos_ [" map "] != "") {
std :: string get_colour_string ( int id ) <nl> } <nl> } <nl>  <nl> - chat :: chat () <nl> + chat :: chat () : <nl> + message_history_ (), <nl> + last_update_ () <nl> { <nl> } <nl>  <nl> ui :: ui ( game_display & disp , const std :: string & title , const config & cfg , chat & c , <nl> chat_textbox_ ( disp . video (), 100 , "", false ), <nl> users_menu_ ( disp . video (), std :: vector < std :: string >(), false , - 1 , - 1 , NULL , & umenu_style ), <nl>  <nl> + user_list_ (), <nl> selected_game_ (""), <nl>  <nl> result_ ( CONTINUE ),
typedef enum { <nl>  <nl> typedef struct { <nl> TIFF * tif ; <nl> + int decoder_ok ; <nl> # ifndef LIBJPEG_ENCAP_EXTERNAL <nl> JMP_BUF exit_jmpbuf ; <nl> # endif <nl> OJPEGPreDecode ( TIFF * tif , uint16 s ) <nl> } <nl> sp -> write_curstrile ++; <nl> } <nl> + sp -> decoder_ok = 1 ; <nl> return ( 1 ); <nl> } <nl>  <nl> OJPEGPreDecodeSkipScanlines ( TIFF * tif ) <nl> static int <nl> OJPEGDecode ( TIFF * tif , uint8 * buf , tmsize_t cc , uint16 s ) <nl> { <nl> + static const char module []=" OJPEGDecode "; <nl> OJPEGState * sp =( OJPEGState *) tif -> tif_data ; <nl> ( void ) s ; <nl> + if ( ! sp -> decoder_ok ) <nl> + { <nl> + TIFFErrorExt ( tif -> tif_clientdata , module ," Cannot decode : decoder not correctly initialized "); <nl> + return 0 ; <nl> + } <nl> if ( sp -> libjpeg_jpeg_query_style == 0 ) <nl> { <nl> if ( OJPEGDecodeRaw ( tif , buf , cc )== 0 )
int lxc_get_config_item ( struct lxc_conf * c , const char * key , char * retv , <nl> return lxc_get_conf_int ( c , retv , inlen , c -> stopsignal ); <nl> else if ( strcmp ( key , " lxc . autodev ") == 0 ) <nl> return lxc_get_conf_int ( c , retv , inlen , c -> autodev ); <nl> + else if ( strcmp ( key , " lxc . kmsg ") == 0 ) <nl> + return lxc_get_conf_int ( c , retv , inlen , c -> kmsg ); <nl> else return - 1 ; <nl>  <nl> if (! v )
static int lxc_mount_auto_mounts ( struct lxc_conf * conf , int flags , struct cgroup <nl> } <nl> r = mount ( source , destination , default_mounts [ i ]. fstype , default_mounts [ i ]. flags , default_mounts [ i ]. options ); <nl> saved_errno = errno ; <nl> + if ( r < 0 ) <nl> + SYSERROR (" error mounting % s on % s ", source , destination ); <nl> free ( source ); <nl> free ( destination ); <nl> if ( r < 0 ) { <nl> - SYSERROR (" error mounting % s ", default_mounts [ i ]. destination ); <nl> errno = saved_errno ; <nl> return - 1 ; <nl> }
static void print_usage ( const struct option longopts []) <nl> exit ( 0 ); <nl> } <nl>  <nl> - static void print_version () <nl> + static void print_version ( void ) <nl> { <nl> printf ("% s \ n ", LXC_VERSION ); <nl> exit ( 0 ); <nl> } <nl>  <nl> - static void print_help () <nl> + static void print_help ( void ) <nl> { <nl> fprintf ( stderr , "\ <nl> Usage : lxc - init -- name = NAME -- COMMAND \ n \
static int receive_answer ( int sock , struct lxc_answer * answer ) <nl> ERROR (" failed to receive answer for the command "); <nl> if ( answer -> pathlen == 0 ) <nl> return ret ; <nl> + if ( answer -> pathlen >= MAXPATHLEN ) { <nl> + ERROR (" cgroup path was too long "); <nl> + return - 1 ; <nl> + } <nl> ret = recv ( sock , answerpath , answer -> pathlen , 0 ); <nl> if ( ret != answer -> pathlen ) { <nl> ERROR (" failed to receive answer for the command ");
static int mount_entry_create_dir_file ( const struct mntent * mntent , <nl> const char * path ) <nl> { <nl> char * pathdirname = NULL ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> FILE * pathfile = NULL ; <nl>  <nl> if ( hasmntopt ( mntent , " create = dir ")) { <nl> static inline int mount_entry_on_generic ( struct mntent * mntent , <nl>  <nl> ret = mount_entry_create_dir_file ( mntent , path ); <nl>  <nl> + if ( ret < 0 ) <nl> + return optional ? 0 : - 1 ; <nl> + <nl> + if ( ret < 0 && ! optional ) <nl> + return - 1 ; <nl> + <nl> cull_mntent_opt ( mntent ); <nl>  <nl> if ( parse_mntopts ( mntent -> mnt_opts , & mntflags , & mntdata ) < 0 ) {
static int get_mtu ( char * name ) <nl> int idx ; <nl>  <nl> idx = if_nametoindex ( name ); <nl> + if ( idx < 0 ) <nl> + return - 1 ; <nl> return netdev_get_mtu ( idx ); <nl> } <nl>  <nl> int main ( int argc , char * argv []) <nl> exit ( EXIT_FAILURE ); <nl> } <nl> host_veth_ifidx = if_nametoindex ( nicname ); <nl> + if (! host_veth_ifidx ) { <nl> + free ( newname ); <nl> + free ( nicname ); <nl> + usernic_error (" Failed to get netdev index : % s \ n ", strerror ( errno )); <nl> + exit ( EXIT_FAILURE ); <nl> + } <nl>  <nl> /* Write names of veth pairs and their ifindeces to stout : <nl> * ( e . g . eth0 : 731 : veth9MT2L4 : 730 )
static int keyboard_feed_evdev ( idev_keyboard * k , idev_data * data ) { <nl> /* TODO : update LEDs */ <nl> } <nl>  <nl> - if ( num < 0 ) <nl> + if ( num < 0 ) { <nl> + r = num ; <nl> goto error ; <nl> + } <nl>  <nl> r = keyboard_fill ( k , & k -> evdata , data -> resync , ev -> code , ev -> value , num , keysyms ); <nl> if ( r < 0 )
static void automount_init ( Unit * u ) { <nl> UNIT ( a )-> ignore_on_isolate = true ; <nl> } <nl>  <nl> - static void repeat_unmout ( const char * path ) { <nl> + static void repeat_unmount ( const char * path ) { <nl> assert ( path ); <nl>  <nl> for (;;) { <nl> static void unmount_autofs ( Automount * a ) { <nl> if ( a -> where && <nl> ( UNIT ( a )-> manager -> exit_code != MANAGER_RELOAD && <nl> UNIT ( a )-> manager -> exit_code != MANAGER_REEXECUTE )) <nl> - repeat_unmout ( a -> where ); <nl> + repeat_unmount ( a -> where ); <nl> } <nl>  <nl> static void automount_done ( Unit * u ) { <nl> fail : <nl> close_nointr_nofail ( ioctl_fd ); <nl>  <nl> if ( mounted ) <nl> - repeat_unmout ( a -> where ); <nl> + repeat_unmount ( a -> where ); <nl>  <nl> log_error_unit ( UNIT ( a )-> id , <nl> " Failed to initialize automounter : % s ", strerror (- r ));
static int transaction_add_job_and_dependencies ( <nl> assert ( type < _JOB_TYPE_MAX ); <nl> assert ( unit ); <nl>  <nl> - if ( unit -> meta . load_state != UNIT_LOADED ) { <nl> + if ( type != JOB_STOP && <nl> + unit -> meta . load_state != UNIT_LOADED ) { <nl> dbus_set_error ( e , BUS_ERROR_LOAD_FAILED , " Unit % s failed to load . See logs for details .", unit -> meta . id ); <nl> return - EINVAL ; <nl> }
static int unit_realize_cgroup_now ( Unit * u ) { <nl> return 0 ; <nl>  <nl> /* First , realize parents */ <nl> - if ( UNIT_ISSET ( u -> slice )) <nl> - unit_realize_cgroup_now ( UNIT_DEREF ( u -> slice )); <nl> + if ( UNIT_ISSET ( u -> slice )) { <nl> + int r ; <nl> + <nl> + r = unit_realize_cgroup_now ( UNIT_DEREF ( u -> slice )); <nl> + if ( r < 0 ) <nl> + return r ; <nl> + } <nl>  <nl> /* And then do the real work */ <nl> return unit_create_cgroups ( u , mask );
int cunescape_length_with_prefix ( const char * s , size_t length , const char * prefi <nl> continue ; <nl> } <nl>  <nl> + free ( r ); <nl> return - EINVAL ; <nl> } <nl>  <nl> int cunescape_length_with_prefix ( const char * s , size_t length , const char * prefi <nl> continue ; <nl> } <nl>  <nl> + free ( r ); <nl> return k ; <nl> } <nl> 
int manager_environment_add ( Manager * m , char ** minus , char ** plus ) { <nl>  <nl> if (! strv_isempty ( plus )) { <nl> b = strv_env_merge ( 2 , l , plus ); <nl> - if (! b ) <nl> + if (! b ) { <nl> + strv_free ( a ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> l = b ; <nl> }
static int netdev_load_one ( Manager * manager , const char * filename ) { <nl> LIST_HEAD_INIT ( netdev -> callbacks ); <nl>  <nl> if ( netdev -> kind == NETDEV_KIND_VETH ) { <nl> - if ( netdev -> ifname_peer ) { <nl> - log_warning (" Veth NetDev without Peer Name configured " <nl> + if (! netdev -> ifname_peer ) { <nl> + log_warning (" Veth NetDev without peer name configured " <nl> " in % s . Ignoring ", filename ); <nl> return 0 ; <nl> }
static int acl_entry_equal ( acl_entry_t a , acl_entry_t b ) { <nl> /* can have only one of those */ <nl> return true ; <nl> case ACL_USER : { <nl> - _cleanup_ ( acl_free_uid_tpp ) uid_t * uid_a , * uid_b ; <nl> + _cleanup_ ( acl_free_uid_tpp ) uid_t * uid_a = NULL , * uid_b = NULL ; <nl>  <nl> uid_a = acl_get_qualifier ( a ); <nl> if (! uid_a ) <nl> static int acl_entry_equal ( acl_entry_t a , acl_entry_t b ) { <nl> return * uid_a == * uid_b ; <nl> } <nl> case ACL_GROUP : { <nl> - _cleanup_ ( acl_free_gid_tpp ) gid_t * gid_a , * gid_b ; <nl> + _cleanup_ ( acl_free_gid_tpp ) gid_t * gid_a = NULL , * gid_b = NULL ; <nl>  <nl> gid_a = acl_get_qualifier ( a ); <nl> if (! gid_a )
static int get_file_to_edit ( <nl> return log_oom (); <nl>  <nl> if ( arg_runtime ) { <nl> - run = strjoin ( paths -> runtime_config , name , NULL ); <nl> + run = strjoin ( paths -> runtime_config , "/", name , NULL ); <nl> if (! run ) <nl> return log_oom (); <nl> }
char * <nl> utf8_prev_char ( const char * p ) <nl> { <nl> - while ( 1 ) <nl> + for (;;) <nl> { <nl> p --; <nl> if ((* p & 0xc0 ) != 0x80 )
static int client_receive_message_udp ( <nl> if ( buflen < 0 ) <nl> return buflen ; <nl>  <nl> + if ( buflen == 0 ) <nl> + buflen = 1 ; <nl> + <nl> message = malloc0 ( buflen ); <nl> if (! message ) <nl> return - ENOMEM ;
int fopen_temporary ( const char * path , FILE ** _f , char ** _temp_path ) { <nl>  <nl> f = fdopen ( fd , " we "); <nl> if (! f ) { <nl> - unlink ( t ); <nl> + unlink_noerrno ( t ); <nl> free ( t ); <nl> safe_close ( fd ); <nl> return - errno ;
int link_save ( Link * link ) { <nl> assert ( link -> state_file ); <nl>  <nl> state = link_state_to_string ( link -> state ); <nl> - if (! state ) <nl> - goto finish ; <nl> + assert ( state ); <nl>  <nl> r = fopen_temporary ( link -> state_file , & f , & temp_path ); <nl> if ( r < 0 )
static int start_unit_one ( <nl>  <nl> if (! sd_bus_error_has_name ( error , BUS_ERROR_NO_SUCH_UNIT ) && <nl> ! sd_bus_error_has_name ( error , BUS_ERROR_UNIT_MASKED )) <nl> - log_error (" See % s logs and ' systemctl % s status % s ' for details .", <nl> + log_error (" See % s logs and ' systemctl % s status % s % s ' for details .", <nl> arg_scope == UNIT_FILE_SYSTEM ? " system " : " user ", <nl> arg_scope == UNIT_FILE_SYSTEM ? "" : " -- user ", <nl> + name [ 0 ] == '-' ? " --" : "", <nl> name ); <nl>  <nl> return r ;
static int output_cat ( sd_journal * j , OutputMode mode , unsigned line , <nl>  <nl> r = sd_journal_get_data ( j , " MESSAGE ", & data , & l ); <nl> if ( r < 0 ) { <nl> + /* An entry without MESSAGE =? */ <nl> + if ( r == - ENOENT ) <nl> + return 0 ; <nl> + <nl> log_error (" Failed to get data : % s ", strerror (- r )); <nl> return r ; <nl> }
static int method_preset_all_unit_files ( sd_bus_message * message , void * userdata , <nl> scope = m -> running_as == MANAGER_SYSTEM ? UNIT_FILE_SYSTEM : UNIT_FILE_USER ; <nl>  <nl> r = unit_file_preset_all ( scope , runtime , NULL , mm , force , & changes , & n_changes ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + unit_file_changes_free ( changes , n_changes ); <nl> return r ; <nl> + } <nl>  <nl> return reply_unit_file_changes_and_free ( m , message , - 1 , changes , n_changes ); <nl> }
int dnssec_verify_dnskey_search ( DnsResourceRecord * dnskey , DnsAnswer * validated_ <nl>  <nl> if ( ds -> key -> type != DNS_TYPE_DS ) <nl> continue ; <nl> - <nl> if ( ds -> key -> class != dnskey -> key -> class ) <nl> continue ; <nl>  <nl> static int nsec3_is_good ( DnsResourceRecord * rr , DnsResourceRecord * nsec3 ) { <nl> if ( rr -> nsec3 . iterations > NSEC3_ITERATIONS_MAX ) <nl> return 0 ; <nl>  <nl> + /* Ignore NSEC3 RRs generated from wildcards */ <nl> + if ( rr -> n_skip_labels_source != 0 ) <nl> + return 0 ; <nl> + /* Ignore NSEC3 RRs that are located anywhere else than one label below the zone */ <nl> + if ( rr -> n_skip_labels_signer != 1 ) <nl> + return 0 ; <nl> + <nl> if (! nsec3 ) <nl> return 1 ; <nl>  <nl> static int nsec3_is_good ( DnsResourceRecord * rr , DnsResourceRecord * nsec3 ) { <nl> if ( r == 0 ) <nl> return 0 ; <nl>  <nl> + /* Make sure both have the same parent */ <nl> return dns_name_equal ( a , b ); <nl> } <nl> 
int udev_monitor_filter_update ( struct udev_monitor * udev_monitor ) <nl> bpf_stmt ( ins , & i , BPF_RET | BPF_K , 0xffffffff ); <nl>  <nl> /* install filter */ <nl> + memset (& filter , 0x00 , sizeof ( filter )); <nl> filter . len = i ; <nl> filter . filter = ins ; <nl> err = setsockopt ( udev_monitor -> sock , SOL_SOCKET , SO_ATTACH_FILTER , & filter , sizeof ( filter ));
hexchat_reinit_timers ( void ) <nl> notify_tag = fe_timeout_add_seconds ( prefs . hex_notify_timeout , <nl> notify_checklist , NULL ); <nl> } <nl> - else if ( notify_tag != 0 ) <nl> + else if (! prefs . hex_notify_timeout && notify_tag != 0 ) <nl> { <nl> fe_timeout_remove ( notify_tag ); <nl> notify_tag = 0 ; <nl> hexchat_reinit_timers ( void ) <nl> { <nl> away_tag = fe_timeout_add_seconds ( prefs . hex_away_timeout , away_check , NULL ); <nl> } <nl> - else if ( away_tag != 0 ) <nl> + else if (! prefs . hex_away_track && away_tag != 0 ) <nl> { <nl> fe_timeout_remove ( away_tag ); <nl> away_tag = 0 ; <nl> hexchat_reinit_timers ( void ) <nl> { <nl> lag_check_update_tag = fe_timeout_add ( 500 , hexchat_lag_check_update , NULL ); <nl> } <nl> - else if ( lag_check_update_tag != 0 ) <nl> + else if (! prefs . hex_gui_lagometer && lag_check_update_tag != 0 ) <nl> { <nl> fe_timeout_remove ( lag_check_update_tag ); <nl> lag_check_update_tag = 0 ; <nl> hexchat_reinit_timers ( void ) <nl> { <nl> lag_check_tag = fe_timeout_add_seconds ( 30 , hexchat_lag_check , NULL ); <nl> } <nl> - else if ( lag_check_tag != 0 ) <nl> + else if ((! prefs . hex_net_ping_timeout && ! prefs . hex_gui_lagometer ) <nl> + && lag_check_tag != 0 ) <nl> { <nl> fe_timeout_remove ( lag_check_tag ); <nl> lag_check_tag = 0 ;
static void sigusr2_handler ( int num ) <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; ais_service [ i ]; i ++) { <nl> - if ( ais_service [ i ]-> exec_dump_fn ) { <nl> + for ( i = 0 ; i < SERVICE_HANDLER_MAXIMUM_COUNT ; i ++) { <nl> + if ( ais_service [ i ] && ais_service [ i ]-> exec_dump_fn ) { <nl> ais_service [ i ]-> exec_dump_fn (); <nl> } <nl> }
static void sync_service_build_handler ( unsigned int nodeid , const void * msg ) <nl> my_service_list [ my_service_list_entries ]. service_id = <nl> req_exec_service_build_message -> service_list [ i ]; <nl> sprintf ( my_service_list [ my_service_list_entries ]. name , <nl> - " Uknown External Service ( id = % d )\ n ", <nl> + " Unknown External Service ( id = % d )\ n ", <nl> req_exec_service_build_message -> service_list [ i ]); <nl> my_service_list [ my_service_list_entries ]. sync_init = <nl> dummy_sync_init ;
int _logsys_system_setup ( <nl> * Setup libqb as a subsys <nl> */ <nl> i = _logsys_subsys_create (" QB ", " array . c "); <nl> + if ( i < 0 ) { <nl> + return - 1 ; <nl> + } <nl> + <nl> _logsys_subsys_filename_add ( i , " log . c "); <nl> _logsys_subsys_filename_add ( i , " log_syslog . c "); <nl> _logsys_subsys_filename_add ( i , " log_blackbox . c ");
static int corosync_main_config_format_set ( <nl> return ( 0 ); <nl>  <nl> parse_error : <nl> - free ( value ); <nl> * error_string = error_reason ; <nl>  <nl> return (- 1 );
static int init_nss_hash ( struct crypto_instance * instance ) <nl> } <nl>  <nl> hash_param . type = siBuffer ; <nl> - hash_param . data = 0 ; <nl> - hash_param . len = 0 ; <nl> + hash_param . data = instance -> private_key ; <nl> + hash_param . len = instance -> private_key_len ; <nl>  <nl> hash_slot = PK11_GetBestSlot ( hash_to_nss [ instance -> crypto_hash_type ], NULL ); <nl> if ( hash_slot == NULL ) {
static const f_pixel * liq_image_get_row_f ( liq_image * img , unsigned int row ) <nl> convert_row_to_f ( img , row_for_thread , row , gamma_lut ); <nl> return row_for_thread ; <nl> } <nl> + <nl> + assert ( omp_get_thread_num () == 0 ); <nl> if (! liq_image_should_use_low_memory ( img )) { <nl> img -> f_pixels = img -> malloc ( sizeof ( img -> f_pixels [ 0 ]) * img -> width * img -> height ); <nl> } <nl> static float remap_to_palette ( liq_image * const input_image , unsigned char * const <nl> const int rows = input_image -> height ; <nl> const unsigned int cols = input_image -> width ; <nl> const float min_opaque_val = input_image -> min_opaque_val ; <nl> - <nl> float remapping_error = 0 ; <nl>  <nl> + if (! liq_image_get_row_f ( input_image , 0 )) { // trigger lazy conversion <nl> + return - 1 ; <nl> + } <nl> + <nl> struct nearest_map * const n = nearest_init ( map , fast ); <nl>  <nl> const unsigned int max_threads = omp_get_max_threads ();
layers_actions_update ( GimpActionGroup * group , <nl> SET_VISIBLE (" layers - text - selection - subtract ", text_layer && ! ac ); <nl> SET_VISIBLE (" layers - text - selection - intersect ", text_layer && ! ac ); <nl>  <nl> - SET_SENSITIVE (" layers - resize ", writable && ! ac ); <nl> + SET_SENSITIVE (" layers - resize ", ( writable || children ) && ! ac ); <nl> SET_SENSITIVE (" layers - resize - to - image ", writable && ! ac ); <nl> SET_SENSITIVE (" layers - scale ", ( writable || children ) && ! ac ); <nl> 
gimp_config_path_expand_only ( const gchar * path , <nl> s = gimp_plug_in_directory (); <nl> else if ( strcmp ( token , " gimp_sysconf_dir ") == 0 ) <nl> s = gimp_sysconf_directory (); <nl> + else if ( strcmp ( token , " gimp_installation_dir ") == 0 ) <nl> + s = gimp_installation_directory (); <nl>  <nl> if (! s ) <nl> s = g_getenv ( token );
wilber_get_extents ( cairo_t * cr ) <nl> cairo_fill_extents ( cr , & wilber_x1 , & wilber_y1 , & wilber_x2 , & wilber_y2 ); <nl>  <nl> wilber_cairo_path = cairo_copy_path ( cr ); <nl> + cairo_new_path ( cr ); <nl>  <nl> cairo_restore ( cr ); <nl> }
gimp_parallel_run_async_set_n_threads ( gint n_threads ) <nl> { <nl> gint i ; <nl>  <nl> + /* FIXME : when the number of GEGL threads is 1 , GEGL disables some thread - <nl> + * safety mechanisms , such that , in particular , concurrent access to the same <nl> + * buffer is not safe . ultimately , it should be possible to configure GEGL <nl> + * to remain thread - safe independently of the number of threads it uses , but <nl> + * for now , we simply disable parallel asynchronous operations when the <nl> + * number of threads is 1 . <nl> + */ <nl> + if ( n_threads == 1 ) <nl> + n_threads = 0 ; <nl> + <nl> n_threads = CLAMP ( n_threads , 0 , GIMP_PARALLEL_RUN_ASYNC_MAX_THREADS ); <nl>  <nl> if ( n_threads > gimp_parallel_run_async_n_threads ) /* need more threads */
gimp_image_map_tool_response ( GtkWidget * widget , <nl>  <nl> gimp_image_flush ( gimp_display_get_image ( tool -> display )); <nl>  <nl> - if ( image_map_tool -> config ) <nl> + if ( image_map_tool -> config && image_map_tool -> settings_box ) <nl> gimp_settings_box_add_current ( GIMP_SETTINGS_BOX ( image_map_tool -> settings_box ), <nl> GIMP_GUI_CONFIG ( tool -> tool_info -> gimp -> config )-> image_map_tool_max_recent ); <nl> }
handle_drop ( GtkWidget * widget , GdkDragContext * context , gint x , gint y , <nl> { <nl> gboolean success = FALSE ; <nl>  <nl> - if ( data -> length >= 0 && data -> format == 8 ) <nl> + if ( gtk_selection_data_get_length ( data ) >= 0 && <nl> + gtk_selection_data_get_format ( data ) == 8 ) <nl> { <nl> - const gchar * text = ( const gchar *) data -> data ; <nl> + const gchar * text = ( const gchar *) gtk_selection_data_get_data ( data ); <nl>  <nl> if ( g_utf8_validate ( text , - 1 , NULL )) <nl> {
save_dialog ( void ) <nl> g_signal_connect ( toggle , " toggled ", <nl> G_CALLBACK ( gimp_toggle_button_update ), <nl> & jsvals . save_xmp ); <nl> + g_signal_connect ( toggle , " toggled ", <nl> + G_CALLBACK ( make_preview ), <nl> + NULL ); <nl>  <nl> gtk_toggle_button_set_active ( GTK_TOGGLE_BUTTON ( toggle ), <nl> jsvals . save_xmp && has_metadata );
load_image ( const gchar * filename , <nl> gint32 volatile image_ID = - 1 ; <nl> gint32 layer_ID ; <nl> int fd ; /* File descriptor */ <nl> - char buf [ BUFLEN ]; /* buffer for random things like scanning */ <nl> + char buf [ BUFLEN + 4 ]; /* buffer for random things like scanning */ <nl> PNMInfo * pnminfo ; <nl> PNMScanner * volatile scan ; <nl> int ctr ;
gimp_image_map_tool_initialize ( GimpTool * tool , <nl>  <nl> window = gimp_display_shell_get_window ( display_shell ); <nl>  <nl> - image_map_tool -> overlay = gimp_image_window_get_fullscreen ( window ); <nl> + /* disabled for at least GIMP 2 . 8 */ <nl> + image_map_tool -> overlay = FALSE ; <nl>  <nl> if ( image_map_tool -> overlay ) <nl> {
hb_font_get_glyph_h_origin_nil ( hb_font_t * font HB_UNUSED , <nl> void * user_data HB_UNUSED ) <nl> { <nl> * x = * y = 0 ; <nl> - return false ; <nl> + return true ; <nl> } <nl> static hb_bool_t <nl> hb_font_get_glyph_h_origin_parent ( hb_font_t * font ,
static int toku_loader_write_ft_from_q ( FTLOADER bl , <nl> result = errno ; <nl> dbout_destroy (& out ); <nl> drain_writer_q ( q ); <nl> + toku_free ( ft . h ); <nl> return result ; <nl> } <nl>  <nl> static int toku_loader_write_ft_from_q ( FTLOADER bl , <nl> subtrees_info_destroy (& sts ); <nl> dbout_destroy (& out ); <nl> drain_writer_q ( q ); <nl> + toku_free ( ft . h ); <nl> return result ; <nl> } <nl>  <nl> static int toku_loader_write_ft_from_q ( FTLOADER bl , <nl> subtrees_info_destroy (& sts ); <nl> dbout_destroy (& out ); <nl> drain_writer_q ( q ); <nl> + toku_free ( ft . h ); <nl>  <nl> return result ; <nl> }
btr_search_guess_on_hash ( <nl> } <nl>  <nl> block = buf_block_align ( rec ); <nl> - page = buf_block_get_frame ( block ); <nl> + page = page_align ( rec ); <nl>  <nl> if ( UNIV_LIKELY (! has_search_latch )) { <nl> 
create_worker_threads ( uint n ) <nl> thd )) { <nl> msg (" compress : pthread_create () failed : " <nl> " errno = % d ", errno ); <nl> + pthread_mutex_unlock (& thd -> ctrl_mutex ); <nl> goto err ; <nl> } <nl> }
int ha_tokudb :: info ( uint flag ) { <nl> if ( flag & HA_STATUS_VARIABLE ) { <nl> // Just to get optimizations right <nl> stats . records = share -> rows + share -> rows_from_locked_table ; <nl> + if ( stats . records == 0 ) { <nl> + stats . records ++; <nl> + } <nl> stats . deleted = 0 ; <nl> if (!( flag & HA_STATUS_NO_LOCK )) { <nl> u_int64_t num_rows = 0 ; <nl> int ha_tokudb :: info ( uint flag ) { <nl> if ( error == 0 ) { <nl> share -> rows = num_rows ; <nl> stats . records = num_rows ; <nl> + if ( stats . records == 0 ) { <nl> + stats . records ++; <nl> + } <nl> } <nl> else { <nl> goto cleanup ;
xbstream_open ( ds_ctxt_t * ctxt , const char * path , MY_STAT * mystat ) <nl> pthread_mutex_lock (& stream_ctxt -> mutex ); <nl> if ( stream_ctxt -> dest_file == NULL ) { <nl> stream_ctxt -> dest_file = ds_open ( dest_ctxt , path , mystat ); <nl> - if ( stream_ctxt -> dest_file == NULL ) { <nl> - return NULL ; <nl> - } <nl> } <nl> pthread_mutex_unlock (& stream_ctxt -> mutex ); <nl> + if ( stream_ctxt -> dest_file == NULL ) { <nl> + return NULL ; <nl> + } <nl>  <nl> file = ( ds_file_t *) my_malloc ( sizeof ( ds_file_t ) + <nl> sizeof ( ds_stream_file_t ), <nl> MYF ( MY_FAE )); <nl> + if (! file ) { <nl> + msg (" my_malloc () failed ."); <nl> + goto err ; <nl> + } <nl> stream_file = ( ds_stream_file_t *) ( file + 1 ); <nl>  <nl> xbstream = stream_ctxt -> xbstream ;
void plugin_shutdown ( void ) <nl> struct st_plugin_int * tmp = dynamic_element (& plugin_array , i , <nl> struct st_plugin_int *); <nl> plugin_deinitialize ( tmp ); <nl> - plugin_del ( tmp ); <nl>  <nl> } <nl> 
my_bool _hash_init ( HASH * hash , CHARSET_INFO * charset , <nl> uint key_length , hash_get_key get_key , <nl> void (* free_element )( void *), uint flags CALLER_INFO_PROTO ); <nl> void hash_free ( HASH * tree ); <nl> - void hash_reset ( HASH * hash ); <nl> + void my_hash_reset ( HASH * hash ); <nl> byte * hash_element ( HASH * hash , uint idx ); <nl> gptr hash_search ( HASH * info , const byte * key , uint length ); <nl> gptr hash_next ( HASH * info , const byte * key , uint length );
int ha_tokudb :: write_row ( uchar * record ) { <nl>  <nl> u_int32_t put_flags = key_type [ primary_key ]; <nl> THD * thd = ha_thd (); <nl> - printf ("% s :% d : unique :% d \ n ", __FILE__ , __LINE__ , <nl> - thd_test_options ( thd , OPTION_RELAXED_UNIQUE_CHECKS )); <nl> - if ( thd_test_options ( thd , OPTION_RELAXED_UNIQUE_CHECKS )) <nl> + if ( thd_test_options ( thd , OPTION_RELAXED_UNIQUE_CHECKS )) { <nl> + if ( 0 ) <nl> + printf ("% s :% d : unique :% d \ n ", __FILE__ , __LINE__ , <nl> + thd_test_options ( thd , OPTION_RELAXED_UNIQUE_CHECKS )); <nl> put_flags = DB_YESOVERWRITE ; <nl> + } <nl>  <nl> table -> insert_or_update = 1 ; // For handling of VARCHAR <nl> if ( table_share -> keys + test ( hidden_primary_key ) == 1 ) {
garbage_helper ( BLOCKNUM b , int64_t UU ( size ), int64_t UU ( address ), void * extra ) { <nl> fill_bfe_for_full_read (& bfe , info -> h ); <nl> int r = toku_deserialize_brtnode_from ( info -> f , b , 0 , & n , & bfe ); <nl> if ( r != 0 ) { <nl> - goto exit ; <nl> + goto no_node ; <nl> } <nl> if ( n -> height > 0 ) { <nl> goto exit ; <nl> garbage_helper ( BLOCKNUM b , int64_t UU ( size ), int64_t UU ( address ), void * extra ) { <nl> } <nl> } <nl> exit : <nl> + toku_brtnode_free (& n ); <nl> + no_node : <nl> return r ; <nl> } <nl> 
static int fix_paths ( void ) <nl> { <nl> if (* opt_secure_file_priv == 0 ) <nl> { <nl> + my_free ( opt_secure_file_priv , MYF ( 0 )); <nl> opt_secure_file_priv = 0 ; <nl> } <nl> else
i_s_zip_fill_low ( <nl>  <nl> DBUG_ENTER (" i_s_zip_fill_low "); <nl>  <nl> + /* deny access to non - superusers */ <nl> + if ( check_global_access ( thd , SUPER_ACL )) { <nl> + <nl> + DBUG_RETURN ( 0 ); <nl> + } <nl> + <nl> /* Determine log2 ( PAGE_ZIP_MIN_SIZE / 2 / BUF_BUDDY_LOW ). */ <nl> for ( uint r = PAGE_ZIP_MIN_SIZE / 2 / BUF_BUDDY_LOW ; r >>= 1 ; y ++); <nl> 
int create_cachetable ( CACHETABLE * result , int n_entries ) { <nl> int i ; <nl> t -> n_in_table = 0 ; <nl> t -> table_size = n_entries ; <nl> - t -> table = toku_calloc ( t -> table_size , sizeof ( struct ctpair )); <nl> + MALLOC_N ( t -> table_size , t -> table ); <nl> assert ( t -> table ); <nl> t -> head = t -> tail = 0 ; <nl> for ( i = 0 ; i < t -> table_size ; i ++) {
void JOIN_TAB :: cleanup () <nl> } <nl> else <nl> { <nl> + TABLE_LIST * tmp = table -> pos_in_table_list ; <nl> end_read_record (& read_record ); <nl> - table -> pos_in_table_list -> jtbm_subselect -> cleanup (); <nl> + tmp -> jtbm_subselect -> cleanup (); <nl> /* <nl> The above call freed the materializedd temptable . Set it to NULL so <nl> that we don ' t attempt to touch it if JOIN_TAB :: cleanup () is invoked <nl> multiple times ( it may be ) <nl> */ <nl> - table = NULL ; <nl> + tmp -> table = NULL ; <nl> + table = NULL ; <nl> } <nl> DBUG_VOID_RETURN ; <nl> }
MI_INFO * mi_open ( const char * name , int mode , uint open_flags ) <nl> offset += share -> rec [ i ]. length ; <nl> } <nl> share -> rec [ i ]. type =( int ) FIELD_LAST ; /* End marker */ <nl> + if ( offset > share -> base . reclength ) <nl> + { <nl> + /* purecov : begin inspected */ <nl> + my_errno = HA_ERR_CRASHED ; <nl> + goto err ; <nl> + /* purecov : end */ <nl> + } <nl>  <nl> if (! lock_error ) <nl> {
bool mysql_make_view ( THD * thd , File_parser * parser , TABLE_LIST * table , <nl> if ( index_list ) <nl> { <nl> DBUG_ASSERT ( index_list -> head ()); // should never fail <nl> - my_error ( ER_KEY_DOES_NOT_EXITS , MYF ( 0 ), index_list -> head ()-> c_ptr_safe (), <nl> + my_error ( ER_KEY_DOES_NOT_EXITS , MYF ( 0 ), index_list -> head ()-> c_ptr (), <nl> table -> table_name ); <nl> DBUG_RETURN ( TRUE ); <nl> }
buf_LRU_block_remove_hashed_page ( <nl> void * data = bpage -> zip . data ; <nl> bpage -> zip . data = NULL ; <nl>  <nl> + ut_ad (! bpage -> in_free_list ); <nl> + ut_ad (! bpage -> in_flush_list ); <nl> + ut_ad (! bpage -> in_LRU_list ); <nl> mutex_exit (&(( buf_block_t *) bpage )-> mutex ); <nl> buf_pool_mutex_exit_forbid (); <nl> buf_buddy_free ( data , page_zip_get_size (& bpage -> zip ));
ulong events_waits_history_per_thread ; <nl> /** Number of instruments class per thread . */ <nl> ulong instr_class_per_thread ; <nl> /** Number of locker lost . @ sa LOCKER_STACK_SIZE . */ <nl> - ulong locker_lost ; <nl> + ulong locker_lost = 0 ; <nl>  <nl> /** <nl> Mutex instrumentation instances array .
trx_undo_parse_page_header ( <nl> mtr_t * mtr ) /*!< in : mtr or NULL */ <nl> { <nl> trx_id_t trx_id ; <nl> + /* Silence a GCC warning about possibly uninitialized variable <nl> + when mach_ull_parse_compressed () is not inlined . */ <nl> + ut_d ( trx_id = 0 ); <nl> + /* Declare the variable uninitialized in Valgrind , so that the <nl> + above initialization will not mask any bugs . */ <nl> + UNIV_MEM_INVALID (& trx_id , sizeof trx_id ); <nl>  <nl> ptr = mach_ull_parse_compressed ( ptr , end_ptr , & trx_id ); <nl> 
row_create_prebuilt ( <nl> prebuilt -> ins_node = NULL ; <nl>  <nl> prebuilt -> ins_upd_rec_buff = NULL ; <nl> - <nl> + <nl> + prebuilt -> hint_need_to_fetch_extra_cols = 0 ; <nl> + <nl> prebuilt -> upd_node = NULL ; <nl> prebuilt -> ins_graph = NULL ; <nl> prebuilt -> upd_graph = NULL ;
int ha_rocksdb :: calc_eq_cond_len ( const Rdb_key_def & kd , <nl> */ <nl> return slice . size () - bytes_changed_by_succ ; <nl> } <nl> + if ( find_flag == HA_READ_PREFIX_LAST_OR_PREV ) { <nl> + /* <nl> + We are interested in records with or without the specified prefix , so <nl> + the only limit is that records are from this index <nl> + */ <nl> + return Rdb_key_def :: INDEX_NUMBER_SIZE ; <nl> + } <nl>  <nl> if ( end_key ) { <nl> * end_key_packed_size =
static uint remove_key ( MI_KEYDEF * keyinfo , uint nod_flag , <nl> else <nl> get_key_length ( rest_length , keypos ); <nl>  <nl> - if ( next_length > prev_length ) <nl> + if ( next_length >= prev_length ) <nl> { /* Key after is based on deleted key */ <nl> uint pack_length , tmp ; <nl> bmove_upp (( char *) keypos ,( char *) ( lastkey + next_length ),
OPJ_BOOL opj_j2k_encode ( opj_j2k_t * p_j2k , <nl> /* now copy this data into the tile component */ <nl> if (! opj_tcd_copy_tile_data ( p_j2k -> m_tcd , l_current_data , l_current_tile_size )) { <nl> opj_event_msg ( p_manager , EVT_ERROR , " Size mismatch between tile data and sent data ." ); <nl> + opj_free ( l_current_data ); <nl> return OPJ_FALSE ; <nl> } <nl> } <nl>  <nl> if (! opj_j2k_post_write_tile ( p_j2k , p_stream , p_manager )) { <nl> + if ( l_current_data ) { <nl> + opj_free ( l_current_data ); <nl> + } <nl> return OPJ_FALSE ; <nl> } <nl> }
static int tga_readheader ( FILE * fp , unsigned int * bits_per_pixel , <nl> if ( fread ( tga , TGA_HEADER_SIZE , 1 , fp ) != 1 ) { <nl> fprintf ( stderr , <nl> "\ nError : fread return a number of element different from the expected .\ n "); <nl> + free ( tga ); <nl> return 0 ; <nl> } <nl> id_len = ( unsigned char ) tga [ 0 ];
rend_cache_store_v2_desc_as_client ( const char * desc , <nl> goto err ; <nl> } <nl> /* Decode / decrypt introduction points . */ <nl> - if ( intro_content ) { <nl> + if ( intro_content && intro_size > 0 ) { <nl> int n_intro_points ; <nl> if ( rend_query -> auth_type != REND_NO_AUTH && <nl> ! tor_mem_is_zero ( rend_query -> descriptor_cookie ,
generate_v2_networkstatus ( void ) <nl> or_options_t * options = get_options (); <nl> char fingerprint [ FINGERPRINT_LEN + 1 ]; <nl> char ipaddr [ INET_NTOA_BUF_LEN + 1 ]; <nl> - char published [ ISO_TIME_LEN ]; <nl> + char published [ ISO_TIME_LEN + 1 ]; <nl> char digest [ DIGEST_LEN ]; <nl> struct in_addr in ; <nl> uint32_t addr ;
rend_config_services ( const or_options_t * options , int validate_only ) <nl> log_warn ( LD_CONFIG , <nl> " HiddenServiceAllowUnknownPorts should be 0 or 1 , not % s ", <nl> line -> value ); <nl> - smartlist_free ( temp_service_list ); <nl> goto free_and_return ; <nl> } <nl> log_info ( LD_CONFIG ,
extern INLINE double U64_TO_DBL ( uint64_t x ) { <nl> # endif <nl>  <nl> /* GCC has several useful attributes . */ <nl> -# ifdef __GNUC__ <nl> +# ifdef __GNUC__ && __GNUC_MAJOR__ >= 3 <nl> # define ATTR_NORETURN __attribute__ (( noreturn )) <nl> # define ATTR_PURE __attribute__ (( pure )) <nl> # define ATTR_MALLOC __attribute__ (( malloc ))
entry_guard_register_connect_status ( const char * digest , int succeeded , <nl> entry -> nickname , buf , tbuf ); <nl> entry -> last_attempted = now ; <nl> } <nl> - entry -> can_retry = 0 ; /* We gave it an early chance ; no good . */ <nl> + if ( entry ) <nl> + entry -> can_retry = 0 ; /* We gave it an early chance ; no good . */ <nl> } <nl>  <nl> if ( first_contact ) {
circuit_resume_edge_reading_helper ( edge_connection_t * first_conn , <nl> int cells_on_queue ; <nl> int cells_per_conn ; <nl> edge_connection_t * chosen_stream = NULL ; <nl> + int max_to_package ; <nl> + <nl> + if ( first_conn == NULL ) { <nl> + /* Don ' t bother to try to do the rest of this if there are no connections <nl> + * to resume . */ <nl> + return 0 ; <nl> + } <nl>  <nl> /* How many cells do we have space for ? It will be the minimum of <nl> * the number needed to exhaust the package window , and the minimum <nl> * needed to fill the cell queue . */ <nl> - int max_to_package = circ -> package_window ; <nl> + max_to_package = circ -> package_window ; <nl> if ( CIRCUIT_IS_ORIGIN ( circ )) { <nl> cells_on_queue = circ -> n_chan_cells . n ; <nl> } else {
policies_parse_exit_policy_internal ( config_line_t * cfg , smartlist_t ** dest , <nl> " address ", fmt_addr ( ipv6_local_address )); <nl> } else { <nl> char buf6 [ POLICY_BUF_LEN ]; <nl> - tor_snprintf ( buf6 , sizeof ( buf6 ), " reject % s :*", <nl> + tor_snprintf ( buf6 , sizeof ( buf6 ), " reject [% s ]:*", <nl> fmt_addr ( ipv6_local_address )); <nl> append_exit_policy_string ( dest , buf6 ); <nl> log_info ( LD_CONFIG , " Adding a reject ExitPolicy '% s ' for our " <nl> policies_parse_exit_policy_internal ( config_line_t * cfg , smartlist_t ** dest , <nl> * address */ <nl> if ( ipv6_local_address == NULL <nl> || ! tor_addr_eq ( ipv6_local_address , a )) { <nl> - tor_snprintf ( bufif , sizeof ( bufif ), " reject6 % s :*", <nl> + tor_snprintf ( bufif , sizeof ( bufif ), " reject6 [% s ]:*", <nl> fmt_addr ( a )); <nl> append_exit_policy_string ( dest , bufif ); <nl> log_info ( LD_CONFIG , " Adding a reject ExitPolicy '% s ' for a local "
tor_tls_check_lifetime ( int severity , tor_tls_t * tls , <nl> * < b > future_tolerance </ b > seconds . If it is live , return 0 . If it is not <nl> * live , log a message and return - 1 . */ <nl> static int <nl> - check_cert_lifetime_internal ( int severity , const X509 * cert , int past_tolerance , <nl> - int future_tolerance ) <nl> + check_cert_lifetime_internal ( int severity , const X509 * cert , <nl> + int past_tolerance , int future_tolerance ) <nl> { <nl> time_t now , t ; <nl> 
static INLINE char * format_msg ( char * buf , size_t buf_len , <nl> /* The message was too long ; overwrite the end of the buffer with <nl> * "[... truncated ]" */ <nl> if ( buf_len >= TRUNCATED_STR_LEN ) { <nl> - /* This is safe , since we have an extra character after buf_len <nl> - to hold the \ 0 . */ <nl> - strlcpy ( buf + buf_len - TRUNCATED_STR_LEN , TRUNCATED_STR , <nl> - buf_len -( buf_len - TRUNCATED_STR_LEN - 1 )); <nl> + int offset = buf_len - TRUNCATED_STR_LEN ; <nl> + /* We have an extra 2 characters after buf_len to hold the \ n \ 0 , <nl> + * so it ' s safe to add 1 to the size here . */ <nl> + strlcpy ( buf + offset , TRUNCATED_STR , buf_len - offset + 1 ); <nl> } <nl> /* Set ' n ' to the end of the buffer , where we ' ll be writing \ n \ 0 . <nl> * Since we already subtracted 2 from buf_len , this is safe .*/ <nl> n = buf_len ; <nl> - <nl> } else { <nl> n += r ; <nl> }
control_event_conf_changed ( smartlist_t * elements ) <nl> for ( i = 0 ; i < smartlist_len ( elements ); i += 2 ) { <nl> char * k = smartlist_get ( elements , i ); <nl> char * v = smartlist_get ( elements , i + 1 ); <nl> + char * tmp ; <nl> if ( v == NULL ) { <nl> - char * tmp ; <nl> tor_asprintf (& tmp , " 650 -% s ", k ); <nl> - smartlist_add ( lines , tmp ); <nl> } else { <nl> - char * tmp ; <nl> tor_asprintf (& tmp , " 650 -% s =% s ", k , v ); <nl> - smartlist_add ( lines , tmp ); <nl> } <nl> + smartlist_add ( lines , tmp ); <nl> } <nl> result = smartlist_join_strings ( lines , "\ r \ n ", 0 , NULL ); <nl> send_control_event ( EVENT_CONF_CHANGED , 0 ,
tor_zlib_process ( tor_zlib_state_t * state , <nl> return Z_OK ; <nl> return TOR_ZLIB_BUF_FULL ; <nl> case Z_OK : <nl> - if ( state -> stream . avail_out == 0 ) <nl> + if ( state -> stream . avail_out == 0 || finish ) <nl> return TOR_ZLIB_BUF_FULL ; <nl> return TOR_ZLIB_OK ; <nl> default :
ECP :: ECP ( BufferedTransformation & bt ) <nl> GetField (). BERDecodeElement ( seq , m_b ); <nl> // skip optional seed <nl> if (! seq . EndReached ()) <nl> - BERDecodeOctetString ( seq , TheBitBucket ()); <nl> + { <nl> + SecByteBlock seed ; <nl> + unsigned int unused ; <nl> + BERDecodeBitString ( seq , seed , unused ); <nl> + } <nl> seq . MessageEnd (); <nl> } <nl> 
void telnetSendChar ( char ch ) { <nl> bool telnetRecv ( JsNetwork * net ) { <nl> if ( tnSrv . sock == 0 || tnSrv . cliSock == 0 ) return false ; <nl>  <nl> - char buff [ 256 ]; <nl> - int r = netRecv ( net , ST_NORMAL , tnSrv . cliSock - 1 , buff , 256 ); <nl> + char buff [ 64 ]; <nl> + if (! jshHasEventSpaceForChars ( sizeof ( buff ))) return false ; <nl> + int r = netRecv ( net , ST_NORMAL , tnSrv . cliSock - 1 , buff , sizeof ( buff )); <nl> if ( r > 0 ) { <nl> jshPushIOCharEvents ( EV_TELNET , buff , ( unsigned int ) r ); <nl> } else if ( r < 0 ) {
extern int job_str_signal ( char * job_id_str , uint16_t signal , uint16_t flags , <nl> while ( job_ptr ) { <nl> if ( job_ptr -> array_job_id == job_id ) <nl> break ; <nl> + job_ptr = job_ptr -> job_array_next_j ; <nl> } <nl> if ( job_ptr && ( job_ptr -> user_id != uid ) && <nl> ! validate_operator ( uid ) &&
static int _get_hash_idx ( const char * name ) <nl> index %= NAME_HASH_LEN ; <nl> if ( index < 0 ) <nl> index += NAME_HASH_LEN ; <nl> + if ( index < 0 ) <nl> + index = 0 /* Can never happen , but clears a Coverity error */ <nl>  <nl> return index ; <nl> }
static int _job_count_bitmap ( struct cr_record * cr_ptr , <nl> if (( gres_cpus < cpu_cnt ) || <nl> ( gres_cpus < job_ptr -> details -> ntasks_per_node ) || <nl> (( job_ptr -> details -> cpus_per_task > 1 ) && <nl> - ( gres_cpus < job_ptr -> details -> cpus_per_task ))) <nl> + ( gres_cpus < job_ptr -> details -> cpus_per_task ))) { <nl> bit_clear ( jobmap , i ); <nl> continue ; <nl> }
_signal_jobstep ( uint32_t jobid , uint32_t stepid , uid_t req_uid , <nl> uid_t uid ; <nl> uint16_t protocol_version ; <nl>  <nl> + /* There will be no stepd if the prolog is still running <nl> + * Return failure so caller can retry . <nl> + */ <nl> + if ( _prolog_is_running ( jobid )) { <nl> + info (" signal % d req for % u .% u while prolog is running ." <nl> + " Returning failure .", signal , jobid , stepid ); <nl> + return SLURM_FAILURE ; <nl> + } <nl> + <nl> fd = stepd_connect ( conf -> spooldir , conf -> node_name , jobid , stepid , <nl> & protocol_version ); <nl> if ( fd == - 1 ) {
extern int assoc_mgr_fill_in_assoc ( void * db_conn , <nl> ret_assoc = found_assoc ; <nl> debug3 (" found association " <nl> " for no partition "); <nl> + continue ; <nl> } else if ( strcasecmp ( assoc -> partition , <nl> - found_assoc -> partition )) <nl> + found_assoc -> partition )) { <nl> debug3 (" not the right partition "); <nl> - continue ; <nl> + continue ; <nl> + } <nl> } <nl> } <nl> ret_assoc = found_assoc ;
static int _schedule ( uint32_t job_limit ) <nl> char job_id_buf [ 32 ]; <nl> char * unavail_node_str = NULL ; <nl> bool fail_by_part ; <nl> - uint32_t deadline_time_limit , save_time_limit ; <nl> + uint32_t deadline_time_limit , save_time_limit = 0 ; <nl> # if HAVE_SYS_PRCTL_H <nl> char get_name [ 16 ]; <nl> # endif
static void _free_step_rec ( struct step_record * step_ptr ) <nl> list_destroy ( step_ptr -> gres_list ); <nl> select_g_select_jobinfo_free ( step_ptr -> select_jobinfo ); <nl> xfree ( step_ptr -> ext_sensors ); <nl> + step_ptr -> job_ptr = NULL ; <nl> xfree ( step_ptr ); <nl> } <nl> 
int * msGetGDALBandList ( layerObj * layer , void * hDS , <nl> " msGetGDALBandList ()", <nl> papszItems [ i ], GDALGetRasterCount ( hDS ) ); <nl> CSLDestroy ( papszItems ); <nl> - CPLFree ( band_list ); <nl> + free ( band_list ); <nl> return NULL ; <nl> } <nl> }
MS_DLL_EXPORT int msMapLoadOWSParameters ( mapObj * map , cgiRequestObj * request , <nl> MS_DLL_EXPORT int msMapIgnoreMissingData ( mapObj * map ); <nl>  <nl> /* mapfile . c */ <nl> - <nl> - MS_DLL_EXPORT int msGetLayerIndex ( mapObj * map , char * name ); /* in mapfile . c */ <nl> + <nl> + MS_DLL_EXPORT int msValidateParameter ( char * value , char * pattern1 , char * pattern2 , char * pattern3 , char * pattern4 ); <nl> + MS_DLL_EXPORT int msGetLayerIndex ( mapObj * map , char * name ); <nl> MS_DLL_EXPORT int msGetSymbolIndex ( symbolSetObj * set , char * name , int try_addimage_if_notfound ); <nl> MS_DLL_EXPORT mapObj * msLoadMap ( char * filename , char * new_mappath ); <nl> MS_DLL_EXPORT int msSaveMap ( mapObj * map , char * filename );
int process_node ( layerObj * layer , expressionObj * filter ) <nl> break ; <nl> /* literal tokens */ <nl> case MS_TOKEN_LITERAL_BOOLEAN : <nl> + if ( layerinfo -> current_node -> tokenval . dblval == MS_TRUE ) <nl> + filter -> native_string = msStringConcatenate ( filter -> native_string , " 1 "); <nl> + else <nl> + filter -> native_string = msStringConcatenate ( filter -> native_string , " 0 "); <nl> + break ; <nl> case MS_TOKEN_LITERAL_NUMBER : <nl> strtmpl = "% lf "; <nl> snippet = ( char *) msSmallMalloc ( strlen ( strtmpl ) + 16 );
void returnQuery () <nl> LRN ++; <nl> } <nl>  <nl> - if ( lp -> footer ) returnPage ( lp -> header , BROWSE ); <nl> + if ( lp -> footer ) returnPage ( lp -> footer , BROWSE ); <nl>  <nl> msLayerClose ( lp ); <nl> ResultLayer = NULL ;
static gboolean <nl> generic_inst_is_sharable ( MonoGenericInst * inst , gboolean allow_type_vars , <nl> gboolean allow_partial ) <nl> { <nl> - gboolean has_refs ; <nl> int i ; <nl>  <nl> - has_refs = FALSE ; <nl> - for ( i = 0 ; i < inst -> type_argc ; ++ i ) { <nl> - MonoType * type = inst -> type_argv [ i ]; <nl> - <nl> - if ( MONO_TYPE_IS_REFERENCE ( type ) || ( allow_type_vars && ( type -> type == MONO_TYPE_VAR || type -> type == MONO_TYPE_MVAR ))) <nl> - has_refs = TRUE ; <nl> - } <nl> - <nl> for ( i = 0 ; i < inst -> type_argc ; ++ i ) { <nl> MonoType * type = inst -> type_argv [ i ]; <nl> 
mono_profiler_init_log ( const char * desc ) <nl> mono_profiler_set_gc_event_callback ( handle , gc_event ); <nl>  <nl> mono_profiler_set_thread_started_callback ( handle , thread_start ); <nl> - mono_profiler_set_thread_stopped_callback ( handle , thread_end ); <nl> + mono_profiler_set_thread_exited_callback ( handle , thread_end ); <nl> mono_profiler_set_thread_name_callback ( handle , thread_name ); <nl>  <nl> mono_profiler_set_domain_loaded_callback ( handle , domain_loaded );
emit_method_info ( MonoAotCompile * acfg , MonoCompile * cfg ) <nl>  <nl> encode_patch_list ( acfg , patches , n_patches , cfg -> compile_llvm , first_got_offset , p , & p ); <nl>  <nl> + g_ptr_array_free ( patches , TRUE ); <nl> + <nl> acfg -> stats . info_size += p - buf ; <nl>  <nl> g_assert ( p - buf < buf_size );
sgen_register_fixed_internal_mem_type ( int type , size_t size ) <nl>  <nl> if ( fixed_type_allocator_indexes [ type ] == - 1 ) <nl> fixed_type_allocator_indexes [ type ] = slot ; <nl> - else <nl> - g_assert ( fixed_type_allocator_indexes [ type ] == slot ); <nl> + else { <nl> + if ( fixed_type_allocator_indexes [ type ] != slot ) <nl> + g_error (" Invalid double registration of type % d old slot % d new slot % d ", type , fixed_type_allocator_indexes [ type ], slot ); <nl> + } <nl> } <nl>  <nl> static const char *
SMySQL :: SMySQL ( const string & database , const string & host , uint16_t port , const <nl> { <nl> mysql_init (& d_db ); <nl> mysql_options (& d_db , MYSQL_READ_DEFAULT_GROUP , " client "); <nl> + my_bool reconnect = 1 ; <nl> + mysql_options (& d_db , MYSQL_OPT_RECONNECT , & reconnect ); <nl> + <nl> if (! mysql_real_connect (& d_db , host . empty () ? 0 : host . c_str (), <nl> user . empty () ? 0 : user . c_str (), <nl> password . empty () ? 0 : password . c_str (),
static bool flow_search_walker ( GenericHashEntry * h , void * user_data ) { <nl> } <nl> } <nl>  <nl> - if ( retriever -> actNumEntries == retriever -> maxNumEntries ) <nl> + if ( retriever -> actNumEntries >= retriever -> maxNumEntries ) <nl> return ( true ); /* Limit reached */ <nl> else <nl> return ( false ); /* false = keep on walking */ <nl> static bool host_search_walker ( GenericHashEntry * he , void * user_data ) { <nl> break ; <nl> } <nl>  <nl> - if ( r -> actNumEntries == r -> maxNumEntries ) <nl> + if ( r -> actNumEntries >= r -> maxNumEntries ) <nl> return ( true ); /* Limit reached */ <nl> else <nl> return ( false ); /* false = keep on walking */
void NetworkInterface :: periodicStatsUpdate () { <nl>  <nl> # ifdef NTOPNG_PRO <nl> if ( aggregated_flows_hash ) { <nl> - if (-- nextFlowAggregation == 0 ) { <nl> + if (( getIfType () == interface_type_DUMMY ) || (-- nextFlowAggregation == 0 )) { <nl> /* Start over */ <nl> aggregated_flows_hash -> cleanup (); <nl> nextFlowAggregation = FLOW_AGGREGATION_DURATION ; <nl> void NetworkInterface :: periodicStatsUpdate () { <nl> " Aggregated flows hash cleared . [ num_items : % i ]", <nl> aggregated_flows_hash -> getCurrentSize ()); <nl> # endif <nl> - <nl> } else <nl> # ifdef AGGREGATED_FLOW_DEBUG <nl> ntop -> getTrace ()-> traceEvent ( TRACE_NORMAL ,
int ecdsa_write_signature ( ecdsa_context * ctx , <nl> void * p_rng ) <nl> { <nl> int ret ; <nl> - unsigned char buf [ MAX_SIG_LEN ]; <nl> - unsigned char * p = buf + MAX_SIG_LEN - 1 ; <nl> + unsigned char buf [ MAX_SIG_LEN + 3 ]; <nl> + unsigned char * p = buf + MAX_SIG_LEN ; <nl> size_t len = 0 ; <nl>  <nl> if ( ( ret = ecdsa_sign ( & ctx -> grp , & ctx -> r , & ctx -> s , & ctx -> d ,
nautilus_path_bar_scroll_down ( NautilusPathBar * path_bar ) <nl> * from the end , removing buttons until we get all the space we <nl> * need . */ <nl> gtk_widget_get_allocation ( BUTTON_DATA ( up_button -> data )-> button , & button_allocation ); <nl> - while ( space_available < space_needed ) { <nl> + while (( space_available < space_needed ) && <nl> + ( up_button != NULL )) { <nl> space_available += button_allocation . width + path_bar -> spacing ; <nl> up_button = up_button -> prev ; <nl> path_bar -> first_scrolled_button = up_button ;
nautilus_view_validate_file_name ( FileNameDialogData * data ) <nl> gboolean duplicated_name ; <nl> gboolean contains_slash ; <nl> gboolean is_empty ; <nl> - const gchar * name ; <nl> + gchar * name ; <nl> GList * files ; <nl> GList * node ; <nl> NautilusFile * file ; <nl> nautilus_view_validate_file_name ( FileNameDialogData * data ) <nl> g_assert ( GTK_IS_DIALOG ( data -> dialog )); <nl> g_assert ( NAUTILUS_IS_VIEW ( data -> view )); <nl>  <nl> - name = gtk_entry_get_text ( GTK_ENTRY ( data -> name_entry )); <nl> + name = g_strstrip ( g_strdup ( gtk_entry_get_text ( GTK_ENTRY ( data -> name_entry )))); <nl> is_empty = strlen ( name ) == 0 ; <nl> contains_slash = strstr ( name , "/") != NULL ; <nl> duplicated_name = FALSE ; <nl> nautilus_view_validate_file_name ( FileNameDialogData * data ) <nl> gtk_dialog_set_response_sensitive ( GTK_DIALOG ( data -> dialog ), <nl> GTK_RESPONSE_OK , <nl> ! is_empty && ! contains_slash && ! duplicated_name ); <nl> + g_free ( name ); <nl> } <nl>  <nl> static void
eel_editable_label_draw ( GtkWidget * widget , <nl> gdk_cairo_region ( cr , clip ); <nl> cairo_clip ( cr ); <nl>  <nl> - state = GTK_STATE_FLAG_SELECTED ; <nl> - if (! gtk_widget_has_focus ( widget )) <nl> - state = GTK_STATE_FLAG_ACTIVE ; <nl> + state = gtk_widget_get_state_flags ( widget ); <nl> + state |= GTK_STATE_FLAG_SELECTED ; <nl>  <nl> gtk_style_context_get_background_color ( style , state , & background_color ); <nl> gdk_cairo_set_source_rgba ( cr , & background_color );
static GF_Err gf_isom_adjust_visual_info ( GF_ISOFile * file , u32 track ) { <nl> HEVCState hevc ; <nl>  <nl> lhvccfg = gf_isom_lhvc_config_get ( file , track , 1 ); <nl> + if (! lhvccfg ) lhvccfg = gf_isom_hevc_config_get ( file , track , 1 ); <nl> if (! lhvccfg ) return GF_OK ; <nl>  <nl> for ( i = 0 ; i < gf_list_count ( lhvccfg -> param_array ); i ++) { <nl> GF_Err gf_media_split_lhvc ( GF_ISOFile * file , u32 track , Bool splitAll , Bool use_ <nl> // update lhvc config <nl> e = gf_isom_lhvc_config_update ( file , track , 1 , NULL , GF_ISOM_LEHVC_WITH_BASE ); <nl> if ( e ) goto exit ; <nl> - <nl> + gf_isom_adjust_visual_info ( file , track ); <nl> + <nl> nal_alloc_size = 10000 ; <nl> nal_data = gf_malloc ( sizeof ( char ) * nal_alloc_size ); <nl> // parse all samples
GF_Err audio_sample_entry_Read ( GF_Box * s , GF_BitStream * bs ) <nl> gf_bs_read_data ( bs , data , size ); <nl> for ( i = 0 ; i < size - 8 ; i ++) { <nl> if ( GF_4CC (( u32 ) data [ i + 4 ], ( u8 ) data [ i + 5 ], ( u8 ) data [ i + 6 ], ( u8 ) data [ i + 7 ]) == GF_ISOM_BOX_TYPE_ESDS ) { <nl> + extern Bool use_dump_mode ; <nl> GF_BitStream * mybs = gf_bs_new ( data + i , size - i , GF_BITSTREAM_READ ); <nl> if ( ptr -> esd ) { <nl> - gf_isom_box_del (( GF_Box *) ptr -> esd ); <nl> + if (! use_dump_mode ) gf_isom_box_del (( GF_Box *) ptr -> esd ); <nl> ptr -> esd = NULL ; <nl> } <nl> 
GF_DownloadSession * gf_dm_sess_new_simple ( GF_DownloadManager * dm , const char * u <nl>  <nl> * e = gf_dm_setup_from_url ( sess , url ); <nl> if (* e ) { <nl> - GF_LOG ( GF_LOG_WARNING , GF_LOG_NETWORK , ("% s :% s gf_dm_sess_new_simple :% d , error =% e at setup \ n ", __FILE__ , __LINE__ , e )); <nl> + GF_LOG ( GF_LOG_WARNING , GF_LOG_NETWORK , ("% s :% d gf_dm_sess_new_simple : error =% s at setup for % s \ n ", __FILE__ , __LINE__ , gf_error_to_string (* e ), url )); <nl> gf_dm_sess_del ( sess ); <nl> return NULL ; <nl> }
GF_Err gf_isom_oinf_read_entry ( void * entry , GF_BitStream * bs ) <nl> op -> output_layer_set_idx = gf_bs_read_u16 ( bs ); <nl> op -> max_temporal_id = gf_bs_read_u8 ( bs ); <nl> op -> layer_count = gf_bs_read_u8 ( bs ); <nl> - if ( op -> layer_count > GF_ARRAY_LENGTH ( op -> layers_info )) <nl> + if ( op -> layer_count > GF_ARRAY_LENGTH ( op -> layers_info )) { <nl> + gf_free ( op ); <nl> return GF_NON_COMPLIANT_BITSTREAM ; <nl> + } <nl> for ( j = 0 ; j < op -> layer_count ; j ++) { <nl> op -> layers_info [ j ]. ptl_idx = gf_bs_read_u8 ( bs ); <nl> op -> layers_info [ j ]. layer_id = gf_bs_read_int ( bs , 6 );
void gf_es_receive_sl_packet ( GF_ClientService * serv , GF_Channel * ch , char * paylo <nl> ch -> net_dts = ch -> net_cts = 0 ; <nl> ch -> CTS = ch -> DTS = gf_clock_time ( ch -> clock ); <nl> } else { <nl> - ch -> net_dts -= ch -> seed_ts ; <nl> + if ( ch -> net_dts > ch -> seed_ts ) ch -> net_dts -= ch -> seed_ts ; <nl> + else ch -> net_dts = 0 ; <nl> ch -> net_cts -= ch -> seed_ts ; <nl> ch -> CTS_past_offset = 0 ; <nl>  <nl> GF_DBUnit * gf_es_get_au ( GF_Channel * ch ) <nl> /* pull from stream - resume clock if needed */ <nl> ch_buffer_off ( ch ); <nl>  <nl> + memset (& slh , 0 , sizeof ( GF_SLHeader )); <nl> + <nl> e = gf_term_channel_get_sl_packet ( ch -> service , ch , ( char **) & ch -> AU_buffer_pull -> data , & ch -> AU_buffer_pull -> dataLength , & slh , & comp , & state , & is_new_data ); <nl> if ( e ) state = e ; <nl> switch ( state ) {
void print_slot_info ( FILE * f , CK_SLOT_INFO * info ) <nl>  <nl> void print_token_info ( FILE * f , CK_TOKEN_INFO * info ) <nl> { <nl> - int i ; <nl> - enum_specs ck_flags [ 18 ] = { <nl> + size_t i ; <nl> + enum_specs ck_flags [] = { <nl> { CKF_RNG , " CKF_RNG " }, <nl> { CKF_WRITE_PROTECTED , " CKF_WRITE_PROTECTED " }, <nl> { CKF_LOGIN_REQUIRED , " CKF_LOGIN_REQUIRED " }, <nl> void print_token_info ( FILE * f , CK_TOKEN_INFO * info ) <nl> fprintf ( f , " firmwareVersion : % d .% d \ n ", info -> firmwareVersion . major , info -> firmwareVersion . minor ); <nl> fprintf ( f , " time : '% 16 . 16s '\ n ", info -> utcTime ); <nl> fprintf ( f , " flags : % 0lx \ n ", info -> flags ); <nl> - for ( i = 0 ; i < 8 ; i ++) { <nl> + for ( i = 0 ; i < sizeof ( ck_flags ) / sizeof (* ck_flags ); i ++) { <nl> if ( info -> flags & ck_flags [ i ]. type ) { <nl> fprintf ( f , " % s \ n ", ck_flags [ i ]. name ); <nl> }
static int asn1_decode_entry ( sc_context_t * ctx , struct sc_asn1_entry * entry , <nl>  <nl> /* Strip off padding zero */ <nl> if (( entry -> flags & SC_ASN1_UNSIGNED ) <nl> - && obj [ 0 ] == 0x00 && objlen > 1 ) { <nl> + && objlen > 1 && obj [ 0 ] == 0x00 ) { <nl> objlen --; <nl> obj ++; <nl> }
sc_parse_ef_atr_content ( struct sc_card * card , unsigned char * buf , size_t buflen ) <nl>  <nl> category = * buf ; <nl>  <nl> + memset (& ef_atr , 0 , sizeof ( struct sc_ef_atr )); <nl> /* IAS / ECC specific : skip second ' zero ' byte */ <nl> if (*(++ buf ) == 0x00 ) <nl> ++ buf ;
sc_pkcs15emu_add_object ( sc_pkcs15_card_t * p15card , int type , <nl> int df_type ; <nl>  <nl> obj = ( sc_pkcs15_object_t *) calloc ( 1 , sizeof (* obj )); <nl> + if (! obj ) <nl> + return SC_ERROR_OUT_OF_MEMORY ; <nl>  <nl> obj -> type = type ; <nl> obj -> data = data ; <nl> sc_pkcs15emu_add_pin ( sc_pkcs15_card_t * p15card , <nl> sc_pkcs15_pin_info_t * info ; <nl>  <nl> info = ( sc_pkcs15_pin_info_t *) calloc ( 1 , sizeof (* info )); <nl> + if (! info ) <nl> + return SC_ERROR_OUT_OF_MEMORY ; <nl> info -> auth_id = * id ; <nl> info -> min_length = min_length ; <nl> info -> max_length = max_length ; <nl> sc_pkcs15emu_add_cert ( sc_pkcs15_card_t * p15card , <nl> /* const char * label = " Certificate "; */ <nl> sc_pkcs15_cert_info_t * info ; <nl> info = ( sc_pkcs15_cert_info_t *) calloc ( 1 , sizeof (* info )); <nl> + if (! info ) <nl> + return SC_ERROR_OUT_OF_MEMORY ; <nl> info -> id = * id ; <nl> info -> authority = authority ; <nl> if ( path ) <nl> sc_pkcs15emu_add_prkey ( sc_pkcs15_card_t * p15card , <nl> sc_pkcs15_prkey_info_t * info ; <nl>  <nl> info = ( sc_pkcs15_prkey_info_t *) calloc ( 1 , sizeof (* info )); <nl> + if (! info ) <nl> + return SC_ERROR_OUT_OF_MEMORY ; <nl> info -> id = * id ; <nl> info -> modulus_length = modulus_length ; <nl> info -> usage = usage ; <nl> sc_pkcs15emu_add_pubkey ( sc_pkcs15_card_t * p15card , <nl> sc_pkcs15_pubkey_info_t * info ; <nl>  <nl> info = ( sc_pkcs15_pubkey_info_t *) calloc ( 1 , sizeof (* info )); <nl> + if (! info ) <nl> + return SC_ERROR_OUT_OF_MEMORY ; <nl> info -> id = * id ; <nl> info -> modulus_length = modulus_length ; <nl> info -> usage = usage ;
* <nl> * <nl> * IDENTIFICATION <nl> - * $ Header : / cvsroot / pgsql / src / backend / utils / adt / acl . c , v 1 . 6 1996 / 11 / 10 03 : 03 : 00 momjian Exp $ <nl> + * $ Header : / cvsroot / pgsql / src / backend / utils / adt / acl . c , v 1 . 7 1996 / 11 / 17 04 : 26 : 59 momjian Exp $ <nl> * <nl> *------------------------------------------------------------------------- <nl> */ <nl> aclitemout ( AclItem * aip ) <nl> ( void ) strcat ( p , " group "); <nl> tmpname = get_groname ( aip -> ai_id ); <nl> ( void ) strncat ( p , tmpname , NAMEDATALEN ); <nl> - pfree ( tmpname ); <nl> break ; <nl> case ACL_IDTYPE_WORLD : <nl> break ;
* <nl> * <nl> * IDENTIFICATION <nl> - * $ PostgreSQL : pgsql / src / backend / parser / parse_func . c , v 1 . 167 2004 / 04 / 02 19 : 06 : 58 tgl Exp $ <nl> + * $ PostgreSQL : pgsql / src / backend / parser / parse_func . c , v 1 . 168 2004 / 04 / 02 21 : 30 : 44 tgl Exp $ <nl> * <nl> *------------------------------------------------------------------------- <nl> */ <nl> unknown_attribute ( ParseState * pstate , Node * relref , char * attname ) <nl> { <nl> RangeTblEntry * rte ; <nl>  <nl> - if ( IsA ( relref , Var )) <nl> + if ( IsA ( relref , Var ) && <nl> + (( Var *) relref )-> varattno == InvalidAttrNumber ) <nl> { <nl> /* Reference the RTE by alias not by actual table name */ <nl> rte = GetRTEByRangeTablePosn ( pstate ,
libpqrcv_identify_system ( TimeLineID * primary_tli ) <nl> GetSystemIdentifier ()); <nl> if ( strcmp ( primary_sysid , standby_sysid ) != 0 ) <nl> { <nl> + primary_sysid = pstrdup ( primary_sysid ); <nl> PQclear ( res ); <nl> ereport ( ERROR , <nl> ( errmsg (" database system identifier differs between the primary and standby "),
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE <nl> SOFTWARE . <nl> */ <nl> -/* $ PostgreSQL : pgsql / contrib / pgcrypto / imath . c , v 1 . 6 2006 / 10 / 04 00 : 29 : 46 momjian Exp $ */ <nl> +/* $ PostgreSQL : pgsql / contrib / pgcrypto / imath . c , v 1 . 7 2007 / 07 / 15 22 : 43 : 40 tgl Exp $ */ <nl>  <nl> # include " postgres . h " <nl> # include " px . h " <nl> s_norm ( mp_int a , mp_int b ) <nl> mp_digit d = b -> digits [ MP_USED ( b ) - 1 ]; <nl> int k = 0 ; <nl>  <nl> - while ( d < ( mp_digit ) ( 1 << ( MP_DIGIT_BIT - 1 ))) <nl> + while ( d < ( mp_digit ) (( mp_digit ) 1 << ( MP_DIGIT_BIT - 1 ))) <nl> { /* d < ( MP_RADIX / 2 ) */ <nl> d <<= 1 ; <nl> ++ k ;
XactLogCommitRecord ( TimestampTz commit_time , <nl> { <nl> XLogRegisterData (( char *) (& xl_twophase ), sizeof ( xl_xact_twophase )); <nl> if ( xl_xinfo . xinfo & XACT_XINFO_HAS_GID ) <nl> - XLogRegisterData (( char *) twophase_gid , strlen ( twophase_gid )); <nl> + XLogRegisterData (( char *) twophase_gid , strlen ( twophase_gid ) + 1 ); <nl> } <nl>  <nl> if ( xl_xinfo . xinfo & XACT_XINFO_HAS_ORIGIN )
* <nl> * 1998 Jan Wieck <nl> * <nl> - * $ Header : / cvsroot / pgsql / src / backend / utils / adt / numeric . c , v 1 . 57 2003 / 03 / 11 21 : 01 : 33 tgl Exp $ <nl> + * $ Header : / cvsroot / pgsql / src / backend / utils / adt / numeric . c , v 1 . 58 2003 / 03 / 14 00 : 15 : 32 tgl Exp $ <nl> * <nl> * ---------- <nl> */ <nl> ln_var ( NumericVar * arg , NumericVar * result ) <nl> break ; <nl>  <nl> add_var ( result , & elem , result ); <nl> + <nl> + if ( elem . weight < ( result -> weight - 2 * global_rscale )) <nl> + break ; <nl> } <nl>  <nl> /* Compensate for argument range reduction , round to caller ' s rscale */
* <nl> * <nl> * IDENTIFICATION <nl> - * $ PostgreSQL : pgsql / src / backend / utils / adt / float . c , v 1 . 130 2006 / 10 / 05 01 : 40 : 45 tgl Exp $ <nl> + * $ PostgreSQL : pgsql / src / backend / utils / adt / float . c , v 1 . 131 2006 / 12 / 23 02 : 13 : 24 momjian Exp $ <nl> * <nl> *------------------------------------------------------------------------- <nl> */ <nl> dtoi4 ( PG_FUNCTION_ARGS ) <nl> float8 num = PG_GETARG_FLOAT8 ( 0 ); <nl> int32 result ; <nl>  <nl> - if (( num < INT_MIN ) || ( num > INT_MAX )) <nl> + if ( num < INT_MIN || num > INT_MAX ) <nl> ereport ( ERROR , <nl> ( errcode ( ERRCODE_NUMERIC_VALUE_OUT_OF_RANGE ), <nl> errmsg (" integer out of range "))); <nl> dtoi2 ( PG_FUNCTION_ARGS ) <nl> float8 num = PG_GETARG_FLOAT8 ( 0 ); <nl> int16 result ; <nl>  <nl> - if (( num < SHRT_MIN ) || ( num > SHRT_MAX )) <nl> + if ( num < SHRT_MIN || num > SHRT_MAX ) <nl> ereport ( ERROR , <nl> ( errcode ( ERRCODE_NUMERIC_VALUE_OUT_OF_RANGE ), <nl> errmsg (" smallint out of range "))); <nl> ftoi4 ( PG_FUNCTION_ARGS ) <nl> float4 num = PG_GETARG_FLOAT4 ( 0 ); <nl> int32 result ; <nl>  <nl> - if (( num < INT_MIN ) || ( num > INT_MAX )) <nl> + if ( num < INT_MIN || num > INT_MAX ) <nl> ereport ( ERROR , <nl> ( errcode ( ERRCODE_NUMERIC_VALUE_OUT_OF_RANGE ), <nl> errmsg (" integer out of range "))); <nl> ftoi2 ( PG_FUNCTION_ARGS ) <nl> float4 num = PG_GETARG_FLOAT4 ( 0 ); <nl> int16 result ; <nl>  <nl> - if (( num < SHRT_MIN ) || ( num > SHRT_MAX )) <nl> + if ( num < SHRT_MIN || num > SHRT_MAX ) <nl> ereport ( ERROR , <nl> ( errcode ( ERRCODE_NUMERIC_VALUE_OUT_OF_RANGE ), <nl> errmsg (" smallint out of range ")));
* <nl> * <nl> * IDENTIFICATION <nl> - * $ Header : / cvsroot / pgsql / src / backend / commands / vacuum . c , v 1 . 176 2000 / 12 / 03 10 : 27 : 27 vadim Exp $ <nl> + * $ Header : / cvsroot / pgsql / src / backend / commands / vacuum . c , v 1 . 177 2000 / 12 / 08 06 : 43 : 44 inoue Exp $ <nl> * <nl> *------------------------------------------------------------------------- <nl> */ <nl> failed to add item with len = % lu to page % u ( free space % lu , nusd % u , noff % u )" <nl> nblocks , blkno , num_moved , <nl> show_rusage (& ru0 )); <nl>  <nl> + /* <nl> + * Reflect the motion of system tuples to catalog cache here . <nl> + */ <nl> + CommandCounterIncrement (); <nl> + <nl> if ( Nvacpagelist . num_pages > 0 ) <nl> { <nl> /* vacuum indices again if needed */
NEVER_INLINE void Marker :: sweep () { <nl> heap_ . iterate ( <nl> [&]( HeapObject * big , size_t big_size ) { // onBig <nl> if ( big -> kind () == HeaderKind :: BigObj ) { <nl> - big = static_cast < MallocNode *>( big ) + 1 ; <nl> - if (! marked ( big )) { <nl> - mm . freeBigSize ( big ); <nl> + HeapObject * h2 = static_cast < MallocNode *>( big ) + 1 ; <nl> + if (! marked ( h2 ) && h2 -> kind () != HeaderKind :: SmallMalloc ) { <nl> + mm . freeBigSize ( h2 ); <nl> } <nl> } <nl> },
namespace HPHP { namespace HHBBC { <nl>  <nl> namespace { <nl>  <nl> + namespace fs = boost :: filesystem ; <nl> + <nl> ////////////////////////////////////////////////////////////////////// <nl>  <nl> std :: string output_repo ; <nl> void compile_repo () { <nl> int main ( int argc , char ** argv ) try { <nl> parse_options ( argc , argv ); <nl>  <nl> - if ( boost :: filesystem :: exists ( output_repo )) { <nl> + if ( fs :: exists ( output_repo )) { <nl> std :: cout << " output repo already exists ; removing it \ n "; <nl> if ( unlink ( output_repo . c_str ())) { <nl> std :: cerr << " failed to unlink output repo : " <nl> int main ( int argc , char ** argv ) try { <nl> return 1 ; <nl> } <nl> } <nl> + if (! fs :: exists ( input_repo )) { <nl> + std :: cerr << " input repo `" << input_repo << "' not found \ n "; <nl> + return 1 ; <nl> + } <nl>  <nl> Hdf config ; <nl> IniSetting :: Map ini = IniSetting :: Map :: object ;
bool SSLSocket :: setupCrypto ( SSLSocket * session /* = NULL */) { <nl> break ; <nl> case CryptoMethod :: ClientTLS : <nl> m_data -> m_client = true ; <nl> - smethod = TLSv1_client_method (); <nl> + smethod = TLS_client_method (); <nl> break ; <nl> case CryptoMethod :: ServerSSLv23 : <nl> m_data -> m_client = false ; <nl> bool SSLSocket :: setupCrypto ( SSLSocket * session /* = NULL */) { <nl>  <nl> case CryptoMethod :: ServerTLS : <nl> m_data -> m_client = false ; <nl> - smethod = TLSv1_server_method (); <nl> + smethod = TLS_server_method (); <nl> break ; <nl> default : <nl> return false ;
Variant HHVM_FUNCTION ( mcrypt_ofb , const String & cipher , const String & key , <nl> } <nl>  <nl> Variant HHVM_FUNCTION ( mcrypt_get_block_size , const String & cipher , <nl> - const Variant & module /* = null_string */) { <nl> + const String & mode ) { <nl> MCRYPT td = mcrypt_module_open (( char *) cipher . data (), <nl> ( char *) MCG ( algorithms_dir ). data (), <nl> - ( char *) module . asCStrRef (). data (), <nl> + ( char *) mode . data (), <nl> ( char *) MCG ( modes_dir ). data ()); <nl> if ( td == MCRYPT_FAILED ) { <nl> MCRYPT_OPEN_MODULE_FAILED (" mcrypt_get_block_size ");
struct SinkPointAnalyzer : private LocalStateHook { <nl>  <nl> assertCanConsume ( value ); <nl>  <nl> + // Drop one IncRef right before a DecRef , so that we don ' t end up <nl> + // missing opportunities because we sunk IncRefs too late . <nl> + if ( m_inst -> is ( DecRef , DecRefNZ ) && valState . optDelta () > 0 ) { <nl> + placeSinkPoint ( value , valState , sinkPoint ); <nl> + } <nl> + <nl> // Note that we ' re treating consumers and observers the same here , which is <nl> // necessary until we have better alias analysis . <nl> observeValue ( value , valState , sinkPoint );
String string_chunk_split ( const char * src , int srclen , const char * end , <nl> int chunks = srclen / chunklen ; // complete chunks ! <nl> int restlen = srclen - chunks * chunklen ; /* srclen % chunklen */ <nl>  <nl> - int out_len = ( chunks + 1 ) * endlen + srclen ; <nl> - String ret ( out_len , ReserveString ); <nl> + String ret ( <nl> + safe_address ( <nl> + chunks + 1 , <nl> + endlen , <nl> + srclen <nl> + ), <nl> + ReserveString <nl> + ); <nl> char * dest = ret . bufferSlice (). ptr ; <nl>  <nl> const char * p ; char * q ;
void AsioContext :: runUntil ( c_WaitableWaitHandle * wait_handle ) { <nl> auto current = m_queue_ready . front (); <nl> m_queue_ready . pop (); <nl> m_current = current ; <nl> + <nl> + auto run_finished_guard = folly :: makeGuard ([&] { <nl> + m_current = nullptr ; <nl> + decRefObj ( current ); <nl> + }); <nl> + <nl> m_current -> run (); <nl> - m_current = nullptr ; <nl> - decRefObj ( current ); <nl>  <nl> if ( wait_handle -> isFinished ()) { <nl> return ;
rewrite : <nl> newbuffer += t ; <nl> } <nl> * newbuffer = '\ 0 '; <nl> - out = out . substr ( 0 , newbuffer - out . data ()); <nl> + out . setSize ( newbuffer - out . data ()); <nl>  <nl> ret = 1 ; <nl> goto clean_up ; <nl> clean_up : <nl> while ( placeholders ) { <nl> plc = placeholders ; <nl> placeholders = plc -> next ; <nl> + plc -> quoted . reset (); <nl> free ( plc ); <nl> } <nl> 
void vtrace ( const char * fmt , va_list ap ) { <nl> if ( moduleEnabledRelease ( Trace :: ringbuffer , 1 )) { <nl> vtraceRingbuffer ( fmt , ap ); <nl> } else { <nl> - vfprintf ( out , fmt , ap ); <nl> ONTRACE ( 1 , pthread_mutex_lock (& mtx )); <nl> ONTRACE ( 1 , fprintf ( out , " t %# 08x : ", int (( int64 ) pthread_self () & 0xFFFFFFFF ))); <nl> + vfprintf ( out , fmt , ap ); <nl> ONTRACE ( 1 , pthread_mutex_unlock (& mtx )); <nl> flush (); <nl> }
/* String - Class */ <nl> /* ======================================================================= */ <nl>  <nl> +# include < string . h > <nl> + <nl> /* <nl> inline void rtl_str_ImplCopy ( IMPL_RTL_STRCODE * pDest , <nl> const IMPL_RTL_STRCODE * pSrc , <nl> void SAL_CALL IMPL_RTL_STRINGNAME ( new_WithLength )( IMPL_RTL_STRINGDATA ** ppThi <nl>  <nl> { <nl> IMPL_RTL_STRCODE * pTempStr = (* ppThis )-> buffer ; <nl> - while ( nLen >= 0 ) <nl> - { <nl> - * pTempStr = 0 ; <nl> - pTempStr ++; <nl> - nLen --; <nl> - } <nl> + memset ( pTempStr , 0 , nLen * sizeof ( IMPL_RTL_STRCODE )); <nl> } <nl> } <nl> }
sal_Bool SAL_CALL osl_assertFailedLine ( <nl>  <nl> /* output backtrace */ <nl> char const * envBacktrace = getenv ( " SAL_DIAGNOSE_BACKTRACE " ); <nl> - if ( envBacktrace != NULL && * envBacktrace != '\ 0 ' ); <nl> + if ( envBacktrace != NULL && * envBacktrace != '\ 0 ' ) <nl> osl_diagnose_backtrace_Impl ( f ); <nl>  <nl> /* release lock and leave */
SAL_DLLPUBLIC void SAL_CALL rtl_uString_newReplaceAllAsciiL ( <nl> @ param to pointer to the replacing substring ; must not be null and must <nl> point to memory of at least \ p toLength ASCII bytes <nl>  <nl> - @ param fromLength the length of the \ p to substring ; must be non - negative <nl> + @ param toLength the length of the \ p to substring ; must be non - negative <nl>  <nl> @ since LibreOffice 5 . 1 <nl> */
defendel ( char * name , int delete ) <nl> if ( dp -> repl != NULL ) /* Free the replacement */ <nl> free ( dp -> repl ); /* if any , and then */ <nl> free (( char *) dp ); /* Free the symbol */ <nl> + dp = NULL ; <nl> } <nl> break ; <nl> }
do { while ( optind < Argc && Legacy ( optind )) {} <nl> switch ( opC ) <nl> { case OpCksum : defCks ( optarg ); <nl> break ; <nl> + case OpCoerce : OpSpec |= DoCoerce ; <nl> + break ; <nl> case OpDebug : OpSpec |= DoDebug ; <nl> if (! a2i ( optarg , & Dlvl , 0 , 3 )) Usage ( 22 ); <nl> break ;
int doCp_xrd2xrd ( XrdClient ** xrddest , const char * src , const char * dst ) { <nl> cout << endl ; <nl> } <nl>  <nl> - if (( unsigned ) cpnfo . len != bytesread ) retvalue = 13 ; <nl> + if ( cpnfo . len != bytesread ) retvalue = 13 ; <nl>  <nl> # ifdef HAVE_XRDCRYPTO <nl> if ( md5 ) MD_5 -> Final (); <nl> int doCp_xrd2loc ( const char * src , const char * dst ) { <nl> cout << endl ; <nl> } <nl>  <nl> - if (( unsigned ) cpnfo . len != bytesread ) retvalue = 13 ; <nl> + if ( cpnfo . len != bytesread ) retvalue = 13 ; <nl>  <nl> # ifdef HAVE_XRDCRYPTO <nl> if ( md5 ) MD_5 -> Final ();
static int show_channel ( channel_t * chptr , struct alis_query * query ) <nl> if ( query -> mode_dir == DIR_SET ) <nl> { <nl> if ((( chptr -> modes & query -> mode ) != query -> mode ) || <nl> - ( query -> mode_key && chptr -> key [ 0 ] == '\ 0 ') || <nl> + ( query -> mode_key && chptr -> key == NULL ) || <nl> ( query -> mode_limit && ! chptr -> limit )) <nl> return 0 ; <nl> for ( i = 0 ; ignore_mode_list [ i ]. mode != '\ 0 '; i ++) <nl> static int show_channel ( channel_t * chptr , struct alis_query * query ) <nl> else if ( query -> mode_dir == DIR_UNSET ) <nl> { <nl> if (( chptr -> modes & query -> mode ) || <nl> - ( query -> mode_key && chptr -> key [ 0 ] != '\ 0 ') || <nl> + ( query -> mode_key && chptr -> key != NULL ) || <nl> ( query -> mode_limit && chptr -> limit )) <nl> return 0 ; <nl> for ( i = 0 ; ignore_mode_list [ i ]. mode != '\ 0 '; i ++) <nl> static int show_channel ( channel_t * chptr , struct alis_query * query ) <nl> } <nl> else if ( query -> mode_dir == DIR_EQUAL ) <nl> { <nl> - if (( chptr -> modes != query -> mode ) || <nl> - ( query -> mode_key && chptr -> key [ 0 ] == '\ 0 ') || <nl> + if ((( chptr -> modes & ~( CMODE_LIMIT | CMODE_KEY )) != query -> mode ) || <nl> + ( query -> mode_key && chptr -> key == NULL ) || <nl> ( query -> mode_limit && ! chptr -> limit )) <nl> return 0 ; <nl> for ( i = 0 ; ignore_mode_list [ i ]. mode != '\ 0 '; i ++)
connection_t * connection_open_tcp ( char * host , char * vhost , unsigned int port , <nl>  <nl> if ( vhost != NULL ) <nl> { <nl> - struct addrinfo * bind_addr = NULL ; <nl> + struct addrinfo hints , * bind_addr = NULL ; <nl>  <nl> - if (( error = getaddrinfo ( vhost , NULL , NULL , & bind_addr ))) <nl> + memset (& hints , 0 , sizeof hints ); <nl> + hints . ai_family = addr -> ai_family ; <nl> + hints . ai_socktype = SOCK_STREAM ; <nl> + hints . ai_flags = AI_PASSIVE ; <nl> + if (( error = getaddrinfo ( vhost , NULL , & hints , & bind_addr ))) <nl> { <nl> slog ( LG_ERROR , " connection_open_tcp (): cant resolve vhost % s : % s ", vhost , gai_strerror ( error )); <nl> close ( s );
static myuser_t * login_user ( sasl_session_t * p ) <nl> /* source_mu is the user whose credentials we verified (" authentication id ") */ <nl> /* target_mu is the user who will be ultimately logged in (" authorization id ") */ <nl>  <nl> - source_mu = myuser_find ( p -> username ); <nl> + source_mu = myuser_find_by_nick ( p -> username ); <nl> if ( source_mu == NULL ) <nl> return NULL ; <nl>  <nl> if ( p -> authzid && * p -> authzid ) <nl> { <nl> - target_mu = myuser_find ( p -> authzid ); <nl> + target_mu = myuser_find_by_nick ( p -> authzid ); <nl> if ( target_mu == NULL ) <nl> return NULL ; <nl> }
static void inspircd_topic_sts ( channel_t * c , const char * setter , time_t ts , time <nl> /* Restoring old topic */ <nl> if ( ts > prevts + 60 || prevts == 0 ) <nl> { <nl> - sts (":% s FTOPIC % s % ld % s :% s ", chansvs . nick , c -> name , ts , setter , topic ); <nl> + sts (":% s FTOPIC % s % ld % s :% s ", chansvs . me -> me -> uid , c -> name , ts , setter , topic ); <nl> return ; <nl> } <nl> /* Tweaking a topic */ <nl> else if ( ts == prevts ) <nl> { <nl> ts += 60 ; <nl> - sts (":% s FTOPIC % s % ld % s :% s ", chansvs . nick , c -> name , ts , setter , topic ); <nl> + sts (":% s FTOPIC % s % ld % s :% s ", chansvs . me -> me -> uid , c -> name , ts , setter , topic ); <nl> c -> topicts = ts ; <nl> return ; <nl> } <nl> - sts (":% s TOPIC % s :% s ", chansvs . nick , c -> name , topic ); <nl> + sts (":% s TOPIC % s :% s ", chansvs . me -> me -> uid , c -> name , topic ); <nl> c -> topicts = CURRTIME ; <nl> } <nl> 
PHP_FUNCTION ( swoole_server_sendfile ) <nl> memcpy ( buffer , filename , send_data . info . len ); <nl> buffer [ send_data . info . len ] = 0 ; <nl> send_data . info . len ++; <nl> + send_data . length = 0 ; <nl>  <nl> send_data . data = buffer ; <nl> SW_CHECK_RETURN ( serv -> factory . finish (& serv -> factory , & send_data ));
void swTimer_node_insert ( swTimer_node ** root , swTimer_node * new_node ) <nl> swTimer_node * tmp = * root ; <nl> while ( 1 ) <nl> { <nl> - if ( tmp -> exec_msec >= new_node -> exec_msec ) <nl> + if ( tmp -> exec_msec > new_node -> exec_msec ) <nl> { <nl> new_node -> prev = tmp -> prev ; <nl> new_node -> next = tmp ;
* the resulting executable , without including the source code for OpenSSL in <nl> * the source distribution . <nl> * <nl> - * $ Id : mod_sql . c , v 1 . 124 2007 - 02 - 15 17 : 01 : 19 castaglia Exp $ <nl> + * $ Id : mod_sql . c , v 1 . 125 2007 - 06 - 12 00 : 59 : 39 castaglia Exp $ <nl> */ <nl>  <nl> # include " conf . h " <nl> MODRET cmd_check ( cmd_rec * cmd ) { <nl> auth_entry -> name ); <nl> success = 1 ; <nl> break ; <nl> + <nl> + } else { <nl> + sql_log ( DEBUG_AUTH , "'% s ' auth handler reports failure ", <nl> + auth_entry -> name ); <nl> } <nl> } <nl> }
class ModuleNickLock : public Module <nl>  <nl> virtual int OnUserPreNick ( userrec * user , const std :: string & newnick ) <nl> { <nl> + if ( isdigit ( newnick [ 0 ])) /* allow a switch to a UID */ <nl> + return 0 ; <nl> + <nl> if ( user -> GetExt (" nick_locked ", n )) <nl> { <nl> user -> WriteServ (" 447 % s : You cannot change your nickname ( your nick is locked )", user -> nick );
private : <nl> */ <nl> char ibuf [ 16384 ]; <nl>  <nl> + /** <nl> + * The output buffer for this socket <nl> + */ <nl> + std :: string Buffer ; <nl> + <nl> /** <nl> * The IP address being connected <nl> * to stored in string form for
int InspIRCd :: Run () <nl>  <nl> int main ( int argc , char ** argv ) <nl> { <nl> - InspIRCd TittyBiscuits = new InspIRCd ( argc , argv ); <nl> + InspIRCd * TittyBiscuits = new InspIRCd ( argc , argv ); <nl> TittyBiscuits -> Run (); <nl> delete TittyBiscuits ; <nl> return 0 ;
class ModuleSilence : public Module <nl> int MatchPattern ( User * dest , User * source , int pattern ) <nl> { <nl> /* Server source */ <nl> - if (! source ) <nl> + if (! source || ! dest ) <nl> return 1 ; <nl>  <nl> silencelist * sl ;
CmdResult CommandKick :: Handle ( const char * const * parameters , int pcnt , User * us <nl> Channel * c = ServerInstance -> FindChan ( parameters [ 0 ]); <nl> User * u = ServerInstance -> FindNick ( parameters [ 1 ]); <nl>  <nl> + if ( ServerInstance -> Parser -> LoopCall ( user , this , parameters , pcnt , 1 )) <nl> + return CMD_SUCCESS ; <nl> + <nl> if (! u || ! c ) <nl> { <nl> user -> WriteServ ( " 401 % s % s : No such nick / channel ", user -> nick , u ? parameters [ 0 ] : parameters [ 1 ]);
std :: string userrec :: ProcessNoticeMasks ( const char * sm ) <nl> { <nl> if ((! IsNoticeMaskSet (* c ) && adding ) || ( IsNoticeMaskSet (* c ) && ! adding )) <nl> { <nl> - if (( oldadding != adding ) || ( sm == c )) <nl> + if (( oldadding != adding ) || ( sm == ( c + 1 ))) <nl> output += ( adding ? '+' : '-'); <nl>  <nl> this -> SetNoticeMask (* c , adding );
bool IsDenied ( userrec * user ) <nl>  <nl> void handle_pass ( char ** parameters , int pcnt , userrec * user ) <nl> { <nl> + // Check to make sure they havnt registered -- Fix by FCS <nl> + if ( user -> registered == 7 ) <nl> + { <nl> + WriteServ ( user -> fd ," 462 % s : You may not reregister ", user -> nick ); <nl> + return ; <nl> + } <nl> if (! strcasecmp ( parameters [ 0 ], Passwd ( user ))) <nl> { <nl> user -> haspassed = true ;
NativeWindowGtk :: NativeWindowGtk ( content :: WebContents * web_contents , <nl> options -> GetInteger ( switches :: kWidth , & width ); <nl> options -> GetInteger ( switches :: kHeight , & height ); <nl>  <nl> - // Fixup the initial window size . <nl> - if ( has_frame_ ) <nl> + bool use_content_size = false ; <nl> + options -> GetBoolean ( switches :: kUseContentSize , & use_content_size ); <nl> + if ( has_frame_ && ! use_content_size ) <nl> SubstractBorderSize (& width , & height ); <nl>  <nl> // Force a size allocation so the web page of hidden window can have correct
Sys_PIDFileName <nl> */ <nl> static char * Sys_PIDFileName ( void ) <nl> { <nl> - const char * homePath = Sys_DefaultHomePath ( ); <nl> + const char * homePath = Cvar_VariableString ( " fs_homepath " ); <nl>  <nl> if ( * homePath != '\ 0 ' ) <nl> return va ( "% s /% s ", homePath , PID_FILENAME );
void RE_RegisterFont ( const char * fontName , int pointSize , fontInfo_t * font ) { <nl> font -> glyphs [ i ]. s2 = readFloat (); <nl> font -> glyphs [ i ]. t2 = readFloat (); <nl> font -> glyphs [ i ]. glyph = readInt (); <nl> - Com_Memcpy ( font -> glyphs [ i ]. shaderName , & fdFile [ fdOffset ], 32 ); <nl> - fdOffset += 32 ; <nl> + Q_strncpyz ( font -> glyphs [ i ]. shaderName , ( const char *)& fdFile [ fdOffset ], sizeof ( font -> glyphs [ i ]. shaderName )); <nl> + fdOffset += sizeof ( font -> glyphs [ i ]. shaderName ); <nl> } <nl> font -> glyphScale = readFloat (); <nl> Com_Memcpy ( font -> name , & fdFile [ fdOffset ], MAX_QPATH );
static void dump_ctx ( struct ctx * ctx ) { <nl> * Values <nl> */ <nl> static void print_tree ( FILE * out , int indent , struct tree * tree ) { <nl> + if ( tree == NULL ) { <nl> + fprintf ( out , "( null tree )\ n "); <nl> + return ; <nl> + } <nl> list_for_each ( t , tree ) { <nl> for ( int i = 0 ; i < indent ; i ++) fputc (' ', out ); <nl> fprintf ( out , "{ ");
static int determinize ( struct fa * fa , struct state_set * ini ) { <nl> E ( points == NULL ); <nl> if ( make_ini ) { <nl> ini = state_set_init (- 1 , S_NONE ); <nl> - if ( ini == NULL || state_set_push ( ini , fa -> initial ) < 0 ) <nl> + if ( ini == NULL || state_set_push ( ini , fa -> initial ) < 0 ) { <nl> + state_set_free ( ini ); <nl> goto error ; <nl> + } <nl> } <nl>  <nl> F ( state_set_list_add (& worklist , ini ));
int main ( int argc , char ** argv ) { <nl> } <nl> } <nl> putchar ('\ n '); <nl> + free ( rx ); <nl> } <nl>  <nl> return 0 ;
namespace tnt <nl> explicit HttpReply ( std :: ostream & s , bool sendStatusLine = true ); <nl>  <nl> void setContentType ( const char * t ) { setHeader ( httpheader :: contentType , t ); } <nl> + void setContentType ( const std :: string & t ) { setHeader ( httpheader :: contentType , t ); } <nl> const char * getContentType () const { return getHeader ( httpheader :: contentType ); } <nl>  <nl> void setHeadRequest ( bool sw = true ) { headRequest = sw ; }
gen_assignment ( codegen_scope * s , node * tree , node * rhs , int sp , int val ) <nl> } <nl> } <nl> if ( tree -> cdr -> car ) { /* keyword arguments */ <nl> - if ( n == 14 ) { <nl> + if ( n == 13 || n == 14 ) { <nl> pop_n ( n ); <nl> genop_2 ( s , OP_ARRAY , cursp (), n ); <nl> push ();
mrb_define_method_raw ( mrb_state * mrb , struct RClass * c , mrb_sym mid , struct RPro <nl> k = kh_put ( mt , mrb , h , mid ); <nl> kh_value ( h , k ) = p ; <nl> if ( p ) { <nl> + p -> c = NULL ; <nl> mrb_field_write_barrier ( mrb , ( struct RBasic *) c , ( struct RBasic *) p ); <nl> } <nl> }
main ( int argc , char ** argv ) <nl> } <nl>  <nl> mrb = mrb_open (); <nl> + if ( mrb == NULL ) { <nl> + fputs (" Invalid mrb_state , exiting mruby - strip \ n ", stderr ); <nl> + return EXIT_FAILURE ; <nl> + } <nl>  <nl> ireps = ( mrb_irep **) malloc ( sizeof ( mrb_irep *) * argc ); <nl> for ( i = args_result ; i < argc ; ++ i ) {
fiber_resume ( mrb_state * mrb , mrb_value self ) <nl> return fiber_switch ( mrb , self , len , a , TRUE ); <nl> } <nl>  <nl> +/* resume thread with given arguments */ <nl> + MRB_API mrb_value <nl> + mrb_fiber_resume ( mrb_state * mrb , mrb_value fib , mrb_int len , const mrb_value * a ) <nl> +{ <nl> + return fiber_switch ( mrb , fib , len , a , TRUE ); <nl> +} <nl> + <nl> /* <nl> * call - seq : <nl> * fiber . alive ? -> true or false
mrb_f_send ( mrb_state * mrb , mrb_value self ) <nl> regs = mrb -> c -> ci -> stack + 1 ; <nl>  <nl> if ( n == 0 ) { <nl> + argnum_error : <nl> mrb_argnum_error ( mrb , 0 , 1 , - 1 ); <nl> } <nl> else if ( n == 15 ) { <nl> + if ( RARRAY_LEN ( regs [ 0 ]) == 0 ) goto argnum_error ; <nl> name = mrb_obj_to_sym ( mrb , RARRAY_PTR ( regs [ 0 ])[ 0 ]); <nl> } <nl> else {
mrb_read_irep_file ( mrb_state * mrb , FILE * fp ) <nl> size_t sirep ; <nl> struct rite_section_header section_header ; <nl> long fpos ; <nl> - const size_t block_size = 1 << 14 ; <nl> + size_t block_size = 1 << 14 ; <nl> + const uint8_t block_fallback_count = 4 ; <nl> + int i ; <nl> const size_t buf_size = sizeof ( struct rite_binary_header ); <nl>  <nl> if (( mrb == NULL ) || ( fp == NULL )) { <nl> mrb_read_irep_file ( mrb_state * mrb , FILE * fp ) <nl> /* verify CRC */ <nl> fpos = ftell ( fp ); <nl> /* You don ' t need use SIZE_ERROR as block_size is enough small . */ <nl> - buf = mrb_malloc ( mrb , block_size ); <nl> + for ( i = 0 ; i < block_fallback_count ; i ++, block_size >>= 1 ){ <nl> + buf = mrb_malloc ( mrb , block_size ); <nl> + if ( buf ) break ; <nl> + } <nl> if (! buf ) { <nl> return MRB_DUMP_GENERAL_FAILURE ; <nl> }
mrb_str_modify ( mrb_state * mrb , struct RString * s ) <nl> if ( RSTR_SHARED_P ( s )) { <nl> mrb_shared_string * shared = s -> as . heap . aux . shared ; <nl>  <nl> - if ( shared -> refcnt == 1 && s -> as . heap . ptr == shared -> ptr ) { <nl> + if ( shared -> nofree == 0 && shared -> refcnt == 1 && s -> as . heap . ptr == shared -> ptr ) { <nl> s -> as . heap . ptr = shared -> ptr ; <nl> s -> as . heap . aux . capa = shared -> len ; <nl> RSTR_PTR ( s )[ s -> as . heap . len ] = '\ 0 ';
RETRY_TRY_BLOCK : <nl> } <nl> } <nl> L_RESCUE : <nl> - irep = ci -> proc -> body . irep ; <nl> + proc = ci -> proc ; <nl> + irep = proc -> body . irep ; <nl> pool = irep -> pool ; <nl> syms = irep -> syms ; <nl> regs = mrb -> c -> stack = ci [ 1 ]. stackent ;
RETRY_TRY_BLOCK : <nl> } <nl> else if ( target_class -> tt == MRB_TT_MODULE ) { <nl> target_class = mrb_vm_ci_target_class ( ci ); <nl> - if ( target_class -> tt != MRB_TT_ICLASS ) { <nl> + if (! target_class || target_class -> tt != MRB_TT_ICLASS ) { <nl> goto super_typeerror ; <nl> } <nl> }
void process_packet_tail ( struct msg_digest * md ) <nl> "% smessage ignored because it contains a payload type (% s ) unexpected by state % s ", <nl> excuse , <nl> enum_show (& ikev1_payload_names , np ), <nl> - st -> st_state -> name ); <nl> + finite_states [ smc -> state ]-> name ); <nl> if (! md -> encrypted ) { <nl> SEND_NOTIFICATION ( INVALID_PAYLOAD_TYPE ); <nl> }
bool load_cert_from_nss ( const char * nssHostCertNickName , <nl> nssHostCertNickName )); <nl> } <nl>  <nl> - <nl> blob . len = nssCert -> derCert . len ; <nl> blob . ptr = alloc_bytes ( blob . len , label ); <nl> memcpy ( blob . ptr , nssCert -> derCert . data , blob . len ); <nl> bool load_cert_from_nss ( const char * nssHostCertNickName , <nl> if (! is_asn1 ( blob )) { <nl> if ( verbose ) <nl> libreswan_log (" cert read from NSS db is not in DER format "); <nl> + pfree ( blob . ptr ); <nl> + /* failure */ <nl> } else { <nl> DBG ( DBG_PARSING , <nl> DBG_log (" file coded in DER format ")); <nl> bool load_cert_from_nss ( const char * nssHostCertNickName , <nl> } <nl> } <nl>  <nl> - /* failure */ <nl> - pfree ( blob . ptr ); <nl> return FALSE ; <nl> } <nl> 
aggr_inI1_outR1_continue1 ( struct pluto_crypto_req_cont * pcrc <nl> /* unpack first calculation */ <nl> unpack_KE ( st , r , & st -> st_gr ); <nl>  <nl> + /* unpack nonce too */ <nl> + unpack_nonce (& st -> st_nr , r ); <nl> + <nl> /* NOTE : the " r " reply will get freed by our caller */ <nl>  <nl> /* set up second calculation */
rsasigkey ( int nbits , char * configdir , char * password ) <nl> mpz_init ( n ); <nl> mpz_init ( e ); <nl>  <nl> - pwdata . source = password ? PW_PLAINTEXT : PW_NONE ; <nl> - pwdata . data = password ? password : NULL ; <nl> - <nl> do { <nl> if (! configdir ) { <nl> fprintf ( stderr , "% s : configdir is required \ n ", me ); <nl> return ; <nl> } <nl>  <nl> + snprintf ( buf , sizeof ( buf ), "% s / nsspassword ", configdir ); <nl> + pwdata . source = password ? ( strcmp ( password , buf )? PW_PLAINTEXT : PW_FROMFILE ) : PW_NONE ; <nl> + pwdata . data = password ? password : NULL ; <nl> + <nl> PR_Init ( PR_USER_THREAD , PR_PRIORITY_NORMAL , 1 ); <nl> snprintf ( buf , sizeof ( buf ), "% s ", configdir ); <nl> if (( rv = NSS_InitReadWrite ( buf )) != SECSuccess ) {
static inline void pcr_init ( struct pluto_crypto_req * r <nl> , enum pluto_crypto_requests pcr_type <nl> , enum crypto_importance pcr_pcim ) <nl> { <nl> - memset ( r , 0 , sizeof ( r )); <nl> + memset ( r , 0 , sizeof (* r )); <nl> r -> pcr_len = sizeof ( struct pluto_crypto_req ); <nl> r -> pcr_type = pcr_type ; <nl> r -> pcr_pcim = pcr_pcim ;
main ( int argc , char ** argv ) <nl> capng_updatev ( CAPNG_ADD , CAPNG_EFFECTIVE | CAPNG_PERMITTED , <nl> CAP_NET_BIND_SERVICE , CAP_NET_ADMIN , CAP_NET_RAW , <nl> CAP_IPC_LOCK , - 1 ); <nl> + /* our children must be able to CAP_NET_ADMIN to change routes . <nl> + */ <nl> + capng_updatev ( CAPNG_ADD , CAPNG_BOUNDING_SET , <nl> + CAP_NET_ADMIN , - 1 ); <nl> capng_apply ( CAPNG_SELECT_BOTH ); <nl> # endif <nl> 
bool ike_alg_enc_ok ( int ealg , unsigned key_len , <nl> /* <nl> * test # 2 : if key_len specified , it must be in range <nl> */ <nl> - if (( key_len ) && (( key_len < enc_desc -> keyminlen ) || <nl> - ( key_len > enc_desc -> keymaxlen ))) { <nl> + if (( key_len ) && (( key_len <= enc_desc -> keyminlen ) || <nl> + ( key_len >= enc_desc -> keymaxlen ))) { <nl> snprintf ( errbuf , sizeof ( errbuf )- 1 , <nl> " key_len not in range : encalg =% d , " <nl> " key_len =% d , keyminlen =% d , keymaxlen =% d ",
cherokee_config_node_add ( cherokee_config_node_t * conf , const char * key , cheroke <nl> } <nl>  <nl> if ( final ) { <nl> + cherokee_buffer_clean (& child -> val ); <nl> cherokee_buffer_add_buffer (& child -> val , val ); <nl> } <nl> 
parse_x_real_ip ( cherokee_logger_t * logger , cherokee_connection_t * conn ) <nl> } <nl>  <nl> p = val ; <nl> - while (* p ) { <nl> + while (* p && ( p - val < len )) { <nl> if ((* p == ' ') || (* p == ',')) { <nl> len = p - val ; <nl> break ;
do_spawn ( void ) <nl> } <nl>  <nl> interpreter = malloc ( sizeof (" exec ") + size ); <nl> - memcpy ( interpreter , " exec ", 5 ); <nl> - memcpy ( interpreter + 5 , p , size + 1 ); <nl> + strncpy ( interpreter , " exec ", 5 ); <nl> + strncpy ( interpreter + 5 , p , size + 1 ); <nl> p += size + 1 ; <nl> ALIGN4 ( p ); <nl> 
ExprResolveLhs ( struct xkb_context * ctx , const ExprDef * expr , <nl> * elem_rtrn = xkb_atom_text ( ctx , expr -> field_ref . element ); <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> field_ref . field ); <nl> * index_rtrn = NULL ; <nl> - return true ; <nl> + return (* elem_rtrn != NULL && * field_rtrn != NULL ); <nl> case EXPR_ARRAY_REF : <nl> * elem_rtrn = xkb_atom_text ( ctx , expr -> array_ref . element ); <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> array_ref . field ); <nl> * index_rtrn = expr -> array_ref . entry ; <nl> + if ( expr -> array_ref . element != XKB_ATOM_NONE && * elem_rtrn == NULL ) <nl> + return false ; <nl> + if (* field_rtrn == NULL ) <nl> + return false ; <nl> return true ; <nl> default : <nl> break ;
skip_more_whitespace_and_comments : <nl>  <nl> /* LHS Keysym . */ <nl> if ( chr ( s , '<')) { <nl> - while ( peek ( s ) != '>' && ! eol ( s )) <nl> + while ( peek ( s ) != '>' && ! eol ( s ) && ! eof ( s )) <nl> buf_append ( s , next ( s )); <nl> if (! chr ( s , '>')) { <nl> scanner_err ( s , " unterminated keysym literal ");
ResolveStateAndPredicate ( ExprDef * expr , enum xkb_match_operation * pred_rtrn , <nl> * pred_rtrn = MATCH_EXACTLY ; <nl> if ( expr -> expr . op == EXPR_ACTION_DECL ) { <nl> const char * pred_txt = xkb_atom_text ( info -> ctx , expr -> action . name ); <nl> - if (! LookupString ( symInterpretMatchMaskNames , pred_txt , pred_rtrn )) { <nl> + if (! LookupString ( symInterpretMatchMaskNames , pred_txt , pred_rtrn ) || <nl> + ! expr -> action . args ) { <nl> log_err ( info -> ctx , <nl> " Illegal modifier predicate \"% s \"; Ignored \ n ", pred_txt ); <nl> return false ;
ExprResolveLhs ( struct xkb_context * ctx , const ExprDef * expr , <nl> * elem_rtrn = NULL ; <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> ident . ident ); <nl> * index_rtrn = NULL ; <nl> - return true ; <nl> + return (* field_rtrn != NULL ); <nl> case EXPR_FIELD_REF : <nl> * elem_rtrn = xkb_atom_text ( ctx , expr -> field_ref . element ); <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> field_ref . field );
TEST ( dlfcn , dt_runpath_smoke ) { <nl> // Bionic specific tests <nl> # if defined ( __BIONIC__ ) <nl>  <nl> -# if defined ( __arm__ ) || defined ( __i386__ ) <nl> +# if defined ( __arm__ ) <nl> const llvm :: ELF :: Elf32_Dyn * to_dynamic_table ( const char * p ) { <nl> return reinterpret_cast < const llvm :: ELF :: Elf32_Dyn *>( p ); <nl> } <nl> void validate_compatibility_of_native_library ( const char * soname ) { <nl> validate_compatibility_of_native_library ( path , elf ); <nl> } <nl>  <nl> -// This is a test for app compatibility workaround for arm and x86 apps <nl> +// This is a test for app compatibility workaround for arm apps <nl> // affected by http :// b / 24465209 <nl> TEST ( dlext , compat_elf_hash_and_relocation_tables ) { <nl> validate_compatibility_of_native_library (" libc . so "); <nl> TEST ( dlext , compat_elf_hash_and_relocation_tables ) { <nl> validate_compatibility_of_native_library (" libjnigraphics . so "); <nl> } <nl>  <nl> -# endif // defined ( __arm__ ) || defined ( __i386__ ) <nl> +# endif // defined ( __arm__ ) <nl>  <nl> TEST ( dlfcn , dt_runpath_absolute_path ) { <nl> std :: string libpath = get_testlib_root () + "/ libtest_dt_runpath_d . so ";
__BEGIN_DECLS <nl>  <nl> typedef struct { <nl> - volatile unsigned int count ; <nl> + volatile unsigned int count ; <nl> +# ifdef __LP64__ <nl> + int __reserved [ 3 ]; <nl> +# endif <nl> } sem_t ; <nl>  <nl> -# define SEM_FAILED NULL <nl> +# define SEM_FAILED NULL <nl>  <nl> extern int sem_init ( sem_t * sem , int pshared , unsigned int value ); <nl> 
TEST ( linker_memory , test_alloc_0 ) { <nl> LinkerMemoryAllocator allocator ; <nl> void * ptr = allocator . alloc ( 0 ); <nl> ASSERT_TRUE ( ptr != nullptr ); <nl> - free ( ptr ); <nl> + allocator . free ( ptr ); <nl> } <nl>  <nl> TEST ( linker_memory , test_free_nullptr ) { <nl> TEST ( linker_memory , test_realloc ) { <nl>  <nl> ASSERT_TRUE ( memcmp ( reallocated_ptr , model , 4000 ) == 0 ); <nl>  <nl> - ASSERT_EQ ( nullptr , realloc ( reallocated_ptr , 0 )); <nl> + ASSERT_EQ ( nullptr , allocator . realloc ( reallocated_ptr , 0 )); <nl> } <nl>  <nl> TEST ( linker_memory , test_small_smoke ) {
void * mmap64 ( void * addr , size_t size , int prot , int flags , int fd , off64_t offse <nl>  <nl> // prevent allocations large enough for ` end - start ` to overflow <nl> size_t rounded = BIONIC_ALIGN ( size , PAGE_SIZE ); <nl> - if ( rounded < size || size > PTRDIFF_MAX ) { <nl> + if ( rounded < size || rounded > PTRDIFF_MAX ) { <nl> errno = ENOMEM ; <nl> return MAP_FAILED ; <nl> }
typedef struct ucontext { <nl> stack_t uc_stack ; <nl> mcontext_t uc_mcontext ; <nl> sigset_t uc_sigmask ; <nl> + char __padding [ 128 - sizeof ( sigset_t )]; <nl> } ucontext_t ; <nl>  <nl> # elif defined ( __mips64__ )
void FrameFactory :: rebuildAggregateFrames ( ID3v2 :: Tag * tag ) const <nl> tag -> frameList (" TDAT "). size () == 1 ) <nl> { <nl> TextIdentificationFrame * tdrc = <nl> - static_cast < TextIdentificationFrame *>( tag -> frameList (" TDRC "). front ()); <nl> + dynamic_cast < TextIdentificationFrame *>( tag -> frameList (" TDRC "). front ()); <nl> UnknownFrame * tdat = static_cast < UnknownFrame *>( tag -> frameList (" TDAT "). front ()); <nl>  <nl> - if ( tdrc -> fieldList (). size () == 1 && <nl> + if ( tdrc && <nl> + tdrc -> fieldList (). size () == 1 && <nl> tdrc -> fieldList (). front (). size () == 4 && <nl> tdat -> data (). size () >= 5 ) <nl> {
void FancyTabWidget :: SetIcon ( int index , const QIcon & icon ) <nl> if ( index > 0 && index < items_ . count ()) { <nl> items_ [ index ]. tab_icon_ = icon ; <nl> } <nl> + if ( menu_ ) { <nl> + menu_ -> deleteLater (); <nl> + menu_ = 0 ; <nl> + } <nl> } <nl>  <nl> void FancyTabWidget :: Recreate ()
void raptor_xml_writer_start_element ( raptor_xml_writer * xml_writer , raptor_xml_e <nl> RAPTOR_API <nl> void raptor_xml_writer_end_element ( raptor_xml_writer * xml_writer , raptor_xml_element * element ); <nl> RAPTOR_API <nl> + void raptor_xml_writer_newline ( raptor_xml_writer * xml_writer ); <nl> + RAPTOR_API <nl> void raptor_xml_writer_cdata ( raptor_xml_writer * xml_writer , const unsigned char * s ); <nl> RAPTOR_API <nl> void raptor_xml_writer_cdata_counted ( raptor_xml_writer * xml_writer , const unsigned char * s , unsigned int len ); <nl> void raptor_xml_writer_comment ( raptor_xml_writer * xml_writer , const unsigned cha <nl> RAPTOR_API <nl> void raptor_xml_writer_comment_counted ( raptor_xml_writer * xml_writer , const unsigned char * s , unsigned int len ); <nl> RAPTOR_API <nl> + void raptor_xml_writer_flush ( raptor_xml_writer * xml_writer ); <nl> + RAPTOR_API <nl> int raptor_xml_writer_features_enumerate ( const raptor_feature feature , const char ** name , raptor_uri ** uri , const char ** label ); <nl> RAPTOR_API <nl> int raptor_xml_writer_set_feature ( raptor_xml_writer * xml_writer , raptor_feature feature , int value );
raptor_xml_writer_start_element_common ( raptor_xml_writer * xml_writer , <nl> size_t nspace_declarations_count = 0 ; <nl> unsigned int i ; <nl>  <nl> - /* max is 1 per element and 1 for each attribute + size of declared */ <nl> if ( nstack ) { <nl> - int nspace_max_count = element -> attribute_count + 1 ; <nl> + int nspace_max_count = element -> attribute_count * 2 ; /* attr and value */ <nl> + if ( element -> name -> nspace ) <nl> + nspace_max_count ++; <nl> if ( element -> declared_nspaces ) <nl> nspace_max_count += raptor_sequence_size ( element -> declared_nspaces ); <nl> if ( element -> xml_language ) <nl> raptor_xml_writer_start_element_common ( raptor_xml_writer * xml_writer , <nl> } <nl> } <nl>  <nl> - /* Add the attribute + value */ <nl> + /* Add the attribute ' s value */ <nl> nspace_declarations [ nspace_declarations_count ]. declaration = <nl> raptor_qname_format_as_xml ( element -> attributes [ i ], <nl> & nspace_declarations [ nspace_declarations_count ]. length );
raptor_new_sax2 ( void * user_data , raptor_error_handlers * error_handlers ) <nl> sax2 -> error_handlers = error_handlers ; <nl>  <nl> # ifdef RAPTOR_XML_LIBXML <nl> - if ( sax2 -> world -> libxml_flags & RAPTOR_LIBXML_FLAGS_GENERIC_ERROR_SAVE ) { <nl> + if ( sax2 -> world -> libxml_flags & RAPTOR_LIBXML_FLAGS_STRUCTURED_ERROR_SAVE ) { <nl> sax2 -> saved_structured_error_context = xmlGenericErrorContext ; <nl> sax2 -> saved_structured_error_handler = xmlStructuredError ; <nl> /* sets xmlGenericErrorContext and xmlStructuredError */ <nl> raptor_new_sax2 ( void * user_data , raptor_error_handlers * error_handlers ) <nl> ( xmlStructuredErrorFunc ) raptor_libxml_xmlStructuredErrorFunc ); <nl> } <nl>  <nl> - if ( sax2 -> world -> libxml_flags & RAPTOR_LIBXML_FLAGS_STRUCTURED_ERROR_SAVE ) { <nl> + if ( sax2 -> world -> libxml_flags & RAPTOR_LIBXML_FLAGS_GENERIC_ERROR_SAVE ) { <nl> sax2 -> saved_generic_error_context = xmlGenericErrorContext ; <nl> sax2 -> saved_generic_error_handler = xmlGenericError ; <nl> /* sets xmlGenericErrorContext and xmlGenericError */
char * rdfa_resolve_curie ( <nl> char * rval = NULL ; <nl> curie_t ctype = rdfa_get_curie_type ( uri ); <nl>  <nl> + if (! uri ) <nl> + return NULL ; <nl> + <nl> if ( ctype == CURIE_TYPE_INVALID ) <nl> { <nl> rval = NULL ;
struct raptor_parser_s { <nl> /* parser callbacks */ <nl> raptor_statement_handler statement_handler ; <nl>  <nl> + raptor_graph_handler graph_handler ; <nl> + <nl> void * generate_id_handler_user_data ; <nl> raptor_generate_id_handler generate_id_handler ; <nl>  <nl> raptor_uri * raptor_new_uri_from_rdf_ordinal ( int ordinal ); <nl> void raptor_init_parser_rdfxml ( void ); <nl> void raptor_init_parser_ntriples ( void ); <nl> void raptor_init_parser_turtle ( void ); <nl> + void raptor_init_parser_trig ( void ); <nl> void raptor_init_parser_n3 ( void ); <nl> void raptor_init_parser_grddl_common ( void ); <nl> void raptor_init_parser_grddl ( void ); <nl> void raptor_parsers_finish ( void ); <nl>  <nl> void raptor_parser_save_content ( raptor_parser * rdf_parser , int save ); <nl> const unsigned char * raptor_parser_get_content ( raptor_parser * rdf_parser , size_t * length_p ); <nl> - <nl> + void raptor_parser_set_graph_name ( raptor_parser * parser , raptor_uri * uri ); <nl>  <nl> /* raptor_rss . c */ <nl> void raptor_init_serializer_rss10 ( void );
ModuleExport MagickBooleanType ReadPSDLayers ( Image * image , <nl> /* <nl> Layer name . <nl> */ <nl> - length =( MagickSizeType ) ReadBlobByte ( image ); <nl> + length =( MagickSizeType ) ( unsigned char ) ReadBlobByte ( image ); <nl> combined_length += length + 1 ; <nl> if ( length > 0 ) <nl> ( void ) ReadBlob ( image ,( size_t ) length ++, layer_info [ i ]. name );
static MagickBooleanType ReadDXT3Pixels ( Image * image , <nl> { <nl> for ( i = 0 ; i < 4 ; i ++) <nl> { <nl> - if (( x + i ) < ( ssize_t ) image -> rows && ( y + j ) < ( ssize_t ) image -> columns ) <nl> + if (( x + i ) < ( ssize_t ) image -> columns && ( y + j ) < ( ssize_t ) image -> rows ) <nl> { <nl> code = ( bits >> (( 4 * j + i )* 2 )) & 0x3 ; <nl> SetPixelRed ( image , ScaleCharToQuantum ( colors . r [ code ]), q );
static MagickBooleanType WriteOnePNGImage ( MngInfo * mng_info , <nl> # endif <nl>  <nl> # if ( MAGICKCORE_QUANTUM_DEPTH >= 16 ) <nl> - if ( image -> depth == 16 && mng_info -> write_png_colortype != 16 ) <nl> + if ( image -> depth == 16 && mng_info -> write_png_depth != 16 ) <nl> if ( mng_info -> write_png8 || LosslessReduceDepthOK ( image ) != MagickFalse ) <nl> image -> depth = 8 ; <nl> # endif <nl> static MagickBooleanType WriteOnePNGImage ( MngInfo * mng_info , <nl> ( number_opaque > 2 && number_opaque < 17 ))) <nl> ping_have_color = MagickTrue ; <nl>  <nl> + <nl> if ( mng_info -> ping_exclude_tRNS != MagickFalse && <nl> ( number_transparent != 0 || number_semitransparent != 0 )) <nl> { <nl> static MagickBooleanType WriteOnePNGImage ( MngInfo * mng_info , <nl> image = DestroyImage ( image ); <nl> return ( MagickFalse ); <nl> } <nl> + <nl> + if (( mng_info -> write_png_colortype == 4 || mng_info -> write_png8 ) && <nl> + ( image -> colors == 0 || image -> colormap == NULL )) <nl> + { <nl> + png_error ( ping , " Cannot write PNG8 or color - type 3 ; colormap is NULL "); <nl> + } <nl> + <nl> /* <nl> Prepare PNG for writing . <nl> */
static Image * ReadPSDImage ( const ImageInfo * image_info , ExceptionInfo * exception ) <nl> if ( blocks == ( unsigned char *) NULL ) <nl> ThrowReaderException ( ResourceLimitError ," MemoryAllocationFailed "); <nl> count = ReadBlob ( image ,( size_t ) length , blocks ); <nl> - if (( count != ( ssize_t ) length ) || <nl> + if (( count != ( ssize_t ) length ) || ( length < 4 ) || <nl> ( LocaleNCompare (( char *) blocks ," 8BIM ", 4 ) != 0 )) <nl> { <nl> blocks =( unsigned char *) RelinquishMagickMemory ( blocks );
static MagickBooleanType WriteBMPImage ( const ImageInfo * image_info , Image * image , <nl> bmp_info . file_size += extra_size ; <nl> bmp_info . offset_bits += extra_size ; <nl> } <nl> + if (( image -> columns != ( signed int ) image -> columns ) || <nl> + ( image -> rows != ( signed int ) image -> rows )) <nl> + ThrowWriterException ( ImageError ," WidthOrHeightExceedsLimit "); <nl> bmp_info . width =( ssize_t ) image -> columns ; <nl> bmp_info . height =( ssize_t ) image -> rows ; <nl> bmp_info . planes = 1 ; <nl> - bmp_info . image_size =( unsigned int ) ( bytes_per_line * image -> rows ); <nl> + bmp_info . image_size =( unsigned long ) ( bytes_per_line * image -> rows ); <nl> bmp_info . file_size += bmp_info . image_size ; <nl> bmp_info . x_pixels = 75 * 39 ; <nl> bmp_info . y_pixels = 75 * 39 ;
static Image * ReadRLEImage ( const ImageInfo * image_info , ExceptionInfo * exception ) <nl> pixel_info_length = image -> columns * image -> rows * <nl> MagickMax ( number_planes_filled , 4 ); <nl> pixels =( unsigned char *) GetVirtualMemoryBlob ( pixel_info ); <nl> + ( void ) ResetMagickMemory ( pixels , 0 , pixel_info_length ); <nl> if (( flags & 0x01 ) && !( flags & 0x02 )) <nl> { <nl> ssize_t
void CRenderTools :: RenderTee ( CAnimState * pAnim , CTeeRenderInfo * pInfo , int Emote <nl> } <nl>  <nl> // draw body ( behind tattoo ) <nl> - if ( Parts & SELECTION_BODY || ( OutLine && RenderBackground )) <nl> + if ( Parts & SELECTION_BODY || ( OutLine && RenderBackground ) || ( RenderBackground && Parts & SELECTION_EYES )) <nl> { <nl> Graphics ()-> TextureSet ( pInfo -> m_aTextures [ 0 ]); <nl> Graphics ()-> QuadsBegin (); <nl> void CRenderTools :: RenderTee ( CAnimState * pAnim , CTeeRenderInfo * pInfo , int Emote <nl> } <nl> else <nl> { <nl> - Graphics ()-> SetColor ( pInfo -> m_aColors [ 0 ]. r , pInfo -> m_aColors [ 0 ]. g , pInfo -> m_aColors [ 0 ]. b , pInfo -> m_aColors [ 0 ]. a ); <nl> + if (!( Parts & SELECTION_BODY ) && Parts & SELECTION_EYES ) <nl> + Graphics ()-> SetColor ( 1 . 0f , 1 . 0f , 1 . 0f , 1 . 0f ); <nl> + else <nl> + Graphics ()-> SetColor ( pInfo -> m_aColors [ 0 ]. r , pInfo -> m_aColors [ 0 ]. g , pInfo -> m_aColors [ 0 ]. b , pInfo -> m_aColors [ 0 ]. a ); <nl> SelectSprite ( SPRITE_TEE_BODY , 0 , 0 , 0 ); <nl> } <nl> Item = BodyItem ;
evthread_make_base_notifiable ( struct event_base * base ) <nl> if ( base -> th_notify_fd [ 0 ] >= 0 ) { <nl> notify = evthread_notify_base_eventfd ; <nl> cb = evthread_notify_drain_eventfd ; <nl> - } else <nl> + } <nl> # endif <nl> # if defined ( _EVENT_HAVE_PIPE ) <nl> - { <nl> + if ( base -> th_notify_fd [ 0 ] < 0 ) { <nl> if (( base -> evsel -> features & EV_FEATURE_FDS )) { <nl> if ( pipe ( base -> th_notify_fd ) < 0 ) <nl> event_warn ("% s : pipe ", __func__ ); <nl> } <nl> } <nl> - if ( base -> th_notify_fd [ 0 ] < 0 ) <nl> # endif <nl>  <nl> # ifdef WIN32 <nl> evthread_make_base_notifiable ( struct event_base * base ) <nl> # else <nl> # define LOCAL_SOCKETPAIR_AF AF_UNIX <nl> # endif <nl> - { <nl> + if ( base -> th_notify_fd [ 0 ] < 0 ) { <nl> if ( evutil_socketpair ( LOCAL_SOCKETPAIR_AF , SOCK_STREAM , 0 , <nl> base -> th_notify_fd ) == - 1 ) { <nl> event_sock_warn (- 1 , "% s : socketpair ", __func__ );
evutil_parse_sockaddr_port ( const char * ip_as_string , struct sockaddr * out , int * <nl>  <nl> cp = strchr ( ip_as_string , ':'); <nl> if (* ip_as_string == '[') { <nl> - int len ; <nl> + size_t len ; <nl> if (!( cp = strchr ( ip_as_string , ']'))) { <nl> return - 1 ; <nl> } <nl> - len = ( int ) ( cp -( ip_as_string + 1 ) ); <nl> - if ( len > ( int ) sizeof ( buf )- 1 ) { <nl> + len = ( cp -( ip_as_string + 1 ) ); <nl> + if ( len > sizeof ( buf )- 1 ) { <nl> return - 1 ; <nl> } <nl> memcpy ( buf , ip_as_string + 1 , len );
# include " file . h " <nl>  <nl> # ifndef lint <nl> - FILE_RCSID ("@(#)$ File : softmagic . c , v 1 . 150 2012 / 05 / 15 17 : 14 : 36 christos Exp $") <nl> + FILE_RCSID ("@(#)$ File : softmagic . c , v 1 . 151 2012 / 09 / 06 14 : 42 : 39 christos Exp $") <nl> # endif /* lint */ <nl>  <nl> # include " magic . h " <nl> match ( struct magic_set * ms , struct magic * magic , uint32_t nmagic , <nl> (! text && ( m -> str_flags & ( STRING_TEXTTEST | STRING_BINTEST )) == STRING_TEXTTEST ))) || <nl> ( m -> flag & mode ) != mode ) { <nl> /* Skip sub - tests */ <nl> - while ( magic [ magindex + 1 ]. cont_level != 0 && <nl> - ++ magindex < nmagic ) <nl> + while ( magindex + 1 < nmagic && <nl> + magic [ magindex + 1 ]. cont_level != 0 && <nl> + ++ magindex ) <nl> continue ; <nl> continue ; /* Skip to next top - level test */ <nl> }
 <nl> # ifndef lint <nl> static char * moduleid = <nl> - "@(#)$ Id : apprentice . c , v 1 . 17 1993 / 09 / 16 20 : 49 : 29 christos Exp $"; <nl> + "@(#)$ Id : apprentice . c , v 1 . 18 1993 / 09 / 23 20 : 19 : 42 christos Exp $"; <nl> # endif /* lint */ <nl>  <nl> # define EATAB { while ( isascii (( unsigned char ) * l ) && \ <nl> int * ndx , check ; <nl> s = l ; <nl> if (* l == '+' || * l == '-') l ++; <nl> if ( isdigit (( unsigned char )* l )) { <nl> - m -> in . offset = strtol ( l , & t , 0 ); <nl> - if (* s == '-') m -> in . offset = - m -> in . offset ; <nl> + m -> in . offset = strtol ( l , & t , 0 ); <nl> + if (* s == '-') m -> in . offset = - m -> in . offset ; <nl> } <nl> + else <nl> + t = l ; <nl> if (* t ++ != ')') <nl> magwarn (" missing ')' in indirect offset "); <nl> l = t ;
int uv_run ( uv_loop_t * loop , uv_run_mode mode ) { <nl>  <nl> uv__update_time ( loop ); <nl> uv__run_timers ( loop ); <nl> + uv__run_pending ( loop ); <nl> uv__run_idle ( loop ); <nl> uv__run_prepare ( loop ); <nl> - uv__run_pending ( loop ); <nl>  <nl> timeout = 0 ; <nl> if (( mode & UV_RUN_NOWAIT ) == 0 )
int uv_async_init ( uv_async_t * async , uv_async_cb async_cb ) { <nl>  <nl> int uv_async_send ( uv_async_t * async ) { <nl> ev_async_send ( EV_DEFAULT_UC_ & async -> async_watcher ); <nl> + return 0 ; <nl> } <nl>  <nl> 
start : <nl> } <nl>  <nl> if ( n < 0 ) { <nl> - if ( errno != EAGAIN ) { <nl> + if ( errno != EAGAIN && errno != EWOULDBLOCK ) { <nl> /* Error */ <nl> req -> error = errno ; <nl> stream -> write_queue_size -= uv__write_req_size ( req );
static const char * static_camera_list [] = <nl> //" Leica S3 ", <nl> " Leica T ( Typ 701 )", <nl> " Leica X1 ", <nl> +" Leica X ( Typ 113 )", <nl> " Leica X2 ", <nl> " Leica V - LUX1 ", <nl> " Leica V - LUX2 ",
struct mk_event_ctx { <nl>  <nl> # define mk_event_foreach ( event , evl ) \ <nl> int __i ; \ <nl> - struct mk_event_ctx * ctx = evl -> data ; \ <nl> + struct mk_event_ctx * __ctx = evl -> data ; \ <nl> \ <nl> if ( evl -> n_events > 0 ) { \ <nl> - event = ctx -> events [ 0 ]. data . ptr ; \ <nl> + event = __ctx -> events [ 0 ]. data . ptr ; \ <nl> } \ <nl> \ <nl> for ( __i = 0 ; \ <nl> __i < evl -> n_events ; \ <nl> __i ++, \ <nl> - event = ctx -> events [ __i ]. data . ptr \ <nl> + event = __ctx -> events [ __i ]. data . ptr \ <nl> ) <nl> # endif
# include < assert . h > <nl> # include < string . h > <nl>  <nl> -# if defined ( __linux__ ) <nl> -# include < ucontext . h > <nl> -# elif defined ( __APPLE__ ) <nl> +# if defined ( __APPLE__ ) <nl> # include < sys / ucontext . h > <nl> +# else <nl> +# include < ucontext . h > <nl> # endif <nl>  <nl> # include < limits . h >
int mk_dirhtml_theme_load () <nl> mk_dirhtml_theme_debug ( mk_dirhtml_tpl_entry , lov_entry ); <nl> mk_dirhtml_theme_debug ( mk_dirhtml_tpl_footer , lov_footer ); <nl> # endif <nl> + <nl> + mk_mem_free ( header ); <nl> + mk_mem_free ( entry ); <nl> + mk_mem_free ( footer ); <nl> + <nl> return 0 ; <nl> } <nl> 
update_focus_app ( ShellWindowTracker * self ) <nl> } <nl>  <nl> set_focus_app ( self , new_focus_app ); <nl> + <nl> + g_clear_object (& new_focus_app ); <nl> } <nl>  <nl> static void
NPP_GetValue ( NPP instance , <nl>  <nl> *( NPObject **) value = funcs . createobject ( instance , & plugin_class ); <nl> break ; <nl> + <nl> + case NPPVpluginNeedsXEmbed : <nl> + *( bool *) value = TRUE ; <nl> + break ; <nl> + <nl> default : <nl> ; <nl> }
static char * <nl> get_atk_bridge_path ( void ) <nl> { <nl> GSettings * atspi_settings = NULL ; <nl> + GVariant * variant = NULL ; <nl> char * value = NULL ; <nl> const char * const * schemas = NULL ; <nl> gboolean found = FALSE ; <nl> get_atk_bridge_path ( void ) <nl> } <nl>  <nl> atspi_settings = g_settings_new ( AT_SPI_SCHEMA ); <nl> - value = g_settings_get_string ( atspi_settings , ATK_BRIDGE_LOCATION_KEY ); <nl> - <nl> + variant = g_settings_get_value ( atspi_settings , ATK_BRIDGE_LOCATION_KEY ); <nl> + value = g_variant_dup_bytestring ( variant , NULL ); <nl> + g_variant_unref ( variant ); <nl> g_object_unref ( atspi_settings ); <nl>  <nl> return value ;
int GetIfaceMTU ( const char * pcap_dev ) <nl> */ <nl> int GetIfaceMaxPacketSize ( const char * pcap_dev ) <nl> { <nl> - int ll_header = GetIfaceMaxHWHeaderLength ( pcap_dev ); <nl> - int mtu = 0 ; <nl> - <nl> if (( pcap_dev == NULL ) || strlen ( pcap_dev ) == 0 ) <nl> return 0 ; <nl>  <nl> - mtu = GetIfaceMTU ( pcap_dev ); <nl> + int mtu = GetIfaceMTU ( pcap_dev ); <nl> switch ( mtu ) { <nl> case 0 : <nl> case - 1 : <nl> return 0 ; <nl> } <nl> + int ll_header = GetIfaceMaxHWHeaderLength ( pcap_dev ); <nl> if ( ll_header == - 1 ) { <nl> /* be conservative , choose a big one */ <nl> ll_header = 16 ;
int RunModeSetIPSAutoFp ( ConfigIPSParserFunc ConfigParser , <nl> exit ( EXIT_FAILURE ); <nl> } <nl> memset ( tname , 0 , sizeof ( tname )); <nl> - snprintf ( tname , sizeof ( tname ), " Recv - Q % s ", cur_queue ); <nl> + snprintf ( tname , sizeof ( tname ), " RX - Q % s ", cur_queue ); <nl>  <nl> ThreadVars * tv_receive = <nl> TmThreadCreatePacketHandler ( tname , <nl> int RunModeSetIPSAutoFp ( ConfigIPSParserFunc ConfigParser , <nl>  <nl> } <nl> for ( thread = 0 ; thread < thread_max ; thread ++) { <nl> - snprintf ( tname , sizeof ( tname ), " Detect % d ", thread + 1 ); <nl> + snprintf ( tname , sizeof ( tname ), " W % 02d ", thread + 1 ); <nl> snprintf ( qname , sizeof ( qname ), " pickup % d ", thread + 1 ); <nl>  <nl> SCLogDebug (" tname % s , qname % s ", tname , qname ); <nl> int RunModeSetIPSAutoFp ( ConfigIPSParserFunc ConfigParser , <nl> /* create the threads */ <nl> for ( int i = 0 ; i < nqueue ; i ++) { <nl> memset ( tname , 0 , sizeof ( tname )); <nl> - snprintf ( tname , sizeof ( tname ), " Verdict % d ", i ); <nl> + snprintf ( tname , sizeof ( tname ), " TX % 02d ", i ); <nl>  <nl> ThreadVars * tv_verdict = <nl> TmThreadCreatePacketHandler ( tname , <nl> int RunModeSetIPSWorker ( ConfigIPSParserFunc ConfigParser , <nl> exit ( EXIT_FAILURE ); <nl> } <nl> memset ( tname , 0 , sizeof ( tname )); <nl> - snprintf ( tname , sizeof ( tname ), " Worker - Q % s ", cur_queue ); <nl> + snprintf ( tname , sizeof ( tname ), " W - Q % s ", cur_queue ); <nl>  <nl> tv = TmThreadCreatePacketHandler ( tname , <nl> " packetpool ", " packetpool ",
int DetectEngineInspectHttpStatCode ( DetectEngineCtx * de_ctx , <nl> } <nl>  <nl> # ifdef DEBUG <nl> - SigMatch * sm = s -> sm_lists [ DETECT_SM_LIST_HSMDMATCH ]; <nl> + SigMatch * sm = s -> sm_lists [ DETECT_SM_LIST_HSCDMATCH ]; <nl> DetectContentData * co = ( DetectContentData *) sm -> ctx ; <nl> SCLogDebug (" co -> id %" PRIu32 , co -> id ); <nl> # endif
int main ( int argc , char ** argv ) <nl> FlowInitConfig ( FLOW_VERBOSE ); <nl>  <nl> DetectEngineCtx * de_ctx = DetectEngineCtxInit (); <nl> + if ( de_ctx == NULL ) { <nl> + SCLogError ( SC_ERR_INITIALIZATION , " initializing detection engine " <nl> + " context failed ."); <nl> + exit ( EXIT_FAILURE ); <nl> + } <nl>  <nl> SCClassConfLoadClassficationConfigFile ( de_ctx ); <nl> SCRConfLoadReferenceConfigFile ( de_ctx );
static int DetectFlowvarSetup ( DetectEngineCtx * de_ctx , Signature * s , char * raws <nl> fd = SCMalloc ( sizeof ( DetectFlowvarData )); <nl> if ( unlikely ( fd == NULL )) <nl> goto error ; <nl> + memset ( fd , 0x00 , sizeof (* fd )); <nl>  <nl> fd -> content = SCMalloc ( contentlen ); <nl> if ( unlikely ( fd -> content == NULL ))
static uint8_t * DetectEngineHHDGetBufferForTX ( int tx_id , <nl> uint8_t flags , <nl> uint32_t * buffer_len ) <nl> { <nl> + uint8_t * headers_buffer = NULL ; <nl> int index = 0 ; <nl> * buffer_len = 0 ; <nl>  <nl> static uint8_t * DetectEngineHHDGetBufferForTX ( int tx_id , <nl> goto end ; <nl>  <nl> htp_header_t * h = NULL ; <nl> - uint8_t * headers_buffer = det_ctx -> hhd_buffers [ index ]; <nl> + headers_buffer = det_ctx -> hhd_buffers [ index ]; <nl> size_t headers_buffer_len = 0 ; <nl>  <nl> table_iterator_reset ( headers );
DetectPcreData * DetectPcreParse ( char * regexstr ) <nl> SCLogError ( SC_ERR_INVALID_SIGNATURE , " regex modifier ' U ' inconsistent with ' I '"); <nl> goto error ; <nl> } <nl> + if ( pd -> flags & DETECT_PCRE_RAWBYTES ) { <nl> + SCLogError ( SC_ERR_INVALID_SIGNATURE , " regex modifier ' U ' inconsistent with ' B '"); <nl> + goto error ; <nl> + } <nl> pd -> flags |= DETECT_PCRE_URI ; <nl> break ; <nl> case ' H ': /* snort ' s option */
LLVMRustSetDataLayoutFromTargetMachine ( LLVMModuleRef Module , <nl> LLVMTargetMachineRef TMR ) { <nl> TargetMachine * Target = unwrap ( TMR ); <nl> # if LLVM_VERSION_MINOR >= 7 <nl> - if ( const DataLayout * DL = Target -> getDataLayout ()) <nl> - unwrap ( Module )-> setDataLayout (* DL ); <nl> + unwrap ( Module )-> setDataLayout ( Target -> createDataLayout ()); <nl> # elif LLVM_VERSION_MINOR >= 6 <nl> if ( const DataLayout * DL = Target -> getSubtargetImpl ()-> getDataLayout ()) <nl> unwrap ( Module )-> setDataLayout ( DL );
static CURLcode Connect ( struct UrlData * data , <nl>  <nl> /* tell ourselves to fetch this range */ <nl> conn -> range = strdup ( resumerange ); <nl> + conn -> bits . use_range = TRUE ; /* enable range download */ <nl> conn -> bits . rangestringalloc = TRUE ; /* mark range string allocated */ <nl> } <nl> 
static CURLcode pop3_doing ( struct connectdata * conn , bool * dophase_done ) <nl> CURLcode result ; <nl> result = pop3_multi_statemach ( conn , dophase_done ); <nl>  <nl> - if (* dophase_done ) { <nl> + if (! result && * dophase_done ) { <nl> result = pop3_dophase_done ( conn , FALSE /* not connected */); <nl>  <nl> DEBUGF ( infof ( conn -> data , " DO phase is complete \ n "));
typedef enum { <nl>  <nl> typedef enum { <nl> CURLMSG_NONE , /* first , not used */ <nl> - CURLMSG_DONE , /* This easy handle has completed . ' whatever ' points to <nl> + CURLMSG_DONE , /* This easy handle has completed . ' result ' contains <nl> the CURLcode of the transfer */ <nl> CURLMSG_LAST /* last , not used */ <nl> } CURLMSG ;
CURLcode Curl_auth_create_plain_message ( struct Curl_easy * data , <nl> plen = strlen ( passwdp ); <nl>  <nl> /* Compute binary message length . Check for overflows . */ <nl> - if (( ulen > SIZE_T_MAX / 2 ) || ( plen > ( SIZE_T_MAX / 2 - 2 ))) <nl> + if (( ulen > SIZE_T_MAX / 4 ) || ( plen > ( SIZE_T_MAX / 2 - 2 ))) <nl> return CURLE_OUT_OF_MEMORY ; <nl> plainlen = 2 * ulen + plen + 2 ; <nl> 
CURLcode Curl_http ( struct connectdata * conn , bool * done ) <nl> if ( http -> writebytecount >= postsize ) { <nl> /* already sent the entire request body , mark the " upload " as <nl> complete */ <nl> - infof ( data , " upload completely sent off : %" FORMAT_OFF_T " out of " <nl> + infof ( data , " upload completely sent off : %" FORMAT_OFF_T " out of " <nl> "%" FORMAT_OFF_T " bytes \ n ", <nl> http -> writebytecount , postsize ); <nl> data -> req . upload_done = TRUE ;
CURLcode Curl_sasl_create_gssapi_user_message ( struct SessionHandle * data , <nl> /* Release the package buffer as it is not required anymore */ <nl> s_pSecFn -> FreeContextBuffer ( SecurityPackage ); <nl>  <nl> + /* Allocate our response buffer */ <nl> + krb5 -> output_token = malloc ( krb5 -> token_max ); <nl> + if (! krb5 -> output_token ) <nl> + return CURLE_OUT_OF_MEMORY ; <nl> + <nl> /* Generate our SPN */ <nl> krb5 -> spn = Curl_sasl_build_spn ( service , data -> easy_conn -> host . name ); <nl> if (! krb5 -> spn ) <nl> CURLcode Curl_sasl_create_gssapi_user_message ( struct SessionHandle * data , <nl>  <nl> /* Allow proper cleanup of the identity structure */ <nl> krb5 -> p_identity = & krb5 -> identity ; <nl> - <nl> - /* Allocate our response buffer */ <nl> - krb5 -> output_token = malloc ( krb5 -> token_max ); <nl> - if (! krb5 -> output_token ) <nl> - return CURLE_OUT_OF_MEMORY ; <nl> } <nl> else <nl> /* Use the current Windows user */
static CURLcode AcceptServerConnect ( struct connectdata * conn ) <nl> } <nl> infof ( data , " Connection accepted from server \ n "); <nl>  <nl> + conn -> sock [ SECONDARYSOCKET ] = s ; <nl> + curlx_nonblock ( s , TRUE ); /* enable non - blocking */ <nl> + conn -> sock_accepted [ SECONDARYSOCKET ] = TRUE ; <nl> + <nl> if ( data -> set . fsockopt ) { <nl> int error = 0 ; <nl>  <nl> static CURLcode AcceptServerConnect ( struct connectdata * conn ) <nl>  <nl> if ( error ) { <nl> Curl_closesocket ( conn , s ); /* close the socket and bail out */ <nl> + conn -> sock [ SECONDARYSOCKET ] = CURL_SOCKET_BAD ; <nl> return CURLE_ABORTED_BY_CALLBACK ; <nl> } <nl> } <nl>  <nl> - conn -> sock [ SECONDARYSOCKET ] = s ; <nl> - curlx_nonblock ( s , TRUE ); /* enable non - blocking */ <nl> - conn -> sock_accepted [ SECONDARYSOCKET ] = TRUE ; <nl> return CURLE_OK ; <nl>  <nl> }
static ParameterError str2double ( double * val , const char * str , long max ) <nl> num = strtod ( str , & endptr ); <nl> if ( errno == ERANGE ) <nl> return PARAM_NUMBER_TOO_LARGE ; <nl> - if (( long ) num > max ) { <nl> + if ( num > max ) { <nl> /* too large */ <nl> return PARAM_NUMBER_TOO_LARGE ; <nl> }
void CQuickSpriteSystem :: Add ( float * pointdata , color4ub_t color , vec2_t fog ) <nl> } <nl>  <nl> curcoord = mVerts [ mNextVert ]; <nl> - memcpy ( curcoord , pointdata , 4 * sizeof ( vec4_t )); <nl> + memcpy ( curcoord , pointdata , sizeof ( vec4_t )); <nl>  <nl> // Set up color <nl> curcolor = & mColors [ mNextVert ];
# include < ruby . h > <nl> # include " dl . h " <nl>  <nl> +# define SafeStringValuePtr ( v ) ( rb_string_value (& v ), rb_check_safe_obj ( v ), RSTRING_PTR ( v )) <nl> + <nl> VALUE rb_cDLHandle ; <nl>  <nl> void <nl> rb_dlhandle_initialize ( int argc , VALUE argv [], VALUE self ) <nl> cflag = RTLD_LAZY | RTLD_GLOBAL ; <nl> break ; <nl> case 1 : <nl> - clib = NIL_P ( lib ) ? NULL : StringValuePtr ( lib ); <nl> + clib = NIL_P ( lib ) ? NULL : SafeStringValuePtr ( lib ); <nl> cflag = RTLD_LAZY | RTLD_GLOBAL ; <nl> break ; <nl> case 2 : <nl> - clib = NIL_P ( lib ) ? NULL : StringValuePtr ( lib ); <nl> + clib = NIL_P ( lib ) ? NULL : SafeStringValuePtr ( lib ); <nl> cflag = NUM2INT ( flag ); <nl> break ; <nl> default : <nl> rb_dlhandle_sym ( VALUE self , VALUE sym ) <nl>  <nl> rb_secure ( 2 ); <nl>  <nl> - name = StringValuePtr ( sym ); <nl> + name = SafeStringValuePtr ( sym ); <nl>  <nl> Data_Get_Struct ( self , struct dl_handle , dlhandle ); <nl> if ( ! dlhandle -> open ){
typedef struct rb_vm_struct { <nl>  <nl> # define RUBY_VM_FIBER_VM_STACK_SIZE ( 16 * 1024 * sizeof ( VALUE )) /* 64 KB or 128 KB */ <nl> # define RUBY_VM_FIBER_VM_STACK_SIZE_MIN ( 2 * 1024 * sizeof ( VALUE )) /* 8 KB or 16 KB */ <nl> -# define RUBY_VM_FIBER_MACHINE_STACK_SIZE ( 64 * 1024 * sizeof ( VALUE )) /* 256 KB or 512 KB */ <nl> +# define RUBY_VM_FIBER_MACHINE_STACK_SIZE ( 64 * 1024 * sizeof ( VALUE )) /* 256 KB or 512 KB */ <nl> +# if defined ( __powerpc64__ ) <nl> +# define RUBY_VM_FIBER_MACHINE_STACK_SIZE_MIN ( 32 * 1024 * sizeof ( VALUE )) /* 128 KB or 256 KB */ <nl> +# else <nl> # define RUBY_VM_FIBER_MACHINE_STACK_SIZE_MIN ( 16 * 1024 * sizeof ( VALUE )) /* 64 KB or 128 KB */ <nl> +# endif <nl>  <nl> /* optimize insn */ <nl> # define INTEGER_REDEFINED_OP_FLAG ( 1 << 0 )
onig_vsnprintf_with_pattern ( UChar buf [], int bufsize , OnigEncoding enc , <nl> need = ( pat_end - pat ) * 4 + 4 ; <nl>  <nl> if ( n + need < ( size_t ) bufsize ) { <nl> - xstrcat (( char * ) buf , ": /", bufsize ); <nl> + static const char sep [] = ": /"; <nl> + memcpy (( char * ) buf + n , sep , sizeof ( sep )); <nl> s = buf + onigenc_str_bytelen_null ( ONIG_ENCODING_ASCII , buf ); <nl>  <nl> p = pat ;
get_vp_value : <nl> RB_GC_GUARD ( vn ) = SSIZET2NUM ( n ); <nl> expo = VpExponent10 ( vx ); <nl> if ( expo < 0 || expo >= 3 ) { <nl> - char buf [ 16 ]; <nl> - snprintf ( buf , 16 , " 1E %" PRIdVALUE , - expo ); <nl> + char buf [ SIZEOF_VALUE * CHAR_BIT / 3 + 4 ]; <nl> + snprintf ( buf , sizeof ( buf ), " 1E %" PRIdVALUE , - expo ); <nl> x = BigDecimal_mult2 ( x , ToValue ( VpCreateRbObject ( 1 , buf )), vn ); <nl> } <nl> else {
str_byte_substr ( VALUE str , long beg , long len , int empty ) <nl> beg += n ; <nl> if ( beg < 0 ) return Qnil ; <nl> } <nl> - if ( beg + len > n ) <nl> + if ( len > n - beg ) <nl> len = n - beg ; <nl> if ( len <= 0 ) { <nl> if (! empty ) return Qnil ;
delete_opm_scanner ( const char * key __unused , int parc __unused , const char ** par <nl>  <nl> rb_dlinkDelete (& proxy -> node , & proxy_scanners ); <nl> rb_free ( proxy ); <nl> + <nl> + if (! rb_dlink_list_length ( proxy_scanners )) <nl> + opm_enable = false ; <nl> } <nl>  <nl> static void <nl> delete_opm_scanner_all ( const char * key __unused , int parc __unused , const char * <nl> { <nl> opm_cancel ( auth ); <nl> } <nl> + <nl> + opm_enable = false ; <nl> } <nl>  <nl> 
void scale_component ( opj_image_comp_t * component , OPJ_UINT32 precision ) <nl> return ; <nl> } <nl> if ( component -> prec < precision ) { <nl> - shift = precision - component -> prec ; <nl> + shift = ( int )( precision - component -> prec ); <nl> } else { <nl> - shift = component -> prec - precision ; <nl> + shift = ( int )( component -> prec - precision ); <nl> } <nl> len = ( OPJ_SIZE_T ) component -> w * ( OPJ_SIZE_T ) component -> h ; <nl>  <nl> static opj_image_t * rawtoimage_common ( const char * filename , opj_cparameters_t * p <nl> } <nl> w = raw_cp -> rawWidth ; <nl> h = raw_cp -> rawHeight ; <nl> - cmptparm = ( opj_image_cmptparm_t *) calloc ( numcomps , sizeof ( opj_image_cmptparm_t )); <nl> + cmptparm = ( opj_image_cmptparm_t *) calloc (( OPJ_UINT32 ) numcomps , sizeof ( opj_image_cmptparm_t )); <nl> if (! cmptparm ) { <nl> fprintf ( stderr , " Failed to allocate image components parameters !!\ n "); <nl> fprintf ( stderr ," Aborting \ n ");
static int generate_subkeys ( mbedtls_cmac_context * ctx ) <nl> multiply_by_u ( ctx -> K1 , L ); <nl> multiply_by_u ( ctx -> K2 , ctx -> K1 ); <nl>  <nl> + mbedtls_zeroize ( L , sizeof ( L ) ); <nl> + <nl> return ( 0 ); <nl> } <nl>  <nl> int mbedtls_aes_cmac_prf_128 ( mbedtls_cmac_context * ctx , <nl> { <nl> return ( ret ); <nl> } <nl> + <nl> + mbedtls_zeroize ( int_key , sizeof ( int_key ) ); <nl> + <nl> return ( mbedtls_cmac_generate ( ctx , input , in_len , tag , 16 ) ); <nl> } <nl> 
static const char * getPCPOpCodeStr ( uint8_t opcode ) <nl> * buffers are same */ <nl> static void copyIPv6IfDifferent ( void * dest , const void * src ) <nl> { <nl> - if ( dest != src ) { <nl> + if ( dest != src && src != NULL ) { <nl> memcpy ( dest , src , sizeof ( struct in6_addr )); <nl> } <nl> }
* Project : miniupnp <nl> * Web : http :// miniupnp . free . fr / <nl> * Author : Thomas BERNARD <nl> - * copyright ( c ) 2005 - 2016 Thomas Bernard <nl> + * copyright ( c ) 2005 - 2017 Thomas Bernard <nl> * This software is subjet to the conditions detailed in the <nl> * provided LICENCE file . */ <nl> /*# include < syslog . h >*/ <nl> connectToMiniSSDPD ( const char * socketpath ) <nl> # endif /* # ifdef MINIUPNPC_SET_SOCKET_TIMEOUT */ <nl> if (! socketpath ) <nl> socketpath = "/ var / run / minissdpd . sock "; <nl> + memset (& addr , 0 , sizeof ( addr )); <nl> addr . sun_family = AF_UNIX ; <nl> strncpy ( addr . sun_path , socketpath , sizeof ( addr . sun_path )); <nl> /* TODO : check if we need to handle the EINTR */
getHTTPResponse ( int s , int * size ) <nl> chunked = 1 ; <nl> } <nl> } <nl> - while ( header_buf [ i ]=='\ r ' || header_buf [ i ] == '\ n ') <nl> + while (( i < ( int ) header_buf_used ) && ( header_buf [ i ]=='\ r ' || header_buf [ i ] == '\ n ')) <nl> i ++; <nl> linestart = i ; <nl> colon = linestart ;
upnp_redirect ( const char * rhost , unsigned short eport , <nl> "% hu ->% s :% hu % s ", eport , iaddr , iport , protocol ); <nl> return - 3 ; <nl> } <nl> + <nl> + if ( desc == NULL ) <nl> + desc = ""; /* assume empty description */ <nl> + <nl> /* IGDv1 ( WANIPConnection : 1 Service Template Version 1 . 01 / Nov 12 , 2001 ) <nl> * - 2 . 2 . 20 . PortMappingDescription : <nl> * Overwriting Previous / Existing Port Mappings :
try_entry ( const SimpleConfEntry * const entry , const char * line , <nl> out_pnt ++; <nl> state = STATE_TEMPLATE_RCHAR ; <nl> } else { <nl> + free ( arg ); <nl> return ENTRYRESULT_INVALID_ENTRY ; <nl> } <nl> continue ;
if ( debug [' G ']) print ("% ux : % s : arm % d \ n ", ( uint32 )( p -> pc ), p -> from . sym -> name , p - <nl> r = p -> reg ; <nl> if ( p -> to . type == D_NONE ) <nl> rt = 0 ; <nl> - if ( p -> as == AMOVW || p -> as == AMVN ) <nl> + if ( p -> as == AMOVB || p -> as == AMOVH || p -> as == AMOVW || p -> as == AMVN ) <nl> r = 0 ; <nl> else <nl> if ( r == NREG )
static struct DWAbbrev { <nl> DW_TAG_subrange_type , DW_CHILDREN_no , <nl> // No name ! <nl> DW_AT_type , DW_FORM_ref_addr , <nl> - DW_AT_upper_bound , DW_FORM_data1 , <nl> + DW_AT_upper_bound , DW_FORM_udata , <nl> 0 , 0 <nl> }, <nl> 
static void list_type ( FUNC_TYPE ft , int one ) <nl> { <nl> FUNCTION * fp ; <nl> int i = 0 ; <nl> - DISPLAY_COLUMNS dc ; <nl> + DISPLAY_COLUMNS dc = { 0 }; <nl>  <nl> if (! one ) <nl> calculate_columns (& dc );
int bn_copy_words ( BN_ULONG * out , const BIGNUM * in , int size ) <nl> return 0 ; <nl>  <nl> memset ( out , 0 , sizeof (* out ) * size ); <nl> - memcpy ( out , in -> d , sizeof (* out ) * in -> top ); <nl> + if ( in -> d != NULL ) <nl> + memcpy ( out , in -> d , sizeof (* out ) * in -> top ); <nl> return 1 ; <nl> } <nl> 
static int do_dir ( const char * dirname , enum Hash h ) <nl> strerror ( errno )); <nl> errs ++; <nl> } <nl> + bit_set ( idmask , nextid ); <nl> } else if ( remove_links ) { <nl> /* Link to be deleted */ <nl> snprintf ( buf , buflen , "% s % s % n % 08x .% s % d ",
static int pkey_rsa_ctrl ( EVP_PKEY_CTX * ctx , int type , int p1 , void * p2 ) <nl> *( const EVP_MD **) p2 = rctx -> md ; <nl> } else { <nl> if ( rsa_pss_restricted ( rctx )) { <nl> - if ( EVP_MD_type ( rctx -> md ) == EVP_MD_type ( p2 )) <nl> + if ( EVP_MD_type ( rctx -> mgf1md ) == EVP_MD_type ( p2 )) <nl> return 1 ; <nl> RSAerr ( RSA_F_PKEY_RSA_CTRL , RSA_R_MGF1_DIGEST_NOT_ALLOWED ); <nl> return 0 ;
static int compute_key ( unsigned char * key , const BIGNUM * pub_key , DH * dh ) <nl> goto err ; <nl> BN_CTX_start ( ctx ); <nl> tmp = BN_CTX_get ( ctx ); <nl> + if ( tmp == NULL ) <nl> + goto err ; <nl>  <nl> if ( dh -> priv_key == NULL ) { <nl> DHerr ( DH_F_COMPUTE_KEY , DH_R_NO_PRIVATE_VALUE );
static void cms_env_set_version ( CMS_EnvelopedData * env ) <nl> env -> version = 2 ; <nl> } <nl> } <nl> - if ( env -> version == 2 ) <nl> - return ; <nl> if ( env -> originatorInfo || env -> unprotectedAttrs ) <nl> env -> version = 2 ; <nl> + if ( env -> version == 2 ) <nl> + return ; <nl> env -> version = 0 ; <nl> } <nl> 
int ASN1_item_verify ( const ASN1_ITEM * it , X509_ALGOR * a , <nl>  <nl> if ( signature -> type == V_ASN1_BIT_STRING && signature -> flags & 0x7 ) <nl> { <nl> - ASN1err ( ASN1_F_ASN1_VERIFY , ASN1_R_INVALID_BIT_STRING_BITS_LEFT ); <nl> + ASN1err ( ASN1_F_ASN1_ITEM_VERIFY , ASN1_R_INVALID_BIT_STRING_BITS_LEFT ); <nl> return - 1 ; <nl> } <nl> 
bool extract_sockaddr ( char * url , char ** sockaddr_url , char ** sockaddr_port ) <nl>  <nl> if ( url_len < 1 ) <nl> return false ; <nl> + <nl> + if ( url_len >= sizeof ( url_address )) <nl> + { <nl> + applog ( LOG_WARNING , "% s : Truncating overflowed address '%.* s '", <nl> + __func__ , url_len , url_begin ); <nl> + url_len = sizeof ( url_address ) - 1 ; <nl> + } <nl>  <nl> sprintf ( url_address , "%.* s ", url_len , url_begin ); <nl> 
retry : <nl> struct stat statbuf ; <nl>  <nl> strcpy ( filename , str ); <nl> + free ( str ); <nl> if (! stat ( filename , & statbuf )) { <nl> wlogprint (" File exists , overwrite ?\ n "); <nl> input = getch (); <nl> retry : <nl> goto retry ; <nl> } <nl> } <nl> + else <nl> + free ( str ); <nl> fcfg = fopen ( filename , " w "); <nl> if (! fcfg ) { <nl> wlogprint (" Cannot open or create file \ n ");
struct pool * add_pool ( void ) <nl> sprintf ( buf , " Pool % d ", pool -> pool_no ); <nl> pool -> poolname = strdup ( buf ); <nl>  <nl> - pools = realloc ( pools , sizeof ( struct pool *) * ( total_pools + 2 )); <nl> + pools = ( struct pool **) realloc ( pools , sizeof ( struct pool *) * ( total_pools + 2 )); <nl> pools [ total_pools ++] = pool ; <nl> mutex_init (& pool -> pool_lock ); <nl> if ( unlikely ( pthread_cond_init (& pool -> cr_cond , NULL )))
CLIENT * handle_starttls ( CLIENT * client , int opt , GArray * servers , uint32_t cflag <nl>  <nl> if ( ret < 0 ) { <nl> gnutls_deinit (* session ); <nl> + g_free ( session ); <nl> return NULL ; <nl> } <nl> client -> tls_session = session ;
void SendCoinsDialog :: on_sendButton_clicked () <nl> QStringList formatted ; <nl> foreach ( const SendCoinsRecipient & rcp , recipients ) <nl> { <nl> - formatted . append ( tr ("% 1 to % 2 (% 3 )"). arg ( BitcoinUnits :: formatWithUnit ( BitcoinUnits :: BTC , rcp . amount ), rcp . label , rcp . address )); <nl> + formatted . append ( tr ("< b >% 1 </ b > to % 2 (% 3 )"). arg ( BitcoinUnits :: formatWithUnit ( BitcoinUnits :: BTC , rcp . amount ), rcp . label , rcp . address )); <nl> } <nl>  <nl> QMessageBox :: StandardButton retval = QMessageBox :: question ( this , tr (" Confirm send coins "),
void ThreadRPCServer2 ( void * parg ) <nl> ip :: tcp :: endpoint endpoint ( bindAddress , GetArg ("- rpcport ", 8332 )); <nl> ip :: tcp :: acceptor acceptor ( io_service , endpoint ); <nl>  <nl> + acceptor . set_option ( boost :: asio :: ip :: tcp :: acceptor :: reuse_address ( true )); <nl> + <nl> # ifdef USE_SSL <nl> ssl :: context context ( io_service , ssl :: context :: sslv23 ); <nl> if ( fUseSSL )
Value listtransactions ( const Array & params , bool fHelp ) <nl> if ( pacentry != 0 ) <nl> AcentryToJSON (* pacentry , strAccount , ret ); <nl>  <nl> - if ( ret . size () >= ( nCount + nFrom )) break ; <nl> + if (( int ) ret . size () >= ( nCount + nFrom )) break ; <nl> } <nl> // ret is newest to oldest <nl> 
static rsRetVal createSocket ( instanceConf_t * info , void ** sock ) { <nl>  <nl> /* Do the bind / connect ... */ <nl> if ( info -> action == ACTION_CONNECT ) { <nl> - rv = zsocket_connect (* sock , info -> description ); <nl> + rv = zsocket_connect (* sock , "% s ", info -> description ); <nl> if ( rv == - 1 ) { <nl> errmsg . LogError ( 0 , <nl> RS_RET_INVALID_PARAMS , <nl> static rsRetVal createSocket ( instanceConf_t * info , void ** sock ) { <nl> } <nl> DBGPRINTF (" imzmq3 : connect for % s successful \ n ", info -> description ); <nl> } else { <nl> - rv = zsocket_bind (* sock , info -> description ); <nl> + rv = zsocket_bind (* sock , "% s ", info -> description ); <nl> if ( rv == - 1 ) { <nl> errmsg . LogError ( 0 , <nl> RS_RET_INVALID_PARAMS ,
# include " lib_ksils12 . h " <nl> # include " lib_ksi_queue . h " <nl>  <nl> - typedef unsigned char uchar ; <nl> # ifndef VERSION <nl> # define VERSION " no - version " <nl> # endif <nl> ksiCreateFile ( rsksictx ctx , const char * path , uid_t uid , gid_t gid , int mode , bo <nl> goto done ; <nl> } <nl>  <nl> - if ( stat_st . st_size == 0 && header != NULL ) <nl> - fwrite ( header , strlen ( header ), 1 , f ); <nl> + if ( stat_st . st_size == 0 && header != NULL ) { <nl> + if ( fwrite ( header , strlen ( header ), 1 , f ) != 1 ) { <nl> + report ( ctx , " ksiOpenSigFile : fwrite for file % s failed : % s ", <nl> + path , strerror ( errno )); <nl> + goto done ; <nl> + } <nl> + } <nl>  <nl> done : <nl> return f ;
* <nl> * Module begun 2008 - 10 - 09 by Rainer Gerhards ( based on previous code from syslogd . c ) <nl> * <nl> - * Copyright 2008 - 2014 Rainer Gerhards and Adiscon GmbH . <nl> + * Copyright 2008 - 2015 Rainer Gerhards and Adiscon GmbH . <nl> * <nl> * This file is part of the rsyslog runtime library . <nl> * <nl> rsRetVal <nl> parserConstructViaModAndName ( modInfo_t * __restrict__ pMod , uchar * const __restrict__ pName , void * pInst ) <nl> { <nl> rsRetVal localRet ; <nl> - parser_t * pParser ; <nl> + parser_t * pParser = NULL ; <nl> DEFiRet ; <nl>  <nl> if ( pInst == NULL && pMod -> mod . pm . newParserInst != NULL ) { <nl> parserConstructViaModAndName ( modInfo_t * __restrict__ pMod , uchar * const __restri <nl> pParser -> pInst = pInst ; <nl> CHKiRet ( parserConstructFinalize ( pParser )); <nl> finalize_it : <nl> + if ( iRet != RS_RET_OK ) <nl> + free ( pParser ); <nl> RETiRet ; <nl> } <nl> BEGINobjDestruct ( parser ) /* be sure to specify the object type also in END and CODESTART macros ! */
weechat_lua_api_config_reload_cb ( void * data , <nl> { <nl> lua_argv [ 0 ] = ( script_callback -> data ) ? script_callback -> data : empty_arg ; <nl> lua_argv [ 1 ] = script_ptr2str ( config_file ); <nl> - lua_argv [ 2 ] = NULL ; <nl>  <nl> rc = ( int *) weechat_lua_exec ( script_callback -> script , <nl> WEECHAT_SCRIPT_EXEC_INT ,
gui_chat_draw ( struct t_gui_buffer * buffer , int clear_chat ) <nl>  <nl> if ( clear_chat ) <nl> { <nl> - snprintf ( format_empty , 32 , "%%-% ds ", ptr_win -> win_chat_width ); <nl> + snprintf ( format_empty , sizeof ( format_empty ), <nl> + "%%-% ds ", ptr_win -> win_chat_width ); <nl> for ( i = 0 ; i < ptr_win -> win_chat_height ; i ++) <nl> { <nl> mvwprintw ( GUI_WINDOW_OBJECTS ( ptr_win )-> win_chat , i , 0 ,
hook_line_free_data ( struct t_hook * hook ) <nl> if (! hook || ! hook -> hook_data ) <nl> return ; <nl>  <nl> + if ( HOOK_LINE ( hook , buffers )) <nl> + { <nl> + string_free_split ( HOOK_LINE ( hook , buffers )); <nl> + HOOK_LINE ( hook , buffers ) = NULL ; <nl> + } <nl> if ( HOOK_LINE ( hook , tags_array )) <nl> { <nl> string_free_split_tags ( HOOK_LINE ( hook , tags_array ));
COMMAND_CALLBACK ( cursor ) <nl> gui_cursor_move_xy ( x , y ); <nl> } <nl> } <nl> + free ( str_x ); <nl> } <nl> } <nl> else
archive_read_format_iso9660_read_data ( struct archive_read * a , <nl> if ( bytes_read == 0 ) <nl> archive_set_error (& a -> archive , ARCHIVE_ERRNO_MISC , <nl> " Truncated input file "); <nl> - if ( buff == NULL ) <nl> + if (* buff == NULL ) <nl> return ( ARCHIVE_FATAL ); <nl> if ( bytes_read > iso9660 -> entry_bytes_remaining ) <nl> bytes_read = iso9660 -> entry_bytes_remaining ;
safe_fprintf ( FILE * f , const char * fmt , ...) <nl> } <nl>  <nl> /* If our output buffer is full , dump it and keep going . */ <nl> - if ( i > ( sizeof ( outbuff ) - 20 )) { <nl> + if ( i > ( sizeof ( outbuff ) - 128 )) { <nl> outbuff [ i ] = '\ 0 '; <nl> fprintf ( f , "% s ", outbuff ); <nl> i = 0 ;
read_Header ( struct _7zip * zip , struct _7z_header_info * h , <nl> /* The high 16 bits of attributes is a posix file mode . */ <nl> entries [ i ]. mode = entries [ i ]. attr >> 16 ; <nl> if ( entries [ i ]. flg & HAS_STREAM ) { <nl> - if ( sindex >= si -> ss . unpack_streams ) <nl> + if (( size_t ) sindex >= si -> ss . unpack_streams ) <nl> return (- 1 ); <nl> if ( entries [ i ]. mode == 0 ) <nl> entries [ i ]. mode = AE_IFREG | 0777 ;
setup_current_filesystem ( struct archive_read_disk * a ) <nl> if (! GetVolumePathName ( tree_current_access_path ( t ), vol , sizeof ( vol ))) { <nl> t -> current_filesystem -> remote = - 1 ; <nl> archive_set_error (& a -> archive , ARCHIVE_ERRNO_MISC , <nl> - " GetVolumePathName failed : % d ", GetLastError ()); <nl> + " GetVolumePathName failed : % d ", ( int ) GetLastError ()); <nl> return ( ARCHIVE_FAILED ); <nl> } <nl> switch ( GetDriveType ( vol )) {
xmlIsID ( xmlDocPtr doc , xmlNodePtr elem , xmlAttrPtr attr ) { <nl> (! strcmp (( char *) attr -> ns -> prefix , " xml "))) <nl> return ( 1 ); <nl> if ( doc == NULL ) return ( 0 ); <nl> - if (( doc -> intSubset == NULL ) && ( doc -> extSubset == NULL )) { <nl> + if (( doc -> intSubset == NULL ) && ( doc -> extSubset == NULL ) && <nl> + ( doc -> type != XML_HTML_DOCUMENT_NODE )) { <nl> return ( 0 ); <nl> } else if ( doc -> type == XML_HTML_DOCUMENT_NODE ) { <nl> if (( xmlStrEqual ( BAD_CAST " id ", attr -> name )) ||
xmlXPathNodeCollectAndTest ( xmlXPathParserContextPtr ctxt , <nl> break ; <nl> } <nl> } else if ( cur -> type == type ) { <nl> - if ( type == XML_NAMESPACE_DECL ) <nl> + if ( cur -> type == XML_NAMESPACE_DECL ) <nl> XP_TEST_HIT_NS <nl> else <nl> XP_TEST_HIT
xz_head ( xz_statep state ) <nl> state -> strm = init ; <nl> state -> strm . avail_in = 0 ; <nl> state -> strm . next_in = NULL ; <nl> - if ( lzma_auto_decoder (& state -> strm , UINT64_MAX , 0 ) != LZMA_OK ) { <nl> + if ( lzma_auto_decoder (& state -> strm , 100000000 , 0 ) != LZMA_OK ) { <nl> xmlFree ( state -> out ); <nl> xmlFree ( state -> in ); <nl> state -> size = 0 ;
retry : <nl>  <nl> written = out -> size - out -> use ; <nl>  <nl> + if ( written > 0 ) <nl> + written --; /* Gennady : count '/ 0 ' */ <nl> + <nl> /* <nl> * First specific handling of in = NULL , i . e . the initialization call <nl> */ <nl> retry : <nl> if ( handler -> output != NULL ) { <nl> ret = handler -> output (& out -> content [ out -> use ], & written , <nl> NULL , & toconv ); <nl> - out -> use += written ; <nl> - out -> content [ out -> use ] = 0 ; <nl> + if ( ret == 0 ) { /* Gennady : check return value */ <nl> + out -> use += written ; <nl> + out -> content [ out -> use ] = 0 ; <nl> + } <nl> } <nl> # ifdef LIBXML_ICONV_ENABLED <nl> else if ( handler -> iconv_out != NULL ) {
static void doXPathDump ( xmlXPathObjectPtr cur ) { <nl> # ifdef LIBXML_OUTPUT_ENABLED <nl> xmlSaveCtxtPtr ctxt ; <nl>  <nl> - if ( cur -> nodesetval -> nodeNr <= 0 ) { <nl> + if (( cur -> nodesetval == NULL ) || ( cur -> nodesetval -> nodeNr <= 0 )) { <nl> fprintf ( stderr , " XPath set is empty \ n "); <nl> progresult = XMLLINT_ERR_XPATH ; <nl> break ;
gen_select_val ( LoaderState * stp , GenOpArg S , GenOpArg Fail , <nl> op -> a [ j + size ] = Fail ; <nl>  <nl> # ifdef DEBUG <nl> - for ( i = 0 ; i < size ; i ++) { <nl> + for ( i = 0 ; i < size - 1 ; i ++) { <nl> ASSERT ( op -> a [ i + 3 ]. val <= op -> a [ i + 4 ]. val ); <nl> } <nl> # endif
erts_port_task_schedule ( Eterm id , <nl> # endif <nl> } <nl>  <nl> - ASSERT (! enq_port || !( runq -> flags & ERTS_RUNQ_FLG_SUSPENDED )); <nl> + ASSERT (! enq_port <nl> + || !( ERTS_RUNQ_FLGS_GET_NOB ( runq ) & ERTS_RUNQ_FLG_SUSPENDED )); <nl>  <nl> if (! pp -> sched . taskq ) <nl> pp -> sched . taskq = port_taskq_init ( port_taskq_alloc (), pp );
# define HAVE_EC <nl> # endif <nl>  <nl> -// ( test for == 1 . 1 . 1pre8 ) <nl> -# if OPENSSL_VERSION_NUMBER == ( PACKED_OPENSSL_VERSION_PLAIN ( 1 , 1 , 1 ) - 7 ) \ <nl> +// ( test for >= 1 . 1 . 1pre8 ) <nl> +# if OPENSSL_VERSION_NUMBER >= ( PACKED_OPENSSL_VERSION_PLAIN ( 1 , 1 , 1 ) - 7 ) \ <nl> && ! defined ( HAS_LIBRESSL ) \ <nl> && defined ( HAVE_EC ) <nl> // EXPERIMENTAL :
imap_auth_res_t imap_auth_sasl ( IMAP_DATA * idata , const char * method ) <nl> irc = IMAP_CMD_CONTINUE ; <nl>  <nl> /* looping protocol */ <nl> - while ( rc == SASL_CONTINUE ) <nl> + while ( rc == SASL_CONTINUE || olen > 0 ) <nl> { <nl> do <nl> irc = imap_cmd_step ( idata ); <nl> imap_auth_res_t imap_auth_sasl ( IMAP_DATA * idata , const char * method ) <nl> mutt_socket_write ( idata -> conn , "*\ r \ n "); <nl> dprint ( 1 , ( debugfile , " imap_auth_sasl : sasl_client_step error % d \ n ", rc )); <nl> } <nl> + <nl> + olen = 0 ; <nl> } <nl>  <nl> while ( irc != IMAP_CMD_OK )
int mutt_index_menu ( void ) <nl> imap_allow_reopen ( Context ); <nl> # endif <nl>  <nl> - index_hint = ( Context -> vcount && menu -> current < Context -> vcount ) ? CURHDR -> index : 0 ; <nl> + index_hint = ( Context -> vcount && menu -> current >= 0 && menu -> current < Context -> vcount ) ? CURHDR -> index : 0 ; <nl>  <nl> if (( check = mx_check_mailbox ( Context , & index_hint , 0 )) < 0 ) <nl> {
static void avalon4_update ( struct cgpu_info * avalon4 ) <nl> if ( info -> set_voltage [ i ] == info -> set_voltage [ 0 ]) <nl> continue ; <nl>  <nl> + avalon4_stratum_set ( avalon4 , pool , i , 0 ); <nl> + } <nl> + } else { <nl> + for ( i = 1 ; i < AVA4_DEFAULT_MODULARS ; i ++) { <nl> + if (! info -> enable [ i ]) <nl> + continue ; <nl> + if ( info -> mod_type [ i ] == AVA4_TYPE_MM40 ) <nl> + continue ; <nl> + <nl> avalon4_stratum_set ( avalon4 , pool , i , 0 ); <nl> } <nl> }
static const char * nodatareturned = " no data returned "; <nl> cgpu -> usbinfo . continuous_ioerr_count = 0 ; \ <nl> } <nl>  <nl> +/* Timeout errors on writes are unusual and should be treated as IO errors . */ <nl> +# define WRITEIOERR_CHECK ( cgpu , err ) \ <nl> + if ( err == LIBUSB_ERROR_IO || err == LIBUSB_ERROR_TIMEOUT ) { \ <nl> + cgpu -> usbinfo . ioerr_count ++; \ <nl> + cgpu -> usbinfo . continuous_ioerr_count ++; \ <nl> + } else { \ <nl> + cgpu -> usbinfo . continuous_ioerr_count = 0 ; \ <nl> + } <nl> + <nl> # if 0 // enable USBDEBUG - only during development testing <nl> static const char * debug_true_str = " true "; <nl> static const char * debug_false_str = " false ";
static void set_work_target ( struct work * work , int diff ) <nl> free ( htarget ); <nl> } <nl> } <nl> - memcpy ( work -> target , target , 256 ); <nl> + memcpy ( work -> target , target , 32 ); <nl> } <nl>  <nl> static void gen_stratum_work ( struct pool * pool , struct work * work )
static bool klondike_init ( struct cgpu_info * klncgpu ) <nl> sscanf ( opt_klondike_options , "% hu ,% lf ,% lf ,% hhu ", & cfgset . hashclock , & temp1 , & temp2 , & cfgset . fantarget ); <nl> cfgset . temptarget = cvtCToKln ( temp1 ); <nl> cfgset . tempcritical = cvtCToKln ( temp2 ); <nl> - cfgset . fantarget = ( int ) 256 * cfgset . fantarget / 100 ; <nl> + cfgset . fantarget = ( int ) 255 * cfgset . fantarget / 100 ; <nl> size = sizeof ( cfgset ); <nl> } <nl>  <nl> static void get_klondike_statline_before ( char * buf , struct cgpu_info * klncgpu ) <nl> { <nl> struct klondike_info * klninfo = ( struct klondike_info *)( klncgpu -> device_data ); <nl> uint8_t temp = 0xFF ; <nl> + uint16_t fan = 0 ; <nl> int dev ; <nl>  <nl> if ( klninfo -> status == NULL ) <nl> static void get_klondike_statline_before ( char * buf , struct cgpu_info * klncgpu ) <nl> for ( dev = 0 ; dev <= klninfo -> status -> slavecount ; dev ++) { <nl> if ( klninfo -> status [ dev ]. temp < temp ) <nl> temp = klninfo -> status [ dev ]. temp ; <nl> + fan += klninfo -> cfg [ dev ]. fantarget ; <nl> } <nl> + fan /= klninfo -> status -> slavecount + 1 ; <nl> rd_unlock (&( klninfo -> stat_lock )); <nl>  <nl> - tailsprintf ( buf , " % 3 . 0fC 1 . 2V | ", cvtKlnToC ( temp )); <nl> + tailsprintf ( buf , " % 3 . 0fC % 3d % | ", cvtKlnToC ( temp ), fan * 100 / 255 ); <nl> } <nl>  <nl> static struct api_data * klondike_api_stats ( struct cgpu_info * klncgpu ) <nl> static struct api_data * klondike_api_stats ( struct cgpu_info * klncgpu ) <nl> sprintf ( buf , " Clock % d ", dev ); <nl> root = api_add_freq ( root , buf , & dClk , true ); <nl>  <nl> - unsigned int iFan = ( unsigned int ) 100 * klninfo -> cfg [ dev ]. fantarget / 256 ; <nl> + unsigned int iFan = ( unsigned int ) 100 * klninfo -> cfg [ dev ]. fantarget / 255 ; <nl> sprintf ( buf , " Fan Percent % d ", dev ); <nl> root = api_add_int ( root , buf , & iFan , true ); <nl> 
static int64_t bitfury_scanhash ( struct thr_info * thr , struct work * work , <nl> * look for the results to prev work . */ <nl> usb_read_timeout ( bitfury , info -> buf , 512 , & amount , 600 , C_BFO_GETRES ); <nl> info -> tot += amount ; <nl> + if ( unlikely ( thr -> work_restart )) <nl> + goto cascade ; <nl>  <nl> /* Now look for the bulk of the previous work results , they will come <nl> * in a batch following the first data . */ <nl> static int64_t bitfury_scanhash ( struct thr_info * thr , struct work * work , <nl> info -> tot += amount ; <nl> }; <nl>  <nl> + if ( unlikely ( thr -> work_restart )) <nl> + goto cascade ; <nl> + <nl> /* Send work */ <nl> usb_write ( bitfury , buf , 45 , & amount , C_BFO_REQWORK ); <nl> /* Get response acknowledging work */
static inline void cg_runlock ( cglock_t * lock ) <nl>  <nl> static inline void cg_wunlock ( cglock_t * lock ) <nl> { <nl> - wr_unlock (& lock -> rwlock ); <nl> + wr_unlock_noyield (& lock -> rwlock ); <nl> mutex_unlock (& lock -> mutex ); <nl> } <nl> 
retry : <nl> struct stat statbuf ; <nl>  <nl> strcpy ( filename , str ); <nl> + free ( str ); <nl> if (! stat ( filename , & statbuf )) { <nl> wlogprint (" File exists , overwrite ?\ n "); <nl> input = getch (); <nl> retry : <nl> goto retry ; <nl> } <nl> } <nl> + else <nl> + free ( str ); <nl> fcfg = fopen ( filename , " w "); <nl> if (! fcfg ) { <nl> wlogprint (" Cannot open or create file \ n ");
static bool klondike_get_stats ( struct cgpu_info * klncgpu ) <nl> for ( dev = 0 ; dev <= slaves ; dev ++) { <nl> char * reply = SendCmdGetReply ( klncgpu , ' S ', dev , 0 , NULL ); <nl> if ( reply != NULL ) <nl> - klninfo -> status [ dev ] = *( WORKSTATUS *)( reply + 2 ); <nl> + memcpy (( void *)(&( klninfo -> status [ dev ])), reply + 2 , sizeof ( klninfo -> status [ dev ])); <nl> } <nl> wr_unlock (&( klninfo -> stat_lock )); <nl> 
void CAnalyzerCCpp :: GetIndependentBuldIdPC ( const std :: string & pBuildIdPC , std :: s <nl> line += pBuildIdPC [ ii ]; <nl> ii ++; <nl> } <nl> - while (! isspace ( line [ jj ]) && jj < line . length ()) <nl> + while ( line [ jj ] != '+' && jj < line . length ()) <nl> { <nl> jj ++; <nl> } <nl> jj ++; <nl> - while (! isspace ( line [ jj ]) && jj < line . length ()) <nl> + while ( line [ jj ] != '@' && jj < line . length ()) <nl> { <nl> - pIndependentBuildIdPC += line [ jj ]; <nl> + if (! isspace ( line [ jj ])) <nl> + { <nl> + pIndependentBuildIdPC += line [ jj ]; <nl> + } <nl> jj ++; <nl> } <nl> ii ++;
int main ( int argc , char ** argv ) <nl> if ( snprintf ( path , sizeof ( path ), "% s /% s - coredump ", g_settings_dump_location , last_slash ) >= sizeof ( path )) <nl> error_msg_and_die (" Error saving '% s ': truncated long file path ", path ); <nl>  <nl> - int abrt_core_fd = xopen3 ( path , O_WRONLY | O_CREAT | O_TRUNC , 0600 ); <nl> + unlink ( path ); <nl> + int abrt_core_fd = xopen3 ( path , O_WRONLY | O_CREAT | O_EXCL , 0600 ); <nl> off_t core_size = copyfd_eof ( STDIN_FILENO , abrt_core_fd , COPYFD_SPARSE ); <nl> if ( core_size < 0 || fsync ( abrt_core_fd ) != 0 ) <nl> {
static bool problem_info_ensure_writable ( problem_info_t * pi ) <nl>  <nl> static problem_info_t * problem_info_new ( const char * dir ) <nl> { <nl> - problem_info_t * pi = xzalloc ( sizeof (* pi )); <nl> + problem_info_t * pi = g_new0 ( problem_info_t , 1 ); <nl> pi -> problem_data = problem_data_new (); <nl> problem_info_set_dir ( pi , dir ); <nl> return pi ; <nl> static void problem_info_free ( problem_info_t * pi ) <nl> return ; <nl>  <nl> problem_data_free ( pi -> problem_data ); <nl> - free ( pi ); <nl> + g_free ( pi ); <nl> } <nl>  <nl> static void run_event_async ( problem_info_t * pi , const char * event_name , int flags ); <nl> struct event_processing_state <nl>  <nl> static struct event_processing_state * new_event_processing_state ( void ) <nl> { <nl> - struct event_processing_state * p = xzalloc ( sizeof (* p )); <nl> + struct event_processing_state * p = g_new0 ( struct event_processing_state , 1 ); <nl> p -> child_pid = - 1 ; <nl> p -> child_stdout_fd = - 1 ; <nl> p -> cmd_output = strbuf_new (); <nl> static void free_event_processing_state ( struct event_processing_state * p ) <nl> return ; <nl>  <nl> strbuf_free ( p -> cmd_output ); <nl> - free ( p ); <nl> + g_free ( p ); <nl> } <nl>  <nl> static char * build_message ( problem_info_t * pi )
wavlike_subchunk_parse ( SF_PRIVATE * psf , int chunk , uint32_t chunk_length ) <nl>  <nl> case exif_MARKER : <nl> psf_log_printf ( psf , " % M \ n ", chunk ) ; <nl> - bytesread += exif_subchunk_parse ( psf , chunk_length - bytesread ) ; <nl> + if ( chunk_length > bytesread ) <nl> + bytesread += exif_subchunk_parse ( psf , chunk_length - bytesread ) ; <nl> continue ; <nl>  <nl> case data_MARKER :
PHP_HTTP_API php_http_message_t * php_http_message_parse ( php_http_message_t * msg , <nl> if ( greedy ) { <nl> flags |= PHP_HTTP_MESSAGE_PARSER_GREEDY ; <nl> } <nl> - if ( FAILURE == php_http_message_parser_parse (& p , & buf , flags , & msg )) { <nl> + if ( PHP_HTTP_MESSAGE_PARSER_STATE_FAILURE == php_http_message_parser_parse (& p , & buf , flags , & msg )) { <nl> if ( free_msg ) { <nl> php_http_message_free (& msg ); <nl> }
ImagingPcdDecode ( Imaging im , ImagingCodecState state , UINT8 * buf , int bytes ) <nl> out [ 0 ] = ptr [ x ]; <nl> out [ 1 ] = ptr [( x + 4 * state -> xsize )/ 2 ]; <nl> out [ 2 ] = ptr [( x + 5 * state -> xsize )/ 2 ]; <nl> - out += 4 ; <nl> + out += 3 ; <nl> } <nl>  <nl> state -> shuffle (( UINT8 *) im -> image [ state -> y ], <nl> ImagingPcdDecode ( Imaging im , ImagingCodecState state , UINT8 * buf , int bytes ) <nl> out [ 0 ] = ptr [ x + state -> xsize ]; <nl> out [ 1 ] = ptr [( x + 4 * state -> xsize )/ 2 ]; <nl> out [ 2 ] = ptr [( x + 5 * state -> xsize )/ 2 ]; <nl> - out += 4 ; <nl> + out += 3 ; <nl> } <nl>  <nl> state -> shuffle (( UINT8 *) im -> image [ state -> y ],
static int parse_cmdline_encoder ( int argc , char ** argv , opj_cparameters_t * param <nl> } <nl>  <nl> /* If subsampled image is provided , automatically disable MCT */ <nl> - if ( (( parameters -> decod_format == RAW_DFMT ) <nl> - || ( parameters -> decod_format == RAWL_DFMT )) <nl> - && (( raw_cp -> rawComps [ 1 ]. dx > 1 ) <nl> - || ( raw_cp -> rawComps [ 1 ]. dy > 1 ) <nl> - || ( raw_cp -> rawComps [ 2 ]. dx > 1 ) <nl> - || ( raw_cp -> rawComps [ 2 ]. dy > 1 ))) { <nl> + if ( (( parameters -> decod_format == RAW_DFMT ) || ( parameters -> decod_format == RAWL_DFMT )) <nl> + && ( (( raw_cp -> rawComp > 1 ) && (( raw_cp -> rawComps [ 1 ]. dx > 1 ) || ( raw_cp -> rawComps [ 1 ]. dy > 1 ))) <nl> + || (( raw_cp -> rawComp > 2 ) && (( raw_cp -> rawComps [ 2 ]. dx > 1 ) || ( raw_cp -> rawComps [ 2 ]. dy > 1 ))) <nl> + )) { <nl> parameters -> tcp_mct = 0 ; <nl> } <nl> 
int get_num_images ( char * imgdirpath ){ <nl> continue ; <nl> num_images ++; <nl> } <nl> + closedir ( dir ); <nl> return num_images ; <nl> } <nl>  <nl> int load_images ( dircnt_t * dirptr , char * imgdirpath ){ <nl> strcpy ( dirptr -> filename [ i ], content -> d_name ); <nl> i ++; <nl> } <nl> + closedir ( dir ); <nl> return 0 ; <nl> } <nl> 
int main ( int argc , char ** argv ) <nl>  <nl> /* Remove the temporary files */ <nl> /* -------------------------- */ <nl> - if ( cp . decod_format != PGX_CFMT ) { /* PNM PGM PPM or BMP */ <nl> + if ( cp . decod_format != PGX_DFMT ) { /* PNM PGM PPM or BMP */ <nl> for ( i = 0 ; i < img . numcomps ; i ++) { <nl> char tmp ; <nl> sprintf (& tmp , " Compo % d ", i );
bool ZrtpSdesStream :: createSdesProfile ( char * cryptoString , size_t * maxLen ) { <nl> /* Get B64 code for master key and master salt */ <nl> b64Len = b64Encode ( keySalt , keyLenBytes + saltLenBytes , b64keySalt , sizeof ( b64keySalt )); <nl> b64keySalt [ b64Len ] = '\ 0 '; <nl> - * maxLen = snprintf ( cryptoString , * maxLen , "% d % s inline :% s ", tag , pSuite -> name , b64keySalt ); <nl> + memset ( cryptoString , 0 , * maxLen ); <nl> + * maxLen = snprintf ( cryptoString , * maxLen - 1 , "% d % s inline :% s ", tag , pSuite -> name , b64keySalt ); <nl>  <nl> memset ( keySalt , 0 , sizeof ( keySalt )); <nl> return true ;
void IRC_Analyzer :: DeliverStream ( int length , const u_char * line , bool orig ) <nl> { <nl> vector < string > parts = SplitWords ( params , ' '); <nl>  <nl> - // Remove nick name . <nl> - parts . erase ( parts . begin ()); <nl> - if ( parts . size () < 2 ) <nl> + if ( parts . size () < 3 ) <nl> { <nl> Weird (" irc_invalid_names_line "); <nl> return ; <nl> } <nl>  <nl> + // Remove nick name . <nl> + parts . erase ( parts . begin ()); <nl> + <nl> string type = parts [ 0 ]; <nl> string channel = parts [ 1 ]; <nl> 
void h2o_init_request ( h2o_req_t * req , h2o_conn_t * conn , h2o_req_t * src ) <nl> COPY ( scheme ); <nl> req -> version = src -> version ; <nl> h2o_vector_reserve (& req -> pool , ( h2o_vector_t *)& req -> headers , sizeof ( h2o_header_t ), src -> headers . size ); <nl> - memcpy ( req -> headers . entries , src -> headers . entries , src -> headers . size ); <nl> + memcpy ( req -> headers . entries , src -> headers . entries , sizeof ( req -> headers . entries [ 0 ]) * src -> headers . size ); <nl> req -> headers . size = src -> headers . size ; <nl> req -> entity = src -> entity ; <nl> req -> http1_is_persistent = src -> http1_is_persistent ;
static void update_idle_timeout ( h2o_http2_conn_t * conn ) <nl> { <nl> h2o_timeout_unlink (& conn -> _timeout_entry ); <nl>  <nl> - if ( conn -> num_streams . pull . half_closed + conn -> num_streams . push . half_closed == 0 ) { <nl> + /* no active streams + no request body upload in progress */ <nl> + if ( conn -> num_streams . pull . half_closed + conn -> num_streams . push . half_closed == 0 && ! conn -> _request_body_in_progress ) { <nl> assert ( h2o_linklist_is_empty (& conn -> _pending_reqs )); <nl> conn -> _timeout_entry . cb = on_idle_timeout ; <nl> h2o_timeout_link ( conn -> super . ctx -> loop , & conn -> super . ctx -> http2 . idle_timeout , & conn -> _timeout_entry );
static problem_data_t * load_problem_data_if_not_yet ( problem_data_t * problem_data <nl> struct dump_dir * dd = dd_opendir ( dump_dir_name , /* flags :*/ 0 ); <nl> if (! dd ) <nl> { <nl> - problem_data_free ( problem_data ); <nl> return NULL ; <nl> } <nl> problem_data = create_problem_data_from_dump_dir ( dd );
void makeAddPoly ( Poly * pp , Agnode_t * n , float xmargin , float ymargin ) <nl> else { <nl> for ( i = 0 ; i < sides ; i ++) { <nl> double h = LEN ( poly -> vertices [ i ]. x , poly -> vertices [ i ]. y ); <nl> - fprintf ( stderr ," v =% g ,% g h =% g \ n ", poly -> vertices [ i ]. x , poly -> vertices [ i ]. y , h ); <nl> verts [ i ]. x = poly -> vertices [ i ]. x * ( 1 . 0 + xmargin / h ); <nl> verts [ i ]. y = poly -> vertices [ i ]. y * ( 1 . 0 + ymargin / h ); <nl> verts [ i ]. x = PS2INCH ( verts [ i ]. x );
int graphcmd ( ClientData clientData , Tcl_Interp * interp , <nl> } <nl> # ifndef WITH_CGRAPH <nl> e = agedge ( g , tail , head ); <nl> - agbindrec ( ce , " Agedgeinfo_t ", sizeof ( Agedgeinfo_t ), TRUE ); <nl> # else <nl> e = agedge ( g , tail , head , NULL , 1 ); <nl> + agbindrec ( e , " Agedgeinfo_t ", sizeof ( Agedgeinfo_t ), TRUE ); <nl> Tcl_AppendResult ( interp , obj2cmd ( e ), NULL ); <nl> # endif <nl> # ifndef WITH_CGRAPH
static void emit_cluster_colors ( GVJ_t * job , graph_t * g ) <nl> emit_cluster_colors ( job , sg ); <nl> if ((( str = agget ( sg , " color ")) != 0 ) && str [ 0 ]) <nl> gvrender_set_pencolor ( job , str ); <nl> + if ((( str = agget ( sg , " pencolor ")) != 0 ) && str [ 0 ]) <nl> + gvrender_set_pencolor ( job , str ); <nl> if ((( str = agget ( sg , " bgcolor ")) != 0 ) && str [ 0 ]) <nl> gvrender_set_pencolor ( job , str ); <nl> if ((( str = agget ( sg , " fillcolor ")) != 0 ) && str [ 0 ])
void fe_common_irc_finish_init ( void ) <nl> for ( tmp = setupservers ; tmp != NULL ; tmp = tmp -> next ) { <nl> SETUP_SERVER_REC * rec = tmp -> data ; <nl>  <nl> - if ( rec -> autoconnect && (* rec -> ircnet == '\ 0 ' || gslist_find_icase_string ( ircnets , rec -> ircnet ) == NULL )) { <nl> + if ( rec -> autoconnect && ( rec -> ircnet == NULL || * rec -> ircnet == '\ 0 ' || <nl> + gslist_find_icase_string ( ircnets , rec -> ircnet ) == NULL )) { <nl> if (* rec -> ircnet != '\ 0 ') <nl> ircnets = g_slist_append ( ircnets , rec -> ircnet ); <nl> 
static int sig_autoremove ( void ) <nl>  <nl> /* Close only logs with private messages */ <nl> logitem = log -> items -> data ; <nl> + if ( logitem -> servertag == NULL ) <nl> + continue ; <nl> + <nl> server = server_find_tag ( logitem -> servertag ); <nl> if ( logitem -> type == LOG_ITEM_TARGET && <nl> server != NULL && ! server -> ischannel (* logitem -> name ))
static void sig_init_finished ( void ) <nl> static char * fix_path ( const char * str ) <nl> { <nl> char * new_str = convert_home ( str ); <nl> + <nl> if (! g_path_is_absolute ( new_str )) { <nl> char * tmp_str = new_str ; <nl> - new_str = g_strdup_printf ("% s /% s ", g_get_current_dir (), tmp_str ); <nl> + char * current_dir = g_get_current_dir (); <nl> + <nl> + new_str = g_build_path ( G_DIR_SEPARATOR_S , current_dir , tmp_str ); <nl> + <nl> + g_free ( current_dir ); <nl> g_free ( tmp_str ); <nl> } <nl> + <nl> return new_str ; <nl> } <nl> 
static char * get_nicks ( IRC_SERVER_REC * server , WI_ITEM_REC * item , <nl> g_hash_table_lookup ( optlist , " yes ") == NULL ) { <nl> /* too many matches */ <nl> g_string_free ( str , TRUE ); <nl> + g_strfreev ( matches ); <nl> cmd_params_free ( free_arg ); <nl>  <nl> signal_emit (" error command ", 1 , <nl> static char * get_nicks ( IRC_SERVER_REC * server , WI_ITEM_REC * item , <nl> if ( str -> len > 0 ) g_string_truncate ( str , str -> len - 1 ); <nl> ret = str -> str ; <nl> g_string_free ( str , FALSE ); <nl> - <nl> + g_strfreev ( matches ); <nl> cmd_params_free ( free_arg ); <nl>  <nl> * ret_channel = channel ;
static void server_init ( IRC_SERVER_REC * server ) <nl> if (* ptr == ':') * ptr = ' _ '; <nl> } <nl>  <nl> + /* don ' t allow hostname to begin with number or '+', '-'. those <nl> + can be interpreted as ircnet ircd ' s mode parameter . <nl> + <nl> + username / hostname parameters should probably be configurable since <nl> + they ' re only needed with some old servers which uses them to count <nl> + unique users . */ <nl> + if (( hostname [ 0 ] >= ' 0 ' && hostname [ 0 ] <= ' 9 ') || <nl> + hostname [ 0 ] == '+' || hostname [ 0 ] == '-') <nl> + hostname [ 0 ] = ' _ '; <nl> + <nl> username = g_strdup ( conn -> username ); <nl> ptr = strchr ( username , ' '); <nl> if ( ptr != NULL ) * ptr = '\ 0 ';
multi_process_incoming_link ( struct multi_context * m , struct multi_instance * inst <nl> orig_buf = c -> c2 . buf . data ; <nl> if ( process_incoming_link_part1 ( c , lsi , floated )) <nl> { <nl> - if ( floated ) <nl> + /* nonzero length means that we have a valid , decrypted packed */ <nl> + if ( floated && c -> c2 . buf . len > 0 ) <nl> { <nl> multi_process_float ( m , m -> pending ); <nl> }
# include " file . h " <nl>  <nl> # ifndef lint <nl> - FILE_RCSID ("@(#)$ File : readelf . c , v 1 . 156 2018 / 10 / 19 00 : 33 : 04 christos Exp $") <nl> + FILE_RCSID ("@(#)$ File : readelf . c , v 1 . 157 2019 / 01 / 02 19 : 44 : 14 christos Exp $") <nl> # endif <nl>  <nl> # ifdef BUILTIN_ELF <nl> do_core_note ( struct magic_set * ms , unsigned char * nbuf , uint32_t type , <nl> char sbuf [ 512 ]; <nl> struct NetBSD_elfcore_procinfo pi ; <nl> memset (& pi , 0 , sizeof ( pi )); <nl> - memcpy (& pi , nbuf + doff , descsz ); <nl> + memcpy (& pi , nbuf + doff , MIN ( descsz , sizeof ( pi ))); <nl>  <nl> if ( file_printf ( ms , ", from '%. 31s ', pid =% u , uid =% u , " <nl> " gid =% u , nlwps =% u , lwp =% u ( signal % u / code % u )",
lexer_process_char_literal ( parser_context_t * context_p , /**< context */ <nl> parser_raise_error ( context_p , PARSER_ERR_LITERAL_LIMIT_REACHED ); <nl> } <nl>  <nl> + if ( length == 0 ) <nl> + { <nl> + has_escape = false ; <nl> + } <nl> + <nl> literal_p = ( lexer_literal_t *) parser_list_append ( context_p , & context_p -> literal_pool ); <nl> literal_p -> prop . length = ( uint16_t ) length ; <nl> literal_p -> type = literal_type ;
getname6 ( netdissect_options * ndo , const u_char * ap ) <nl> return ( p -> name ); <nl> } <nl>  <nl> - static const char hex [] = " 0123456789abcdef "; <nl> + static const char hex [ 16 ] = " 0123456789abcdef "; <nl>  <nl>  <nl> /* Find the hash node that corresponds the ether address ' ep ' */
lldp_private_8021_print ( netdissect_options * ndo , <nl> int subtype , hexdump = FALSE ; <nl> u_int sublen ; <nl> u_int tval ; <nl> - uint8_t i ; <nl> + u_int i ; <nl>  <nl> if ( tlv_len < 4 ) { <nl> return hexdump ; <nl> lldp_private_8021_print ( netdissect_options * ndo , <nl> ND_PRINT (( ndo , "\ n \ t Application Priority Table ")); <nl> while ( i < sublen ) { <nl> tval =*( tptr + i + 5 ); <nl> - ND_PRINT (( ndo , "\ n \ t Priority : % d , RES : % d , Sel : % d ", <nl> - tval >> 5 , ( tval >> 3 ) & 0x03 , ( tval & 0x07 ))); <nl> - ND_PRINT (( ndo , " Protocol ID : % d ", EXTRACT_16BITS ( tptr + i + 5 ))); <nl> + ND_PRINT (( ndo , "\ n \ t Priority : % u , RES : % u , Sel : % u , Protocol ID : % u ", <nl> + tval >> 5 , ( tval >> 3 ) & 0x03 , ( tval & 0x07 ), <nl> + EXTRACT_16BITS ( tptr + i + 5 ))); <nl> i = i + 3 ; <nl> } <nl> break ;
dhcp6opt_print ( const u_char * cp , const u_char * ep ) <nl> if ( ep < cp + sizeof (* dh6o )) <nl> goto trunc ; <nl> dh6o = ( struct dhcp6opt *) cp ; <nl> + TCHECK (* dh6o ); <nl> optlen = EXTRACT_16BITS (& dh6o -> dh6opt_len ); <nl> if ( ep < cp + sizeof (* dh6o ) + optlen ) <nl> goto trunc ;
ospf6_print_lshdr ( netdissect_options * ndo , <nl> { <nl> if (( const u_char *)( lshp + 1 ) > dataend ) <nl> goto trunc ; <nl> - ND_TCHECK ( lshp -> ls_type ); <nl> - ND_TCHECK ( lshp -> ls_seq ); <nl> + ND_TCHECK ( lshp -> ls_length ); /* last field of struct lsa6_hdr */ <nl>  <nl> ND_PRINT (( ndo , "\ n \ t Advertising Router % s , seq 0x % 08x , age % us , length % u ", <nl> ipaddr_string ( ndo , & lshp -> ls_router ),
ieee802_11_print ( netdissect_options * ndo , <nl> hdrlen = roundup2 ( hdrlen , 4 ); <nl> if ( ndo -> ndo_Hflag && FC_TYPE ( fc ) == T_DATA && <nl> DATA_FRAME_IS_QOS ( FC_SUBTYPE ( fc ))) { <nl> + if ( caplen < hdrlen + 1 ) { <nl> + ND_PRINT (( ndo , "% s ", tstr )); <nl> + return hdrlen ; <nl> + } <nl> meshdrlen = extract_mesh_header_length ( p + hdrlen ); <nl> hdrlen += meshdrlen ; <nl> } else
dhcpv4_print ( netdissect_options * ndo , <nl>  <nl> i = 0 ; <nl> while ( i < length ) { <nl> + if ( i + 2 > length ) <nl> + return - 1 ; <nl> tlv = cp + i ; <nl> type = ( uint8_t ) tlv [ 0 ]; <nl> optlen = ( uint8_t ) tlv [ 1 ]; <nl> dhcpv4_print ( netdissect_options * ndo , <nl>  <nl> ND_PRINT (( ndo , "% s ", tok2str ( dh4opt_str , " Unknown ", type ))); <nl> ND_PRINT (( ndo ," (% u )", optlen + 2 )); <nl> + if ( i + 2 + optlen > length ) <nl> + return - 1 ; <nl>  <nl> switch ( type ) { <nl> case DH4OPT_DNS_SERVERS :
vrrp_print ( netdissect_options * ndo , <nl>  <nl> vec [ 0 ]. ptr = bp ; <nl> vec [ 0 ]. len = len ; <nl> - if ( in_cksum ( vec , 1 )) <nl> + if ( in_cksum ( vec , 1 )) { <nl> + ND_TCHECK_16BITS (& bp [ 6 ]); <nl> ND_PRINT (( ndo , ", ( bad vrrp cksum % x )", <nl> EXTRACT_16BITS (& bp [ 6 ]))); <nl> + } <nl> } <nl>  <nl> if ( version == 3 && ND_TTEST2 ( bp [ 0 ], len )) {
smb_fdata ( netdissect_options * ndo , <nl> while ( buf < maxbuf ) { <nl> const u_char * buf2 ; <nl> depth ++; <nl> - buf2 = smb_fdata ( ndo , buf , fmt , maxbuf , unicodestr ); <nl> + /* Not sure how this relates with the protocol specification , <nl> + * but in order to avoid stack exhaustion recurse at most that <nl> + * many levels . <nl> + */ <nl> + if ( depth == 10 ) <nl> + ND_PRINT (( ndo , "( too many nested levels , not recursing )")); <nl> + else <nl> + buf2 = smb_fdata ( ndo , buf , fmt , maxbuf , unicodestr ); <nl> depth --; <nl> if ( buf2 == NULL ) <nl> return ( NULL );
static char * <nl> get_next_file ( FILE * VFile , char * ptr ) <nl> { <nl> char * ret ; <nl> + size_t len ; <nl>  <nl> ret = fgets ( ptr , PATH_MAX , VFile ); <nl> if (! ret ) <nl> return NULL ; <nl>  <nl> - if ( ptr [ strlen ( ptr ) - 1 ] == '\ n ') <nl> - ptr [ strlen ( ptr ) - 1 ] = '\ 0 '; <nl> + len = strlen ( ptr ); <nl> + if ( len > 0 && ptr [ len - 1 ] == '\ n ') <nl> + ptr [ len - 1 ] = '\ 0 '; <nl>  <nl> return ret ; <nl> }
icmp6_nodeinfo_print ( netdissect_options * ndo , u_int icmp6len , const u_char * bp , <nl> } else <nl> dnsname_print ( ndo , cp , ep ); <nl> if (( EXTRACT_16BITS (& ni6 -> ni_flags ) & 0x01 ) != 0 ) <nl> - ND_PRINT (( ndo ," [ TTL =% u ]", *( uint32_t *)( ni6 + 1 ))); <nl> + ND_PRINT (( ndo ," [ TTL =% u ]", EXTRACT_32BITS ( ni6 + 1 ))); <nl> break ; <nl> case NI_QTYPE_NODEADDR : <nl> if ( needcomma )
vrrp_print ( netdissect_options * ndo , <nl> if ( version == 3 && ND_TTEST2 ( bp [ 0 ], len )) { <nl> uint16_t cksum = nextproto4_cksum ( ndo , ( const struct ip *) bp2 , bp , <nl> len , len , IPPROTO_VRRP ); <nl> - if ( cksum ) <nl> + if ( cksum ) { <nl> + ND_TCHECK_16BITS (& bp [ 6 ]); <nl> ND_PRINT (( ndo , ", ( bad vrrp cksum % x )", <nl> EXTRACT_16BITS (& bp [ 6 ]))); <nl> + } <nl> } <nl>  <nl> ND_PRINT (( ndo , ", addrs "));
bgp_capabilities_print ( netdissect_options * ndo , <nl> opt [ i + 5 ])); <nl> break ; <nl> case BGP_CAPCODE_RESTART : <nl> + /* Restart Flags ( 4 bits ), Restart Time in seconds ( 12 bits ) */ <nl> + ND_TCHECK_16BITS ( opt + i + 2 ); <nl> ND_PRINT (( ndo , "\ n \ t \ tRestart Flags : [% s ], Restart Time % us ", <nl> (( opt [ i + 2 ])& 0x80 ) ? " R " : " none ", <nl> EXTRACT_16BITS ( opt + i + 2 )& 0xfff ));
AP4_HdlrAtom :: AP4_HdlrAtom ( AP4_UI32 size , <nl> stream . ReadUI32 ( m_Reserved [ 2 ]); <nl>  <nl> // read the name unless it is empty <nl> - int name_size = size -( AP4_FULL_ATOM_HEADER_SIZE + 20 ); <nl> - if ( name_size == 0 ) return ; <nl> + if ( size < AP4_FULL_ATOM_HEADER_SIZE + 20 ) return ; <nl> + AP4_UI32 name_size = size -( AP4_FULL_ATOM_HEADER_SIZE + 20 ); <nl> char * name = new char [ name_size + 1 ]; <nl> + if ( name == NULL ) return ; <nl> stream . Read ( name , name_size ); <nl> name [ name_size ] = '\ 0 '; // force a null termination <nl> // handle a special case : the Quicktime files have a pascal
AP4_AtomSampleTable :: GetSample ( AP4_Ordinal index , <nl> AP4_UI32 cts_offset = 0 ; <nl> AP4_UI64 dts = 0 ; <nl> AP4_UI32 duration = 0 ; <nl> - result = m_SttsAtom -> GetDts ( index , dts , & duration ); <nl> - if ( AP4_FAILED ( result )) return result ; <nl> + if ( m_SttsAtom ) { <nl> + result = m_SttsAtom -> GetDts ( index , dts , & duration ); <nl> + if ( AP4_FAILED ( result )) return result ; <nl> + } <nl> sample . SetDuration ( duration ); <nl> sample . SetDts ( dts ); <nl> if ( m_CttsAtom == NULL ) {
static bool parse_namefile ( struct HBAName * hname , const char * fn , bool is_db ) <nl>  <nl> f = fopen ( fn , " r "); <nl> if (! f ) { <nl> - free ( fn ); <nl> return false ; <nl> } <nl> for ( linenr = 1 ; ; linenr ++) {
int WavpackVerifySingleBlock ( unsigned char * buffer , int verify_checksum ) <nl> # endif <nl>  <nl> if ( meta_bc == 4 ) { <nl> - if (* dp ++ != ( csum & 0xff ) || * dp ++ != (( csum >> 8 ) & 0xff ) || * dp ++ != (( csum >> 16 ) & 0xff ) || * dp ++ != (( csum >> 24 ) & 0xff )) <nl> + if (* dp != ( csum & 0xff ) || dp [ 1 ] != (( csum >> 8 ) & 0xff ) || dp [ 2 ] != (( csum >> 16 ) & 0xff ) || dp [ 3 ] != (( csum >> 24 ) & 0xff )) <nl> return FALSE ; <nl> } <nl> else { <nl> csum ^= csum >> 16 ; <nl>  <nl> - if (* dp ++ != ( csum & 0xff ) || * dp ++ != (( csum >> 8 ) & 0xff )) <nl> + if (* dp != ( csum & 0xff ) || dp [ 1 ] != (( csum >> 8 ) & 0xff )) <nl> return FALSE ; <nl> } <nl> 
request_starting_cb ( WebKitWebView * view , WebKitWebFrame * frame , WebKitWebResour <nl>  <nl> g_object_ref ( decision -> request ); <nl> request_decision ( uri , decision ); <nl> + <nl> + g_free ( decision ); <nl> } <nl>  <nl> void
* OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE . <nl> */ <nl>  <nl> -# define RCSID "$ Id : auth . c , v 1 . 90 2002 / 12 / 04 23 : 03 : 32 paulus Exp $" <nl> +# define RCSID "$ Id : auth . c , v 1 . 91 2003 / 02 / 16 22 : 25 : 49 paulus Exp $" <nl>  <nl> # include < stdio . h > <nl> # include < stddef . h > <nl> link_established ( unit ) <nl> && protp -> lowerup != NULL ) <nl> (* protp -> lowerup )( unit ); <nl>  <nl> + if (! auth_required && noauth_addrs != NULL ) <nl> + set_allowed_addrs ( unit , NULL , NULL ) <nl> + <nl> if ( auth_required && !( go -> neg_upap || go -> neg_chap || go -> neg_eap )) { <nl> /* <nl> * We wanted the peer to authenticate itself , and it refused :
void dispatch_conn_new ( int sfd , int init_state , int event_flags , <nl>  <nl> cq_push (& thread -> new_conn_queue , item ); <nl>  <nl> - MEMCACHED_CONN_DISPATCH ( sfd , threads [ thread ]. thread_id ); <nl> + MEMCACHED_CONN_DISPATCH ( sfd , thread -> thread_id ); <nl> if ( write ( thread -> notify_send_fd , "", 1 ) != 1 ) { <nl> perror (" Writing to thread notify pipe "); <nl> }
void iobuf_remove ( struct iobuf * io , size_t n ) { <nl>  <nl> static size_t ns_out ( struct ns_connection * nc , const void * buf , size_t len ) { <nl> if ( nc -> flags & NSF_UDP ) { <nl> - long n = sendto ( nc -> sock , buf , len , 0 , & nc -> sa . sa , sizeof ( nc -> sa . sin )); <nl> + long n = sendto ( nc -> sock , ( const char *) buf , len , 0 , & nc -> sa . sa , <nl> + sizeof ( nc -> sa . sin )); <nl> DBG (("% p % d send % ld (% d % s )", nc , nc -> sock , n , errno , strerror ( errno ))); <nl> return n < 0 ? 0 : n ; <nl> } else {
static int mg_http_multipart_wait_for_boundary ( struct mg_connection * c ) { <nl> struct mbuf * io = & c -> recv_mbuf ; <nl> struct mg_http_proto_data * pd = mg_http_get_proto_data ( c ); <nl>  <nl> + if ( pd -> mp_stream . boundary == NULL ) { <nl> + pd -> mp_stream . state = MPS_FINALIZE ; <nl> + DBG ((" Invalid request : boundary not initilaized ")); <nl> + return 0 ; <nl> + } <nl> + <nl> if (( int ) io -> len < pd -> mp_stream . boundary_len + 2 ) { <nl> return 0 ; <nl> }
static pid_t start_process ( char * interp , const char * cmd , const char * env , <nl> } <nl>  <nl> if ( interp != NULL ) { <nl> - GetFullPathNameA ( interp , sizeof ( buf ), buf , NULL ); <nl> + GetFullPathName ( interp , sizeof ( buf ), buf , NULL ); <nl> interp = buf ; <nl> } <nl> - GetFullPathNameA ( dir , sizeof ( full_dir ), full_dir , NULL ); <nl> + GetFullPathName ( dir , sizeof ( full_dir ), full_dir , NULL ); <nl> mg_snprintf ( cmdline , sizeof ( cmdline ), "% s % s \"% s \"", <nl> interp ? interp : "", interp ? " " : "", cmd ); <nl> 
static int mg_deliver_websocket_data ( struct mg_connection * nc ) { <nl> } <nl>  <nl> frame_len = header_len + data_len ; <nl> - ok = frame_len > 0 && frame_len <= buf_len ; <nl> + ok = ( frame_len > 0 && frame_len <= buf_len ); <nl> + <nl> + /* Check for overflow */ <nl> + if ( frame_len < header_len || frame_len < data_len ) { <nl> + ok = 0 ; <nl> + nc -> flags |= MG_F_CLOSE_IMMEDIATELY ; <nl> + } <nl>  <nl> if ( ok ) { <nl> struct websocket_message wsm ;
static st_ret_t _st_db_put ( st_driver_t drv , const char * type , const char * owner , <nl> DB_TXN * t ; <nl> st_ret_t ret ; <nl>  <nl> + if ( dbd == NULL ) { <nl> + return st_FAILED ; <nl> + } <nl> + <nl> if ( os_count ( os ) == 0 ) <nl> return st_SUCCESS ; <nl> 
static int jpc_pi_nextrpcl ( register jpc_pi_t * pi ) <nl> for ( rlvlno = 0 , pirlvl = picomp -> pirlvls ; rlvlno < <nl> picomp -> numrlvls ; ++ rlvlno , ++ pirlvl ) { <nl> // Check for the potential for overflow problems . <nl> - if ( pirlvl -> prcwidthexpn + pi -> picomp -> numrlvls > <nl> + if ( pirlvl -> prcwidthexpn + picomp -> numrlvls > <nl> JAS_UINTFAST32_NUMBITS - 2 || <nl> - pirlvl -> prcheightexpn + pi -> picomp -> numrlvls > <nl> + pirlvl -> prcheightexpn + picomp -> numrlvls > <nl> JAS_UINTFAST32_NUMBITS - 2 ) { <nl> return - 1 ; <nl> }
int JPC_SEGPASSCNT ( int passno , int firstpassno , int numpasses , int bypass , int t <nl> } else { <nl> ret = JPC_PREC * 3 - 2 ; <nl> } <nl> - ret = JAS_MIN ( ret , numpasses - passno ); <nl> + if ( passno < numpasses ) <nl> + ret = JAS_MIN ( ret , numpasses - passno ); <nl> return ret ; <nl> } <nl> 
static int oidc_post_config ( apr_pool_t * pool , apr_pool_t * p1 , apr_pool_t * p2 , <nl> } <nl>  <nl> ap_log_error ( APLOG_MARK , APLOG_INFO , 0 , s , <nl> - "% s - init - cjose % s , % s , EC =% s , GCM =% s , Redis =% s ", <nl> + "% s - init - cjose % s , % s , EC =% s , GCM =% s , Redis =% s , JQ =% s ", <nl> NAMEVERSION , <nl> cjose_version (), <nl> OPENSSL_VERSION_TEXT , <nl> static int oidc_post_config ( apr_pool_t * pool , apr_pool_t * p1 , apr_pool_t * p2 , <nl> # else <nl> " no " <nl> # endif <nl> - ); <nl> + , <nl> +# ifdef USE_LIBJQ <nl> + " yes " <nl> +# else <nl> + " no " <nl> +# endif <nl> + ); <nl>  <nl> curl_global_init ( CURL_GLOBAL_ALL ); <nl> OpenSSL_add_all_digests ();
using namespace SDE ; <nl>  <nl> int main ( int argc , char ** argv ) { <nl> + QString display = ": 0 "; <nl> + // grab DISPLAY environment variable if set <nl> + if ( getenv (" DISPLAY ") != nullptr ) <nl> + display = getenv (" DISPLAY "); <nl> + <nl> QString configPath = "/ etc / sddm . conf "; <nl> QString themePath = ""; <nl> bool testing = false ; <nl> int main ( int argc , char ** argv ) { <nl> Cookie cookie ; <nl> // create display manager <nl> DisplayManager displayManager ; <nl> - displayManager . setDisplay (": 0 "); <nl> + displayManager . setDisplay ( display ); <nl> displayManager . setCookie ( cookie ); <nl> // start the display manager , except when in test mode <nl> if (( testing == false ) && ( displayManager . start () == false )) { <nl> int main ( int argc , char ** argv ) { <nl> } <nl> // create session manager <nl> SessionManager sessionManager ; <nl> - sessionManager . setDisplay (": 0 "); <nl> + sessionManager . setDisplay ( display ); <nl> sessionManager . setCookie ( cookie ); <nl> // auto login <nl> if ( first && ! Configuration :: instance ()-> autoUser (). isEmpty ()) {
namespace SDDM { <nl> if ( index != - 1 ) <nl> env . insert ( s . left ( index ), s . mid ( index + 1 )); <nl> } <nl> +# else <nl> + // we strdup ' d the string before in this branch <nl> + free ( mapped ); <nl> # endif <nl> env . insert (" HOME ", pw -> pw_dir ); <nl> env . insert (" PWD ", pw -> pw_dir );
PyArray_EquivTypenums ( int typenum1 , int typenum2 ) <nl> PyArray_Descr * d1 , * d2 ; <nl> npy_bool ret ; <nl>  <nl> + if ( typenum1 == typenum2 ) { <nl> + return NPY_SUCCEED ; <nl> + } <nl> + <nl> d1 = PyArray_DescrFromType ( typenum1 ); <nl> d2 = PyArray_DescrFromType ( typenum2 ); <nl> ret = PyArray_EquivTypes ( d1 , d2 );
const TSS2_TCTI_INFO * tpm2_tcti_ldr_getinfo ( void ) { <nl> bool tpm2_tcti_ldr_is_tcti_present ( const char * name ) { <nl>  <nl> char path [ PATH_MAX ]; <nl> - snprintf ( path , sizeof ( path ), " libtss2 - tcti -% s . so ", name ); <nl> + snprintf ( path , sizeof ( path ), " libtss2 - tcti -% s . so . 0 ", name ); <nl>  <nl> void * handle = dlopen ( path , RTLD_LAZY ); <nl> if ( handle ) {
static bool init ( int argc , char * argv [], tpm_hmac_ctx * ctx ) { <nl> } <nl>  <nl> /* <nl> - * Options g , i , o must be specified and k or c must be specified . <nl> + * Options g , I , o must be specified and k or c must be specified . <nl> */ <nl> if (!(( flags . k || flags . c ) && flags . I && flags . o && flags . g )) { <nl> LOG_ERR (" Must specify options g , i , o and k or c ");
static void receive_udppacket ( node_t * n , vpn_packet_t * inpkt ) { <nl> void receive_tcppacket ( connection_t * c , const char * buffer , int len ) { <nl> vpn_packet_t outpkt ; <nl>  <nl> + if ( len > sizeof outpkt . data ) <nl> + return ; <nl> + <nl> outpkt . len = len ; <nl> if ( c -> options & OPTION_TCPONLY ) <nl> outpkt . priority = 0 ;
/* <nl> meta . c -- handle the meta communication <nl> - Copyright ( C ) 2000 - 2009 Guus Sliepen < guus @ tinc - vpn . org >, <nl> + Copyright ( C ) 2000 - 2013 Guus Sliepen < guus @ tinc - vpn . org >, <nl> 2000 - 2005 Ivo Timmermans <nl> 2006 Scott Lamb < slamb @ slamb . org > <nl>  <nl> bool receive_meta ( connection_t * c ) { <nl> logger ( LOG_ERR , " Invalid response from proxy server "); <nl> return false ; <nl> } <nl> - if ( c -> buffer [ 1 ] == 0xff ) { <nl> + if ( c -> buffer [ 1 ] == ( char ) 0xff ) { <nl> logger ( LOG_ERR , " Proxy request rejected : unsuitable authentication method "); <nl> return false ; <nl> }
static void printAddrMode3OffsetOperand ( MCInst * MI , unsigned OpNum , SStream * O ) <nl> printRegName ( MI -> csh , O , MCOperand_getReg ( MO1 )); <nl> if ( MI -> csh -> detail ) { <nl> MI -> flat_insn -> detail -> arm . operands [ MI -> flat_insn -> detail -> arm . op_count ]. type = ARM_OP_REG ; <nl> - MI -> flat_insn -> detail -> arm . operands [ MI -> flat_insn -> detail -> arm . op_count ]. imm = MCOperand_getReg ( MO1 ); <nl> + MI -> flat_insn -> detail -> arm . operands [ MI -> flat_insn -> detail -> arm . op_count ]. reg = MCOperand_getReg ( MO1 ); <nl> MI -> flat_insn -> detail -> arm . op_count ++; <nl> } <nl> return ;
static void CG_DrawStatusBarKeys () { <nl>  <nl> // ( SA ) just don ' t draw this stuff for now . It ' s got fog issues I need to clean up <nl>  <nl> - return ; <nl> +// return ; <nl>  <nl>  <nl>  <nl> static void CG_DrawStatusBar ( void ) { <nl> // CG_DrawField ( 185 , STATUSBARHEIGHT , 3 , value ); <nl> { <nl> char printme [ 16 ]; <nl> - sprintf ( printme , "% d ", value ); <nl> + Com_sprintf ( printme , sizeof ( printme ), "% d ", value ); <nl> // CG_DrawBigString ( 185 , STATUSBARHEIGHT , printme , cg_hudAlpha . value ); <nl> CG_DrawBigString2 ( 16 + 23 + 43 , STATUSBARHEIGHT , printme , cg_hudAlpha . value ); <nl> } <nl> static void CG_DrawStatusBar ( void ) { <nl> // CG_DrawField ( 370 , STATUSBARHEIGHT , 3 , value ); <nl> { <nl> char printme [ 16 ]; <nl> - sprintf ( printme , "% d ", value ); <nl> + Com_sprintf ( printme , sizeof ( printme ), "% d ", value ); <nl> // CG_DrawBigString ( 370 , STATUSBARHEIGHT , printme , cg_hudAlpha . value ); <nl> CG_DrawBigString2 ( 200 , STATUSBARHEIGHT , printme , cg_hudAlpha . value ); <nl> } <nl> static float CG_DrawPowerups ( float y ) { <nl> x = 640 - ICON_SIZE - CHAR_WIDTH * 2 ; <nl> for ( i = 0 ; i < active ; i ++ ) { <nl>  <nl> - continue ; // ( SA ) FIXME : TEMP : as I ' m getting powerup business going <nl> +// continue ; // ( SA ) FIXME : TEMP : as I ' m getting powerup business going <nl>  <nl> item = BG_FindItemForPowerup ( sorted [ i ] ); <nl> 
HwProbe :: hd2value ( hd_t * hd ) <nl> out -> add ( YCPString (" sub_vendor "), YCPString ( s )); <nl> } <nl>  <nl> + // HAL udi <nl> + <nl> + s = hd -> udi ; <nl> + if ( s ) <nl> + { <nl> + out -> add ( YCPString (" udi "), YCPString ( s )); <nl> + } <nl> + <nl> // unique key <nl>  <nl> s = hd -> unique_id ;
skip_probe : <nl> len = getESSID ( fessid ); <nl> if (! len ) <nl> { <nl> - strcpy ( fessid , " default "); <nl> + strncpy ( fessid , " default ", sizeof ( fessid ) - 1 ); <nl> len = strlen ( fessid ); <nl> } <nl> packet [ z + 12 ] = 0x00 ; <nl> void beacon_thread ( void * arg ) <nl> flushESSID (); <nl> essid_len = getNextESSID ( essid ); <nl> if (! essid_len ) { <nl> - strcpy ( essid , " default "); <nl> + strncpy ( essid , " default ", sizeof ( essid ) - 1 ); <nl> essid_len = strlen (" default "); <nl> } <nl> 
static int wait_for_beacon ( uint8_t * bssid , uint8_t * capa , char * essid ) <nl> if ( tagtype != 0 ) continue ; <nl> if ( taglen <= 1 ) <nl> { <nl> - if ( memcmp ( bssid , pkt_sniff + 10 , 6 ) == 0 ) break ; <nl> + if ( bssid != NULL && memcmp ( bssid , pkt_sniff + 10 , 6 ) == 0 ) break ; <nl> else continue ; <nl> } <nl> if ( pos + 2 + taglen > len ) continue ; <nl>  <nl> if ( taglen > 32 ) taglen = 32 ; <nl>  <nl> - if (( pkt_sniff + pos + 2 )[ 0 ] < 32 && memcmp ( bssid , pkt_sniff + 10 , 6 ) == 0 ) <nl> + if (( pkt_sniff + pos + 2 )[ 0 ] < 32 && bssid != NULL && memcmp ( bssid , pkt_sniff + 10 , 6 ) == 0 ) <nl> { <nl> break ; <nl> }
__no_resolve : <nl> if (( off = lseek ( _state . s_wpafd , 0 , SEEK_CUR )) == ( off_t ) - 1 ) <nl> err ( 1 , " lseek ()"); <nl>  <nl> + if ( lseek ( _state . s_wpafd , 0 , SEEK_SET ) == ( off_t ) - 1 ) <nl> + err ( 1 , " lseek ()"); <nl> + <nl> while ( tot ) { <nl> int l = tot ; <nl> 
static int resolve_deps ( const char * src ) <nl> if ( strstr ( buf , " not regular file ")) <nl> break ; <nl>  <nl> + if ( strstr ( buf , " cannot read header ")) <nl> + break ; <nl> + <nl> + if ( strstr ( buf , destrootdir )) <nl> + break ; <nl> + <nl> p = strstr ( buf , "/"); <nl> if ( p ) { <nl> int r ;
QPDFWriter :: enqueueObject ( QPDFObjectHandle object ) <nl> // here . Instead , enqueue the object stream . Object <nl> // streams always have generation 0 . <nl> int stream_id = this -> object_to_object_stream [ og ]; <nl> + // Detect loops by storing invalid object ID 0 , which <nl> + // will get overwritten later . <nl> + obj_renumber [ og ] = 0 ; <nl> enqueueObject ( this -> pdf . getObjectByID ( stream_id , 0 )); <nl> } <nl> else <nl> QPDFWriter :: enqueueObject ( QPDFObjectHandle object ) <nl> } <nl> } <nl> } <nl> + else if ( obj_renumber [ og ] == 0 ) <nl> + { <nl> + // This can happen if a specially constructed file <nl> + // indicates that an object stream is inside itself . <nl> + QTC :: TC (" qpdf ", " QPDFWriter ignore self - referential object stream "); <nl> + } <nl> } <nl> else if ( object . isArray ()) <nl> {
int main ( int argc , char ** argv ) <nl> info . subusers . erase ( uiter ); <nl> if ( purge_keys ) { <nl> map < string , RGWAccessKey > * keys_map ; <nl> - access_key = subuser ; <nl> + access_key = info . user_id ; <nl> access_key . append (":"); <nl> access_key . append ( subuser ); <nl> keys_map = & info . swift_keys ;
extern " C " int ceph_create ( struct ceph_mount_info ** cmount , const char * const i <nl> extern " C " void ceph_shutdown ( struct ceph_mount_info * cmount ) <nl> { <nl> cmount -> shutdown (); <nl> + delete cmount ; <nl> } <nl>  <nl> extern " C " int ceph_conf_read_file ( struct ceph_mount_info * cmount , const char * path )
ssize_t AsyncConnection :: _try_send ( bool send , bool more ) <nl> // trim already sent for outcoming_bl <nl> if ( sent_bytes ) { <nl> if ( sent_bytes < outcoming_bl . length ()) { <nl> - bufferlist bl ; <nl> - outcoming_bl . splice ( sent_bytes , outcoming_bl . length ()- sent_bytes , & bl ); <nl> - bl . swap ( outcoming_bl ); <nl> + outcoming_bl . splice ( 0 , sent_bytes ); <nl> } else { <nl> outcoming_bl . clear (); <nl> }
static int do_cache_evict ( IoCtx & io_ctx , string oid ) <nl>  <nl> static int do_cache_flush_evict_all ( IoCtx & io_ctx , bool blocking ) <nl> { <nl> - int r ; <nl> int errors = 0 ; <nl> try { <nl> librados :: ObjectIterator i = io_ctx . objects_begin (); <nl> librados :: ObjectIterator i_end = io_ctx . objects_end (); <nl> for (; i != i_end ; ++ i ) { <nl> + int r ; <nl> cout << i -> first << "\ t " << i -> second << std :: endl ; <nl> if ( i -> second . size ()) { <nl> io_ctx . locator_set_key ( i -> second );
int aio_bench ( Rados & rados , rados_pool_t pool , int secondsToRun , int concurrenti <nl> time (& initialTime ); <nl> stringstream initialTimeS (""); <nl> initialTimeS << initialTime ; <nl> - const char * iTime = initialTimeS . str (). c_str (); <nl> + char iTime [ 100 ]; <nl> + strcpy ( iTime , initialTimeS . str (). c_str ()); <nl> maxLatency . set_from_double ( 0 ); <nl> // set up writes so I can start them together <nl> for ( int i = 0 ; i < concurrentios ; ++ i ) {
static int rgw_reshard_list ( cls_method_context_t hctx , bufferlist * in , bufferlis <nl> bufferlist :: iterator iter ; <nl> map < string , bufferlist > vals ; <nl> string filter_prefix ; <nl> - # define MAX_RESHARD_LIST_ENTRIES 1000 <nl> +# define MAX_RESHARD_LIST_ENTRIES 1000 <nl> /* one extra entry for identifying truncation */ <nl> int32_t max = ( op . max < MAX_RESHARD_LIST_ENTRIES ? op . max : MAX_RESHARD_LIST_ENTRIES ) + 1 ; <nl> int ret = cls_cxx_map_get_vals ( hctx , op . marker , filter_prefix , max , & vals ); <nl> static int rgw_reshard_list ( cls_method_context_t hctx , bufferlist * in , bufferlis <nl> return ret ; <nl> map < string , bufferlist >:: iterator it ; <nl> cls_rgw_reshard_entry entry ; <nl> - for ( it = vals . begin (); it != vals . end (); ++ it ) { <nl> + int i = 0 ; <nl> + for ( it = vals . begin (); i < ( int ) op . max && it != vals . end (); ++ it , ++ i ) { <nl> iter = it -> second . begin (); <nl> try { <nl> :: decode ( entry , iter ); <nl> static int rgw_reshard_list ( cls_method_context_t hctx , bufferlist * in , bufferlis <nl> } <nl> op_ret . entries . push_back ( entry ); <nl> } <nl> - op_ret . is_truncated = op . max && ( op_ret . entries . size () >= op . max ); <nl> + op_ret . is_truncated = op . max && ( vals . size () > op . max ); <nl> :: encode ( op_ret , * out ); <nl> return 0 ; <nl> }
int ReplicatedPG :: start_flush ( <nl> // nonblocking can join anything <nl> // blocking can only join a blocking flush <nl> dout ( 20 ) << __func__ << " piggybacking on existing flush " << dendl ; <nl> - fop -> dup_ops . push_back ( op ); <nl> + if ( op ) <nl> + fop -> dup_ops . push_back ( op ); <nl> return - EAGAIN ; // clean up this ctx ; op will retry later <nl> } <nl> 
void PG :: start_peering_interval ( <nl> state_clear ( PG_STATE_REMAPPED ); <nl>  <nl> int role = osdmap -> calc_pg_role ( osd -> whoami , acting , acting . size ()); <nl> - if ( role == pg_whoami . shard ) <nl> + if ( pool . info . is_replicated () || role == pg_whoami . shard ) <nl> set_role ( role ); <nl> else <nl> set_role (- 1 );
int buffer :: list :: read_file ( const char * fn ) <nl> :: fstat ( fd , & st ); <nl> int s = ROUND_UP_TO ( st . st_size , PAGE_SIZE ); <nl> bufferptr bp = buffer :: create_page_aligned ( s ); <nl> + bp . set_length ( st . st_size ); <nl> append ( bp ); <nl> :: read ( fd , ( void *) c_str (), length ()); <nl> :: close ( fd );
void Server :: _rename_prepare ( MDRequestRef & mdr , <nl> } <nl> if ( tpi ) { <nl> tpi -> ctime = mdr -> get_op_stamp (); <nl> + destdn -> make_path_string ( tpi -> stray_prior_path ); <nl> tpi -> nlink --; <nl> if ( tpi -> nlink == 0 ) <nl> oldin -> state_set ( CInode :: STATE_ORPHAN );
bool OSDMonitor :: update_pools_status () <nl> if (! pool_is_full ) <nl> continue ; <nl>  <nl> - if (( uint64_t ) sum . num_bytes >= pool . quota_max_bytes ) { <nl> + if ( pool . quota_max_bytes > 0 && <nl> + ( uint64_t ) sum . num_bytes >= pool . quota_max_bytes ) { <nl> mon -> clog . warn () << " pool '" << pool_name << "' is full " <nl> << " ( reached quota ' s max_bytes : " <nl> << si_t ( pool . quota_max_bytes ) << ")"; <nl> - } else if (( uint64_t ) sum . num_objects >= pool . quota_max_objects ) { <nl> + } else if ( pool . quota_max_objects > 0 && <nl> + ( uint64_t ) sum . num_objects >= pool . quota_max_objects ) { <nl> mon -> clog . warn () << " pool '" << pool_name << "' is full " <nl> << " ( reached quota ' s max_objects : " <nl> << pool . quota_max_objects << ")";
struct SnapContext { <nl> // seq >= snaps [ 0 ] <nl> if ( snaps [ 0 ] > seq ) <nl> return false ; <nl> - // snaps is descending <nl> + // snaps [] is descending <nl> snapid_t t = snaps [ 0 ]; <nl> for ( unsigned i = 1 ; i < snaps . size (); i ++) { <nl> - if ( snaps [ i ] >= t ) <nl> + if ( snaps [ i ] >= t || t == 0 ) <nl> return false ; <nl> t = snaps [ i ]; <nl> }
protected : <nl> public : <nl> Message () { <nl> memset (& header , 0 , sizeof ( header )); <nl> + memset (& footer , 0 , sizeof ( footer )); <nl> }; <nl> Message ( int t ) { <nl> memset (& header , 0 , sizeof ( header )); <nl> header . type = t ; <nl> header . priority = 0 ; // undef <nl> header . data_off = 0 ; <nl> + memset (& footer , 0 , sizeof ( footer )); <nl> } <nl> virtual ~ Message () { } <nl> 
void apply ( uint64_t off , <nl> std :: function < void ( uint64_t , boost :: dynamic_bitset <> &)> f ) { <nl> auto end = ROUND_UP_TO ( off + len , granularity ); <nl> while ( off < end ) { <nl> - uint64_t pos = off / granularity ; <nl> - f ( pos , bitset ); <nl> + uint64_t pos = off / granularity ; <nl> + f ( pos , bitset ); <nl> off += granularity ; <nl> } <nl> }
struct MOSDScrub : public Message { <nl> MOSDScrub () {} <nl> MOSDScrub ( ceph_fsid & f ) : <nl> Message ( MSG_OSD_SCRUB ), <nl> - fsid ( f ) {} <nl> + fsid ( f ), repair ( false ) {} <nl> MOSDScrub ( ceph_fsid & f , vector < pg_t >& pgs , bool r ) : <nl> Message ( MSG_OSD_SCRUB ), <nl> fsid ( f ), scrub_pgs ( pgs ), repair ( r ) {}
void ReplicatedPG :: handle_watch_timeout ( WatchRef watch ) <nl>  <nl> // obc ref swallowed by repop ! <nl> simple_repop_submit ( repop ); <nl> + <nl> + // apply new object state . <nl> + ctx -> obc -> obs = ctx -> new_obs ; <nl> } <nl>  <nl> ObjectContextRef ReplicatedPG :: create_object_context ( const object_info_t & oi ,
int RGWCreateBucket_ObjStore_S3 :: get_params () <nl> op_ret = rgw_rest_read_all_input ( s , & data , & len , CREATE_BUCKET_MAX_REQ_LEN ); <nl> if (( op_ret < 0 ) && ( op_ret != - ERR_LENGTH_REQUIRED )) <nl> return op_ret ; <nl> + <nl> + auto data_deleter = std :: unique_ptr < char , decltype ( free )*>{ data , free }; <nl>  <nl> if ( s -> aws4_auth_needs_complete ) { <nl> int ret_auth = do_aws4_auth_completion (); <nl> int RGWCreateBucket_ObjStore_S3 :: get_params () <nl>  <nl> if (! success ) { <nl> ldout ( s -> cct , 0 ) << " failed to parse input : " << data << dendl ; <nl> - free ( data ); <nl> return - EINVAL ; <nl> } <nl> - free ( data ); <nl>  <nl> if (! parser . get_location_constraint ( location_constraint )) { <nl> ldout ( s -> cct , 0 ) << " provided input did not specify location constraint correctly " << dendl ;
void ObjectCopyRequest < I >:: send_update_object_map () { <nl> m_local_image_ctx -> snap_lock . put_read (); <nl> finish ( 0 ); <nl> return ; <nl> + } else if ( m_local_image_ctx -> object_map == nullptr ) { <nl> + // possible that exclusive lock was lost in background <nl> + derr << ": object map is not initialized " << dendl ; <nl> + <nl> + m_local_image_ctx -> snap_lock . put_read (); <nl> + finish (- EINVAL ); <nl> + return ; <nl> } <nl>  <nl> assert ( m_local_image_ctx -> object_map != nullptr );
void OSD :: requeue_failures () <nl> unsigned old_pending = failure_pending . size (); <nl> for ( map < int , pair < utime_t , entity_inst_t > >:: iterator p = <nl> failure_pending . begin (); <nl> - p != failure_pending . end (); <nl> - ++ p ) { <nl> + p != failure_pending . end (); ) { <nl> failure_queue [ p -> first ] = p -> second . first ; <nl> + failure_pending . erase ( p ++); <nl> } <nl> dout ( 10 ) << __func__ << " " << old_queue << " + " << old_pending << " -> " <nl> << failure_queue . size () << dendl ;
public : <nl> monc_lock (" MonClient :: monc_lock "), <nl> timer ( monc_lock ), <nl> hunting ( false ), <nl> + want_monmap ( false ), <nl> + want_keys ( 0 ), <nl> mounting ( 0 ), mount_err ( 0 ), <nl> auth ( NULL ) { } <nl> ~ MonClient () {
static int run_command ( const char * command ) <nl>  <nl> if ( status < 0 ) { <nl> char error_buf [ 80 ]; <nl> + strerror_r ( errno , error_buf , sizeof ( error_buf )); <nl> fprintf ( stderr , " couldn ' t run '% s ': % s \ n ", command , <nl> - strerror_r ( errno , error_buf , sizeof ( error_buf ))); <nl> + error_buf ); <nl> } else if ( WIFSIGNALED ( status )) { <nl> fprintf ( stderr , "'% s ' killed by signal % d \ n ", command , <nl> WTERMSIG ( status ));
void OSD :: dump_ops_in_flight ( ostream & ss ) <nl> m -> print ( name ); <nl> jf . open_object_section (" op "); <nl> jf . dump_string (" description ", name . str (). c_str ()); // this OpRequest <nl> - jf . dump_float (" received_at ", (* p )-> received_time ); <nl> + jf . dump_stream (" received_at ") << (* p )-> received_time ; <nl> jf . dump_float (" age ", now - (* p )-> received_time ); <nl> jf . dump_string (" flag_point ", (* p )-> state_string ()); <nl> if ( m -> get_orig_source (). is_client ()) {
void FileStore :: _set_replay_guard ( int fd , const SequencerPosition & spos ) <nl> // first make sure the previous operation commits <nl> :: fsync ( fd ); <nl>  <nl> + // sync object_map too . even if this object has a header or keys , <nl> + // it have had them in the past and then removed them , so always <nl> + // sync . <nl> + object_map -> sync (); <nl> + <nl> // then record that we did it <nl> bufferlist v ( 40 ); <nl> :: encode ( spos , v );
int aio_write ( ImageCtx * ictx , off_t off , uint64_t len , const char * buf , <nl> librados :: AioCompletion * rados_completion = <nl> Rados :: aio_create_completion ( block_completion , NULL , rados_cb ); <nl> r = ictx -> data_ctx . aio_write ( oid , rados_completion , bl , write_len , block_ofs ); <nl> + rados_completion -> release (); <nl> + delete rados_completion ; <nl> if ( r < 0 ) <nl> goto done ; <nl> total_write += write_len ; <nl> int aio_read ( ImageCtx * ictx , off_t off , uint64_t len , <nl> r = ictx -> data_ctx . aio_sparse_read ( oid , rados_completion , <nl> & block_completion -> m , & block_completion -> data_bl , <nl> read_len , block_ofs ); <nl> + rados_completion -> release (); <nl> + delete rados_completion ; <nl> if ( r < 0 && r == - ENOENT ) <nl> r = 0 ; <nl> if ( r < 0 ) {
int main ( int argc , const char ** argv , const char * envp []) <nl> return - 1 ; <nl> } <nl>  <nl> + int ret = 0 ; <nl> + <nl> if ( observe ) { <nl> lock . Lock (); <nl> send_observe_requests (); <nl> int main ( int argc , const char ** argv , const char * envp []) <nl>  <nl> string rs ; <nl> bufferlist odata ; <nl> - do_command ( vcmd , indata , rs , odata ); <nl> + ret = do_command ( vcmd , indata , rs , odata ); <nl>  <nl> int len = odata . length (); <nl> if ( len ) { <nl> int main ( int argc , const char ** argv , const char * envp []) <nl> // wait for messenger to finish <nl> messenger -> wait (); <nl> messenger -> destroy (); <nl> - return 0 ; <nl> + return ret ; <nl> } <nl> 
void OSDMonitor :: prime_pg_temp ( <nl> return ; <nl> } <nl> } <nl> + if (! osdmap . have_pg_pool ( pgid . pool ())) { <nl> + return ; <nl> + } <nl>  <nl> vector < int > up , acting ; <nl> mapping . get ( pgid , & up , nullptr , & acting , nullptr );
void Monitor :: handle_probe ( MonOpRequestRef op ) <nl> } <nl> } <nl>  <nl> -/** <nl> - * @ todo fix this . This is going to cause trouble . <nl> - */ <nl> void Monitor :: handle_probe_probe ( MonOpRequestRef op ) <nl> { <nl> MMonProbe * m = static_cast < MMonProbe *>( op -> get_req ());
bool OSDMonitor :: preprocess_pgtemp ( MonOpRequestRef op ) <nl> goto ignore ; <nl> } <nl>  <nl> + if ( m -> forced ) { <nl> + return false ; <nl> + } <nl> + <nl> for ( auto p = m -> pg_temp . begin (); p != m -> pg_temp . end (); ++ p ) { <nl> dout ( 20 ) << " " << p -> first <nl> << ( osdmap . pg_temp -> count ( p -> first ) ? osdmap . pg_temp -> get ( p -> first ) : empty )
bool CephXAuthorizer :: verify_reply ( bufferlist :: iterator & indata ) <nl> } <nl> } catch ( buffer :: error * e ) { <nl> dout ( 0 ) << " verify_authorizer_reply exception in decode_decrypt with " << session_key << dendl ; <nl> + delete e ; <nl> return false ; <nl> } <nl> 
BYTE * DecompressRTF ( variableLength * p , int * size ) { <nl> ALLOCCHECK_CHAR ( dst ); <nl> memcpy ( dst , comp_Prebuf . data , comp_Prebuf . size ); <nl> out = comp_Prebuf . size ; <nl> - while ( out < ( comp_Prebuf . size + uncompressedSize )) { <nl> + while (( out < ( comp_Prebuf . size + uncompressedSize )) && ( in < p -> size )) { <nl> // each flag byte flags 8 literals / references , 1 per bit <nl> flags = ( flagCount ++ % 8 == 0 ) ? src [ in ++] : flags >> 1 ; <nl> if (( flags & 1 ) == 1 ) { // each flag bit is 1 for reference , 0 for literal
void winwidget_render_image ( winwidget winwid , int resize , int alias ) <nl> smaller = (( winwid -> im_w < max_w ) <nl> && ( winwid -> im_h < max_h )); <nl>  <nl> - if (! smaller || ( opt . zoom_mode == ZOOM_MODE_FILL )) { <nl> + if (! smaller || opt . zoom_mode ) { <nl> double ratio = 0 . 0 ; <nl>  <nl> /* Image is larger than the screen ( so wants shrinking ), or it ' s <nl> double feh_calc_needed_zoom ( double * zoom , int orig_w , int orig_h , int dest_w , in <nl>  <nl> ratio = (( double ) orig_w / orig_h ) / (( double ) dest_w / dest_h ); <nl>  <nl> + if ( opt . zoom_mode == ZOOM_MODE_MAX ) <nl> + ratio = 1 . 0 / ratio ; <nl> + <nl> if ( ratio > 1 . 0 ) <nl> * zoom = (( double ) dest_w / orig_w ); <nl> else
extern <nl> /* Sadly we can ' t include < malloc . h > as it causes a redefinition error */ <nl> size_t malloc_usable_size ( void *); <nl> # elif defined ( __APPLE__ ) <nl> - # include < malloc . h > <nl> + # if TARGET_OS_IPHONE <nl> + # include < malloc / malloc . h > <nl> + # else <nl> + # include < malloc . h > <nl> + # endif <nl> # else <nl> # error Do not know what to do here <nl> # endif
is_allowed_callout_name ( OnigEncoding enc , UChar * name , UChar * name_end ) <nl> UChar * p ; <nl> OnigCodePoint c ; <nl>  <nl> + if ( name >= name_end ) return 0 ; <nl> + <nl> p = name ; <nl> while ( p < name_end ) { <nl> c = ONIGENC_MBC_TO_CODE ( enc , p , name_end ); <nl> if (! IS_ALLOWED_CODE_IN_CALLOUT_NAME ( c )) <nl> return 0 ; <nl>  <nl> + if ( p == name ) { <nl> + if ( c >= ' 0 ' && c <= ' 9 ') return 0 ; <nl> + } <nl> + <nl> p += ONIGENC_MBC_ENC_LEN ( enc , p ); <nl> } <nl> 
onig_builtin_monitor ( OnigCalloutArgs * args , void * user_data ) <nl>  <nl> in = onig_get_callout_in_by_callout_args ( args ); <nl> if ( in == ONIG_CALLOUT_IN_PROGRESS ) { <nl> - if ( val . c == '-') <nl> + if ( val . c == '<') <nl> return ONIG_CALLOUT_SUCCESS ; <nl> } <nl> else { <nl> - if ( val . c != '+' && val . c != '-') <nl> + if ( val . c != ' X ' && val . c != '<') <nl> return ONIG_CALLOUT_SUCCESS ; <nl> } <nl>  <nl> onig_setup_builtin_monitors_by_ascii_encoded_name ( void ) <nl>  <nl> name = " MON "; <nl> ts [ 0 ] = ONIG_TYPE_CHAR ; <nl> - opts [ 0 ]. c = ' '; <nl> + opts [ 0 ]. c = '>'; <nl> BC_B_O ( name , monitor , 1 , ts , 1 , opts ); <nl>  <nl> return ONIG_NORMAL ;
int Secure_Channel :: StartEnrollment ( BYTE p1 , BYTE p2 , Buffer * wrapped_challenge , <nl> RA :: Debug (" Secure_Channel :: GenerateKey ", <nl> " Secure_Channel :: GenerateKey "); <nl> generate_key_apdu = new Generate_Key_APDU ( p1 , p2 , alg , keysize , option , <nl> - 0x85 , * wrapped_challenge , * key_check ); <nl> + alg , * wrapped_challenge , * key_check ); <nl> rc = ComputeAPDU ( generate_key_apdu ); <nl> if ( rc == - 1 ) <nl> goto loser ;
int CActIf :: takeAction ( CContext & c , sockstream & fs ) <nl> StoreResult (& m_coSql , res ); <nl> CheckForRows (* res , 1 ); <nl> FetchRow (* res , row ); <nl> + if ( row [ 0 ] == NULL ) <nl> + return ERR_NODATA ; <nl> if ( modifier == CMS_ST_ARTICLE && param . attrType () != "" && ! m_bStrictType ) <nl> { <nl> field = param . attribute ();
private : <nl> // Create an xml reader fed with the resource from file name . <nl> inline CXMLReader :: CXMLReader ( const char * p_pchFilename ) throw ( invalid_argument ) <nl> { <nl> + if ( p_pchFilename == NULL ) <nl> + throw invalid_argument ( string (" Invalid file NULL ")); <nl> m_pReader = xmlNewTextReaderFilename ( p_pchFilename ); <nl> if ( m_pReader == NULL ) <nl> throw invalid_argument ( string (" Invalid file ") + p_pchFilename ); <nl> inline bool CXMLReader :: isValid () const <nl> inline bool CXMLReader :: moveToAttribute ( const char * p_pchAttribute ) <nl> throw ( xml_parse_error , invalid_argument ) <nl> { <nl> + if ( p_pchAttribute == NULL ) <nl> + throw invalid_argument ( string (" Invalid attribute NULL ")); <nl> int nRes = xmlTextReaderMoveToAttribute ( m_pReader , ( const xmlChar *) p_pchAttribute ); <nl> if ( nRes == - 1 ) <nl> throw xml_parse_error ("");
rsvg_filter_primitive_free ( gpointer impl ) <nl> { <nl> RsvgFilterPrimitive * primitive = impl ; <nl>  <nl> - g_string_free ( primitive -> in , TRUE ); <nl> - g_string_free ( primitive -> result , TRUE ); <nl> + if ( primitive -> in ) { <nl> + g_string_free ( primitive -> in , TRUE ); <nl> + } <nl> + <nl> + if ( primitive -> result ) { <nl> + g_string_free ( primitive -> result , TRUE ); <nl> + } <nl>  <nl> g_free ( primitive ); <nl> }
int main ( int argc , char * argv []) <nl> " dosfslabel : labels can be no longer than 11 characters \ n "); <nl> exit ( 1 ); <nl> } <nl> - for ( i = 0 ; i < 11 ; i ++) <nl> + for ( i = 0 ; label [ i ] && i < 11 ; i ++) <nl> /* don ' t know if here should be more strict ! uppercase ( label [ i ])*/ <nl> if ( islower ( label [ i ])) { <nl> fprintf ( stderr ,
int json_string_setn_nocheck ( json_t * json , const char * value , size_t len ) <nl> string = json_to_string ( json ); <nl> jsonp_free ( string -> value ); <nl> string -> value = dup ; <nl> + string -> length = len ; <nl>  <nl> return 0 ; <nl> }
_gsp_app_manager_handle_delete ( GspAppManager * manager , <nl> return ; <nl> } <nl>  <nl> + if ( index < position ) { <nl> + /* it got deleted , but in a position earlier than the current <nl> + * one . This happens when the user file was changed and became <nl> + * identical to the system file ; in this case , the user file is <nl> + * simply removed . */ <nl> + g_assert ( index == 0 ); <nl> + return ; <nl> + } <nl> + <nl> if ( position == index && <nl> ( system_position == index || system_position == G_MAXUINT )) { <nl> /* the file used by the user was deleted , and there ' s no other
display_error ( pam_handle_t * pamh , const char * message ) { <nl> } <nl>  <nl> D ((" conv returned : '% s '", resp -> resp )); <nl> + free ( resp ); <nl> return retval ; <nl> } <nl> # endif /* HAVE_CR */ <nl> pam_sm_authenticate ( pam_handle_t * pamh , <nl> struct pam_conv * conv ; <nl> const struct pam_message * pmsg [ 1 ]; <nl> struct pam_message msg [ 1 ]; <nl> - struct pam_response * resp ; <nl> + struct pam_response * resp = NULL ; <nl> int nargs = 1 ; <nl> ykclient_t * ykc = NULL ; <nl> struct cfg cfg_st ; <nl> pam_sm_authenticate ( pam_handle_t * pamh , <nl> } <nl> } <nl> msg [ 0 ]. msg_style = cfg -> verbose_otp ? PAM_PROMPT_ECHO_ON : PAM_PROMPT_ECHO_OFF ; <nl> - resp = NULL ; <nl>  <nl> retval = conv -> conv ( nargs , pmsg , & resp , conv -> appdata_ptr ); <nl>  <nl> done : <nl> DBG ((" done . [% s ]", pam_strerror ( pamh , retval ))); <nl> pam_set_data ( pamh , " yubico_setcred_return ", ( void *) ( intptr_t ) retval , NULL ); <nl>  <nl> + if ( resp ) <nl> + free ( resp ); <nl> + <nl> return retval ; <nl> } <nl> 
de_dotdot ( char * file ) <nl> while ( strncmp ( file , "./", 2 ) == 0 ) <nl> ( void ) memmove ( file , file + 2 , strlen ( file ) - 1 ); <nl> while ( ( cp = strstr ( file , "/./") ) != ( char *) 0 ) <nl> - ( void ) memmove ( cp , cp + 2 , strlen ( file ) - 1 ); <nl> + ( void ) memmove ( cp , cp + 2 , strlen ( cp ) - 1 ); <nl>  <nl> /* Alternate between removing leading ../ and removing xxx /../ */ <nl> for (;;)
int usbmuxd_read_pair_record ( const char * record_id , char ** record_data , uint32_t <nl> if ( node && plist_get_node_type ( node ) == PLIST_DATA ) { <nl> uint64_t int64val = 0 ; <nl> plist_get_data_val ( node , record_data , & int64val ); <nl> - if ( record_data && int64val > 0 ) { <nl> + if (* record_data && int64val > 0 ) { <nl> * record_size = ( uint32_t ) int64val ; <nl> ret = 0 ; <nl> }
S_study_chunk ( pTHX_ RExC_state_t * pRExC_state , regnode ** scanp , <nl> RExC_precomp ))); <nl> } <nl>  <nl> + if ( ( minnext > 0 && mincount >= SSize_t_MAX / minnext ) <nl> + || min >= SSize_t_MAX - minnext * mincount ) <nl> + { <nl> + FAIL (" Regexp out of space "); <nl> + } <nl> + <nl> min += minnext * mincount ; <nl> is_inf_internal |= deltanext == SSize_t_MAX <nl> || ( maxcount == REG_INFTY && minnext + deltanext > 0 );
# define OPER_SAVE 0 <nl> # define OPER_RESTORE 1 <nl>  <nl> +# define MAX_CURSOR_SIZE 1024 <nl> + <nl> # define RGB24_TO_PIXEL ( bpp , r , g , b ) \ <nl> (((( uint ## bpp ## _t )( r ) & 0xFF ) * client -> format . redMax + 127 ) / 255 \ <nl> << client -> format . redShift | \ <nl> rfbBool HandleCursorShape ( rfbClient * client , int xhot , int yhot , int width , int h <nl> if ( width * height == 0 ) <nl> return TRUE ; <nl>  <nl> + if ( width >= MAX_CURSOR_SIZE || height >= MAX_CURSOR_SIZE ) <nl> + return FALSE ; <nl> + <nl> /* Allocate memory for pixel data and temporary mask data . */ <nl> if ( client -> rcSource ) <nl> free ( client -> rcSource );
qemuProcessHandleMonitorEOF ( qemuMonitorPtr mon , <nl> /* We don ' t want this EOF handler to be called over and over while the <nl> * thread is waiting for a job . <nl> */ <nl> + virObjectLock ( mon ); <nl> qemuMonitorUnregister ( mon ); <nl> + virObjectUnlock ( mon ); <nl>  <nl> /* We don ' t want any cleanup from EOF handler ( or any other <nl> * thread ) to enter qemu namespace . */
ofproto_rule_insert__ ( struct ofproto * ofproto , struct rule * rule ) <nl> const struct rule_actions * actions = rule_get_actions ( rule ); <nl>  <nl> /* A rule may not be reinserted . */ <nl> - ovs_assert ( rule -> state == RULE_INITIALIZED ); <nl> + ovs_assert ( rule -> state != RULE_INSERTED ); <nl>  <nl> if ( rule -> hard_timeout || rule -> idle_timeout ) { <nl> ovs_list_insert (& ofproto -> expirable , & rule -> expirable );
namespace Sass { <nl> if ( lex < variable >()) <nl> { return SASS_MEMORY_NEW ( Variable , pstate , Util :: normalize_underscores ( lexed )); } <nl>  <nl> - // Special case handling for `%` proceeding an interpolant . <nl> - if ( lex < sequence < exactly <'%'>, optional < percentage > > >()) <nl> - { return SASS_MEMORY_NEW ( String_Constant , pstate , lexed ); } <nl> - <nl> css_error (" Invalid CSS ", " after ", ": expected expression ( e . g . 1px , bold ), was "); <nl>  <nl> // unreachable statement
matchCurrentInput ( <nl> const InString * input , int pos , const widechar * passInstructions , int passIC ) { <nl> int k ; <nl> int kk = pos ; <nl> - for ( k = passIC + 2 ; k < passIC + 2 + passInstructions [ passIC + 1 ]; k ++) <nl> + for ( k = passIC + 2 ; <nl> + (( k < passIC + 2 + passInstructions [ passIC + 1 ]) && ( kk < input -> length )); <nl> + k ++) <nl> if ( input -> chars [ kk ] == ENDSEGMENT || passInstructions [ k ] != input -> chars [ kk ++]) <nl> return 0 ; <nl> return 1 ;
int af_get_page ( AFFILE * af , int64_t pagenum , unsigned char * data , size_t * bytes ) <nl> return - 3 ; // read error <nl> } <nl>  <nl> + /* Sanity check to avoid undefined behaviour when calling malloc below with pagesize from a corrupt AFF image . */ <nl> + if ( af -> image_pagesize <= 0 || af -> image_pagesize > 16 * 1024 * 1024 ) <nl> + return - 1 ; <nl> + <nl> + <nl> /* Now uncompress directly into the buffer provided by the caller , unless the caller didn ' t <nl> * provide a buffer . If that happens , allocate our own ... <nl> */
tcp_write ( struct tcp_pcb * pcb , const void * arg , u16_t len , u8_t apiflags ) <nl> if ( oversize > 0 ) { <nl> LWIP_ASSERT (" inconsistent oversize vs . space ", oversize_used <= space ); <nl> seg = last_unsent ; <nl> - oversize_used = oversize < len ? oversize : len ; <nl> + oversize_used = LWIP_MIN ( space , LWIP_MIN ( oversize , len )); <nl> pos += oversize_used ; <nl> oversize -= oversize_used ; <nl> space -= oversize_used ;
void clean_up ( bool print_message ) <nl> make sure that handlers finish up <nl> what they have that is dependent on the binlog <nl> */ <nl> - sql_print_information (" Binlog end "); <nl> + if (( opt_help == 0 ) || ( opt_verbose > 0 )) <nl> + sql_print_information (" Binlog end "); <nl> ha_binlog_end ( current_thd ); <nl>  <nl> logger . cleanup_base ();
struct buf_pool_t { <nl>  <nl> /** @ name General fields */ <nl> /* @{ */ <nl> - ib_mutex_t mutex ; /*!< Buffer pool mutex of this <nl> + ib_mutex_t mutex ; /*!< Buffer pool mutex of this <nl> instance */ <nl> - ib_mutex_t zip_mutex ; /*!< Zip mutex of this buffer <nl> + ib_mutex_t zip_mutex ; /*!< Zip mutex of this buffer <nl> pool instance , protects compressed <nl> only pages ( of type buf_page_t , not <nl> buf_block_t */
__weak_alias ( vis , _vis ) <nl> # define MAXEXTRAS 5 <nl>  <nl>  <nl> - char * MAKEEXTRALIST ( uint flag , const char * orig ) <nl> + char * MAKEEXTRALIST ( unsigned int flag , const char * orig ) <nl> { <nl> const char * o = orig ; <nl> char * e , * extra ;
struct NodeReceiverGroup { <nl>  <nl> template < unsigned T > struct SignalT <nl> { <nl> + SignalT () { bzero (& header , sizeof ( header )); } <nl> Uint32 m_sectionPtrI [ 3 ]; <nl> SignalHeader header ; <nl> union {
TABLE_COUNTER_TYPE Query_cache :: is_cacheable ( THD * thd , uint32 query_len , <nl> ( tables_used -> db_length == 5 && <nl> # ifdef FN_NO_CASE_SENCE <nl> // TODO : latin1 charset should be replaced with system charset <nl> - my_strncasecmp ( my_charset_latin1 , tables_used -> db ," mysql ", 5 ) == 0 <nl> + my_strncasecmp (& my_charset_latin1 , <nl> + tables_used -> db , <nl> + " mysql ", 5 ) == 0 <nl> # else <nl> tables_used -> db [ 0 ]==' m ' && <nl> tables_used -> db [ 1 ]==' y ' &&
compare_errors : <nl> has already been dropped . To ignore such irrelevant " table does <nl> not exist errors ", we silently clear the error if TEMPORARY was used . <nl> */ <nl> - if ( thd -> lex -> drop_temporary && <nl> - thd -> net . last_errno == ER_BAD_TABLE_ERROR && ! expected_error ) <nl> + if ( thd -> net . last_errno == ER_BAD_TABLE_ERROR && <nl> + ! expected_error && thd -> lex -> drop_temporary ) <nl> thd -> clear_error (); <nl> /* <nl> If we expected a non - zero error code , and we don ' t get the same error
btr_search_drop ( dict_index_t * index ) <nl> continue ; <nl> } <nl>  <nl> - const dict_index_t * index = <nl> + const dict_index_t * found_index = <nl> reinterpret_cast < const buf_block_t *>( <nl> bpage )-> index ; <nl> - if ( index != NULL ) { <nl> + if ( found_index == index ) { <nl> drop [ n_drop ]. copy_from ( bpage -> id ); <nl> if (++ n_drop == DROP_BATCH ) { <nl> break ;
bool mysql_create_table ( THD * thd , TABLE_LIST * create_table , <nl> if (! result && ! thd -> is_plugin_fake_ddl ()) <nl> result = trans_commit_stmt ( thd ) || trans_commit_implicit ( thd ); <nl>  <nl> - if ( result ) <nl> + if ( result && ! thd -> is_plugin_fake_ddl ()) <nl> { <nl> trans_rollback_stmt ( thd ); <nl> /*
my_bool cli_read_prepare_result ( MYSQL * mysql , MYSQL_STMT * stmt ) <nl> /* skip parameters data : we don ' t support it yet */ <nl> if (!( cli_read_metadata ( mysql , param_count , 7 ))) <nl> DBUG_RETURN ( 1 ); <nl> + /* free memory allocated by cli_read_metadata () for parameters data */ <nl> + free_root (& mysql -> field_alloc , MYF ( 0 )); <nl> } <nl>  <nl> if ( field_count != 0 )
int <nl> Tablespace_client :: get_tablespace_info ( CreateFilegroupImplReq * rep ) <nl> { <nl> Ptr < Tsman :: Tablespace > ts_ptr ; <nl> - if ( m_tsman -> m_tablespace_hash . find ( ts_ptr , m_tablespace_id )); <nl> + if ( m_tsman -> m_tablespace_hash . find ( ts_ptr , m_tablespace_id )) <nl> { <nl> rep -> tablespace . extent_size = ts_ptr . p -> m_extent_size ; <nl> rep -> tablespace . logfile_group_id =
history_save ( History * h , const char * fname ) <nl> retval = HPREV ( h , & ev ), i ++) { <nl> len = strlen ( ev . str ) * 4 ; <nl> if ( len >= max_size ) { <nl> - max_size = ( len + 1023 ) & 1023 ; <nl> + max_size = ( len + 1023 ) & ~ 1023 ; <nl> ptr = h_realloc ( ptr , max_size ); <nl> } <nl> ( void ) strvis ( ptr , ev . str , VIS_WHITE );
public : <nl> character set is utf - 8 , we can safely assume that no <nl> character starts with a zero byte . <nl> */ <nl> - return memcmp ( m_ptr , rhs -> m_ptr , min ( m_length , rhs -> m_length )+ 1 ); <nl> + return memcmp ( m_ptr , rhs -> m_ptr , min ( m_length , rhs -> m_length )); <nl> } <nl>  <nl> MDL_key ( const MDL_key * rhs )
NDB_Modifiers :: parse ( THD * thd , <nl> } <nl> my_error ( ER_ILLEGAL_HA_CREATE_OPTION , MYF ( 0 ), ndbcluster_hton_name , <nl> " Syntax error in COMMENT modifier "); <nl> + if ( source != _source ) <nl> + { <nl> + delete [] source ; <nl> + } <nl> return - 1 ; <nl> } <nl> 
zblob_reader_t :: setup_zstream () <nl> dberr_t <nl> zblob_reader_t :: fetch () <nl> { <nl> - dberr_t err ; <nl> + dberr_t err = DB_SUCCESS ; <nl> DBUG_ENTER (" zblob_reader_t :: fetch "); <nl>  <nl> ut_ad ( is_valid_blob ());
byte ft_simple_get_word ( CHARSET_INFO * cs , byte ** start , byte * end , <nl> FT_WORD * word ) <nl> { <nl> byte * doc = * start ; <nl> - uint mwc , length ; <nl> + uint mwc , length , mbl ; <nl> DBUG_ENTER (" ft_simple_get_word "); <nl>  <nl> while ( doc < end ) <nl> byte ft_simple_get_word ( CHARSET_INFO * cs , byte ** start , byte * end , <nl> } <nl>  <nl> mwc = length = 0 ; <nl> - for ( word -> pos = doc ; doc < end ; length ++, doc += my_mbcharlen ( cs , *( uchar *) doc )) <nl> + for ( word -> pos = doc ; doc < end ; length ++, mbl = my_mbcharlen ( cs , *( uchar *) doc ), doc +=( mbl ? mbl : 1 )) <nl> if ( true_word_char ( cs ,* doc )) <nl> mwc = 0 ; <nl> else if (! misc_word_char (* doc ) || mwc ++)
ha_innopart :: repair ( <nl>  <nl> /* Only repair partitions for MEDIUM or EXTENDED options . */ <nl> if (( repair_opt -> flags & ( T_MEDIUM | T_EXTEND )) == 0 ) { <nl> - return ( HA_ADMIN_OK ); <nl> + DBUG_RETURN ( HA_ADMIN_OK ); <nl> } <nl> if ( set_altered_partitions ()) { <nl> ut_ad ( 0 ); // Already checked by set_part_state ()! <nl> - return ( HA_ADMIN_INVALID ); <nl> + DBUG_RETURN ( HA_ADMIN_INVALID ); <nl> } <nl> for ( uint i = m_part_info -> get_first_used_partition (); <nl> i < m_tot_parts ;
inline void Operation :: readColumn ( int id ) { <nl> /* Methods for writing to the key record */ <nl>  <nl> inline size_t Operation :: requiredKeyBuffer () { <nl> - return plan -> key_record -> rec_size ; <nl> + /* Malloc checkers complain if this + 1 is not present . Not sure why . <nl> + Theory : because the terminating null of a C - string may be written there . */ <nl> + return plan -> key_record -> rec_size + 1 ; <nl> } <nl>  <nl> inline void Operation :: clearKeyNullBits () { <nl> inline void Operation :: setKeyPartNull ( int id ) { <nl> /* Methods for writing to the row */ <nl>  <nl> inline size_t Operation :: requiredBuffer () { <nl> - return record -> rec_size ; <nl> + return record -> rec_size + 1 ; <nl> } <nl>  <nl> inline void Operation :: setNullBits () {
RWPool :: handle_invalid_release ( Ptr < void > ptr ) <nl> Uint32 * record_ptr_i = ( m_memroot + pageI )-> m_data + pos ; <nl>  <nl> Uint32 magic = * ( record_ptr_p + m_record_info . m_offset_magic ); <nl> - snprintf ( buf , sizeof ( buf ), <nl> + BaseString :: snprintf ( buf , sizeof ( buf ), <nl> " Invalid memory release : ptr (% x % p % p ) magic : (%. 8x %. 8x ) memroot : % p page : % x ", <nl> ptr . i , ptr . p , record_ptr_i , magic , m_record_info . m_type_id , <nl> m_memroot , <nl> RWPool :: handle_invalid_get_ptr ( Uint32 ptrI ) <nl> Uint32 * record_ptr_i = ( m_memroot + pageI )-> m_data + pos ; <nl>  <nl> Uint32 magic = * ( record_ptr_i + m_record_info . m_offset_magic ); <nl> - snprintf ( buf , sizeof ( buf ), <nl> + BaseString :: snprintf ( buf , sizeof ( buf ), <nl> " Invalid memory access : ptr (% x % p ) magic : (%. 8x %. 8x ) memroot : % p page : % x ", <nl> ptrI , record_ptr_i , magic , m_record_info . m_type_id , <nl> m_memroot ,
PFS_thread * create_thread ( PFS_thread_class * klass , const void * identity , <nl> pfs -> m_processlist_info_length = 0 ; <nl> pfs -> m_connection_type = NO_VIO_TYPE ; <nl>  <nl> + pfs -> m_thd = NULL ; <nl> pfs -> m_host = NULL ; <nl> pfs -> m_user = NULL ; <nl> pfs -> m_account = NULL ;
private : <nl> const bool is_temp <nl> = id . m_space_id == srv_tmp_space . space_id (); <nl>  <nl> - return ( is_ibuf || is_temp ); <nl> + return ( is_ibuf || is_temp <nl> + || ( id . m_index_id & 0xFFFFFFFF00000000ULL ) != 0 ); <nl> } <nl>  <nl> /** ( key , value ) storage . */
int SetCipherList ( WOLFSSL_CTX * ctx , Suites * suites , const char * list ) <nl> } <nl> # endif /* WOLFSSL_DTLS */ <nl>  <nl> + if ( idx + 1 >= WOLFSSL_MAX_SUITE_SZ ) { <nl> + WOLFSSL_MSG (" WOLFSSL_MAX_SUITE_SZ set too low "); <nl> + return 0 ; /* suites buffer not large enough , error out */ <nl> + } <nl> + <nl> suites -> suites [ idx ++] = ( XSTRSTR ( name , " TLS13 ")) ? TLS13_BYTE <nl> : ( XSTRSTR ( name , " CHACHA ")) ? CHACHA_BYTE <nl> : ( XSTRSTR ( name , " QSH ")) ? QSH_BYTE
int activate_fd ( int irq , int fd , int type , void * dev_id ) <nl> . events = events , <nl> . current_events = 0 } ); <nl>  <nl> + err = - EBUSY ; <nl> spin_lock_irqsave (& irq_lock , flags ); <nl> for ( irq_fd = active_fds ; irq_fd != NULL ; irq_fd = irq_fd -> next ) { <nl> if (( irq_fd -> fd == fd ) && ( irq_fd -> type == type )) {
static int max77686_pmic_dt_parse_pdata ( struct max77686_dev * iodev , <nl> for ( i = 0 ; i < pdata -> num_regulators ; i ++) { <nl> rmatch . name = regulators [ i ]. name ; <nl> rmatch . init_data = NULL ; <nl> + rmatch . of_node = NULL ; <nl> of_regulator_match ( iodev -> dev , regulators_np , & rmatch , 1 ); <nl> rdata [ i ]. initdata = rmatch . init_data ; <nl> }
static int ivtv_getsda_old ( void * data ) <nl> /* template for i2c - bit - algo */ <nl> static struct i2c_adapter ivtv_i2c_adap_template = { <nl> . name = " ivtv i2c driver ", <nl> - . id = I2C_HW_B_CX2341X , /* algo - bit is OR ' d with this */ <nl> + . id = I2C_HW_B_CX2341X , <nl> . algo = NULL , /* set by i2c - algo - bit */ <nl> . algo_data = NULL , /* filled from template */ <nl> . client_register = attach_inform ,
static void ati_remote_input_report ( struct urb * urb , struct pt_regs * regs ) <nl> input_regs ( dev , regs ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 1 ); <nl> + input_sync ( dev ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 0 ); <nl> input_sync ( dev );
static int __init fcoe_init ( void ) <nl> /* Setup link change notification */ <nl> fcoe_dev_setup (); <nl>  <nl> - fcoe_if_init (); <nl> + rc = fcoe_if_init (); <nl> + if ( rc ) <nl> + goto out_free ; <nl>  <nl> return 0 ; <nl> 
static int __devinit cciss_init_one ( struct pci_dev * pdev , <nl> int j = 0 ; <nl> int rc ; <nl> int dac , return_code ; <nl> - InquiryData_struct * inq_buff = NULL ; <nl> + InquiryData_struct * inq_buff ; <nl>  <nl> if ( reset_devices ) { <nl> /* Reset the controller with a PCI power - cycle */ <nl> static int __devinit cciss_init_one ( struct pci_dev * pdev , <nl> printk ( KERN_WARNING " cciss : unable to determine firmware " <nl> " version of controller \ n "); <nl> } <nl> + kfree ( inq_buff ); <nl>  <nl> cciss_procinit ( i ); <nl>  <nl> static int __devinit cciss_init_one ( struct pci_dev * pdev , <nl> return 1 ; <nl>  <nl> clean4 : <nl> - kfree ( inq_buff ); <nl> kfree ( hba [ i ]-> cmd_pool_bits ); <nl> if ( hba [ i ]-> cmd_pool ) <nl> pci_free_consistent ( hba [ i ]-> pdev ,
void perf_cgroup_switch ( struct task_struct * task , int mode ) <nl>  <nl> list_for_each_entry_rcu ( pmu , & pmus , entry ) { <nl> cpuctx = this_cpu_ptr ( pmu -> pmu_cpu_context ); <nl> + if ( cpuctx -> unique_pmu != pmu ) <nl> + continue ; /* ensure we process each cpuctx once */ <nl>  <nl> /* <nl> * perf_cgroup_events says at least one <nl> void perf_cgroup_switch ( struct task_struct * task , int mode ) <nl>  <nl> if ( mode & PERF_CGROUP_SWIN ) { <nl> WARN_ON_ONCE ( cpuctx -> cgrp ); <nl> - /* set cgrp before ctxsw in to <nl> - * allow event_filter_match () to not <nl> - * have to pass task around <nl> + /* <nl> + * set cgrp before ctxsw in to allow <nl> + * event_filter_match () to not have to pass <nl> + * task around <nl> */ <nl> cpuctx -> cgrp = perf_cgroup_from_task ( task ); <nl> cpu_ctx_sched_in ( cpuctx , EVENT_ALL , task );
static ssize_t auerchar_write ( struct file * file , const char __user * buf , size_t <nl> int ret ; <nl> wait_queue_t wait ; <nl>  <nl> - dbg (" auerchar_write % d bytes ", len ); <nl> + dbg (" auerchar_write % zd bytes ", len ); <nl>  <nl> /* Error checking */ <nl> if (! ccp )
show_regs ( struct pt_regs * regs ) <nl> void <nl> start_thread ( struct pt_regs * regs , unsigned long pc , unsigned long sp ) <nl> { <nl> - set_fs ( USER_DS ); <nl> regs -> pc = pc ; <nl> regs -> ps = 8 ; <nl> wrusp ( sp );
static void kvm_mmu_remove_some_alloc_mmu_pages ( struct kvm * kvm , <nl> { <nl> struct kvm_mmu_page * page ; <nl>  <nl> + if ( list_empty (& kvm -> arch . active_mmu_pages )) <nl> + return ; <nl> + <nl> page = container_of ( kvm -> arch . active_mmu_pages . prev , <nl> struct kvm_mmu_page , link ); <nl> kvm_mmu_prepare_zap_page ( kvm , page , invalid_list );
static int iwl_read_ucode ( struct iwl_priv * priv ) <nl> priv -> ucode_data_backup . len = data_size ; <nl> iwl_alloc_fw_desc ( priv -> pci_dev , & priv -> ucode_data_backup ); <nl>  <nl> + if (! priv -> ucode_code . v_addr || ! priv -> ucode_data . v_addr || <nl> + ! priv -> ucode_data_backup . v_addr ) <nl> + goto err_pci_alloc ; <nl> + <nl> /* Initialization instructions and data */ <nl> if ( init_size && init_data_size ) { <nl> priv -> ucode_init . len = init_size ;
static int nci_add_new_protocol ( struct nci_dev * ndev , <nl> struct rf_tech_specific_params_nfcf_poll * nfcf_poll ; <nl> __u32 protocol ; <nl>  <nl> - if ( rf_protocol == NCI_RF_PROTOCOL_T2T ) <nl> + if ( rf_protocol == NCI_RF_PROTOCOL_T1T ) <nl> + protocol = NFC_PROTO_JEWEL_MASK ; <nl> + else if ( rf_protocol == NCI_RF_PROTOCOL_T2T ) <nl> protocol = NFC_PROTO_MIFARE_MASK ; <nl> else if ( rf_protocol == NCI_RF_PROTOCOL_ISO_DEP ) <nl> if ( rf_tech_and_mode == NCI_NFC_A_PASSIVE_POLL_MODE )
int rxrpc_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> if ( copy > len - copied ) <nl> copy = len - copied ; <nl>  <nl> - if ( skb -> ip_summed == CHECKSUM_UNNECESSARY ) { <nl> + if ( skb -> ip_summed == CHECKSUM_UNNECESSARY || <nl> + skb -> ip_summed == CHECKSUM_PARTIAL ) { <nl> ret = skb_copy_datagram_iovec ( skb , offset , <nl> msg -> msg_iov , copy ); <nl> } else {
void unregister_memory_isolate_notifier ( struct notifier_block * nb ) <nl> } <nl> EXPORT_SYMBOL ( unregister_memory_isolate_notifier ); <nl>  <nl> + static void memory_block_release ( struct device * dev ) <nl> +{ <nl> + struct memory_block * mem = container_of ( dev , struct memory_block , dev ); <nl> + <nl> + kfree ( mem ); <nl> +} <nl> + <nl> /* <nl> * register_memory - Setup a sysfs device for a memory block <nl> */ <nl> int register_memory ( struct memory_block * memory ) <nl>  <nl> memory -> dev . bus = & memory_subsys ; <nl> memory -> dev . id = memory -> start_section_nr / sections_per_block ; <nl> + memory -> dev . release = memory_block_release ; <nl>  <nl> error = device_register (& memory -> dev ); <nl> return error ; <nl> int remove_memory_block ( unsigned long node_id , struct mem_section * section , <nl> mem_remove_simple_file ( mem , phys_device ); <nl> mem_remove_simple_file ( mem , removable ); <nl> unregister_memory ( mem ); <nl> - kfree ( mem ); <nl> } else <nl> kobject_put (& mem -> dev . kobj ); <nl> 
fl_create ( struct net * net , struct in6_flowlabel_req * freq , char __user * optval , <nl> goto done ; <nl> fl -> share = freq -> flr_share ; <nl> addr_type = ipv6_addr_type (& freq -> flr_dst ); <nl> - if (( addr_type & IPV6_ADDR_MAPPED ) <nl> - || addr_type == IPV6_ADDR_ANY ) { <nl> + if (( addr_type & IPV6_ADDR_MAPPED ) || <nl> + addr_type == IPV6_ADDR_ANY ) { <nl> err = - EINVAL ; <nl> goto done ; <nl> } <nl> static int mem_check ( struct sock * sk ) <nl>  <nl> if ( room <= 0 || <nl> (( count >= FL_MAX_PER_SOCK || <nl> - ( count > 0 && room < FL_MAX_SIZE / 2 ) || room < FL_MAX_SIZE / 4 ) <nl> - && ! capable ( CAP_NET_ADMIN ))) <nl> + ( count > 0 && room < FL_MAX_SIZE / 2 ) || room < FL_MAX_SIZE / 4 ) && <nl> + ! capable ( CAP_NET_ADMIN ))) <nl> return - ENOBUFS ; <nl>  <nl> return 0 ;
static struct platform_pwm_backlight_data zoom_backlight_data = { <nl> . max_brightness = 127 , <nl> . dft_brightness = 127 , <nl> . pwm_period_ns = 7812500 , <nl> + . enable_gpio = - 1 , <nl> }; <nl>  <nl> static struct platform_device zoom_backlight_pwm = {
int of_dma_controller_register ( struct device_node * np , <nl> if (! nbcells ) { <nl> pr_err ("% s : # dma - cells property is missing or invalid \ n ", <nl> __func__ ); <nl> + kfree ( ofdma ); <nl> return - EINVAL ; <nl> } <nl> 
void hw_cursor_setData ( struct lynx_cursor * cursor , <nl> iowrite16 ( data , pbuffer ); <nl>  <nl> /* assume pitch is 1 , 2 , 4 , 8 ,...*/ <nl> - if (( i + 1 ) % pitch == 0 ) <nl> - { <nl> + if (( i + 1 ) % pitch == 0 ) { <nl> /* need a return */ <nl> pstart += offset ; <nl> pbuffer = pstart ;
static inline int test_and_set_bit ( unsigned long nr , <nl> unsigned short bit = nr & SZLONG_MASK ; <nl> unsigned long res ; <nl>  <nl> + smp_llsc_mb (); <nl> + <nl> if ( cpu_has_llsc && R10000_LLSC_WAR ) { <nl> unsigned long * m = (( unsigned long *) addr ) + ( nr >> SZLONG_LOG ); <nl> unsigned long temp ; <nl> static inline int test_and_clear_bit ( unsigned long nr , <nl> unsigned short bit = nr & SZLONG_MASK ; <nl> unsigned long res ; <nl>  <nl> + smp_llsc_mb (); <nl> + <nl> if ( cpu_has_llsc && R10000_LLSC_WAR ) { <nl> unsigned long * m = (( unsigned long *) addr ) + ( nr >> SZLONG_LOG ); <nl> unsigned long temp ; <nl> static inline int test_and_change_bit ( unsigned long nr , <nl> unsigned short bit = nr & SZLONG_MASK ; <nl> unsigned long res ; <nl>  <nl> + smp_llsc_mb (); <nl> + <nl> if ( cpu_has_llsc && R10000_LLSC_WAR ) { <nl> unsigned long * m = (( unsigned long *) addr ) + ( nr >> SZLONG_LOG ); <nl> unsigned long temp ;
struct connect_attr { <nl> void * arg ; <nl> enum AUTHTYPE auth_type ; <nl> u8 ch ; <nl> - void * pJoinParams ; <nl> + void * params ; <nl> }; <nl>  <nl> struct rcvd_async_info { <nl> static s32 Handle_Connect ( struct host_if_drv * hif_drv , <nl>  <nl> PRINT_INFO ( HOSTINF_DBG , " Saving connection parameters in global structure \ n "); <nl>  <nl> - ptstrJoinBssParam = ( struct join_bss_param *) pstrHostIFconnectAttr -> pJoinParams ; <nl> + ptstrJoinBssParam = ( struct join_bss_param *) pstrHostIFconnectAttr -> params ; <nl> if ( ptstrJoinBssParam == NULL ) { <nl> PRINT_ER (" Required BSSID not found \ n "); <nl> s32Error = - ENOENT ; <nl> s32 host_int_set_join_req ( struct host_if_drv * hif_drv , u8 * pu8bssid , <nl> msg . body . con_info . ch = u8channel ; <nl> msg . body . con_info . result = pfConnectResult ; <nl> msg . body . con_info . arg = pvUserArg ; <nl> - msg . body . con_info . pJoinParams = pJoinParams ; <nl> + msg . body . con_info . params = pJoinParams ; <nl> msg . drv = hif_drv ; <nl>  <nl> if ( pu8bssid != NULL ) {
void eth_header_cache_update ( struct hh_cache * hh , struct net_device * dev , <nl> static int eth_mac_addr ( struct net_device * dev , void * p ) <nl> { <nl> struct sockaddr * addr = p ; <nl> + <nl> if ( netif_running ( dev )) <nl> return - EBUSY ; <nl> + if (! is_valid_ether_addr ( addr -> sa_data )) <nl> + return - EADDRNOTAVAIL ; <nl> memcpy ( dev -> dev_addr , addr -> sa_data , dev -> addr_len ); <nl> return 0 ; <nl> }
idtoname_request ( struct cache_detail * cd , struct cache_head * ch , char ** bpp , <nl> char idstr [ 11 ]; <nl>  <nl> qword_add ( bpp , blen , ent -> authname ); <nl> - snprintf ( idstr , sizeof ( idstr ), "% d ", ent -> id ); <nl> + snprintf ( idstr , sizeof ( idstr ), "% u ", ent -> id ); <nl> qword_add ( bpp , blen , ent -> type == IDMAP_TYPE_GROUP ? " group " : " user "); <nl> qword_add ( bpp , blen , idstr ); <nl>  <nl> idtoname_show ( struct seq_file * m , struct cache_detail * cd , struct cache_head * h ) <nl> return 0 ; <nl> } <nl> ent = container_of ( h , struct ent , h ); <nl> - seq_printf ( m , "% s % s % d ", ent -> authname , <nl> + seq_printf ( m , "% s % s % u ", ent -> authname , <nl> ent -> type == IDMAP_TYPE_GROUP ? " group " : " user ", <nl> ent -> id ); <nl> if ( test_bit ( CACHE_VALID , & h -> flags )) <nl> nametoid_show ( struct seq_file * m , struct cache_detail * cd , struct cache_head * h ) <nl> ent -> type == IDMAP_TYPE_GROUP ? " group " : " user ", <nl> ent -> name ); <nl> if ( test_bit ( CACHE_VALID , & h -> flags )) <nl> - seq_printf ( m , " % d ", ent -> id ); <nl> + seq_printf ( m , " % u ", ent -> id ); <nl> seq_printf ( m , "\ n "); <nl> return 0 ; <nl> }
static int acm_probe ( struct usb_interface * intf , <nl> } <nl>  <nl> while ( buflen > 0 ) { <nl> + elength = buffer [ 0 ]; <nl> + if (! elength ) { <nl> + dev_err (& intf -> dev , " skipping garbage byte \ n "); <nl> + elength = 1 ; <nl> + goto next_desc ; <nl> + } <nl> if ( buffer [ 1 ] != USB_DT_CS_INTERFACE ) { <nl> dev_err (& intf -> dev , " skipping garbage \ n "); <nl> goto next_desc ; <nl> } <nl> - elength = buffer [ 0 ]; <nl>  <nl> switch ( buffer [ 2 ]) { <nl> case USB_CDC_UNION_TYPE : /* we ' ve found it */
int iwl_mvm_mac_setup_register ( struct iwl_mvm * mvm ) <nl>  <nl> hw -> wiphy -> max_remain_on_channel_duration = 10000 ; <nl> hw -> max_listen_interval = IWL_CONN_MAX_LISTEN_INTERVAL ; <nl> + /* we can compensate an offset of up to 3 channels = 15 MHz */ <nl> + hw -> wiphy -> max_adj_channel_rssi_comp = 3 * 5 ; <nl>  <nl> /* Extract MAC address */ <nl> memcpy ( mvm -> addresses [ 0 ]. addr , mvm -> nvm_data -> hw_addr , ETH_ALEN );
static int ext4_alloc_branch ( handle_t * handle , struct inode * inode , <nl> return 0 ; <nl> failed : <nl> for (; i >= 0 ; i --) { <nl> - if ( i != indirect_blks && branch [ i ]. bh ) <nl> + /* <nl> + * We want to ext4_forget () only freshly allocated indirect <nl> + * blocks . Buffer for new_blocks [ i - 1 ] is at branch [ i ]. bh and <nl> + * buffer at branch [ 0 ]. bh is indirect block / inode already <nl> + * existing before ext4_alloc_branch () was called . <nl> + */ <nl> + if ( i > 0 && i != indirect_blks && branch [ i ]. bh ) <nl> ext4_forget ( handle , 1 , inode , branch [ i ]. bh , <nl> branch [ i ]. bh -> b_blocknr ); <nl> ext4_free_blocks ( handle , inode , NULL , new_blocks [ i ],
static int __devinit gen_74x164_probe ( struct spi_device * spi ) <nl> } <nl>  <nl> chip -> gpio_chip . ngpio = GEN_74X164_NUMBER_GPIOS * chip -> registers ; <nl> - chip -> buffer = devm_kzalloc (& spi -> dev , chip -> gpio_chip . ngpio , GFP_KERNEL ); <nl> + chip -> buffer = devm_kzalloc (& spi -> dev , chip -> registers , GFP_KERNEL ); <nl> if (! chip -> buffer ) { <nl> ret = - ENOMEM ; <nl> goto exit_destroy ;
static int __devinit corgipm_init ( void ) <nl> { <nl> int ret ; <nl>  <nl> + if (! machine_is_corgi () && ! machine_is_shepherd () <nl> + && ! machine_is_husky ()) <nl> + return - ENODEV ; <nl> + <nl> corgipm_device = platform_device_alloc (" sharpsl - pm ", - 1 ); <nl> if (! corgipm_device ) <nl> return - ENOMEM ;
static int virtscsi_probe ( struct virtio_device * vdev ) <nl> u32 num_queues ; <nl> struct scsi_host_template * hostt ; <nl>  <nl> + if (! vdev -> config -> get ) { <nl> + dev_err (& vdev -> dev , "% s failure : config access disabled \ n ", <nl> + __func__ ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> /* We need to know how many queues before we allocate . */ <nl> num_queues = virtscsi_config_get ( vdev , num_queues ) ? : 1 ; <nl> 
iblock_get_bio ( struct se_task * task , sector_t lba , u32 sg_num ) <nl> struct iblock_req * ib_req = IBLOCK_REQ ( task ); <nl> struct bio * bio ; <nl>  <nl> + /* <nl> + * Only allocate as many vector entries as the bio code allows us to , <nl> + * we ' ll loop later on until we have handled the whole request . <nl> + */ <nl> + if ( sg_num > BIO_MAX_PAGES ) <nl> + sg_num = BIO_MAX_PAGES ; <nl> + <nl> bio = bio_alloc_bioset ( GFP_NOIO , sg_num , ib_dev -> ibd_bio_set ); <nl> if (! bio ) { <nl> pr_err (" Unable to allocate memory for bio \ n ");
static void qeth_clear_output_buffer ( struct qeth_qdio_out_q * queue , <nl> buf -> buffer -> element [ i ]. addr = NULL ; <nl> buf -> buffer -> element [ i ]. flags = 0 ; <nl> } <nl> + buf -> buffer -> element [ 15 ]. flags = 0 ; <nl> buf -> next_element_to_fill = 0 ; <nl> atomic_set (& buf -> state , QETH_QDIO_BUF_EMPTY ); <nl> }
static void ib_ucm_cleanup_events ( struct ib_ucm_context * ctx ) <nl> struct ib_ucm_event , ctx_list ); <nl> list_del (& uevent -> file_list ); <nl> list_del (& uevent -> ctx_list ); <nl> + mutex_unlock (& ctx -> file -> file_mutex ); <nl>  <nl> /* clear incoming connections . */ <nl> if ( ib_ucm_new_cm_id ( uevent -> resp . event )) <nl> ib_destroy_cm_id ( uevent -> cm_id ); <nl>  <nl> kfree ( uevent ); <nl> + mutex_lock (& ctx -> file -> file_mutex ); <nl> } <nl> mutex_unlock (& ctx -> file -> file_mutex ); <nl> }
static void rcu_eqs_enter_common ( struct rcu_dynticks * rdtp , long long oldval , <nl> { <nl> trace_rcu_dyntick ( TPS (" Start "), oldval , rdtp -> dynticks_nesting ); <nl> if (! user && ! is_idle_task ( current )) { <nl> - struct task_struct * idle = idle_task ( smp_processor_id ()); <nl> + struct task_struct * idle __maybe_unused = <nl> + idle_task ( smp_processor_id ()); <nl>  <nl> trace_rcu_dyntick ( TPS (" Error on entry : not idle task "), oldval , 0 ); <nl> ftrace_dump ( DUMP_ORIG ); <nl> static void rcu_eqs_exit_common ( struct rcu_dynticks * rdtp , long long oldval , <nl> rcu_cleanup_after_idle ( smp_processor_id ()); <nl> trace_rcu_dyntick ( TPS (" End "), oldval , rdtp -> dynticks_nesting ); <nl> if (! user && ! is_idle_task ( current )) { <nl> - struct task_struct * idle = idle_task ( smp_processor_id ()); <nl> + struct task_struct * idle __maybe_unused = <nl> + idle_task ( smp_processor_id ()); <nl>  <nl> trace_rcu_dyntick ( TPS (" Error on exit : not idle task "), <nl> oldval , rdtp -> dynticks_nesting );
static int dvb_frontend_ioctl_properties ( struct inode * inode , struct file * file , <nl>  <nl> /* Put an arbitrary limit on the number of messages that can <nl> * be sent at once */ <nl> - if ( tvps -> num > DTV_IOCTL_MAX_MSGS ) <nl> + if (( tvps -> num == 0 ) || ( tvps -> num > DTV_IOCTL_MAX_MSGS )) <nl> return - EINVAL ; <nl>  <nl> tvp = ( struct dtv_property *) kmalloc ( tvps -> num * <nl> static int dvb_frontend_ioctl_properties ( struct inode * inode , struct file * file , <nl>  <nl> /* Put an arbitrary limit on the number of messages that can <nl> * be sent at once */ <nl> - if ( tvps -> num > DTV_IOCTL_MAX_MSGS ) <nl> + if (( tvps -> num == 0 ) || ( tvps -> num > DTV_IOCTL_MAX_MSGS )) <nl> return - EINVAL ; <nl>  <nl> tvp = ( struct dtv_property *) kmalloc ( tvps -> num *
static int ipu_probe ( struct platform_device * pdev ) <nl> if ( ret ) <nl> goto out_failed_irq ; <nl>  <nl> - ipu_reset ( ipu ); <nl> + ret = ipu_reset ( ipu ); <nl> + if ( ret ) <nl> + goto out_failed_reset ; <nl>  <nl> /* Set MCU_T to divide MCU access window into 2 */ <nl> ipu_cm_write ( ipu , 0x00400000L | ( IPU_MCU_T_DEFAULT << 18 ), <nl> failed_add_clients : <nl> ipu_submodules_exit ( ipu ); <nl> failed_submodules_init : <nl> ipu_irq_exit ( ipu ); <nl> + out_failed_reset : <nl> out_failed_irq : <nl> clk_disable_unprepare ( ipu -> clk ); <nl> failed_clk_get :
static int fuse_rename ( struct inode * olddir , struct dentry * oldent , <nl> fuse_invalidate_attr ( newdir ); <nl>  <nl> /* newent will end up negative */ <nl> - if ( newent -> d_inode ) <nl> + if ( newent -> d_inode ) { <nl> + fuse_invalidate_attr ( newent -> d_inode ); <nl> fuse_invalidate_entry_cache ( newent ); <nl> + } <nl> } else if ( err == - EINTR ) { <nl> /* If request was interrupted , DEITY only knows if the <nl> rename actually took place . If the invalidation
static int irda_usb_probe ( struct usb_interface * intf , <nl> self -> needspatch = ( ret < 0 ); <nl> if ( ret < 0 ) { <nl> printk (" patch_device failed \ n "); <nl> - goto err_out_4 ; <nl> + goto err_out_5 ; <nl> } <nl>  <nl> /* replace IrDA class descriptor with what patched device is now reporting */ <nl> irda_desc = irda_usb_find_class_desc ( self -> usbintf ); <nl> if ( irda_desc == NULL ) { <nl> ret = - ENODEV ; <nl> - goto err_out_4 ; <nl> + goto err_out_5 ; <nl> } <nl> if ( self -> irda_desc ) <nl> kfree ( self -> irda_desc ); <nl> static int irda_usb_probe ( struct usb_interface * intf , <nl>  <nl> return 0 ; <nl>  <nl> + err_out_5 : <nl> + unregister_netdev ( self -> netdev ); <nl> err_out_4 : <nl> kfree ( self -> speed_buff ); <nl> err_out_3 :
int cfg80211_mgd_wext_giwessid ( struct net_device * dev , <nl> data -> flags = 1 ; <nl> data -> length = wdev -> wext . connect . ssid_len ; <nl> memcpy ( ssid , wdev -> wext . connect . ssid , data -> length ); <nl> - } else <nl> - data -> flags = 0 ; <nl> + } <nl> wdev_unlock ( wdev ); <nl>  <nl> return 0 ; <nl> int cfg80211_mgd_wext_giwap ( struct net_device * dev , <nl> wdev_lock ( wdev ); <nl> if ( wdev -> current_bss ) <nl> memcpy ( ap_addr -> sa_data , wdev -> current_bss -> pub . bssid , ETH_ALEN ); <nl> - else if ( wdev -> wext . connect . bssid ) <nl> - memcpy ( ap_addr -> sa_data , wdev -> wext . connect . bssid , ETH_ALEN ); <nl> else <nl> memset ( ap_addr -> sa_data , 0 , ETH_ALEN ); <nl> wdev_unlock ( wdev );
lnet_ping ( lnet_process_id_t id , int timeout_ms , lnet_process_id_t * ids , int n_i <nl>  <nl> rc = - EFAULT ; /* If I SEGV ... */ <nl>  <nl> + memset (& tmpid , 0 , sizeof ( tmpid )); <nl> for ( i = 0 ; i < n_ids ; i ++) { <nl> tmpid . pid = info -> pi_pid ; <nl> tmpid . nid = info -> pi_ni [ i ]. ns_nid ;
static unsigned int bsg_poll ( struct file * file , poll_table * wait ) <nl> spin_lock_irq (& bd -> lock ); <nl> if (! list_empty (& bd -> done_list )) <nl> mask |= POLLIN | POLLRDNORM ; <nl> - if ( bd -> queued_cmds >= bd -> max_queue ) <nl> + if ( bd -> queued_cmds < bd -> max_queue ) <nl> mask |= POLLOUT ; <nl> spin_unlock_irq (& bd -> lock ); <nl> 
device_receive_frame ( <nl> } <nl>  <nl> ev . src_addr . sa_family = ARPHRD_ETHER ; <nl> - memcpy ( ev . src_addr . sa_data , pMACHeader -> abyAddr2 , ETH_ALEN ); <nl> + ether_addr_copy ( ev . src_addr . sa_data , <nl> + pMACHeader -> abyAddr2 ); <nl> memset (& wrqu , 0 , sizeof ( wrqu )); <nl> wrqu . data . length = sizeof ( ev ); <nl> wireless_send_event ( pDevice -> dev , IWEVMICHAELMICFAILURE , & wrqu , ( char *)& ev );
*/ <nl> typedef union <nl> { <nl> - __u32 a4 ; <nl> - __u32 a6 [ 4 ]; <nl> + __be32 a4 ; <nl> + __be32 a6 [ 4 ]; <nl> } xfrm_address_t ; <nl>  <nl> /* Ident of a specific xfrm_state . It is used on input to lookup
static void __exit moxa_exit ( void ) <nl> printk (" Couldn ' t unregister MOXA Intellio family serial driver \ n "); <nl> put_tty_driver ( moxaDriver ); <nl>  <nl> - for ( i = 0 ; i < MAX_BOARDS ; i ++) <nl> + for ( i = 0 ; i < MAX_BOARDS ; i ++) { <nl> + if ( moxaBaseAddr [ i ]) <nl> + iounmap ( moxaBaseAddr [ i ]); <nl> if ( moxa_boards [ i ]. busType == MOXA_BUS_TYPE_PCI ) <nl> pci_dev_put ( moxa_boards [ i ]. pciInfo . pdev ); <nl> + } <nl>  <nl> if ( verbose ) <nl> printk (" Done \ n ");
static int exynos_dsi_parse_dt ( struct exynos_dsi * dsi ) <nl>  <nl> ep = of_graph_get_next_endpoint ( node , NULL ); <nl> if (! ep ) { <nl> - ret = - ENXIO ; <nl> + ret = - EINVAL ; <nl> goto end ; <nl> } <nl>  <nl> dsi -> bridge_node = of_graph_get_remote_port_parent ( ep ); <nl> if (! dsi -> bridge_node ) { <nl> - ret = - ENXIO ; <nl> + ret = - EINVAL ; <nl> goto end ; <nl> } <nl> end :
int mmc_send_if_cond ( struct mmc_host * host , u32 ocr ) <nl> static const u8 test_pattern = 0xAA ; <nl> u8 result_pattern ; <nl>  <nl> + memset (& cmd , 0 , sizeof ( struct mmc_command )); <nl> + <nl> /* <nl> * To support SD 2 . 0 cards , we must always invoke SD_SEND_IF_COND <nl> * before SD_APP_OP_COND . This command will harmlessly fail for
static int sep_construct_dma_tables_from_lli ( <nl> table_data_size ); <nl>  <nl> /* If info entry is null - this is the first table built */ <nl> - if ( info_in_entry_ptr == NULL ) { <nl> + if ( info_in_entry_ptr == NULL || info_out_entry_ptr == NULL ) { <nl> /* Set the output parameters to physical addresses */ <nl> * lli_table_in_ptr = <nl> sep_shared_area_virt_to_bus ( sep , dma_in_lli_table_ptr );
void ath10k_thermal_set_throttling ( struct ath10k * ar ) <nl>  <nl> lockdep_assert_held (& ar -> conf_mutex ); <nl>  <nl> + if (! ar -> wmi . ops -> gen_pdev_set_quiet_mode ) <nl> + return ; <nl> + <nl> if ( ar -> state != ATH10K_STATE_ON ) <nl> return ; <nl> 
void __init spear13xx_l2x0_init ( void ) <nl> * write alloc and ' Full line of zero ' options <nl> * <nl> */ <nl> + if (! IS_ENABLED ( CONFIG_CACHE_L2X0 )) <nl> + return ; <nl>  <nl> writel_relaxed ( 0x06 , VA_L2CC_BASE + L2X0_PREFETCH_CTRL ); <nl> 
static int filename_trans_write ( struct policydb * p , void * fp ) <nl> __le32 buf [ 1 ]; <nl> int rc ; <nl>  <nl> + if ( p -> policyvers < POLICYDB_VERSION_FILENAME_TRANS ) <nl> + return 0 ; <nl> + <nl> nel = 0 ; <nl> rc = hashtab_map ( p -> filename_trans , hashtab_cnt , & nel ); <nl> if ( rc )
module_i2c_driver ( si2157_driver ); <nl> MODULE_DESCRIPTION (" Silicon Labs Si2157 / Si2158 silicon tuner driver "); <nl> MODULE_AUTHOR (" Antti Palosaari < crope @ iki . fi >"); <nl> MODULE_LICENSE (" GPL "); <nl> + MODULE_FIRMWARE ( SI2158_A20_FIRMWARE );
store_vrm ( struct device * dev , struct device_attribute * attr , <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
int rtw_tkip_encrypt23a ( struct rtw_adapter * padapter , <nl> arcfour_encrypt (& mycontext , payload , payload , length ); <nl> arcfour_encrypt (& mycontext , payload + length , crc , 4 ); <nl>  <nl> - pframe += pxmitpriv -> frag_len ; <nl> - pframe = PTR_ALIGN ( pframe , 4 ); <nl> + pframe += pxmitpriv -> frag_len ; <nl> + pframe = PTR_ALIGN ( pframe , 4 ); <nl> } <nl> } <nl> 
# ifndef _ASM_ARM_SYSCALL_H <nl> # define _ASM_ARM_SYSCALL_H <nl>  <nl> +# include < linux / audit . h > /* for AUDIT_ARCH_ * */ <nl> +# include < linux / elf . h > /* for ELF_EM */ <nl> # include < linux / err . h > <nl> # include < linux / sched . h > <nl>  <nl> static inline void syscall_set_arguments ( struct task_struct * task , <nl> memcpy (& regs -> ARM_r0 + i , args , n * sizeof ( args [ 0 ])); <nl> } <nl>  <nl> + static inline int syscall_get_arch ( struct task_struct * task , <nl> + struct pt_regs * regs ) <nl> +{ <nl> + /* ARM tasks don ' t change audit architectures on the fly . */ <nl> + return AUDIT_ARCH_ARM ; <nl> +} <nl> + <nl> # endif /* _ASM_ARM_SYSCALL_H */
u32 omap_prcm_get_reset_sources ( void ) <nl> return prm_read_mod_reg ( WKUP_MOD , OMAP2_RM_RSTST ) & 0x7f ; <nl> if ( cpu_is_omap44xx ()) <nl> return prm_read_mod_reg ( WKUP_MOD , OMAP4_RM_RSTST ) & 0x7f ; <nl> + <nl> + return 0 ; <nl> } <nl> EXPORT_SYMBOL ( omap_prcm_get_reset_sources ); <nl>  <nl> /* Resets clock rates and reboots the system . Only called from system . h */ <nl> void omap_prcm_arch_reset ( char mode ) <nl> { <nl> - s16 prcm_offs ; <nl> + s16 prcm_offs = 0 ; <nl>  <nl> if ( cpu_is_omap24xx ()) { <nl> omap2xxx_clk_prepare_for_reboot ();
static int stmmac_init_phy ( struct net_device * dev ) <nl> interface ); <nl> } <nl>  <nl> - if ( IS_ERR ( phydev )) { <nl> + if ( IS_ERR_OR_NULL ( phydev )) { <nl> pr_err ("% s : Could not attach to PHY \ n ", dev -> name ); <nl> + if (! phydev ) <nl> + return - ENODEV ; <nl> + <nl> return PTR_ERR ( phydev ); <nl> } <nl> 
static int sysfs_dentry_revalidate ( struct dentry * dentry , struct nameidata * nd ) <nl> { <nl> struct sysfs_dirent * sd ; <nl> int is_dir ; <nl> + int type ; <nl>  <nl> if ( nd -> flags & LOOKUP_RCU ) <nl> return - ECHILD ; <nl> static int sysfs_dentry_revalidate ( struct dentry * dentry , struct nameidata * nd ) <nl> if ( strcmp ( dentry -> d_name . name , sd -> s_name ) != 0 ) <nl> goto out_bad ; <nl>  <nl> + /* The sysfs dirent has been moved to a different namespace */ <nl> + type = KOBJ_NS_TYPE_NONE ; <nl> + if ( sd -> s_parent ) <nl> + type = sysfs_ns_type ( sd -> s_parent ); <nl> + if ( type && ( sysfs_info ( dentry -> d_sb )-> ns [ type ] != sd -> s_ns )) <nl> + goto out_bad ; <nl> + <nl> mutex_unlock (& sysfs_mutex ); <nl> out_valid : <nl> return 1 ;
static int build_mlx_header ( struct mlx4_ib_sqp * sqp , struct ib_send_wr * wr , <nl> ( be32_to_cpu ( ah -> av . sl_tclass_flowlabel ) >> 20 ) & 0xff ; <nl> sqp -> ud_header . grh . flow_label = <nl> ah -> av . sl_tclass_flowlabel & cpu_to_be32 ( 0xfffff ); <nl> + sqp -> ud_header . grh . hop_limit = ah -> av . hop_limit ; <nl> ib_get_cached_gid ( ib_dev , be32_to_cpu ( ah -> av . port_pd ) >> 24 , <nl> ah -> av . gid_index , & sqp -> ud_header . grh . source_gid ); <nl> memcpy ( sqp -> ud_header . grh . destination_gid . raw ,
static void perf_syscall_enter ( void * ignore , struct pt_regs * regs , long id ) <nl> int size ; <nl>  <nl> syscall_nr = syscall_get_nr ( current , regs ); <nl> + if ( syscall_nr < 0 ) <nl> + return ; <nl> if (! test_bit ( syscall_nr , enabled_perf_enter_syscalls )) <nl> return ; <nl>  <nl> static void perf_syscall_exit ( void * ignore , struct pt_regs * regs , long ret ) <nl> int size ; <nl>  <nl> syscall_nr = syscall_get_nr ( current , regs ); <nl> + if ( syscall_nr < 0 ) <nl> + return ; <nl> if (! test_bit ( syscall_nr , enabled_perf_exit_syscalls )) <nl> return ; <nl> 
qla2x00_process_loopback ( struct fc_bsg_job * bsg_job ) <nl> ql_log ( ql_log_warn , vha , 0x701f , <nl> " Get port config failed .\ n "); <nl> rval = - EPERM ; <nl> - goto done_free_dma_req ; <nl> + goto done_free_dma_rsp ; <nl> } <nl>  <nl> ql_dbg ( ql_dbg_user , vha , 0x70c0 , <nl> qla2x00_process_loopback ( struct fc_bsg_job * bsg_job ) <nl>  <nl> if ( rval ) { <nl> rval = - EPERM ; <nl> - goto done_free_dma_req ; <nl> + goto done_free_dma_rsp ; <nl> } <nl>  <nl> type = " FC_BSG_HST_VENDOR_LOOPBACK "; <nl> qla2x00_process_loopback ( struct fc_bsg_job * bsg_job ) <nl> } <nl>  <nl> rval = - EIO ; <nl> - goto done_free_dma_req ; <nl> + goto done_free_dma_rsp ; <nl> } <nl> } else { <nl> type = " FC_BSG_HST_VENDOR_LOOPBACK "; <nl> qla2x00_process_loopback ( struct fc_bsg_job * bsg_job ) <nl> fw_sts_ptr += sizeof ( response ); <nl> * fw_sts_ptr = command_sent ; <nl>  <nl> + done_free_dma_rsp : <nl> dma_free_coherent (& ha -> pdev -> dev , rsp_data_len , <nl> rsp_data , rsp_data_dma ); <nl> done_free_dma_req :
int tipc_node_get_linkname ( u32 bearer_id , u32 addr , char * linkname , size_t len ) <nl> struct tipc_link * link ; <nl> struct tipc_node * node = tipc_node_find ( addr ); <nl>  <nl> - if (( bearer_id > MAX_BEARERS ) || ! node ) <nl> + if (( bearer_id >= MAX_BEARERS ) || ! node ) <nl> return - EINVAL ; <nl> tipc_node_lock ( node ); <nl> link = node -> links [ bearer_id ];
legacy_init_iomem_resources ( struct resource * code_resource , struct resource * dat <nl> probe_roms (); <nl> for ( i = 0 ; i < e820 . nr_map ; i ++) { <nl> struct resource * res ; <nl> + if ( e820 . map [ i ]. addr + e820 . map [ i ]. size > 0x100000000ULL ) <nl> + continue ; <nl> res = kzalloc ( sizeof ( struct resource ), GFP_ATOMIC ); <nl> switch ( e820 . map [ i ]. type ) { <nl> case E820_RAM : res -> name = " System RAM "; break ;
static void ath_ant_comb_scan ( struct ath_softc * sc , struct ath_rx_status * rs ) <nl> main_ant_conf = ( rs -> rs_rssi_ctl2 >> ATH_ANT_RX_MAIN_SHIFT ) & <nl> ATH_ANT_RX_MASK ; <nl>  <nl> - /* Record packet only when alt_rssi is positive */ <nl> - if ( alt_rssi > 0 ) { <nl> + /* Record packet only when both main_rssi and alt_rssi is positive */ <nl> + if ( main_rssi > 0 && alt_rssi > 0 ) { <nl> antcomb -> total_pkt_count ++; <nl> antcomb -> main_total_rssi += main_rssi ; <nl> antcomb -> alt_total_rssi += alt_rssi ;
static void __net_exit ip6mr_rules_exit ( struct net * net ) <nl> { <nl> struct mr6_table * mrt , * next ; <nl>  <nl> + rtnl_lock (); <nl> list_for_each_entry_safe ( mrt , next , & net -> ipv6 . mr6_tables , list ) { <nl> list_del (& mrt -> list ); <nl> ip6mr_free_table ( mrt ); <nl> } <nl> + rtnl_unlock (); <nl> fib_rules_unregister ( net -> ipv6 . mr6_rules_ops ); <nl> } <nl> # else <nl> static int __net_init ip6mr_rules_init ( struct net * net ) <nl>  <nl> static void __net_exit ip6mr_rules_exit ( struct net * net ) <nl> { <nl> + rtnl_lock (); <nl> ip6mr_free_table ( net -> ipv6 . mrt6 ); <nl> + net -> ipv6 . mrt6 = NULL ; <nl> + rtnl_unlock (); <nl> } <nl> # endif <nl> 
static int irda_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> struct sock * sk = sock -> sk ; <nl> struct irda_sock * self = irda_sk ( sk ); <nl>  <nl> + memset (& saddr , 0 , sizeof ( saddr )); <nl> if ( peer ) { <nl> if ( sk -> sk_state != TCP_ESTABLISHED ) <nl> return - ENOTCONN ;
void msp_set_scart ( struct i2c_client * client , int in , int out ) <nl> msp_write_dsp ( client , 0x13 , state -> acb ); <nl>  <nl> /* Sets I2S speed 0 = 1 . 024 Mbps , 1 = 2 . 048 Mbps */ <nl> - msp_write_dem ( client , 0x40 , state -> i2s_mode ); <nl> + if ( state -> has_i2s_conf ) <nl> + msp_write_dem ( client , 0x40 , state -> i2s_mode ); <nl> } <nl>  <nl> void msp_set_mute ( struct i2c_client * client )
static int wm5100_probe ( struct snd_soc_codec * codec ) <nl> return 0 ; <nl>  <nl> err_gpio : <nl> + if ( i2c -> irq ) <nl> + free_irq ( i2c -> irq , codec ); <nl> wm5100_free_gpio ( codec ); <nl> err_reset : <nl> if ( wm5100 -> pdata . reset ) { <nl> err_core : <nl> static int wm5100_remove ( struct snd_soc_codec * codec ) <nl> { <nl> struct wm5100_priv * wm5100 = snd_soc_codec_get_drvdata ( codec ); <nl> + struct i2c_client * i2c = to_i2c_client ( codec -> dev ); <nl>  <nl> wm5100_set_bias_level ( codec , SND_SOC_BIAS_OFF ); <nl> if ( wm5100 -> pdata . hp_pol ) { <nl> gpio_free ( wm5100 -> pdata . hp_pol ); <nl> } <nl> + if ( i2c -> irq ) <nl> + free_irq ( i2c -> irq , codec ); <nl> wm5100_free_gpio ( codec ); <nl> if ( wm5100 -> pdata . reset ) { <nl> gpio_set_value_cansleep ( wm5100 -> pdata . reset , 1 );
static int cgroupstats_user_cmd ( struct sk_buff * skb , struct genl_info * info ) <nl> na = nla_reserve ( rep_skb , CGROUPSTATS_TYPE_CGROUP_STATS , <nl> sizeof ( struct cgroupstats )); <nl> if ( na == NULL ) { <nl> + nlmsg_free ( rep_skb ); <nl> rc = - EMSGSIZE ; <nl> goto err ; <nl> }
nouveau_cli_destroy ( struct nouveau_cli * cli ) <nl> nvkm_vm_ref ( NULL , & nvxx_client (& cli -> base )-> vm , NULL ); <nl> nvif_client_fini (& cli -> base ); <nl> usif_client_fini ( cli ); <nl> + kfree ( cli ); <nl> } <nl>  <nl> static void
/* <nl> * Literals <nl> */ <nl> -# define IPR_DRIVER_VERSION " 2 . 1 . 2 " <nl> -# define IPR_DRIVER_DATE "( February 8 , 2006 )" <nl> +# define IPR_DRIVER_VERSION " 2 . 1 . 3 " <nl> +# define IPR_DRIVER_DATE "( March 29 , 2006 )" <nl>  <nl> /* <nl> * IPR_MAX_CMD_PER_LUN : This defines the maximum number of outstanding
struct mxs_dma_chan { <nl> int chan_irq ; <nl> struct mxs_dma_ccw * ccw ; <nl> dma_addr_t ccw_phys ; <nl> + int desc_count ; <nl> dma_cookie_t last_completed ; <nl> enum dma_status status ; <nl> unsigned int flags ; <nl> static struct dma_async_tx_descriptor * mxs_dma_prep_slave_sg ( <nl> struct scatterlist * sg ; <nl> int i , j ; <nl> u32 * pio ; <nl> - static int idx ; <nl> + int idx = append ? mxs_chan -> desc_count : 0 ; <nl>  <nl> if ( mxs_chan -> status == DMA_IN_PROGRESS && ! append ) <nl> return NULL ; <nl> static struct dma_async_tx_descriptor * mxs_dma_prep_slave_sg ( <nl> } <nl> } <nl> } <nl> + mxs_chan -> desc_count = idx ; <nl>  <nl> return & mxs_chan -> desc ; <nl>  <nl> static struct dma_async_tx_descriptor * mxs_dma_prep_dma_cyclic ( <nl>  <nl> i ++; <nl> } <nl> + mxs_chan -> desc_count = i ; <nl>  <nl> return & mxs_chan -> desc ; <nl> 
retry_snap : <nl> goto retry_snap ; <nl> } <nl> } else { <nl> + loff_t old_size = inode -> i_size ; <nl> /* <nl> * No need to acquire the i_truncate_mutex . Because <nl> * the MDS revokes Fwb caps before sending truncate <nl> retry_snap : <nl> written = generic_file_buffered_write ( iocb , iov , nr_segs , <nl> pos , & iocb -> ki_pos , <nl> count , 0 ); <nl> + if ( inode -> i_size > old_size ) <nl> + ceph_fscache_update_objectsize ( inode ); <nl> mutex_unlock (& inode -> i_mutex ); <nl> } <nl> 
static int ceph_link ( struct dentry * old_dentry , struct inode * dir , <nl> req -> r_locked_dir = dir ; <nl> req -> r_dentry_drop = CEPH_CAP_FILE_SHARED ; <nl> req -> r_dentry_unless = CEPH_CAP_FILE_EXCL ; <nl> + /* release LINK_SHARED on source inode ( mds will lock it ) */ <nl> + req -> r_old_inode_drop = CEPH_CAP_LINK_SHARED ; <nl> err = ceph_mdsc_do_request ( mdsc , dir , req ); <nl> if ( err ) { <nl> d_drop ( dentry );
int xhci_add_endpoint ( struct usb_hcd * hcd , struct usb_device * udev , <nl> * for usb_set_interface () and usb_set_configuration () claim ). <nl> */ <nl> if ( xhci_endpoint_init ( xhci , xhci -> devs [ udev -> slot_id ], <nl> - udev , ep , GFP_KERNEL ) < 0 ) { <nl> + udev , ep , GFP_NOIO ) < 0 ) { <nl> dev_dbg (& udev -> dev , "% s - could not initialize ep %# x \ n ", <nl> __func__ , ep -> desc . bEndpointAddress ); <nl> return - ENOMEM ;
int ipath_create_rcvhdrq ( struct ipath_devdata * dd , <nl> " for port % u rcvhdrqtailaddr failed \ n ", <nl> pd -> port_port ); <nl> ret = - ENOMEM ; <nl> + dma_free_coherent (& dd -> pcidev -> dev , amt , <nl> + pd -> port_rcvhdrq , pd -> port_rcvhdrq_phys ); <nl> + pd -> port_rcvhdrq = NULL ; <nl> goto bail ; <nl> } <nl> pd -> port_rcvhdrqtailaddr_phys = phys_hdrqtail ;
static int udf_load_partdesc ( struct super_block * sb , sector_t block ) <nl> ret = udf_fill_partdesc_info ( sb , p , i ); <nl> if ( ret ) <nl> goto out_bh ; <nl> + /* <nl> + * Mark filesystem read - only if we have a partition with virtual map <nl> + * since we don ' t handle writing to it ( we overwrite blocks instead of <nl> + * relocating them ). <nl> + */ <nl> + sb -> s_flags |= MS_RDONLY ; <nl> + printk ( KERN_NOTICE " UDF - fs : Filesystem marked read - only because " <nl> + " writing to pseudooverwrite partition is not implemented .\ n "); <nl>  <nl> ret = udf_load_vat ( sb , i , type1_idx ); <nl> out_bh :
static irqreturn_t dwc3_process_event_buf ( struct dwc3 * dwc , u32 buf ) <nl> static irqreturn_t dwc3_thread_interrupt ( int irq , void * _dwc ) <nl> { <nl> struct dwc3 * dwc = _dwc ; <nl> + unsigned long flags ; <nl> irqreturn_t ret = IRQ_NONE ; <nl> int i ; <nl>  <nl> - spin_lock (& dwc -> lock ); <nl> + spin_lock_irqsave (& dwc -> lock , flags ); <nl>  <nl> for ( i = 0 ; i < dwc -> num_event_buffers ; i ++) <nl> ret |= dwc3_process_event_buf ( dwc , i ); <nl>  <nl> - spin_unlock (& dwc -> lock ); <nl> + spin_unlock_irqrestore (& dwc -> lock , flags ); <nl>  <nl> return ret ; <nl> }
 <nl> # define FIRST_VM86_IRQ 3 <nl> # define LAST_VM86_IRQ 15 <nl> -# define invalid_vm86_irq ( irq ) (( irq ) < 3 || ( irq ) > 15 ) <nl> + <nl> +# ifndef __ASSEMBLY__ <nl> + static inline int invalid_vm86_irq ( int irq ) <nl> +{ <nl> + return irq < 3 || irq > 15 ; <nl> +} <nl> +# endif <nl>  <nl> /* <nl> * Size the maximum number of interrupts .
static void line6_data_received ( struct urb * urb ) <nl> line6 -> message_length = done ; <nl> line6_midi_receive ( line6 , line6 -> buffer_message , done ); <nl>  <nl> - switch ( line6 -> usbdev -> descriptor . idProduct ) { <nl> + switch ( le16_to_cpu ( line6 -> usbdev -> descriptor . idProduct )) { <nl> case LINE6_DEVID_BASSPODXT : <nl> case LINE6_DEVID_BASSPODXTLIVE : <nl> case LINE6_DEVID_BASSPODXTPRO : <nl> static void line6_disconnect ( struct usb_interface * interface ) <nl> dev_err ( line6 -> ifcdev , <nl> " driver bug : inconsistent usb device \ n "); <nl>  <nl> - switch ( line6 -> usbdev -> descriptor . idProduct ) { <nl> + switch ( le16_to_cpu ( line6 -> usbdev -> descriptor . idProduct )) { <nl> case LINE6_DEVID_BASSPODXT : <nl> case LINE6_DEVID_BASSPODXTLIVE : <nl> case LINE6_DEVID_BASSPODXTPRO : <nl> static int line6_reset_resume ( struct usb_interface * interface ) <nl> { <nl> struct usb_line6 * line6 = usb_get_intfdata ( interface ); <nl>  <nl> - switch ( line6 -> usbdev -> descriptor . idProduct ) { <nl> + switch ( le16_to_cpu ( line6 -> usbdev -> descriptor . idProduct )) { <nl> case LINE6_DEVID_PODSTUDIO_GX : <nl> case LINE6_DEVID_PODSTUDIO_UX1 : <nl> case LINE6_DEVID_PODSTUDIO_UX2 :
void afu_release_irqs ( struct cxl_context * ctx , void * cookie ) <nl>  <nl> afu_irq_name_free ( ctx ); <nl> cxl_release_irq_ranges (& ctx -> irqs , ctx -> afu -> adapter ); <nl> + <nl> + kfree ( ctx -> irq_bitmap ); <nl> + ctx -> irq_bitmap = NULL ; <nl> + ctx -> irq_count = 0 ; <nl> }
omap2_mcspi_txrx_pio ( struct spi_device * spi , struct spi_transfer * xfer ) <nl> rx_reg = base + OMAP2_MCSPI_RX0 ; <nl> chstat_reg = base + OMAP2_MCSPI_CHSTAT0 ; <nl>  <nl> + if ( c < ( word_len >> 3 )) <nl> + return 0 ; <nl> + <nl> if ( word_len <= 8 ) { <nl> u8 * rx ; <nl> const u8 * tx ; <nl> omap2_mcspi_txrx_pio ( struct spi_device * spi , struct spi_transfer * xfer ) <nl> dev_vdbg (& spi -> dev , " read -% d % 02x \ n ", <nl> word_len , *( rx - 1 )); <nl> } <nl> - } while ( c ); <nl> + } while ( c > ( word_len >> 3 )); <nl> } else if ( word_len <= 16 ) { <nl> u16 * rx ; <nl> const u16 * tx ; <nl> omap2_mcspi_txrx_pio ( struct spi_device * spi , struct spi_transfer * xfer ) <nl> dev_vdbg (& spi -> dev , " read -% d % 04x \ n ", <nl> word_len , *( rx - 1 )); <nl> } <nl> - } while ( c ); <nl> + } while ( c > ( word_len >> 3 )); <nl> } else if ( word_len <= 32 ) { <nl> u32 * rx ; <nl> const u32 * tx ; <nl> omap2_mcspi_txrx_pio ( struct spi_device * spi , struct spi_transfer * xfer ) <nl> dev_vdbg (& spi -> dev , " read -% d % 08x \ n ", <nl> word_len , *( rx - 1 )); <nl> } <nl> - } while ( c ); <nl> + } while ( c > ( word_len >> 3 )); <nl> } <nl>  <nl> /* for TX_ONLY mode , be sure all words have shifted out */
static void setexposure ( struct gspca_dev * gspca_dev ) <nl> buf [ i ++] = 0x35 ; /* reg = global gain */ <nl> buf [ i ++] = 0x00 ; /* val H */ <nl> buf [ i ++] = sensor -> i2c_dum ; <nl> - buf [ i ++] = sd -> gain ; /* val L */ <nl> + buf [ i ++] = 0x80 + sd -> gain / 2 ; /* val L */ <nl> buf [ i ++] = 0x00 ; <nl> buf [ i ++] = 0x00 ; <nl> buf [ i ++] = 0x00 ;
static int brcms_b_attach ( struct brcms_c_info * wlc , struct bcma_device * core , <nl>  <nl> /* check device id ( srom , nvram etc .) to set bands */ <nl> if ( wlc_hw -> deviceid == BCM43224_D11N_ID || <nl> - wlc_hw -> deviceid == BCM43224_D11N_ID_VEN1 ) <nl> + wlc_hw -> deviceid == BCM43224_D11N_ID_VEN1 || <nl> + wlc_hw -> deviceid == BCM43224_CHIP_ID ) <nl> /* Dualband boards */ <nl> wlc_hw -> _nbands = 2 ; <nl> else <nl> static bool brcms_c_chipmatch_pci ( struct bcma_device * core ) <nl> return false ; <nl> } <nl>  <nl> - if ( device == BCM43224_D11N_ID_VEN1 ) <nl> + if ( device == BCM43224_D11N_ID_VEN1 || device == BCM43224_CHIP_ID ) <nl> return true ; <nl> if (( device == BCM43224_D11N_ID ) || ( device == BCM43225_D11N2G_ID )) <nl> return true ;
static int md_open ( struct block_device * bdev , fmode_t mode ) <nl> struct mddev * mddev = mddev_find ( bdev -> bd_dev ); <nl> int err ; <nl>  <nl> + if (! mddev ) <nl> + return - ENODEV ; <nl> + <nl> if ( mddev -> gendisk != bdev -> bd_disk ) { <nl> /* we are racing with mddev_put which is discarding this <nl> * bd_disk .
static struct ath_buf * ath_clone_txbuf ( struct ath_softc * sc , struct ath_buf * bf ) <nl> tbf -> aphy = bf -> aphy ; <nl> tbf -> bf_mpdu = bf -> bf_mpdu ; <nl> tbf -> bf_buf_addr = bf -> bf_buf_addr ; <nl> - *( tbf -> bf_desc ) = *( bf -> bf_desc ); <nl> + memcpy ( tbf -> bf_desc , bf -> bf_desc , sc -> sc_ah -> caps . tx_desc_len ); <nl> tbf -> bf_state = bf -> bf_state ; <nl> tbf -> bf_dmacontext = bf -> bf_dmacontext ; <nl> 
bad_area : <nl> bad_area_nosemaphore : <nl> /* User mode accesses just cause a SIGSEGV */ <nl> if ( user_mode ( regs )) { <nl> - pr_alert ("% s : unhandled page fault (% d ) at 0x % 08lx , " <nl> - " cause % ld \ n ", current -> comm , SIGSEGV , address , cause ); <nl> - show_regs ( regs ); <nl> + if ( unhandled_signal ( current , SIGSEGV ) && printk_ratelimit ()) { <nl> + pr_info ("% s : unhandled page fault (% d ) at 0x % 08lx , " <nl> + " cause % ld \ n ", current -> comm , SIGSEGV , address , cause ); <nl> + show_regs ( regs ); <nl> + } <nl> _exception ( SIGSEGV , regs , code , address ); <nl> return ; <nl> }
static int __init encx24j600_init ( void ) <nl> } <nl> module_init ( encx24j600_init ); <nl>  <nl> - void encx24j600_exit ( void ) <nl> + static void encx24j600_exit ( void ) <nl> { <nl> spi_unregister_driver (& encx24j600_spi_net_driver ); <nl> }
static void bfs_delete_inode ( struct inode * inode ) <nl> brelse ( bh ); <nl>  <nl> if ( bi -> i_dsk_ino ) { <nl> - info -> si_freeb += BFS_FILEBLOCKS ( bi ); <nl> + if ( bi -> i_sblock ) <nl> + info -> si_freeb += bi -> i_eblock + 1 - bi -> i_sblock ; <nl> info -> si_freei ++; <nl> clear_bit ( ino , info -> si_imap ); <nl> dump_imap (" delete_inode ", s );
static int rd_build_device_space ( struct rd_dev * rd_dev ) <nl> rd_dev -> rd_page_count ); <nl> return - EINVAL ; <nl> } <nl> + <nl> + /* Don ' t need backing pages for NULLIO */ <nl> + if ( rd_dev -> rd_flags & RDF_NULLIO ) <nl> + return 0 ; <nl> + <nl> total_sg_needed = rd_dev -> rd_page_count ; <nl>  <nl> sg_tables = ( total_sg_needed / max_sg_per_table ) + 1 ;
static int __dma_supported ( struct device * dev , u64 mask , bool warn ) <nl> */ <nl> if ( sizeof ( mask ) != sizeof ( dma_addr_t ) && <nl> mask > ( dma_addr_t )~ 0 && <nl> - dma_to_pfn ( dev , ~ 0 ) < max_pfn ) { <nl> + dma_to_pfn ( dev , ~ 0 ) < max_pfn - 1 ) { <nl> if ( warn ) { <nl> dev_warn ( dev , " Coherent DMA mask %# llx is larger than dma_addr_t allows \ n ", <nl> mask );
# define gadget_is_musbhsfc ( g ) 0 <nl> # endif <nl>  <nl> -/* Mentor high speed " dual role " controller , peripheral mode */ <nl> -# ifdef CONFIG_USB_GADGET_MUSBHDRC <nl> -# define gadget_is_musbhdrc ( g ) ! strcmp (" musbhdrc_udc ", ( g )-> name ) <nl> +/* Mentor high speed " dual role " controller , in peripheral role */ <nl> +# ifdef CONFIG_USB_GADGET_MUSB_HDRC <nl> +# define gadget_is_musbhdrc ( g ) ! strcmp (" musb_hdrc ", ( g )-> name ) <nl> # else <nl> # define gadget_is_musbhdrc ( g ) 0 <nl> # endif
int video_register_device ( struct video_device * vfd , int type , int nr ) <nl> vfd -> class_dev . devt = MKDEV ( VIDEO_MAJOR , vfd -> minor ); <nl> sprintf ( vfd -> class_dev . class_id , "% s % d ", name_base , i - base ); <nl> ret = class_device_register (& vfd -> class_dev ); <nl> - if ( ret ) { <nl> + if ( ret < 0 ) { <nl> printk ( KERN_ERR "% s : class_device_register failed \ n ", <nl> __FUNCTION__ ); <nl> goto fail_minor ;
ret_orig : <nl> kfree_skb ( clone ); <nl> return skb ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( nf_ct_frag6_gather ); <nl>  <nl> void nf_ct_frag6_consume_orig ( struct sk_buff * skb ) <nl> {
 <nl> # include < linux / seq_file . h > <nl> # include < linux / list . h > <nl> +# include < linux / vmalloc . h > <nl> # include " debug . h " <nl> # include " ath5k . h " <nl> # include " reg . h "
ip_vs_new_dest ( struct ip_vs_service * svc , struct ip_vs_dest_user_kern * udest , <nl> return - EINVAL ; <nl> } <nl>  <nl> - dest = kzalloc ( sizeof ( struct ip_vs_dest ), GFP_ATOMIC ); <nl> + dest = kzalloc ( sizeof ( struct ip_vs_dest ), GFP_KERNEL ); <nl> if ( dest == NULL ) { <nl> pr_err ("% s (): no memory .\ n ", __func__ ); <nl> return - ENOMEM ; <nl> ip_vs_add_service ( struct ip_vs_service_user_kern * u , <nl> } <nl> # endif <nl>  <nl> - svc = kzalloc ( sizeof ( struct ip_vs_service ), GFP_ATOMIC ); <nl> + svc = kzalloc ( sizeof ( struct ip_vs_service ), GFP_KERNEL ); <nl> if ( svc == NULL ) { <nl> IP_VS_DBG ( 1 , "% s (): no memory \ n ", __func__ ); <nl> ret = - ENOMEM ;
static int option_probe ( struct usb_serial * serial , <nl> ( serial -> dev -> descriptor . idProduct == HUAWEI_PRODUCT_K3765 || <nl> serial -> dev -> descriptor . idProduct == HUAWEI_PRODUCT_K4505 || <nl> serial -> dev -> descriptor . idProduct == HUAWEI_PRODUCT_K4605 ) && <nl> - serial -> interface -> cur_altsetting -> desc . bInterfaceNumber == 1 ) <nl> + ( serial -> interface -> cur_altsetting -> desc . bInterfaceNumber == 1 || <nl> + serial -> interface -> cur_altsetting -> desc . bInterfaceNumber == 2 )) <nl> return - ENODEV ; <nl>  <nl> /* Don ' t bind network interface on Samsung GT - B3730 , it is handled by a separate module */
int nfs4_setup_sequence ( struct nfs_client * clp , <nl> goto out ; <nl> ret = nfs41_setup_sequence ( clp -> cl_session , args , res , cache_reply , <nl> task ); <nl> - if ( ret != - EAGAIN ) { <nl> + if ( ret && ret != - EAGAIN ) { <nl> /* terminate rpc task */ <nl> task -> tk_status = ret ; <nl> task -> tk_action = NULL ;
static void scsi_sysfs_add_devices ( struct Scsi_Host * shost ) <nl> /* target removed before the device could be added */ <nl> if ( sdev -> sdev_state == SDEV_DEL ) <nl> continue ; <nl> + /* If device is already visible , skip adding it to sysfs */ <nl> + if ( sdev -> is_visible ) <nl> + continue ; <nl> if (! scsi_host_scan_allowed ( shost ) || <nl> scsi_sysfs_add_sdev ( sdev ) != 0 ) <nl> __scsi_remove_device ( sdev );
__rb_reserve_next ( struct ring_buffer_per_cpu * cpu_buffer , <nl> write &= RB_WRITE_MASK ; <nl> tail = write - length ; <nl>  <nl> + /* <nl> + * If this is the first commit on the page , then it has the same <nl> + * timestamp as the page itself . <nl> + */ <nl> + if (! tail ) <nl> + delta = 0 ; <nl> + <nl> /* See if we shot pass the end of this buffer page */ <nl> if ( unlikely ( write > BUF_PAGE_SIZE )) <nl> return rb_move_tail ( cpu_buffer , length , tail ,
static void __exit_signal ( struct task_struct * tsk ) <nl> tty = sig -> tty ; <nl> sig -> tty = NULL ; <nl> } else { <nl> + /* <nl> + * This can only happen if the caller is de_thread (). <nl> + * FIXME : this is the temporary hack , we should teach <nl> + * posix - cpu - timers to handle this case correctly . <nl> + */ <nl> + if ( unlikely ( has_group_leader_pid ( tsk ))) <nl> + posix_cpu_timers_exit_group ( tsk ); <nl> + <nl> /* <nl> * If there is any task waiting for the group exit <nl> * then notify it :
static int __init d40_of_probe ( struct platform_device * pdev , <nl> struct device_node * np ) <nl> { <nl> struct stedma40_platform_data * pdata ; <nl> - int num_memcpy = 0 ; <nl> + int num_phy = 0 , num_memcpy = 0 ; <nl> const const __be32 * list ; <nl>  <nl> pdata = devm_kzalloc (& pdev -> dev , <nl> static int __init d40_of_probe ( struct platform_device * pdev , <nl> if (! pdata ) <nl> return - ENOMEM ; <nl>  <nl> + /* If absent this value will be obtained from h / w . */ <nl> + of_property_read_u32 ( np , " dma - channels ", & num_phy ); <nl> + if ( num_phy > 0 ) <nl> + pdata -> num_of_phy_chans = num_phy ; <nl> + <nl> list = of_get_property ( np , " memcpy - channels ", & num_memcpy ); <nl> num_memcpy /= sizeof (* list ); <nl> 
static ide_startstop_t cdrom_read_intr ( ide_drive_t * drive ) <nl> */ <nl> if ( dma ) { <nl> info -> dma = 0 ; <nl> - if (( dma_error = HWIF ( drive )-> ide_dma_end ( drive ))) <nl> + dma_error = HWIF ( drive )-> ide_dma_end ( drive ); <nl> + if ( dma_error ) { <nl> + printk ( KERN_ERR "% s : DMA read error \ n ", drive -> name ); <nl> ide_dma_off ( drive ); <nl> + } <nl> } <nl>  <nl> if ( cdrom_decode_status ( drive , 0 , & stat ))
static int abituguru3_read_increment_offset ( struct abituguru3_data * data , <nl>  <nl> for ( i = 0 ; i < offset_count ; i ++) <nl> if (( x = abituguru3_read ( data , bank , offset + i , count , <nl> - buf + i * count )) != count ) <nl> - return i * count + ( i && ( x < 0 )) ? 0 : x ; <nl> + buf + i * count )) != count ) { <nl> + if ( x < 0 ) <nl> + return x ; <nl> + return i * count + x ; <nl> + } <nl>  <nl> return i * count ; <nl> }
static void mesh_path_move_to_queue ( struct mesh_path * gate_mpath , <nl> } <nl>  <nl>  <nl> - static struct mesh_path * path_lookup ( struct mesh_table * tbl , u8 * dst , <nl> + static struct mesh_path * mpath_lookup ( struct mesh_table * tbl , u8 * dst , <nl> struct ieee80211_sub_if_data * sdata ) <nl> { <nl> struct mesh_path * mpath ; <nl> static struct mesh_path * path_lookup ( struct mesh_table * tbl , u8 * dst , <nl> */ <nl> struct mesh_path * mesh_path_lookup ( u8 * dst , struct ieee80211_sub_if_data * sdata ) <nl> { <nl> - return path_lookup ( rcu_dereference ( mesh_paths ), dst , sdata ); <nl> + return mpath_lookup ( rcu_dereference ( mesh_paths ), dst , sdata ); <nl> } <nl>  <nl> struct mesh_path * mpp_path_lookup ( u8 * dst , struct ieee80211_sub_if_data * sdata ) <nl> { <nl> - return path_lookup ( rcu_dereference ( mpp_paths ), dst , sdata ); <nl> + return mpath_lookup ( rcu_dereference ( mpp_paths ), dst , sdata ); <nl> } <nl>  <nl> 
static void stmmac_clk_csr_set ( struct stmmac_priv * priv ) <nl> priv -> clk_csr = STMMAC_CSR_100_150M ; <nl> else if (( clk_rate >= CSR_F_150M ) && ( clk_rate < CSR_F_250M )) <nl> priv -> clk_csr = STMMAC_CSR_150_250M ; <nl> - else if (( clk_rate >= CSR_F_250M ) && ( clk_rate < CSR_F_300M )) <nl> + else if (( clk_rate >= CSR_F_250M ) && ( clk_rate <= CSR_F_300M )) <nl> priv -> clk_csr = STMMAC_CSR_250_300M ; <nl> } <nl> }
static inline void kvm_s390_get_base_disp_sse ( struct kvm_vcpu * vcpu , <nl>  <nl> static inline void kvm_s390_get_regs_rre ( struct kvm_vcpu * vcpu , int * r1 , int * r2 ) <nl> { <nl> - * r1 = ( vcpu -> arch . sie_block -> ipb & 0x00f00000 ) >> 20 ; <nl> - * r2 = ( vcpu -> arch . sie_block -> ipb & 0x000f0000 ) >> 16 ; <nl> + if ( r1 ) <nl> + * r1 = ( vcpu -> arch . sie_block -> ipb & 0x00f00000 ) >> 20 ; <nl> + if ( r2 ) <nl> + * r2 = ( vcpu -> arch . sie_block -> ipb & 0x000f0000 ) >> 16 ; <nl> } <nl>  <nl> static inline u64 kvm_s390_get_base_disp_rsy ( struct kvm_vcpu * vcpu )
static hva_t kvmppc_mmu_book3s_64_get_pteg ( <nl> dprintk (" MMU : page = 0x % x sdr1 = 0x % llx pteg = 0x % llx vsid = 0x % llx \ n ", <nl> page , vcpu_book3s -> sdr1 , pteg , slbe -> vsid ); <nl>  <nl> - r = gfn_to_hva ( vcpu_book3s -> vcpu . kvm , pteg >> PAGE_SHIFT ); <nl> + /* When running a PAPR guest , SDR1 contains a HVA address instead <nl> + of a GPA */ <nl> + if ( vcpu_book3s -> vcpu . arch . papr_enabled ) <nl> + r = pteg ; <nl> + else <nl> + r = gfn_to_hva ( vcpu_book3s -> vcpu . kvm , pteg >> PAGE_SHIFT ); <nl> + <nl> if ( kvm_is_error_hva ( r )) <nl> return r ; <nl> return r | ( pteg & ~ PAGE_MASK );
static inline int ccid_hc_tx_init ( struct ccid * ccid , struct sock * sk ) <nl>  <nl> static inline void ccid_hc_rx_exit ( struct ccid * ccid , struct sock * sk ) <nl> { <nl> - if ( ccid -> ccid_hc_rx_exit != NULL && <nl> + if ( ccid != NULL && ccid -> ccid_hc_rx_exit != NULL && <nl> dccp_sk ( sk )-> dccps_hc_rx_ccid_private != NULL ) <nl> ccid -> ccid_hc_rx_exit ( sk ); <nl> } <nl>  <nl> static inline void ccid_hc_tx_exit ( struct ccid * ccid , struct sock * sk ) <nl> { <nl> - if ( ccid -> ccid_hc_tx_exit != NULL && <nl> + if ( ccid != NULL && ccid -> ccid_hc_tx_exit != NULL && <nl> dccp_sk ( sk )-> dccps_hc_tx_ccid_private != NULL ) <nl> ccid -> ccid_hc_tx_exit ( sk ); <nl> }
static struct ib_cq * iwch_create_cq ( struct ib_device * ibdev , int entries , int ve <nl> mm -> len = PAGE_ALIGN ((( 1UL << uresp . size_log2 ) + 1 ) * <nl> sizeof ( struct t3_cqe )); <nl> uresp . memsize = mm -> len ; <nl> + uresp . reserved = 0 ; <nl> resplen = sizeof uresp ; <nl> } <nl> if ( ib_copy_to_udata ( udata , & uresp , resplen )) {
static ssize_t dgrp_class_pollrate_store ( struct device * c , <nl> struct device_attribute * attr , <nl> const char * buf , size_t count ) <nl> { <nl> - sscanf ( buf , " 0x % x \ n ", & dgrp_poll_tick ); <nl> + if ( sscanf ( buf , " 0x % x \ n ", & dgrp_poll_tick ) != 1 ) <nl> + return - EINVAL ; <nl> + <nl> return count ; <nl> } <nl> static DEVICE_ATTR ( pollrate , 0600 , dgrp_class_pollrate_show ,
EXPORT_SYMBOL_GPL ( usbnet_resume ); <nl>  <nl> static int __init usbnet_init ( void ) <nl> { <nl> - /* compiler should optimize this out */ <nl> - BUILD_BUG_ON ( sizeof ((( struct sk_buff *) 0 )-> cb ) <nl> - < sizeof ( struct skb_data )); <nl> + /* Compiler should optimize this out . */ <nl> + BUILD_BUG_ON ( <nl> + FIELD_SIZEOF ( struct sk_buff , cb ) < sizeof ( struct skb_data )); <nl>  <nl> random_ether_addr ( node_id ); <nl> return 0 ;
regulator_register ( const struct regulator_desc * regulator_desc , <nl> if ( ret != 0 ) { <nl> rdev_err ( rdev , " Failed to request enable GPIO % d : % d \ n ", <nl> config -> ena_gpio , ret ); <nl> - goto clean ; <nl> + goto wash ; <nl> } <nl>  <nl> rdev -> ena_gpio = config -> ena_gpio ; <nl> scrub : <nl> if ( rdev -> ena_gpio ) <nl> gpio_free ( rdev -> ena_gpio ); <nl> kfree ( rdev -> constraints ); <nl> + wash : <nl> device_unregister (& rdev -> dev ); <nl> /* device core frees rdev */ <nl> rdev = ERR_PTR ( ret );
static int afiucv_hs_rcv ( struct sk_buff * skb , struct net_device * dev , <nl> break ; <nl> case 0 : <nl> /* plain data frame */ <nl> + memcpy ( CB_TRGCLS ( skb ), & trans_hdr -> iucv_hdr . class , <nl> + CB_TRGCLS_LEN ); <nl> err = afiucv_hs_callback_rx ( sk , skb ); <nl> break ; <nl> default :
static bool batadv_purge_orig_node ( struct batadv_priv * bat_priv , <nl> { <nl> struct batadv_neigh_node * best_neigh_node ; <nl> struct batadv_hard_iface * hard_iface ; <nl> - bool changed ; <nl> + bool changed_ifinfo , changed_neigh ; <nl>  <nl> if ( batadv_has_timed_out ( orig_node -> last_seen , <nl> 2 * BATADV_PURGE_TIMEOUT )) { <nl> static bool batadv_purge_orig_node ( struct batadv_priv * bat_priv , <nl> jiffies_to_msecs ( orig_node -> last_seen )); <nl> return true ; <nl> } <nl> - changed = batadv_purge_orig_ifinfo ( bat_priv , orig_node ); <nl> - changed = changed || batadv_purge_orig_neighbors ( bat_priv , orig_node ); <nl> + changed_ifinfo = batadv_purge_orig_ifinfo ( bat_priv , orig_node ); <nl> + changed_neigh = batadv_purge_orig_neighbors ( bat_priv , orig_node ); <nl>  <nl> - if (! changed ) <nl> + if (! changed_ifinfo && ! changed_neigh ) <nl> return false ; <nl>  <nl> /* first for NULL ... */
static struct usb_device_id id_table [] = { <nl> { USB_DEVICE ( 0x1199 , 0x6851 ) }, /* Sierra Wireless AirCard 881 */ <nl> { USB_DEVICE ( 0x1199 , 0x6852 ) }, /* Sierra Wireless AirCard 880 E */ <nl> { USB_DEVICE ( 0x1199 , 0x6853 ) }, /* Sierra Wireless AirCard 881 E */ <nl> + { USB_DEVICE ( 0x1199 , 0x6856 ) }, /* Sierra Wireless AirCard 881 U */ <nl>  <nl> { USB_DEVICE ( 0x1199 , 0x0112 ), . driver_info = DEVICE_1_PORT }, /* Sierra Wireless AirCard 580 */ <nl> { USB_DEVICE ( 0x0F3D , 0x0112 ), . driver_info = DEVICE_1_PORT }, /* Airprime / Sierra PC 5220 */ <nl> static struct usb_device_id id_table_3port [] = { <nl> { USB_DEVICE ( 0x1199 , 0x6851 ) }, /* Sierra Wireless AirCard 881 */ <nl> { USB_DEVICE ( 0x1199 , 0x6852 ) }, /* Sierra Wireless AirCard 880E */ <nl> { USB_DEVICE ( 0x1199 , 0x6853 ) }, /* Sierra Wireless AirCard 881E */ <nl> + { USB_DEVICE ( 0x1199 , 0x6856 ) }, /* Sierra Wireless AirCard 881U */ <nl> { } <nl> }; <nl> 
mwifiex_config_scan ( struct mwifiex_private * priv , <nl> wildcard_ssid_tlv -> max_ssid_length = <nl> IEEE80211_MAX_SSID_LEN ; <nl>  <nl> + if (! memcmp ( user_scan_in -> ssid_list [ i ]. ssid , <nl> + " DIRECT -", 7 )) <nl> + wildcard_ssid_tlv -> max_ssid_length = 0xfe ; <nl> + <nl> memcpy ( wildcard_ssid_tlv -> ssid , <nl> user_scan_in -> ssid_list [ i ]. ssid , ssid_len ); <nl> 
static struct mtd_partition * newpart ( char * s , <nl> s ++; <nl> } else { <nl> size = memparse ( s , & s ); <nl> - if ( size < PAGE_SIZE ) { <nl> - printk ( KERN_ERR ERRP " partition size too small (% llx )\ n ", <nl> - size ); <nl> + if (! size ) { <nl> + printk ( KERN_ERR ERRP " partition has size 0 \ n "); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> }
int i915_gpu_idle ( struct drm_device * dev ) <nl> /* Is the device fubar ? */ <nl> if ( WARN_ON (! list_empty (& ring -> gpu_write_list ))) <nl> return - EBUSY ; <nl> + <nl> + ret = i915_switch_context ( ring , NULL , DEFAULT_CONTEXT_ID ); <nl> + if ( ret ) <nl> + return ret ; <nl> } <nl>  <nl> return 0 ;
enum thread_state { <nl> struct work_atom { <nl> struct list_head list ; <nl> enum thread_state state ; <nl> + u64 sched_out_time ; <nl> u64 wake_up_time ; <nl> u64 sched_in_time ; <nl> u64 runtime ; <nl> lat_sched_out ( struct task_atoms * atoms , <nl> if (! atom ) <nl> die (" Non memory "); <nl>  <nl> + atom -> sched_out_time = timestamp ; <nl> + <nl> if ( sched_out_state ( switch_event ) == ' R ') { <nl> atom -> state = THREAD_WAIT_CPU ; <nl> - atom -> wake_up_time = timestamp ; <nl> + atom -> wake_up_time = atom -> sched_out_time ; <nl> } <nl>  <nl> atom -> runtime = delta ; <nl> latency_wakeup_event ( struct trace_wakeup_event * wakeup_event , <nl> if ( atom -> state != THREAD_SLEEPING ) <nl> return ; <nl>  <nl> + if ( atom -> sched_out_time > timestamp ) <nl> + return ; <nl> + <nl> atom -> state = THREAD_WAIT_CPU ; <nl> atom -> wake_up_time = timestamp ; <nl> }
tracing_buffers_splice_read ( struct file * file , loff_t * ppos , <nl> }; <nl> struct buffer_ref * ref ; <nl> int entries , size , i ; <nl> - ssize_t ret ; <nl> + ssize_t ret = 0 ; <nl>  <nl> mutex_lock (& trace_types_lock ); <nl>  <nl> tracing_buffers_splice_read ( struct file * file , loff_t * ppos , <nl> int r ; <nl>  <nl> ref = kzalloc ( sizeof (* ref ), GFP_KERNEL ); <nl> - if (! ref ) <nl> + if (! ref ) { <nl> + ret = - ENOMEM ; <nl> break ; <nl> + } <nl>  <nl> ref -> ref = 1 ; <nl> ref -> buffer = iter -> trace_buffer -> buffer ; <nl> ref -> page = ring_buffer_alloc_read_page ( ref -> buffer , iter -> cpu_file ); <nl> if (! ref -> page ) { <nl> + ret = - ENOMEM ; <nl> kfree ( ref ); <nl> break ; <nl> } <nl> tracing_buffers_splice_read ( struct file * file , loff_t * ppos , <nl>  <nl> /* did we read anything ? */ <nl> if (! spd . nr_pages ) { <nl> + if ( ret ) <nl> + goto out ; <nl> + <nl> if (( file -> f_flags & O_NONBLOCK ) || ( flags & SPLICE_F_NONBLOCK )) { <nl> ret = - EAGAIN ; <nl> goto out ;
int pci_dev_present ( const struct pci_device_id * ids ) <nl> WARN_ON ( in_interrupt ()); <nl> while ( ids -> vendor || ids -> subvendor || ids -> class_mask ) { <nl> found = pci_get_dev_by_id ( ids , NULL ); <nl> - if ( found ) <nl> - goto exit ; <nl> + if ( found ) { <nl> + pci_dev_put ( found ); <nl> + return 1 ; <nl> + } <nl> ids ++; <nl> } <nl> - exit : <nl> - if ( found ) <nl> - return 1 ; <nl> + <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL ( pci_dev_present );
static int ep93xx_gpio_irq_type ( struct irq_data * d , unsigned int type ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - __irq_set_handler_locked ( d -> irq , handler ); <nl> + irq_set_handler_locked ( d , handler ); <nl>  <nl> gpio_int_enabled [ port ] |= port_mask ; <nl> 
nfs4_ff_layout_prepare_ds ( struct pnfs_layout_segment * lseg , u32 ds_idx , <nl> pnfs_error_mark_layout_for_return ( ino , lseg ); <nl> } else <nl> pnfs_error_mark_layout_for_return ( ino , lseg ); <nl> + ds = NULL ; <nl> + goto out ; <nl> } <nl> out_update_creds : <nl> if ( ff_layout_update_mirror_cred ( mirror , ds ))
static int gprs_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> dev -> name , err ); <nl> dev -> stats . tx_aborted_errors ++; <nl> dev -> stats . tx_errors ++; <nl> - dev_kfree_skb ( skb ); <nl> } else { <nl> dev -> stats . tx_packets ++; <nl> dev -> stats . tx_bytes += len ;
struct e820entry ; <nl> char * __init machine_specific_memory_setup ( void ); <nl> char * memory_setup ( void ); <nl>  <nl> - int __init copy_e820_map ( struct e820entry * biosmap , int nr_map ); <nl> - int __init sanitize_e820_map ( struct e820entry * biosmap , char * pnr_map ); <nl> + int __init copy_e820_map ( struct e820entry * biosmap , int nr_map ); <nl> + int __init sanitize_e820_map ( struct e820entry * biosmap , char * pnr_map ); <nl> void __init add_memory_region ( unsigned long long start , <nl> unsigned long long size , int type ); <nl> 
static int wm8731_set_bias_level ( struct snd_soc_codec * codec , <nl>  <nl> switch ( level ) { <nl> case SND_SOC_BIAS_ON : <nl> - if ( wm8731 -> mclk ) <nl> - clk_prepare_enable ( wm8731 -> mclk ); <nl> + if ( wm8731 -> mclk ) { <nl> + ret = clk_prepare_enable ( wm8731 -> mclk ); <nl> + if ( ret ) <nl> + return ret ; <nl> + } <nl> break ; <nl> case SND_SOC_BIAS_PREPARE : <nl> break ;
static int snd_pcm_drain ( struct snd_pcm_substream * substream ) <nl>  <nl> snd_pcm_stream_lock_irq ( substream ); <nl> /* resume pause */ <nl> - if ( runtime -> status -> state == SNDRV_PCM_STATE_PAUSED ) <nl> + if ( substream -> runtime -> status -> state == SNDRV_PCM_STATE_PAUSED ) <nl> snd_pcm_pause ( substream , 0 ); <nl>  <nl> /* pre - start / stop - all running streams are changed to DRAINING state */
# undef __get_str <nl>  <nl> # undef TP_printk <nl> -# define TP_printk ( fmt , args ...) "% s , % s \ n ", # fmt , __stringify ( args ) <nl> +# define TP_printk ( fmt , args ...) "\"% s \", % s \ n ", fmt , __stringify ( args ) <nl>  <nl> # undef TP_fast_assign <nl> # define TP_fast_assign ( args ...) args
static int __devinit cciss_init_one ( struct pci_dev * pdev , <nl> h -> scatter_list = kmalloc ( h -> max_commands * <nl> sizeof ( struct scatterlist *), <nl> GFP_KERNEL ); <nl> + if (! h -> scatter_list ) <nl> + goto clean4 ; <nl> + <nl> for ( k = 0 ; k < h -> nr_cmds ; k ++) { <nl> h -> scatter_list [ k ] = kmalloc ( sizeof ( struct scatterlist ) * <nl> h -> maxsgentries ,
static int machines__deliver_event ( struct machines * machines , <nl>  <nl> switch ( event -> header . type ) { <nl> case PERF_RECORD_SAMPLE : <nl> - dump_sample ( evsel , event , sample ); <nl> if ( evsel == NULL ) { <nl> ++ evlist -> stats . nr_unknown_id ; <nl> return 0 ; <nl> } <nl> + dump_sample ( evsel , event , sample ); <nl> if ( machine == NULL ) { <nl> ++ evlist -> stats . nr_unprocessable_samples ; <nl> return 0 ;
static int parse_raid_params ( struct raid_set * rs , char ** argv , <nl> rs -> ti -> error = " write_mostly option is only valid for RAID1 "; <nl> return - EINVAL ; <nl> } <nl> - if ( value > rs -> md . raid_disks ) { <nl> + if ( value >= rs -> md . raid_disks ) { <nl> rs -> ti -> error = " Invalid write_mostly drive index given "; <nl> return - EINVAL ; <nl> }
static netdev_tx_t r6040_start_xmit ( struct sk_buff * skb , <nl> /* Set TX descriptor & Transmit it */ <nl> lp -> tx_free_desc --; <nl> descptr = lp -> tx_insert_ptr ; <nl> - if ( skb -> len < MISR ) <nl> - descptr -> len = MISR ; <nl> + if ( skb -> len < ETH_ZLEN ) <nl> + descptr -> len = ETH_ZLEN ; <nl> else <nl> descptr -> len = skb -> len ; <nl> 
int rtl8188eu_init_recv_priv ( struct adapter * padapter ) <nl> _rtw_init_queue (& precvpriv -> free_recv_buf_queue ); <nl>  <nl> precvpriv -> pallocated_recv_buf = <nl> - kzalloc ( NR_RECVBUFF * sizeof ( struct recv_buf ) + 4 , GFP_KERNEL ); <nl> + kzalloc ( NR_RECVBUFF * sizeof ( struct recv_buf ), GFP_KERNEL ); <nl> if ( precvpriv -> pallocated_recv_buf == NULL ) { <nl> res = _FAIL ; <nl> RT_TRACE ( _module_rtl871x_recv_c_ , _drv_err_ , <nl> (" alloc recv_buf fail !\ n ")); <nl> goto exit ; <nl> } <nl> - memset ( precvpriv -> pallocated_recv_buf , 0 , <nl> - NR_RECVBUFF * sizeof ( struct recv_buf ) + 4 ); <nl>  <nl> - precvpriv -> precv_buf = ( u8 *) N_BYTE_ALIGMENT (( size_t ) <nl> - ( precvpriv -> pallocated_recv_buf ), 4 ); <nl> + precvpriv -> precv_buf = precvpriv -> pallocated_recv_buf ; <nl>  <nl>  <nl> precvbuf = ( struct recv_buf *) precvpriv -> precv_buf ;
static s32 igb_get_invariants_82575 ( struct e1000_hw * hw ) <nl> */ <nl> size += NVM_WORD_SIZE_BASE_SHIFT ; <nl>  <nl> + /* <nl> + * Check for invalid size <nl> + */ <nl> + if (( hw -> mac . type == e1000_82576 ) && ( size > 15 )) { <nl> + printk (" igb : The NVM size is not valid , " <nl> + " defaulting to 32K .\ n "); <nl> + size = 15 ; <nl> + } <nl> nvm -> word_size = 1 << size ; <nl> if ( nvm -> word_size == ( 1 << 15 )) <nl> nvm -> page_size = 128 ;
static int gpmc_probe_dt ( struct platform_device * pdev ) <nl> of_node_cmp ( child -> name , " nor ") == 0 ) <nl> ret = gpmc_probe_generic_child ( pdev , child ); <nl>  <nl> - if ( ret < 0 ) { <nl> + if ( WARN ( ret < 0 , "% s : probing gpmc child % s failed \ n ", <nl> + __func__ , child -> full_name )) <nl> of_node_put ( child ); <nl> - return ret ; <nl> - } <nl> } <nl>  <nl> return 0 ;
* ( C ) Copyright 2000 Alex deVries < alex @ onefishtwo . ca > <nl> * ( C ) Copyright 2001 John Marvin < jsm fc hp com > <nl> * ( C ) Copyright 2003 Grant Grundler < grundler parisc - linux org > <nl> + * ( C ) Copyright 2005 Kyle McMartin < kyle @ parisc - linux . org > <nl> * <nl> * This program is free software ; you can redistribute it and / or <nl> * modify it under the terms of the GNU General Public License as <nl> static void __devinit superio_serial_init ( void ) <nl>  <nl> serial [ 0 ]. iobase = sio_dev . sp1_base ; <nl> serial [ 0 ]. irq = SP1_IRQ ; <nl> + spin_lock_init (& serial [ 0 ]. lock ); <nl>  <nl> retval = early_serial_setup (& serial [ 0 ]); <nl> if ( retval < 0 ) { <nl> static void __devinit superio_serial_init ( void ) <nl>  <nl> serial [ 1 ]. iobase = sio_dev . sp2_base ; <nl> serial [ 1 ]. irq = SP2_IRQ ; <nl> + spin_lock_init (& serial [ 1 ]. lock ); <nl> retval = early_serial_setup (& serial [ 1 ]); <nl>  <nl> if ( retval < 0 )
static int super_90_load ( struct md_rdev * rdev , struct md_rdev * refdev , int minor <nl> ret = 0 ; <nl> } <nl> rdev -> sectors = rdev -> sb_start ; <nl> - /* Limit to 4TB as metadata cannot record more than that */ <nl> - if ( rdev -> sectors >= ( 2ULL << 32 )) <nl> + /* Limit to 4TB as metadata cannot record more than that . <nl> + * ( not needed for Linear and RAID0 as metadata doesn ' t <nl> + * record this size ) <nl> + */ <nl> + if ( rdev -> sectors >= ( 2ULL << 32 ) && sb -> level >= 1 ) <nl> rdev -> sectors = ( 2ULL << 32 ) - 2 ; <nl>  <nl> if ( rdev -> sectors < (( sector_t ) sb -> size ) * 2 && sb -> level >= 1 ) <nl> super_90_rdev_size_change ( struct md_rdev * rdev , sector_t num_sectors ) <nl> /* Limit to 4TB as metadata cannot record more than that . <nl> * 4TB == 2 ^ 32 KB , or 2 * 2 ^ 32 sectors . <nl> */ <nl> - if ( num_sectors >= ( 2ULL << 32 )) <nl> + if ( num_sectors >= ( 2ULL << 32 ) && rdev -> mddev -> level >= 1 ) <nl> num_sectors = ( 2ULL << 32 ) - 2 ; <nl> md_super_write ( rdev -> mddev , rdev , rdev -> sb_start , rdev -> sb_size , <nl> rdev -> sb_page );
static struct task_struct * copy_process ( unsigned long clone_flags , <nl> current -> signal -> flags & SIGNAL_UNKILLABLE ) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> + /* <nl> + * If the new process will be in a different pid namespace <nl> + * don ' t allow the creation of threads . <nl> + */ <nl> + if (( clone_flags & ( CLONE_VM | CLONE_NEWPID )) && <nl> + ( task_active_pid_ns ( current ) != current -> nsproxy -> pid_ns )) <nl> + return ERR_PTR (- EINVAL ); <nl> + <nl> retval = security_task_create ( clone_flags ); <nl> if ( retval ) <nl> goto fork_out ;
xfs_showargs ( <nl>  <nl> if ( mp -> m_logbufs > 0 ) <nl> seq_printf ( m , "," MNTOPT_LOGBUFS "=% d ", mp -> m_logbufs ); <nl> - <nl> if ( mp -> m_logbsize > 0 ) <nl> seq_printf ( m , "," MNTOPT_LOGBSIZE "=% dk ", mp -> m_logbsize >> 10 ); <nl>  <nl> if ( mp -> m_logname ) <nl> seq_printf ( m , "," MNTOPT_LOGDEV "=% s ", mp -> m_logname ); <nl> - <nl> if ( mp -> m_rtname ) <nl> seq_printf ( m , "," MNTOPT_RTDEV "=% s ", mp -> m_rtname ); <nl>  <nl> if ( mp -> m_dalign > 0 ) <nl> seq_printf ( m , "," MNTOPT_SUNIT "=% d ", <nl> ( int ) XFS_FSB_TO_BB ( mp , mp -> m_dalign )); <nl> - <nl> if ( mp -> m_swidth > 0 ) <nl> seq_printf ( m , "," MNTOPT_SWIDTH "=% d ", <nl> ( int ) XFS_FSB_TO_BB ( mp , mp -> m_swidth )); <nl>  <nl> if (!( mp -> m_flags & XFS_MOUNT_COMPAT_ATTR )) <nl> seq_printf ( m , "," MNTOPT_ATTR2 ); <nl> - <nl> if (!( mp -> m_flags & XFS_MOUNT_COMPAT_IOSIZE )) <nl> seq_printf ( m , "," MNTOPT_LARGEIO ); <nl> + if (!( mp -> m_flags & XFS_MOUNT_BARRIER )) <nl> + seq_printf ( m , "," MNTOPT_NOBARRIER ); <nl>  <nl> if (!( vfsp -> vfs_flag & VFS_32BITINODES )) <nl> seq_printf ( m , "," MNTOPT_64BITINODE ); <nl> - <nl> - if (!( vfsp -> vfs_flag & XFS_MOUNT_BARRIER )) <nl> - seq_printf ( m , "," MNTOPT_NOBARRIER ); <nl> - <nl> if ( vfsp -> vfs_flag & VFS_GRPID ) <nl> seq_printf ( m , "," MNTOPT_GRPID ); <nl> 
int drm_vblank_get ( struct drm_device * dev , int crtc ) <nl> unsigned long irqflags ; <nl> int ret = 0 ; <nl>  <nl> + if (! dev -> num_crtcs ) <nl> + return - EINVAL ; <nl> + <nl> if ( WARN_ON ( crtc >= dev -> num_crtcs )) <nl> return - EINVAL ; <nl> 
struct gen_74x164_chip { <nl> u8 port_config ; <nl> }; <nl>  <nl> - static struct gen_74x164_chip * gpio_to_chip ( struct gpio_chip * gc ) <nl> + static struct gen_74x164_chip * gpio_to_74x164_chip ( struct gpio_chip * gc ) <nl> { <nl> return container_of ( gc , struct gen_74x164_chip , gpio_chip ); <nl> } <nl> static int __gen_74x164_write_config ( struct gen_74x164_chip * chip ) <nl>  <nl> static int gen_74x164_get_value ( struct gpio_chip * gc , unsigned offset ) <nl> { <nl> - struct gen_74x164_chip * chip = gpio_to_chip ( gc ); <nl> + struct gen_74x164_chip * chip = gpio_to_74x164_chip ( gc ); <nl> int ret ; <nl>  <nl> mutex_lock (& chip -> lock ); <nl> static int gen_74x164_get_value ( struct gpio_chip * gc , unsigned offset ) <nl> static void gen_74x164_set_value ( struct gpio_chip * gc , <nl> unsigned offset , int val ) <nl> { <nl> - struct gen_74x164_chip * chip = gpio_to_chip ( gc ); <nl> + struct gen_74x164_chip * chip = gpio_to_74x164_chip ( gc ); <nl>  <nl> mutex_lock (& chip -> lock ); <nl> if ( val )
static int usblp_check_status ( struct usblp * usblp , int err ) <nl>  <nl> error = usblp_read_status ( usblp , usblp -> statusbuf ); <nl> if ( error < 0 ) { <nl> - err (" usblp % d : error % d reading printer status ", <nl> - usblp -> minor , error ); <nl> + if ( printk_ratelimit ()) <nl> + err (" usblp % d : error % d reading printer status ", <nl> + usblp -> minor , error ); <nl> return 0 ; <nl> } <nl>  <nl> static int usblp_ioctl ( struct inode * inode , struct file * file , unsigned int cmd , <nl>  <nl> case LPGETSTATUS : <nl> if ( usblp_read_status ( usblp , usblp -> statusbuf )) { <nl> - err (" usblp % d : failed reading printer status ", usblp -> minor ); <nl> + if ( printk_ratelimit ()) <nl> + err (" usblp % d : failed reading printer status ", <nl> + usblp -> minor ); <nl> retval = - EIO ; <nl> goto done ; <nl> }
static void xen_pcibk_control_isr ( struct pci_dev * dev , int reset ) <nl> enable ? " enable " : " disable "); <nl>  <nl> if ( enable ) { <nl> + /* <nl> + * The MSI or MSI - X should not have an IRQ handler . Otherwise <nl> + * if the guest terminates we BUG_ON in free_msi_irqs . <nl> + */ <nl> + if ( dev -> msi_enabled || dev -> msix_enabled ) <nl> + goto out ; <nl> + <nl> rc = request_irq ( dev_data -> irq , <nl> xen_pcibk_guest_interrupt , IRQF_SHARED , <nl> dev_data -> irq_name , dev );
enum ipi_message_type { <nl> }; <nl>  <nl> /* Set to a secondary ' s cpuid when it comes online . */ <nl> - static int smp_secondary_alive __initdata = 0 ; <nl> + static int smp_secondary_alive __devinitdata = 0 ; <nl>  <nl> /* Which cpus ids came online . */ <nl> cpumask_t cpu_online_map ; <nl> smp_callin ( void ) <nl> } <nl>  <nl> /* Wait until hwrpb -> txrdy is clear for cpu . Return - 1 on timeout . */ <nl> - static int __init <nl> + static int __devinit <nl> wait_for_txrdy ( unsigned long cpumask ) <nl> { <nl> unsigned long timeout ; <nl> secondary_cpu_start ( int cpuid , struct task_struct * idle ) <nl> /* <nl> * Bring one cpu online . <nl> */ <nl> - static int __init <nl> + static int __devinit <nl> smp_boot_one_cpu ( int cpuid ) <nl> { <nl> struct task_struct * idle ;
static int pvscsi_queue_ring ( struct pvscsi_adapter * adapter , <nl> memcpy ( e -> cdb , cmd -> cmnd , e -> cdbLen ); <nl>  <nl> e -> tag = SIMPLE_QUEUE_TAG ; <nl> - if ( sdev -> tagged_supported && <nl> - ( cmd -> tag == HEAD_OF_QUEUE_TAG || <nl> - cmd -> tag == ORDERED_QUEUE_TAG )) <nl> - e -> tag = cmd -> tag ; <nl>  <nl> if ( cmd -> sc_data_direction == DMA_FROM_DEVICE ) <nl> e -> flags = PVSCSI_FLAG_CMD_DIR_TOHOST ;
static void ext4_mb_group_or_file ( struct ext4_allocation_context * ac ) <nl> return ; <nl> } <nl>  <nl> + if ( sbi -> s_mb_group_prealloc <= 0 ) { <nl> + ac -> ac_flags |= EXT4_MB_STREAM_ALLOC ; <nl> + return ; <nl> + } <nl> + <nl> /* don ' t use group allocation for large files */ <nl> size = max ( size , isize ); <nl> if ( size > sbi -> s_mb_stream_request ) {
static int setup_netdev ( struct l2cap_chan * chan , struct lowpan_dev ** dev ) <nl> set_dev_addr ( netdev , & chan -> src , chan -> src_type ); <nl>  <nl> netdev -> netdev_ops = & netdev_ops ; <nl> - SET_NETDEV_DEV ( netdev , & chan -> conn -> hcon -> dev ); <nl> + SET_NETDEV_DEV ( netdev , & chan -> conn -> hcon -> hdev -> dev ); <nl> SET_NETDEV_DEVTYPE ( netdev , & bt_type ); <nl>  <nl> err = register_netdev ( netdev );
static ssize_t dlpar_cpu_probe ( const char * buf , size_t count ) <nl> if ( rc ) <nl> return - EINVAL ; <nl>  <nl> + rc = dlpar_acquire_drc ( drc_index ); <nl> + if ( rc ) <nl> + return - EINVAL ; <nl> + <nl> parent = of_find_node_by_path ("/ cpus "); <nl> if (! parent ) <nl> return - ENODEV ; <nl> static ssize_t dlpar_cpu_probe ( const char * buf , size_t count ) <nl>  <nl> of_node_put ( parent ); <nl>  <nl> - rc = dlpar_acquire_drc ( drc_index ); <nl> - if ( rc ) { <nl> - dlpar_free_cc_nodes ( dn ); <nl> - return - EINVAL ; <nl> - } <nl> - <nl> rc = dlpar_attach_node ( dn ); <nl> if ( rc ) { <nl> dlpar_release_drc ( drc_index );
static const char * i2400ms_bus_fw_names [] = { <nl> }; <nl>  <nl>  <nl> + static const struct i2400m_poke_table i2400ms_pokes [] = { <nl> + I2400M_FW_POKE ( 0x6BE260 , 0x00000088 ), <nl> + I2400M_FW_POKE ( 0x080550 , 0x00000005 ), <nl> + I2400M_FW_POKE ( 0xAE0000 , 0x00000000 ), <nl> + I2400M_FW_POKE ( 0x000000 , 0x00000000 ), /* MUST be 0 terminated or bad <nl> + * things will happen */ <nl> +}; <nl> + <nl> /* <nl> * Enable the SDIO function <nl> * <nl> int i2400ms_probe ( struct sdio_func * func , <nl> i2400m -> bus_bm_wait_for_ack = i2400ms_bus_bm_wait_for_ack ; <nl> i2400m -> bus_fw_names = i2400ms_bus_fw_names ; <nl> i2400m -> bus_bm_mac_addr_impaired = 1 ; <nl> + i2400m -> bus_bm_pokes_table = & i2400ms_pokes [ 0 ]; <nl>  <nl> sdio_claim_host ( func ); <nl> result = sdio_set_block_size ( func , I2400MS_BLK_SIZE );
static void request_firmware_work_func ( struct work_struct * work ) <nl> put_device ( fw_work -> device ); /* taken in request_firmware_nowait () */ <nl>  <nl> module_put ( fw_work -> module ); <nl> + kfree_const ( fw_work -> name ); <nl> kfree ( fw_work ); <nl> } <nl>  <nl> request_firmware_nowait ( <nl> return - ENOMEM ; <nl>  <nl> fw_work -> module = module ; <nl> - fw_work -> name = name ; <nl> + fw_work -> name = kstrdup_const ( name , gfp ); <nl> + if (! fw_work -> name ) <nl> + return - ENOMEM ; <nl> fw_work -> device = device ; <nl> fw_work -> context = context ; <nl> fw_work -> cont = cont ; <nl> request_firmware_nowait ( <nl> ( uevent ? FW_OPT_UEVENT : FW_OPT_USERHELPER ); <nl>  <nl> if (! try_module_get ( module )) { <nl> + kfree_const ( fw_work -> name ); <nl> kfree ( fw_work ); <nl> return - EFAULT ; <nl> }
static inline void * __alloc_percpu ( size_t size , size_t align ) <nl> * percpu sections on SMP for which this path isn ' t used . <nl> */ <nl> WARN_ON_ONCE ( align > __alignof__ ( unsigned long long )); <nl> - return kzalloc ( size , gfp ); <nl> + return kzalloc ( size , GFP_KERNEL ); <nl> } <nl>  <nl> static inline void free_percpu ( void * p )
static int k3_dma_probe ( struct platform_device * op ) <nl> d -> slave . device_issue_pending = k3_dma_issue_pending ; <nl> d -> slave . device_control = k3_dma_control ; <nl> d -> slave . copy_align = DMA_ALIGN ; <nl> - d -> slave . chancnt = d -> dma_requests ; <nl>  <nl> /* init virtual channel */ <nl> d -> chans = devm_kzalloc (& op -> dev ,
static void __devinit pci_fixed_bar_fixup ( struct pci_dev * dev ) <nl> u32 size ; <nl> int i ; <nl>  <nl> + /* Must have extended configuration space */ <nl> + if ( dev -> cfg_size < PCIE_CAP_OFFSET + 4 ) <nl> + return ; <nl> + <nl> /* Fixup the BAR sizes for fixed BAR devices and make them unmoveable */ <nl> offset = fixed_bar_cap ( dev -> bus , dev -> devfn ); <nl> if (! offset || PCI_DEVFN ( 2 , 0 ) == dev -> devfn ||
static int mem_cgroup_move_parent ( struct page * page , <nl> unsigned long uninitialized_var ( flags ); <nl> int ret ; <nl>  <nl> - /* Is ROOT ? */ <nl> - if ( mem_cgroup_is_root ( child )) <nl> - return - EINVAL ; <nl> + VM_BUG_ON ( mem_cgroup_is_root ( child )); <nl>  <nl> ret = - EBUSY ; <nl> if (! get_page_unless_zero ( page )) <nl> static int mem_cgroup_force_empty_write ( struct cgroup * cont , unsigned int event ) <nl> struct mem_cgroup * memcg = mem_cgroup_from_cont ( cont ); <nl> int ret ; <nl>  <nl> + if ( mem_cgroup_is_root ( memcg )) <nl> + return - EINVAL ; <nl> css_get (& memcg -> css ); <nl> ret = mem_cgroup_force_empty ( memcg ); <nl> css_put (& memcg -> css );
static int changed_cb ( struct btrfs_root * left_root , <nl> if ( ret < 0 ) <nl> goto out ; <nl>  <nl> + /* Ignore non - FS objects */ <nl> + if ( key -> objectid == BTRFS_FREE_INO_OBJECTID || <nl> + key -> objectid == BTRFS_FREE_SPACE_OBJECTID ) <nl> + goto out ; <nl> + <nl> if ( key -> type == BTRFS_INODE_ITEM_KEY ) <nl> ret = changed_inode ( sctx , result ); <nl> else if ( key -> type == BTRFS_INODE_REF_KEY )
static struct nft_stats __percpu * nft_stats_alloc ( const struct nlattr * attr ) <nl> static void nft_chain_stats_replace ( struct nft_base_chain * chain , <nl> struct nft_stats __percpu * newstats ) <nl> { <nl> + if ( newstats == NULL ) <nl> + return ; <nl> + <nl> if ( chain -> stats ) { <nl> struct nft_stats __percpu * oldstats = <nl> nft_dereference ( chain -> stats );
int xgbe_mdio_register ( struct xgbe_prv_data * pdata ) <nl> of_node_put ( phy_node ); <nl> goto err_phy_device ; <nl> } <nl> + if (! phydev -> dev . driver ) { <nl> + dev_err ( pdata -> dev , " phy driver probe failed \ n "); <nl> + ret = - EIO ; <nl> + goto err_phy_device ; <nl> + } <nl>  <nl> /* Add a reference to the PHY driver so it can ' t be unloaded */ <nl> - pdata -> phy_module = phydev -> dev . driver ? <nl> - phydev -> dev . driver -> owner : NULL ; <nl> + pdata -> phy_module = phydev -> dev . driver -> owner ; <nl> if (! try_module_get ( pdata -> phy_module )) { <nl> dev_err ( pdata -> dev , " try_module_get failed \ n "); <nl> ret = - EIO ;
static int bond_xmit_roundrobin ( struct sk_buff * skb , struct net_device * bond_dev <nl> * send the join / membership reports . The curr_active_slave found <nl> * will send all of this type of traffic . <nl> */ <nl> - if (( iph -> protocol == htons ( IPPROTO_IGMP )) && <nl> + if (( iph -> protocol == IPPROTO_IGMP ) && <nl> ( skb -> protocol == htons ( ETH_P_IP ))) { <nl>  <nl> read_lock (& bond -> curr_slave_lock );
int __kvm_set_memory_region ( struct kvm * kvm , <nl> goto out_free ; <nl> } <nl>  <nl> - kvm_free_physmem_slot (& old , & new ); <nl> + kvm_free_physmem_slot (& old , npages ? & new : NULL ); <nl> + /* Slot deletion case : we have to update the current slot */ <nl> + if (! npages ) <nl> + * memslot = old ; <nl> # ifdef CONFIG_DMAR <nl> /* map the pages in iommu page table */ <nl> r = kvm_iommu_map_pages ( kvm , base_gfn , npages );
kiblnd_nid2peerlist ( lnet_nid_t nid ) <nl> unsigned int hash = <nl> (( unsigned int ) nid ) % kiblnd_data . kib_peer_hash_size ; <nl>  <nl> - return (& kiblnd_data . kib_peers [ hash ]); <nl> + return & kiblnd_data . kib_peers [ hash ]; <nl> } <nl>  <nl> static inline int <nl> kiblnd_peer_active ( kib_peer_t * peer ) <nl> { <nl> /* Am I in the peer hash table ? */ <nl> - return (! list_empty (& peer -> ibp_list )); <nl> + return ! list_empty (& peer -> ibp_list ); <nl> } <nl>  <nl> static inline kib_conn_t * <nl> kiblnd_wreqid2ptr ( __u64 wreqid ) <nl> static inline int <nl> kiblnd_wreqid2type ( __u64 wreqid ) <nl> { <nl> - return ( wreqid & IBLND_WID_MASK ); <nl> + return wreqid & IBLND_WID_MASK ; <nl> } <nl>  <nl> static inline void
static void bnx2x_tpa_stop ( struct bnx2x * bp , struct bnx2x_fastpath * fp , <nl> skb = build_skb ( data ); <nl>  <nl> if ( likely ( skb )) { <nl> - <nl> # ifdef BNX2X_STOP_ON_ERROR <nl> if ( pad + len > fp -> rx_buf_size ) { <nl> BNX2X_ERR (" skb_put is about to fail ... " <nl> static void bnx2x_tpa_stop ( struct bnx2x * bp , struct bnx2x_fastpath * fp , <nl>  <nl> return ; <nl> } <nl> - <nl> + kfree ( new_data ); <nl> drop : <nl> /* drop the packet and keep the buffer in the bin */ <nl> DP ( NETIF_MSG_RX_STATUS ,
restart : <nl> mutex_unlock (& mutex ); <nl> } <nl>  <nl> +/* <nl> + * sync everything . Start out by waking pdflush , because that writes back <nl> + * all queues in parallel . <nl> + */ <nl> SYSCALL_DEFINE0 ( sync ) <nl> { <nl> + wakeup_pdflush ( 0 ); <nl> sync_filesystems ( 0 ); <nl> sync_filesystems ( 1 ); <nl> if ( unlikely ( laptop_mode ))
static int efx_init_lm87 ( struct efx_nic * efx , struct i2c_board_info * info , <nl> if (! client ) <nl> return - EIO ; <nl>  <nl> + /* Read - to - clear alarm / interrupt status */ <nl> + i2c_smbus_read_byte_data ( client , LM87_REG_ALARMS1 ); <nl> + i2c_smbus_read_byte_data ( client , LM87_REG_ALARMS2 ); <nl> + <nl> rc = efx_poke_lm87 ( client , reg_values ); <nl> if ( rc ) <nl> goto err ;
static struct scsi_host_template qla4xxx_driver_template = { <nl> . slave_alloc = qla4xxx_slave_alloc , <nl> . slave_destroy = qla4xxx_slave_destroy , <nl>  <nl> + . scan_finished = iscsi_scan_finished , <nl> + <nl> . this_id = - 1 , <nl> . cmd_per_lun = 3 , <nl> . use_clustering = ENABLE_CLUSTERING , <nl> static int __devinit qla4xxx_probe_adapter ( struct pci_dev * pdev , <nl> qla4xxx_version_str , ha -> pdev -> device , pci_name ( ha -> pdev ), <nl> ha -> host_no , ha -> firmware_version [ 0 ], ha -> firmware_version [ 1 ], <nl> ha -> patch_number , ha -> build_number ); <nl> - <nl> + scsi_scan_host ( host ); <nl> return 0 ; <nl>  <nl> remove_host :
static struct mtd_partition collie_partitions [] = { <nl>  <nl> static void collie_set_vpp ( int vpp ) <nl> { <nl> - write_scoop_reg (& colliescoop_device . dev , SCOOP_GPCR , read_scoop_reg ( SCOOP_GPCR ) | COLLIE_SCP_VPEN ); <nl> + write_scoop_reg (& colliescoop_device . dev , SCOOP_GPCR , read_scoop_reg (& colliescoop_device . dev , SCOOP_GPCR ) | COLLIE_SCP_VPEN ); <nl> if ( vpp ) <nl> - write_scoop_reg (& colliescoop_device . dev , SCOOP_GPWR , read_scoop_reg ( SCOOP_GPWR ) | COLLIE_SCP_VPEN ); <nl> + write_scoop_reg (& colliescoop_device . dev , SCOOP_GPWR , read_scoop_reg (& colliescoop_device . dev , SCOOP_GPWR ) | COLLIE_SCP_VPEN ); <nl> else <nl> - write_scoop_reg (& colliescoop_device . dev , SCOOP_GPWR , read_scoop_reg ( SCOOP_GPWR ) & ~ COLLIE_SCP_VPEN ); <nl> + write_scoop_reg (& colliescoop_device . dev , SCOOP_GPWR , read_scoop_reg (& colliescoop_device . dev , SCOOP_GPWR ) & ~ COLLIE_SCP_VPEN ); <nl> } <nl>  <nl> static struct flash_platform_data collie_flash_data = {
static int pci_probe ( struct pci_dev * dev , <nl>  <nl> reg_write ( ohci , OHCI1394_IsoXmitIntMaskSet , ~ 0 ); <nl> ohci -> it_context_support = reg_read ( ohci , OHCI1394_IsoXmitIntMaskSet ); <nl> + /* JMicron JMB38x often shows 0 at first read , just ignore it */ <nl> + if (! ohci -> it_context_support ) { <nl> + ohci_notice ( ohci , " overriding IsoXmitIntMask \ n "); <nl> + ohci -> it_context_support = 0xf ; <nl> + } <nl> reg_write ( ohci , OHCI1394_IsoXmitIntMaskClear , ~ 0 ); <nl> ohci -> it_context_mask = ohci -> it_context_support ; <nl> ohci -> n_it = hweight32 ( ohci -> it_context_mask );
static int oti6858_ioctl ( struct usb_serial_port * port , struct file * file , <nl>  <nl> switch ( cmd ) { <nl> case TCGETS : <nl> - if ( copy_to_user ( user_arg , port -> tty -> termios , <nl> - sizeof ( struct ktermios ))) { <nl> + if ( kernel_termios_to_user_termios (( struct ktermios __user *) arg , <nl> + port -> tty -> termios )) <nl> return - EFAULT ; <nl> - } <nl> return 0 ; <nl>  <nl> case TCSETS : <nl> case TCSETSW : /* FIXME : this is not the same ! */ <nl> case TCSETSF : /* FIXME : this is not the same ! */ <nl> - if ( copy_from_user ( port -> tty -> termios , user_arg , <nl> - sizeof ( struct ktermios ))) { <nl> + if ( user_termios_to_kernel_termios ( port -> tty -> termios , <nl> + ( struct ktermios __user *) arg )) <nl> return - EFAULT ; <nl> - } <nl> oti6858_set_termios ( port , NULL ); <nl> return 0 ; <nl> 
static const struct pvr2_ctl_info control_defs [] = { <nl> . internal_id = PVR2_CID_CROPW , <nl> . default_value = 720 , <nl> DEFREF ( cropw ), <nl> + DEFINT ( 0 , 864 ), <nl> . get_max_value = ctrl_cropw_max_get , <nl> . get_def_value = ctrl_get_cropcapdw , <nl> }, { <nl> static const struct pvr2_ctl_info control_defs [] = { <nl> . internal_id = PVR2_CID_CROPH , <nl> . default_value = 480 , <nl> DEFREF ( croph ), <nl> + DEFINT ( 0 , 576 ), <nl> . get_max_value = ctrl_croph_max_get , <nl> . get_def_value = ctrl_get_cropcapdh , <nl> }, {
static void ath9k_hw_read_revisions ( struct ath_hw * ah ) <nl> val = REG_READ ( ah , AR_SREV ); <nl> ah -> hw_version . macRev = MS ( val , AR_SREV_REVISION2 ); <nl> return ; <nl> + case AR9300_DEVID_QCA955X : <nl> + ah -> hw_version . macVersion = AR_SREV_VERSION_9550 ; <nl> + return ; <nl> } <nl>  <nl> val = REG_READ ( ah , AR_SREV ) & AR_SREV_ID ;
int decode_seq ( bitstr_t * bs , field_t * f , char * base , int level ) <nl> CHECK_BOUND ( bs , 2 ); <nl> len = get_len ( bs ); <nl> CHECK_BOUND ( bs , len ); <nl> - if (! base ) { <nl> + if (! base || !( son -> attr & DECODE )) { <nl> PRINT ("%*. s % s \ n ", ( level + 1 ) * TAB_SIZE , <nl> " ", son -> name ); <nl> bs -> cur += len ; <nl> int decode_choice ( bitstr_t * bs , field_t * f , char * base , int level ) <nl> } else { <nl> ext = 0 ; <nl> type = get_bits ( bs , f -> sz ); <nl> + if ( type >= f -> lb ) <nl> + return H323_ERROR_RANGE ; <nl> } <nl>  <nl> /* Write Type */
ssize_t usb_store_new_id ( struct usb_dynids * dynids , <nl> if ( fields > 4 ) { <nl> const struct usb_device_id * id = id_table ; <nl>  <nl> + if (! id ) <nl> + return - ENODEV ; <nl> + <nl> for (; id -> match_flags ; id ++) <nl> if ( id -> idVendor == refVendor && id -> idProduct == refProduct ) <nl> break ;
static int sunxi_pinctrl_gpio_get ( struct gpio_chip * chip , unsigned offset ) <nl> u8 index = sunxi_data_offset ( offset ); <nl> u32 set_mux = pctl -> desc -> irq_read_needs_mux && <nl> test_bit ( FLAG_USED_AS_IRQ , & chip -> desc [ offset ]. flags ); <nl> + u32 pin = offset + chip -> base ; <nl> u32 val ; <nl>  <nl> if ( set_mux ) <nl> - sunxi_pmx_set ( pctl -> pctl_dev , offset , SUN4I_FUNC_INPUT ); <nl> + sunxi_pmx_set ( pctl -> pctl_dev , pin , SUN4I_FUNC_INPUT ); <nl>  <nl> val = ( readl ( pctl -> membase + reg ) >> index ) & DATA_PINS_MASK ; <nl>  <nl> if ( set_mux ) <nl> - sunxi_pmx_set ( pctl -> pctl_dev , offset , SUN4I_FUNC_IRQ ); <nl> + sunxi_pmx_set ( pctl -> pctl_dev , pin , SUN4I_FUNC_IRQ ); <nl>  <nl> return !! val ; <nl> }
void ieee80211_key_free ( struct ieee80211_key * key ) <nl> if (! key ) <nl> return ; <nl>  <nl> + if (! key -> sdata ) { <nl> + /* The key has not been linked yet , simply free it <nl> + * and don ' t Oops */ <nl> + if ( key -> conf . alg == ALG_CCMP ) <nl> + ieee80211_aes_key_free ( key -> u . ccmp . tfm ); <nl> + kfree ( key ); <nl> + return ; <nl> + } <nl> + <nl> spin_lock_irqsave (& key -> sdata -> local -> key_lock , flags ); <nl> __ieee80211_key_free ( key ); <nl> spin_unlock_irqrestore (& key -> sdata -> local -> key_lock , flags );
twobyte_insn : <nl> break ; <nl> case 0xc3 : /* movnti */ <nl> ctxt -> dst . bytes = ctxt -> op_bytes ; <nl> - ctxt -> dst . val = ( ctxt -> op_bytes == 4 ) ? ( u32 ) ctxt -> src . val : <nl> - ( u64 ) ctxt -> src . val ; <nl> + ctxt -> dst . val = ( ctxt -> op_bytes == 8 ) ? ( u64 ) ctxt -> src . val : <nl> + ( u32 ) ctxt -> src . val ; <nl> break ; <nl> default : <nl> goto cannot_emulate ;
static int btrfs_remount ( struct super_block * sb , int * flags , char * data ) <nl> struct btrfs_root * root = btrfs_sb ( sb ); <nl> int ret ; <nl>  <nl> + ret = btrfs_parse_options ( root , data ); <nl> + if ( ret ) <nl> + return - EINVAL ; <nl> + <nl> if ((* flags & MS_RDONLY ) == ( sb -> s_flags & MS_RDONLY )) <nl> return 0 ; <nl> 
int perf_event__parse_sample ( const union perf_event * event , u64 type , <nl> u32 val32 [ 2 ]; <nl> } u ; <nl>  <nl> - <nl> + memset ( data , 0 , sizeof (* data )); <nl> data -> cpu = data -> pid = data -> tid = - 1 ; <nl> data -> stream_id = data -> id = data -> time = - 1ULL ; <nl> 
static struct tgfx __init * tgfx_probe ( int parport , int * n_buttons , int n_devs ) <nl> if ( n_buttons [ i ] < 1 ) <nl> continue ; <nl>  <nl> - if ( n_buttons [ i ] > 6 ) { <nl> + if ( n_buttons [ i ] > ARRAY_SIZE ( tgfx_buttons )) { <nl> printk ( KERN_ERR " turbografx . c : Invalid number of buttons % d \ n ", n_buttons [ i ]); <nl> err = - EINVAL ; <nl> goto err_unreg_devs ;
static int da9052_dcdc_set_current_limit ( struct regulator_dev * rdev , int min_uA , <nl> else if ( offset == 0 ) <nl> row = 1 ; <nl>  <nl> - if ( min_uA > da9052_current_limits [ row ][ DA9052_MAX_UA ] || <nl> - max_uA < da9052_current_limits [ row ][ DA9052_MIN_UA ]) <nl> - return - EINVAL ; <nl> - <nl> for ( i = DA9052_CURRENT_RANGE - 1 ; i >= 0 ; i --) { <nl> - if ( da9052_current_limits [ row ][ i ] <= max_uA ) { <nl> + if (( min_uA <= da9052_current_limits [ row ][ i ]) && <nl> + ( da9052_current_limits [ row ][ i ] <= max_uA )) { <nl> reg_val = i ; <nl> break ; <nl> } <nl> } <nl>  <nl> + if ( i < 0 ) <nl> + return - EINVAL ; <nl> + <nl> /* Determine the even or odd position of the buck current limit <nl> * register field <nl> */
static void rv6xx_program_display_gap ( struct radeon_device * rdev ) <nl> u32 tmp = RREG32 ( CG_DISPLAY_GAP_CNTL ); <nl>  <nl> tmp &= ~( DISP1_GAP_MCHG_MASK | DISP2_GAP_MCHG_MASK ); <nl> - if ( RREG32 ( AVIVO_D1CRTC_CONTROL ) & AVIVO_CRTC_EN ) { <nl> + if ( rdev -> pm . dpm . new_active_crtcs & 1 ) { <nl> tmp |= DISP1_GAP_MCHG ( R600_PM_DISPLAY_GAP_VBLANK ); <nl> tmp |= DISP2_GAP_MCHG ( R600_PM_DISPLAY_GAP_IGNORE ); <nl> - } else if ( RREG32 ( AVIVO_D2CRTC_CONTROL ) & AVIVO_CRTC_EN ) { <nl> + } else if ( rdev -> pm . dpm . new_active_crtcs & 2 ) { <nl> tmp |= DISP1_GAP_MCHG ( R600_PM_DISPLAY_GAP_IGNORE ); <nl> tmp |= DISP2_GAP_MCHG ( R600_PM_DISPLAY_GAP_VBLANK ); <nl> } else {
static int alps_enter_command_mode ( struct psmouse * psmouse , <nl> return - 1 ; <nl> } <nl>  <nl> - if ( param [ 0 ] != 0x88 && param [ 1 ] != 0x07 ) { <nl> + if ( param [ 0 ] != 0x88 || ( param [ 1 ] != 0x07 && param [ 1 ] != 0x08 )) { <nl> psmouse_dbg ( psmouse , <nl> " unknown response while entering command mode \ n "); <nl> return - 1 ;
static gpa_t FNAME ( gva_to_gpa )( struct kvm_vcpu * vcpu , gva_t vaddr ) <nl> static void FNAME ( prefetch_page )( struct kvm_vcpu * vcpu , <nl> struct kvm_mmu_page * sp ) <nl> { <nl> - int i ; <nl> + int i , offset = 0 ; <nl> pt_element_t * gpt ; <nl> struct page * page ; <nl>  <nl> - if ( sp -> role . metaphysical || PTTYPE == 32 ) { <nl> + if ( sp -> role . metaphysical <nl> + || ( PTTYPE == 32 && sp -> role . level > PT_PAGE_TABLE_LEVEL )) { <nl> nonpaging_prefetch_page ( vcpu , sp ); <nl> return ; <nl> } <nl>  <nl> + if ( PTTYPE == 32 ) <nl> + offset = sp -> role . quadrant << PT64_LEVEL_BITS ; <nl> page = gfn_to_page ( vcpu -> kvm , sp -> gfn ); <nl> gpt = kmap_atomic ( page , KM_USER0 ); <nl> for ( i = 0 ; i < PT64_ENT_PER_PAGE ; ++ i ) <nl> - if ( is_present_pte ( gpt [ i ])) <nl> + if ( is_present_pte ( gpt [ offset + i ])) <nl> sp -> spt [ i ] = shadow_trap_nonpresent_pte ; <nl> else <nl> sp -> spt [ i ] = shadow_notrap_nonpresent_pte ;
int msp_pcibios_config_access ( unsigned char access_type , <nl> unsigned long intr ; <nl> unsigned long value ; <nl> static char pciirqflag ; <nl> + int ret ; <nl> # if defined ( CONFIG_PMC_MSP7120_GW ) || defined ( CONFIG_PMC_MSP7120_EVAL ) <nl> unsigned int vpe_status ; <nl> # endif <nl> int msp_pcibios_config_access ( unsigned char access_type , <nl> * allocation assigns an interrupt handler to the interrupt . <nl> */ <nl> if ( pciirqflag == 0 ) { <nl> - request_irq ( MSP_INT_PCI ,/* Hardcoded internal MSP7120 wiring */ <nl> + ret = request_irq ( MSP_INT_PCI ,/* Hardcoded internal MSP7120 wiring */ <nl> bpci_interrupt , <nl> IRQF_SHARED | IRQF_DISABLED , <nl> " PMC MSP PCI Host ", <nl> preg ); <nl> + if ( ret != 0 ) <nl> + return ret ; <nl> pciirqflag = ~ 0 ; <nl> } <nl> 
nsm_monitor ( struct nlm_host * host ) <nl> int status ; <nl>  <nl> dprintk (" lockd : nsm_monitor (% s )\ n ", nsm -> sm_name ); <nl> - BUG_ON ( nsm == NULL ); <nl>  <nl> if ( nsm -> sm_monitored ) <nl> return 0 ;
static int time_cpufreq_notifier ( struct notifier_block * nb , unsigned long val , <nl> tsc_khz = cpufreq_scale ( tsc_khz_ref , ref_freq , freq -> new ); <nl> if (!( freq -> flags & CPUFREQ_CONST_LOOPS )) <nl> mark_tsc_unstable (" cpufreq changes "); <nl> - } <nl>  <nl> - set_cyc2ns_scale ( tsc_khz , freq -> cpu ); <nl> + set_cyc2ns_scale ( tsc_khz , freq -> cpu ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static void __init mark_kpte_bitmap ( unsigned long start , unsigned long end ) <nl> while ( start < end ) { <nl> long remains ; <nl>  <nl> + remains = end - start ; <nl> + if ( remains < size_256MB ) <nl> + break ; <nl> + <nl> if ( start & mask_256MB ) { <nl> start = ( start + size_256MB ) & ~ mask_256MB ; <nl> continue ; <nl> } <nl>  <nl> - remains = end - start ; <nl> while ( remains >= size_256MB ) { <nl> unsigned long index = start >> shift_256MB ; <nl> 
void ieee80211_beacon_connection_loss_work ( struct work_struct * work ) <nl> struct sta_info * sta ; <nl>  <nl> if ( ifmgd -> associated ) { <nl> + rcu_read_lock (); <nl> sta = sta_info_get ( sdata , ifmgd -> bssid ); <nl> if ( sta ) <nl> sta -> beacon_loss_count ++; <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> if ( sdata -> local -> hw . flags & IEEE80211_HW_CONNECTION_MONITOR )
static void kswapd_try_to_sleep ( pg_data_t * pgdat , int order , int classzone_idx ) <nl> * them before going back to sleep . <nl> */ <nl> set_pgdat_percpu_threshold ( pgdat , calculate_normal_threshold ); <nl> - schedule (); <nl> + <nl> + if (! kthread_should_stop ()) <nl> + schedule (); <nl> + <nl> set_pgdat_percpu_threshold ( pgdat , calculate_pressure_threshold ); <nl> } else { <nl> if ( remaining )
static int opp_parse_supplies ( struct dev_pm_opp * opp , struct device * dev , <nl>  <nl> /* Search for " opp - microvolt -< name >" */ <nl> if ( dev_opp -> prop_name ) { <nl> - sprintf ( name , " opp - microvolt -% s ", dev_opp -> prop_name ); <nl> + snprintf ( name , sizeof ( name ), " opp - microvolt -% s ", <nl> + dev_opp -> prop_name ); <nl> prop = of_find_property ( opp -> np , name , NULL ); <nl> } <nl>  <nl> static int opp_parse_supplies ( struct dev_pm_opp * opp , struct device * dev , <nl> /* Search for " opp - microamp -< name >" */ <nl> prop = NULL ; <nl> if ( dev_opp -> prop_name ) { <nl> - sprintf ( name , " opp - microamp -% s ", dev_opp -> prop_name ); <nl> + snprintf ( name , sizeof ( name ), " opp - microamp -% s ", <nl> + dev_opp -> prop_name ); <nl> prop = of_find_property ( opp -> np , name , NULL ); <nl> } <nl> 
kvm_irqfd_assign ( struct kvm * kvm , int fd , int gsi ) <nl> events = file -> f_op -> poll ( file , & irqfd -> pt ); <nl>  <nl> list_add_tail (& irqfd -> list , & kvm -> irqfds . items ); <nl> - spin_unlock_irq (& kvm -> irqfds . lock ); <nl>  <nl> /* <nl> * Check if there was an event already pending on the eventfd <nl> kvm_irqfd_assign ( struct kvm * kvm , int fd , int gsi ) <nl> if ( events & POLLIN ) <nl> schedule_work (& irqfd -> inject ); <nl>  <nl> + spin_unlock_irq (& kvm -> irqfds . lock ); <nl> + <nl> /* <nl> * do not drop the file until the irqfd is fully initialized , otherwise <nl> * we might race against the POLLHUP
static int chaoskey_rng_read ( struct hwrng * rng , void * data , <nl> if ( this_time > max ) <nl> this_time = max ; <nl>  <nl> - memcpy ( data , dev -> buf , this_time ); <nl> + memcpy ( data , dev -> buf + dev -> used , this_time ); <nl>  <nl> dev -> used += this_time ; <nl> 
static inline void update_sg_lb_stats ( struct lb_env * env , <nl> struct sched_group * group , int load_idx , <nl> int local_group , struct sg_lb_stats * sgs ) <nl> { <nl> - unsigned long nr_running ; <nl> unsigned long load ; <nl> int i ; <nl>  <nl> static inline void update_sg_lb_stats ( struct lb_env * env , <nl> for_each_cpu_and ( i , sched_group_cpus ( group ), env -> cpus ) { <nl> struct rq * rq = cpu_rq ( i ); <nl>  <nl> - nr_running = rq -> nr_running ; <nl> - <nl> /* Bias balancing toward cpus of our domain */ <nl> if ( local_group ) <nl> load = target_load ( i , load_idx ); <nl> static inline void update_sg_lb_stats ( struct lb_env * env , <nl> load = source_load ( i , load_idx ); <nl>  <nl> sgs -> group_load += load ; <nl> - sgs -> sum_nr_running += nr_running ; <nl> + sgs -> sum_nr_running += rq -> nr_running ; <nl> # ifdef CONFIG_NUMA_BALANCING <nl> sgs -> nr_numa_running += rq -> nr_numa_running ; <nl> sgs -> nr_preferred_running += rq -> nr_preferred_running ;
static int mx2_camera_set_bus_param ( struct soc_camera_device * icd , <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if ( common_flags & SOCAM_PCLK_SAMPLE_RISING ) <nl> + csicr1 |= CSICR1_REDGE ; <nl> if ( common_flags & SOCAM_PCLK_SAMPLE_FALLING ) <nl> csicr1 |= CSICR1_INV_PCLK ; <nl> if ( common_flags & SOCAM_VSYNC_ACTIVE_HIGH )
parse_init_table ( struct nvbios * bios , uint16_t offset , struct init_exec * iexec ) <nl> int count = 0 , i , ret ; <nl> uint8_t id ; <nl>  <nl> + /* catch NULL script pointers */ <nl> + if ( offset == 0 ) <nl> + return 0 ; <nl> + <nl> /* <nl> * Loop until INIT_DONE causes us to break out of the loop <nl> * ( or until offset > bios length just in case ... )
static struct mem_cgroup * try_get_mem_cgroup_from_swapcache ( struct page * page ) <nl> return NULL ; <nl>  <nl> pc = lookup_page_cgroup ( page ); <nl> - /* <nl> - * Used bit of swapcache is solid under page lock . <nl> - */ <nl> + lock_page_cgroup ( pc ); <nl> if ( PageCgroupUsed ( pc )) { <nl> mem = pc -> mem_cgroup ; <nl> if ( mem && ! css_tryget (& mem -> css )) <nl> static struct mem_cgroup * try_get_mem_cgroup_from_swapcache ( struct page * page ) <nl> mem = NULL ; <nl> rcu_read_unlock (); <nl> } <nl> + unlock_page_cgroup ( pc ); <nl> return mem ; <nl> } <nl> 
static bool use_mmio_flip ( struct intel_engine_cs * ring , <nl> * So using MMIO flips there would disrupt this mechanism . <nl> */ <nl>  <nl> + if ( ring == NULL ) <nl> + return true ; <nl> + <nl> if ( INTEL_INFO ( ring -> dev )-> gen < 5 ) <nl> return false ; <nl>  <nl> static int intel_crtc_page_flip ( struct drm_crtc * crtc , <nl>  <nl> if ( IS_VALLEYVIEW ( dev )) { <nl> ring = & dev_priv -> ring [ BCS ]; <nl> + if ( obj -> tiling_mode != work -> old_fb_obj -> tiling_mode ) <nl> + /* vlv : DISPLAY_FLIP fails to change tiling */ <nl> + ring = NULL ; <nl> } else if ( IS_IVYBRIDGE ( dev )) { <nl> ring = & dev_priv -> ring [ BCS ]; <nl> } else if ( INTEL_INFO ( dev )-> gen >= 7 ) {
int nf_conntrack_init_net ( struct net * net ) <nl> int cpu ; <nl>  <nl> atomic_set (& net -> ct . count , 0 ); <nl> + seqcount_init (& net -> ct . generation ); <nl>  <nl> net -> ct . pcpu_lists = alloc_percpu ( struct ct_pcpu ); <nl> if (! net -> ct . pcpu_lists )
void ata_bmdma_error_handler ( struct ata_port * ap ) <nl> */ <nl> void ata_bmdma_post_internal_cmd ( struct ata_queued_cmd * qc ) <nl> { <nl> - ata_bmdma_stop ( qc ); <nl> + if ( qc -> ap -> ioaddr . bmdma_addr ) <nl> + ata_bmdma_stop ( qc ); <nl> } <nl>  <nl> # ifdef CONFIG_PCI
static int nl80211_start_radar_detection ( struct sk_buff * skb , <nl> struct net_device * dev = info -> user_ptr [ 1 ]; <nl> struct wireless_dev * wdev = dev -> ieee80211_ptr ; <nl> struct cfg80211_chan_def chandef ; <nl> + enum nl80211_dfs_regions dfs_region ; <nl> int err ; <nl>  <nl> + dfs_region = reg_get_dfs_region ( wdev -> wiphy ); <nl> + if ( dfs_region == NL80211_DFS_UNSET ) <nl> + return - EINVAL ; <nl> + <nl> err = nl80211_parse_chandef ( rdev , info , & chandef ); <nl> if ( err ) <nl> return err ;
static int init_timers_cpu ( int cpu ) <nl> /* <nl> * The APs use this path later in boot <nl> */ <nl> - base = kmalloc_node ( sizeof (* base ), <nl> - GFP_KERNEL | __GFP_ZERO , <nl> - cpu_to_node ( cpu )); <nl> + base = kzalloc_node ( sizeof (* base ), GFP_KERNEL , <nl> + cpu_to_node ( cpu )); <nl> if (! base ) <nl> return - ENOMEM ; <nl> 
static int __devinit bttv_probe ( struct pci_dev * dev , <nl> ( unsigned long long ) pci_resource_start ( dev , 0 )); <nl> schedule (); <nl>  <nl> - btv -> bt848_mmio = ioremap ( pci_resource_start ( dev , 0 ), 0x1000 ); <nl> - if ( NULL == ioremap ( pci_resource_start ( dev , 0 ), 0x1000 )) { <nl> + btv -> bt848_mmio = ioremap ( pci_resource_start ( dev , 0 ), 0x1000 ); <nl> + if ( NULL == btv -> bt848_mmio ) { <nl> printk (" bttv % d : ioremap () failed \ n ", btv -> c . nr ); <nl> result = - EIO ; <nl> goto fail1 ;
static int dp83867_config_init ( struct phy_device * phydev ) <nl> return ret ; <nl> } <nl>  <nl> - if (( phydev -> interface >= PHY_INTERFACE_MODE_RGMII_ID ) || <nl> + if (( phydev -> interface >= PHY_INTERFACE_MODE_RGMII_ID ) && <nl> ( phydev -> interface <= PHY_INTERFACE_MODE_RGMII_RXID )) { <nl> val = phy_read_mmd_indirect ( phydev , DP83867_RGMIICTL , <nl> DP83867_DEVADDR , phydev -> addr );
int MoxaDriverIoctl ( unsigned int cmd , unsigned long arg , int port ) <nl> case MOXA_FIND_BOARD : <nl> case MOXA_LOAD_C320B : <nl> case MOXA_LOAD_CODE : <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> break ; <nl> } <nl> 
static void sport_shutdown ( struct uart_port * port ) <nl> } <nl>  <nl> static void sport_set_termios ( struct uart_port * port , <nl> - struct termios * termios , struct termios * old ) <nl> + struct ktermios * termios , struct ktermios * old ) <nl> { <nl> pr_debug ("% s enter , c_cflag :% 08x \ n ", __func__ , termios -> c_cflag ); <nl> uart_update_timeout ( port , CS8 , port -> uartclk );
static struct inotify_watch * create_watch ( struct inotify_device * dev , <nl> return ERR_PTR ( ret ); <nl> } <nl>  <nl> - dev -> last_wd = ret ; <nl> + dev -> last_wd = watch -> wd ; <nl> watch -> mask = mask ; <nl> atomic_set (& watch -> count , 0 ); <nl> INIT_LIST_HEAD (& watch -> d_list );
static int __init parse_crashkernel_mem ( char * cmdline , <nl> } while (* cur ++ == ','); <nl>  <nl> if (* crash_size > 0 ) { <nl> - while (* cur != ' ' && * cur != '@') <nl> + while (* cur && * cur != ' ' && * cur != '@') <nl> cur ++; <nl> if (* cur == '@') { <nl> cur ++;
# include < linux / rcupdate . h > <nl> # include < linux / hrtimer . h > <nl> # include < linux / sched / rt . h > <nl> +# include < linux / freezer . h > <nl>  <nl> # include < asm / uaccess . h > <nl>  <nl> int poll_schedule_timeout ( struct poll_wqueues * pwq , int state , <nl>  <nl> set_current_state ( state ); <nl> if (! pwq -> triggered ) <nl> - rc = schedule_hrtimeout_range ( expires , slack , HRTIMER_MODE_ABS ); <nl> + rc = freezable_schedule_hrtimeout_range ( expires , slack , <nl> + HRTIMER_MODE_ABS ); <nl> __set_current_state ( TASK_RUNNING ); <nl>  <nl> /*
static void ctx_sched_out ( struct perf_event_context * ctx , <nl> struct perf_event * event ; <nl>  <nl> raw_spin_lock (& ctx -> lock ); <nl> + perf_pmu_disable ( ctx -> pmu ); <nl> ctx -> is_active = 0 ; <nl> if ( likely (! ctx -> nr_events )) <nl> goto out ; <nl> static void ctx_sched_out ( struct perf_event_context * ctx , <nl> group_sched_out ( event , cpuctx , ctx ); <nl> } <nl> out : <nl> + perf_pmu_enable ( ctx -> pmu ); <nl> raw_spin_unlock (& ctx -> lock ); <nl> } <nl>  <nl> void perf_event_context_sched_in ( struct perf_event_context * ctx ) <nl> if ( cpuctx -> task_ctx == ctx ) <nl> return ; <nl>  <nl> + perf_pmu_disable ( ctx -> pmu ); <nl> /* <nl> * We want to keep the following priority order : <nl> * cpu pinned ( that don ' t need to move ), task pinned , <nl> void perf_event_context_sched_in ( struct perf_event_context * ctx ) <nl> * cpu - context we got scheduled on is actually rotating . <nl> */ <nl> perf_pmu_rotate_start ( ctx -> pmu ); <nl> + perf_pmu_enable ( ctx -> pmu ); <nl> } <nl>  <nl> /* <nl> static enum hrtimer_restart perf_event_context_tick ( struct hrtimer * timer ) <nl> rotate = 1 ; <nl> } <nl>  <nl> + perf_pmu_disable ( cpuctx -> ctx . pmu ); <nl> perf_ctx_adjust_freq (& cpuctx -> ctx , cpuctx -> timer_interval ); <nl> if ( ctx ) <nl> perf_ctx_adjust_freq ( ctx , cpuctx -> timer_interval ); <nl> static enum hrtimer_restart perf_event_context_tick ( struct hrtimer * timer ) <nl> task_ctx_sched_in ( ctx , EVENT_FLEXIBLE ); <nl>  <nl> done : <nl> + perf_pmu_enable ( cpuctx -> ctx . pmu ); <nl> hrtimer_forward_now ( timer , ns_to_ktime ( cpuctx -> timer_interval )); <nl>  <nl> return restart ;
static int team_device_event ( struct notifier_block * unused , <nl> case NETDEV_UP : <nl> if ( netif_carrier_ok ( dev )) <nl> team_port_change_check ( port , true ); <nl> + break ; <nl> case NETDEV_DOWN : <nl> team_port_change_check ( port , false ); <nl> + break ; <nl> case NETDEV_CHANGE : <nl> if ( netif_running ( port -> dev )) <nl> team_port_change_check ( port ,
static int stv680_start_stream ( struct usb_stv * stv680 ) <nl> return 0 ; <nl>  <nl> nomem_err : <nl> - for ( i = 0 ; i < STV680_NUMSCRATCH ; i ++) { <nl> - kfree ( stv680 -> scratch [ i ]. data ); <nl> - stv680 -> scratch [ i ]. data = NULL ; <nl> - } <nl> for ( i = 0 ; i < STV680_NUMSBUF ; i ++) { <nl> usb_kill_urb ( stv680 -> urb [ i ]); <nl> usb_free_urb ( stv680 -> urb [ i ]); <nl> static int stv680_start_stream ( struct usb_stv * stv680 ) <nl> kfree ( stv680 -> sbuf [ i ]. data ); <nl> stv680 -> sbuf [ i ]. data = NULL ; <nl> } <nl> + /* used in irq , free only as all URBs are dead */ <nl> + for ( i = 0 ; i < STV680_NUMSCRATCH ; i ++) { <nl> + kfree ( stv680 -> scratch [ i ]. data ); <nl> + stv680 -> scratch [ i ]. data = NULL ; <nl> + } <nl> return - ENOMEM ; <nl>  <nl> }
# include < linux / blkdev . h > <nl> # include < linux / random . h > <nl> # include < linux / iocontext . h > <nl> +# include < linux / capability . h > <nl> # include < asm / div64 . h > <nl> # include " compat . h " <nl> # include " ctree . h " <nl> int btrfs_balance ( struct btrfs_root * dev_root ) <nl> if ( dev_root -> fs_info -> sb -> s_flags & MS_RDONLY ) <nl> return - EROFS ; <nl>  <nl> + if (! capable ( CAP_SYS_ADMIN )) <nl> + return - EPERM ; <nl> + <nl> mutex_lock (& dev_root -> fs_info -> volume_mutex ); <nl> dev_root = dev_root -> fs_info -> dev_root ; <nl> 
int vmbus_open ( struct vmbus_channel * newchannel , u32 send_ringbuffer_size , <nl> sizeof ( struct vmbus_channel_open_channel )); <nl>  <nl> if ( ret != 0 ) <nl> - goto Cleanup ; <nl> + goto cleanup ; <nl>  <nl> t = wait_for_completion_timeout (& openInfo -> waitevent , HZ ); <nl> if ( t == 0 ) { <nl> int vmbus_open ( struct vmbus_channel * newchannel , u32 send_ringbuffer_size , <nl> if ( openInfo -> response . open_result . status ) <nl> err = openInfo -> response . open_result . status ; <nl>  <nl> - Cleanup : <nl> + cleanup : <nl> spin_lock_irqsave (& vmbus_connection . channelmsg_lock , flags ); <nl> list_del (& openInfo -> msglistentry ); <nl> spin_unlock_irqrestore (& vmbus_connection . channelmsg_lock , flags ); <nl> int vmbus_establish_gpadl ( struct vmbus_channel * channel , void * kbuffer , <nl> ret = vmbus_post_msg ( gpadlmsg , msginfo -> msgsize - <nl> sizeof (* msginfo )); <nl> if ( ret != 0 ) <nl> - goto Cleanup ; <nl> + goto cleanup ; <nl>  <nl> if ( msgcount > 1 ) { <nl> list_for_each ( curr , & msginfo -> submsglist ) { <nl> int vmbus_establish_gpadl ( struct vmbus_channel * channel , void * kbuffer , <nl> submsginfo -> msgsize - <nl> sizeof (* submsginfo )); <nl> if ( ret != 0 ) <nl> - goto Cleanup ; <nl> + goto cleanup ; <nl>  <nl> } <nl> } <nl> int vmbus_establish_gpadl ( struct vmbus_channel * channel , void * kbuffer , <nl> /* At this point , we received the gpadl created msg */ <nl> * gpadl_handle = gpadlmsg -> gpadl ; <nl>  <nl> - Cleanup : <nl> + cleanup : <nl> spin_lock_irqsave (& vmbus_connection . channelmsg_lock , flags ); <nl> list_del (& msginfo -> msglistentry ); <nl> spin_unlock_irqrestore (& vmbus_connection . channelmsg_lock , flags );
int read_log ( struct tpm_bios_log * log ) <nl> log -> bios_event_log_end = log -> bios_event_log + len ; <nl>  <nl> virt = acpi_os_map_memory ( start , len ); <nl> + if (! virt ) { <nl> + kfree ( log -> bios_event_log ); <nl> + printk ("% s : ERROR - Unable to map memory \ n ", __func__ ); <nl> + return - EIO ; <nl> + } <nl>  <nl> memcpy ( log -> bios_event_log , virt , len ); <nl> 
static void rndis_wlan_media_specific_indication ( struct usbnet * usbdev , <nl> struct rndis_indicate * msg , int buflen ) <nl> { <nl> struct ndis_80211_status_indication * indication ; <nl> - int len , offset ; <nl> + unsigned int len , offset ; <nl>  <nl> offset = offsetof ( struct rndis_indicate , status ) + <nl> le32_to_cpu ( msg -> offset ); <nl> static void rndis_wlan_media_specific_indication ( struct usbnet * usbdev , <nl> return ; <nl> } <nl>  <nl> - if ( offset + len > buflen ) { <nl> + if ( len > buflen || offset > buflen || offset + len > buflen ) { <nl> netdev_info ( usbdev -> net , " media specific indication , too large to fit to buffer (% i > % i )\ n ", <nl> offset + len , buflen ); <nl> return ;
static int snd_usb_pcm_check_knot ( struct snd_pcm_runtime * runtime , <nl> struct snd_usb_substream * subs ) <nl> { <nl> struct audioformat * fp ; <nl> + int * rate_list ; <nl> int count = 0 , needs_knot = 0 ; <nl> int err ; <nl>  <nl> static int snd_usb_pcm_check_knot ( struct snd_pcm_runtime * runtime , <nl> if (! needs_knot ) <nl> return 0 ; <nl>  <nl> - subs -> rate_list . list = kmalloc ( sizeof ( int ) * count , GFP_KERNEL ); <nl> + subs -> rate_list . list = rate_list = <nl> + kmalloc ( sizeof ( int ) * count , GFP_KERNEL ); <nl> if (! subs -> rate_list . list ) <nl> return - ENOMEM ; <nl> subs -> rate_list . count = count ; <nl> static int snd_usb_pcm_check_knot ( struct snd_pcm_runtime * runtime , <nl> list_for_each_entry ( fp , & subs -> fmt_list , list ) { <nl> int i ; <nl> for ( i = 0 ; i < fp -> nr_rates ; i ++) <nl> - subs -> rate_list . list [ count ++] = fp -> rate_table [ i ]; <nl> + rate_list [ count ++] = fp -> rate_table [ i ]; <nl> } <nl> err = snd_pcm_hw_constraint_list ( runtime , 0 , SNDRV_PCM_HW_PARAM_RATE , <nl> & subs -> rate_list );
out_unlock : <nl>  <nl> struct irq_desc * move_irq_desc ( struct irq_desc * desc , int node ) <nl> { <nl> - /* those all static , do move them */ <nl> - if ( desc -> irq < NR_IRQS_LEGACY ) <nl> + /* those static or target node is - 1 , do not move them */ <nl> + if ( desc -> irq < NR_IRQS_LEGACY || node == - 1 ) <nl> return desc ; <nl>  <nl> if ( desc -> node != node )
void card_send_command ( struct ft1000_device * ft1000dev , void * ptempbuffer , <nl>  <nl> DEBUG (" card_send_command : enter card_send_command ... size =% d \ n ", size ); <nl>  <nl> - commandbuf = ( unsigned char *) kmalloc ( size + 2 , GFP_KERNEL ); <nl> + commandbuf = kmalloc ( size + 2 , GFP_KERNEL ); <nl> memcpy (( void *) commandbuf + 2 , ( void *) ptempbuffer , size ); <nl>  <nl> ft1000_read_register ( ft1000dev , & temp , FT1000_REG_DOORBELL );
static int set_connectable ( struct sock * sk , struct hci_dev * hdev , void * data , <nl>  <nl> no_scan_update : <nl> /* Update the advertising parameters if necessary */ <nl> - if ( hci_dev_test_flag ( hdev , HCI_ADVERTISING )) <nl> + if ( hci_dev_test_flag ( hdev , HCI_ADVERTISING ) || <nl> + hci_dev_test_flag ( hdev , HCI_ADVERTISING_INSTANCE )) <nl> enable_advertising (& req ); <nl>  <nl> err = hci_req_run (& req , set_connectable_complete );
void ip6_tnl_dst_reset ( struct ip6_tnl * t ) <nl> int i ; <nl>  <nl> for_each_possible_cpu ( i ) <nl> - ip6_tnl_per_cpu_dst_set ( raw_cpu_ptr ( t -> dst_cache ), NULL ); <nl> + ip6_tnl_per_cpu_dst_set ( per_cpu_ptr ( t -> dst_cache , i ), NULL ); <nl> } <nl> EXPORT_SYMBOL_GPL ( ip6_tnl_dst_reset ); <nl> 
void __devinit bttv_idcard ( struct bttv * btv ) <nl>  <nl> if ( UNSET != audiomux [ 0 ]) { <nl> gpiobits = 0 ; <nl> - for ( i = 0 ; i < 5 ; i ++) { <nl> + for ( i = 0 ; i < 4 ; i ++) { <nl> bttv_tvcards [ btv -> c . type ]. gpiomux [ i ] = audiomux [ i ]; <nl> gpiobits |= audiomux [ i ]; <nl> } <nl> } else { <nl> gpiobits = audioall ; <nl> - for ( i = 0 ; i < 5 ; i ++) { <nl> + for ( i = 0 ; i < 4 ; i ++) { <nl> bttv_tvcards [ btv -> c . type ]. gpiomux [ i ] = audioall ; <nl> } <nl> }
int rt2x00mac_set_key ( struct ieee80211_hw * hw , enum set_key_cmd cmd , <nl> crypto . cipher = rt2x00crypto_key_to_cipher ( key ); <nl> if ( crypto . cipher == CIPHER_NONE ) <nl> return - EOPNOTSUPP ; <nl> + if ( crypto . cipher == CIPHER_TKIP && rt2x00_is_usb ( rt2x00dev )) <nl> + return - EOPNOTSUPP ; <nl>  <nl> crypto . cmd = cmd ; <nl> 
write_begin_failed : <nl> ext3_journal_stop ( handle ); <nl> unlock_page ( page ); <nl> page_cache_release ( page ); <nl> + /* <nl> + * block_write_begin may have instantiated a few blocks <nl> + * outside i_size . Trim these off again . Don ' t need <nl> + * i_size_read because we hold i_mutex . <nl> + */ <nl> + if ( pos + len > inode -> i_size ) <nl> + vmtruncate ( inode , inode -> i_size ); <nl> } <nl> if ( ret == - ENOSPC && ext3_should_retry_alloc ( inode -> i_sb , & retries )) <nl> goto retry ;
void sctp_transport_lower_cwnd ( struct sctp_transport * transport , <nl> transport -> ssthresh = max ( transport -> cwnd / 2 , <nl> 4 * transport -> asoc -> pathmtu ); <nl> transport -> cwnd = transport -> asoc -> pathmtu ; <nl> + <nl> + /* T3 - rtx also clears fast recovery on the transport */ <nl> + transport -> fast_recovery = 0 ; <nl> break ; <nl>  <nl> case SCTP_LOWER_CWND_FAST_RTX :
static int nr_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> if ( sax != NULL ) { <nl> + memset ( sax , 0 , sizeof ( sax )); <nl> sax -> sax25_family = AF_NETROM ; <nl> skb_copy_from_linear_data_offset ( skb , 7 , sax -> sax25_call . ax25_call , <nl> AX25_ADDR_LEN );
static int replace_system_preds ( struct event_subsystem * system , <nl> * replace the filter for the call . <nl> */ <nl> filter = call -> filter ; <nl> - call -> filter = filter_item -> filter ; <nl> + rcu_assign_pointer ( call -> filter , filter_item -> filter ); <nl> filter_item -> filter = filter ; <nl>  <nl> fail = false ; <nl> int apply_event_filter ( struct ftrace_event_call * call , char * filter_string ) <nl> filter = call -> filter ; <nl> if (! filter ) <nl> goto out_unlock ; <nl> - call -> filter = NULL ; <nl> + RCU_INIT_POINTER ( call -> filter , NULL ); <nl> /* Make sure the filter is not being used */ <nl> synchronize_sched (); <nl> __free_filter ( filter ); <nl> out : <nl> * string <nl> */ <nl> tmp = call -> filter ; <nl> - call -> filter = filter ; <nl> + rcu_assign_pointer ( call -> filter , filter ); <nl> if ( tmp ) { <nl> /* Make sure the call is done with the filter */ <nl> synchronize_sched ();
static int __devinit vortex_probe1 ( struct device * gendev , <nl> printk (" *** INVALID CHECKSUM % 4 . 4x *** ", checksum ); <nl> for ( i = 0 ; i < 3 ; i ++) <nl> (( u16 *) dev -> dev_addr )[ i ] = htons ( eeprom [ i + 10 ]); <nl> + memcpy ( dev -> perm_addr , dev -> dev_addr , dev -> addr_len ); <nl> if ( print_info ) { <nl> for ( i = 0 ; i < 6 ; i ++) <nl> printk ("% c % 2 . 2x ", i ? ':' : ' ', dev -> dev_addr [ i ]); <nl> static struct ethtool_ops vortex_ethtool_ops = { <nl> . set_settings = vortex_set_settings , <nl> . get_link = vortex_get_link , <nl> . nway_reset = vortex_nway_reset , <nl> + . get_perm_addr = ethtool_op_get_perm_addr , <nl> }; <nl>  <nl> # ifdef CONFIG_PCI
void hugetlb_unreserve_pages ( struct inode * inode , long offset , long freed ) <nl> long chg = region_truncate (& inode -> i_mapping -> private_list , offset ); <nl>  <nl> spin_lock (& inode -> i_lock ); <nl> - inode -> i_blocks -= blocks_per_huge_page ( h ); <nl> + inode -> i_blocks -= ( blocks_per_huge_page ( h ) * freed ); <nl> spin_unlock (& inode -> i_lock ); <nl>  <nl> hugetlb_put_quota ( inode -> i_mapping , ( chg - freed ));
# include < linux / syscore_ops . h > <nl> # include < linux / mutex . h > <nl> # include < linux / module . h > <nl> +# include < linux / interrupt . h > <nl>  <nl> static LIST_HEAD ( syscore_ops_list ); <nl> static DEFINE_MUTEX ( syscore_ops_lock ); <nl> int syscore_suspend ( void ) <nl> struct syscore_ops * ops ; <nl> int ret = 0 ; <nl>  <nl> + pr_debug (" Checking wakeup interrupts \ n "); <nl> + <nl> + /* Return error code if there are any wakeup interrupts pending . */ <nl> + ret = check_wakeup_irqs (); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> WARN_ONCE (! irqs_disabled (), <nl> " Interrupts enabled before system core suspend .\ n "); <nl> 
static const struct net_device_ops amd8111e_netdev_ops = { <nl> static int amd8111e_probe_one ( struct pci_dev * pdev , <nl> const struct pci_device_id * ent ) <nl> { <nl> - int err , i , pm_cap ; <nl> + int err , i ; <nl> unsigned long reg_addr , reg_len ; <nl> struct amd8111e_priv * lp ; <nl> struct net_device * dev ; <nl> static int amd8111e_probe_one ( struct pci_dev * pdev , <nl> pci_set_master ( pdev ); <nl>  <nl> /* Find power - management capability . */ <nl> - if (( pm_cap = pci_find_capability ( pdev , PCI_CAP_ID_PM ))== 0 ){ <nl> + if (! pdev -> pm_cap ) { <nl> printk ( KERN_ERR " amd8111e : No Power Management capability , " <nl> " exiting .\ n "); <nl> err = - ENODEV ; <nl> static int amd8111e_probe_one ( struct pci_dev * pdev , <nl> lp = netdev_priv ( dev ); <nl> lp -> pci_dev = pdev ; <nl> lp -> amd8111e_net_dev = dev ; <nl> - lp -> pm_cap = pm_cap ; <nl> + lp -> pm_cap = pdev -> pm_cap ; <nl>  <nl> spin_lock_init (& lp -> lock ); <nl> 
static void __init xen_rebuild_p2m_list ( unsigned long * p2m ) <nl> p2m_missing_pte : p2m_identity_pte ; <nl> for ( i = 0 ; i < PMDS_PER_MID_PAGE ; i ++) { <nl> pmdp = populate_extra_pmd ( <nl> - ( unsigned long )( p2m + pfn + i * PTRS_PER_PTE )); <nl> + ( unsigned long )( p2m + pfn ) + i * PMD_SIZE ); <nl> set_pmd ( pmdp , __pmd ( __pa ( ptep ) | _KERNPG_TABLE )); <nl> } <nl> }
static int crypto_ccm_auth ( struct aead_request * req , struct scatterlist * plain , <nl> if ( assoclen ) { <nl> pctx -> ilen = format_adata ( idata , assoclen ); <nl> get_data_to_compute ( cipher , pctx , req -> assoc , req -> assoclen ); <nl> + } else { <nl> + pctx -> ilen = 0 ; <nl> } <nl>  <nl> /* compute plaintext into mac */
static struct sms_board sms_boards [] = { <nl> . board_cfg . leds_power = 26 , <nl> . board_cfg . led0 = 27 , <nl> . board_cfg . led1 = 28 , <nl> + . board_cfg . ir = 9 , <nl> . led_power = 26 , <nl> . led_lo = 27 , <nl> . led_hi = 28 ,
int scsi_error_handler ( void * data ) <nl> set_current_state ( TASK_INTERRUPTIBLE ); <nl> } <nl>  <nl> + __set_current_state ( TASK_RUNNING ); <nl> + <nl> SCSI_LOG_ERROR_RECOVERY ( 1 , printk (" Error handler scsi_eh_ % d " <nl> " exiting \ n ", shost -> host_no )); <nl> 
static bool compliance_mode_recovery_timer_quirk_check ( void ) <nl>  <nl> dmi_product_name = dmi_get_system_info ( DMI_PRODUCT_NAME ); <nl> dmi_sys_vendor = dmi_get_system_info ( DMI_SYS_VENDOR ); <nl> + if (! dmi_product_name || ! dmi_sys_vendor ) <nl> + return false ; <nl>  <nl> if (!( strstr ( dmi_sys_vendor , " Hewlett - Packard "))) <nl> return false ;
nouveau_connector_helper_funcs = { <nl> static const struct drm_connector_funcs <nl> nouveau_connector_funcs = { <nl> . dpms = drm_helper_connector_dpms , <nl> - . save = NULL , <nl> - . restore = NULL , <nl> . detect = nouveau_connector_detect , <nl> . destroy = nouveau_connector_destroy , <nl> . fill_modes = drm_helper_probe_single_connector_modes , <nl> nouveau_connector_funcs = { <nl> static const struct drm_connector_funcs <nl> nouveau_connector_funcs_lvds = { <nl> . dpms = drm_helper_connector_dpms , <nl> - . save = NULL , <nl> - . restore = NULL , <nl> . detect = nouveau_connector_detect_lvds , <nl> . destroy = nouveau_connector_destroy , <nl> . fill_modes = drm_helper_probe_single_connector_modes , <nl> nouveau_connector_dp_dpms ( struct drm_connector * connector , int mode ) <nl> static const struct drm_connector_funcs <nl> nouveau_connector_funcs_dp = { <nl> . dpms = nouveau_connector_dp_dpms , <nl> - . save = NULL , <nl> - . restore = NULL , <nl> . detect = nouveau_connector_detect , <nl> . destroy = nouveau_connector_destroy , <nl> . fill_modes = drm_helper_probe_single_connector_modes ,
static ssize_t iio_ring_rip_outer ( struct file * filp , char __user * buf , <nl> return - EINVAL ; <nl> copied = rb -> access . rip_lots ( rb , count , & data , & dead_offset ); <nl>  <nl> - if ( copied < 0 ) { <nl> + if ( copied <= 0 ) { <nl> ret = copied ; <nl> goto error_ret ; <nl> }
static int dr_interception ( struct vcpu_svm * svm ) <nl> kvm_register_write (& svm -> vcpu , reg , val ); <nl> } <nl>  <nl> + skip_emulated_instruction (& svm -> vcpu ); <nl> + <nl> return 1 ; <nl> } <nl> 
static const struct attribute_group isl29108_group = { <nl> static int isl29018_chip_init ( struct isl29018_chip * chip ) <nl> { <nl> int status ; <nl> - int new_adc_bit ; <nl> + unsigned int new_adc_bit ; <nl> unsigned int new_range ; <nl>  <nl> /* Code added per Intersil Application Note 1534 :
enum pm_qos_req_action { <nl>  <nl> static inline int dev_pm_qos_request_active ( struct dev_pm_qos_request * req ) <nl> { <nl> - return req -> dev != 0 ; <nl> + return req -> dev != NULL ; <nl> } <nl>  <nl> int pm_qos_update_target ( struct pm_qos_constraints * c , struct plist_node * node ,
static int corsair_input_mapping ( struct hid_device * dev , <nl> { <nl> int gkey ; <nl>  <nl> + if (( usage -> hid & HID_USAGE_PAGE ) != HID_UP_KEYBOARD ) <nl> + return 0 ; <nl> + <nl> gkey = corsair_usage_to_gkey ( usage -> hid & HID_USAGE ); <nl> if ( gkey != 0 ) { <nl> hid_map_usage_clear ( input , usage , bit , max , EV_KEY ,
xfs_itruncate_start ( <nl> mp = ip -> i_mount ; <nl>  <nl> /* wait for the completion of any pending DIOs */ <nl> - if ( new_size < ip -> i_size ) <nl> + if ( new_size == 0 || new_size < ip -> i_size ) <nl> vn_iowait ( ip ); <nl>  <nl> /*
static int ptp_dp83640_enable ( struct ptp_clock_info * ptp , <nl> if ( on ) { <nl> gpio_num = gpio_tab [ EXTTS0_GPIO + index ]; <nl> evnt |= ( gpio_num & EVNT_GPIO_MASK ) << EVNT_GPIO_SHIFT ; <nl> - evnt |= EVNT_RISE ; <nl> + if ( rq -> extts . flags & PTP_FALLING_EDGE ) <nl> + evnt |= EVNT_FALL ; <nl> + else <nl> + evnt |= EVNT_RISE ; <nl> } <nl> ext_write ( 0 , phydev , PAGE5 , PTP_EVNT , evnt ); <nl> return 0 ;
static int do_end_io ( struct multipath * m , struct request * clone , <nl> if (! error && ! clone -> errors ) <nl> return 0 ; /* I / O complete */ <nl>  <nl> - if ( error == - EOPNOTSUPP || error == - EREMOTEIO ) <nl> + if ( error == - EOPNOTSUPP || error == - EREMOTEIO || error == - EILSEQ ) <nl> return error ; <nl>  <nl> if ( mpio -> pgpath )
int snd_hda_parse_pin_def_config ( struct hda_codec * codec , <nl> cfg -> input_pins [ AUTO_PIN_AUX ] = nid ; <nl> break ; <nl> case AC_JACK_SPDIF_OUT : <nl> + case AC_JACK_DIG_OTHER_OUT : <nl> cfg -> dig_out_pin = nid ; <nl> break ; <nl> case AC_JACK_SPDIF_IN : <nl> + case AC_JACK_DIG_OTHER_IN : <nl> cfg -> dig_in_pin = nid ; <nl> break ; <nl> }
out : <nl>  <nl> static int jfs_ci_revalidate ( struct dentry * dentry , struct nameidata * nd ) <nl> { <nl> - if ( nd -> flags & LOOKUP_RCU ) <nl> + if ( nd && nd -> flags & LOOKUP_RCU ) <nl> return - ECHILD ; <nl> /* <nl> * This is not negative dentry . Always valid .
static const char * sky2_name ( u8 chipid , char * buf , int sz ) <nl> " Optima ", /* 0xbc */ <nl> }; <nl>  <nl> - if ( chipid >= CHIP_ID_YUKON_XL && chipid < CHIP_ID_YUKON_OPT ) <nl> + if ( chipid >= CHIP_ID_YUKON_XL && chipid <= CHIP_ID_YUKON_OPT ) <nl> strncpy ( buf , name [ chipid - CHIP_ID_YUKON_XL ], sz ); <nl> else <nl> snprintf ( buf , sz , "( chip %# x )", chipid );
cputime_to_timeval ( const cputime_t cputime , struct timeval * value ) <nl> value -> tv_usec = rp . subreg . even / 4096 ; <nl> value -> tv_sec = rp . subreg . odd ; <nl> # else <nl> - value -> tv_usec = cputime % 4096000000ULL ; <nl> + value -> tv_usec = ( cputime % 4096000000ULL ) / 4096 ; <nl> value -> tv_sec = cputime / 4096000000ULL ; <nl> # endif <nl> }
static int aes_cipher ( u8 * key , uint hdrlen , u8 * pframe , uint plen ) <nl> num_blocks = plen / 16 ; <nl>  <nl> /* Find start of payload */ <nl> - payload_index = ( hdrlen + 8 ); <nl> + payload_index = hdrlen + 8 ; <nl>  <nl> /* Calculate MIC */ <nl> aes128k128d ( key , mic_iv , aes_out ); <nl> static int aes_decipher ( u8 * key , uint hdrlen , <nl> num_blocks = ( plen - 8 ) / 16 ; <nl>  <nl> /* Find start of payload */ <nl> - payload_index = ( hdrlen + 8 ); <nl> + payload_index = hdrlen + 8 ; <nl>  <nl> /* Calculate MIC */ <nl> aes128k128d ( key , mic_iv , aes_out );
static irqreturn_t et131x_isr ( int irq , void * dev_id ) <nl> { <nl> bool handled = true ; <nl> bool enable_interrupts = true ; <nl> - struct net_device * netdev = ( struct net_device *) dev_id ; <nl> + struct net_device * netdev = dev_id ; <nl> struct et131x_adapter * adapter = netdev_priv ( netdev ); <nl> struct address_map __iomem * iomem = adapter -> regs ; <nl> struct rx_ring * rx_ring = & adapter -> rx_ring ;
spider_net_stop ( struct net_device * netdev ) <nl> /* release chains */ <nl> spider_net_release_tx_chain ( card , 1 ); <nl>  <nl> + spider_net_free_rx_chain_contents ( card ); <nl> + <nl> spider_net_free_chain ( card , & card -> tx_chain ); <nl> spider_net_free_chain ( card , & card -> rx_chain ); <nl> 
static int __devinit cs5520_init_one ( struct pci_dev * dev , const struct pci_devic <nl>  <nl> /* We must not grab the entire device , it has ' ISA ' space in its <nl> BARS too and we will freak out other bits of the kernel */ <nl> - if ( pci_enable_device_bars ( dev , 1 << 2 )) <nl> - { <nl> + if ( pci_enable_device_bars ( dev , 1 << 2 )) { <nl> printk ( KERN_WARNING "% s : Unable to enable 55x0 .\ n ", d -> name ); <nl> - return 1 ; <nl> + return - ENODEV ; <nl> } <nl> pci_set_master ( dev ); <nl> if ( pci_set_dma_mask ( dev , DMA_32BIT_MASK )) {
static int pstore_unlink ( struct inode * dir , struct dentry * dentry ) <nl> struct pstore_private * p = dentry -> d_inode -> i_private ; <nl>  <nl> p -> erase ( p -> id ); <nl> - kfree ( p ); <nl>  <nl> return simple_unlink ( dir , dentry ); <nl> } <nl>  <nl> + static void pstore_evict_inode ( struct inode * inode ) <nl> +{ <nl> + end_writeback ( inode ); <nl> + kfree ( inode -> i_private ); <nl> +} <nl> + <nl> static const struct inode_operations pstore_dir_inode_operations = { <nl> . lookup = simple_lookup , <nl> . unlink = pstore_unlink , <nl> static struct inode * pstore_get_inode ( struct super_block * sb , <nl> static const struct super_operations pstore_ops = { <nl> . statfs = simple_statfs , <nl> . drop_inode = generic_delete_inode , <nl> + . evict_inode = pstore_evict_inode , <nl> . show_options = generic_show_options , <nl> }; <nl> 
static int generic_set_freq ( struct dvb_frontend * fe , <nl> goto err ; <nl>  <nl> rc = r820t_sysfreq_sel ( priv , freq , type , std , delsys ); <nl> + if ( rc < 0 ) <nl> + goto err ; <nl> + <nl> + tuner_dbg ("% s : PLL locked on frequency % d Hz , gain =% d \ n ", <nl> + __func__ , freq , r820t_read_gain ( priv )); <nl> + <nl> err : <nl>  <nl> if ( rc < 0 )
xfs_ioc_trim ( <nl>  <nl> if (! capable ( CAP_SYS_ADMIN )) <nl> return - XFS_ERROR ( EPERM ); <nl> + if (! blk_queue_discard ( q )) <nl> + return - XFS_ERROR ( EOPNOTSUPP ); <nl> if ( copy_from_user (& range , urange , sizeof ( range ))) <nl> return - XFS_ERROR ( EFAULT ); <nl> 
static int __devinit synaptics_rmi4_probe <nl> retval = input_register_device ( rmi4_data -> input_dev ); <nl> if ( retval ) { <nl> dev_err (& client -> dev , "% s : input register failed \ n ", __func__ ); <nl> - goto err_input_register ; <nl> + goto err_query_dev ; <nl> } <nl>  <nl> /* Clear interrupts */ <nl> static int __devinit synaptics_rmi4_probe <nl> err_request_irq : <nl> free_irq ( platformdata -> irq_number , rmi4_data ); <nl> input_unregister_device ( rmi4_data -> input_dev ); <nl> - err_input_register : <nl> - i2c_set_clientdata ( client , NULL ); <nl> err_query_dev : <nl> if ( platformdata -> regulator_en ) { <nl> regulator_disable ( rmi4_data -> regulator );
static int __init crossbar_of_init ( struct device_node * node ) <nl> int i , size , max , reserved = 0 , entry ; <nl> const __be32 * irqsr ; <nl>  <nl> - cb = kzalloc ( sizeof ( struct cb_device *), GFP_KERNEL ); <nl> + cb = kzalloc ( sizeof (* cb ), GFP_KERNEL ); <nl>  <nl> if (! cb ) <nl> return - ENOMEM ;
static ssize_t channel_dimm_label_show ( struct device * dev , <nl> if (! rank -> dimm -> label [ 0 ]) <nl> return 0 ; <nl>  <nl> - return snprintf ( data , EDAC_MC_LABEL_LEN , "% s \ n ", <nl> + return snprintf ( data , sizeof ( rank -> dimm -> label ) + 1 , "% s \ n ", <nl> rank -> dimm -> label ); <nl> } <nl>  <nl> static ssize_t dimmdev_label_show ( struct device * dev , <nl> if (! dimm -> label [ 0 ]) <nl> return 0 ; <nl>  <nl> - return snprintf ( data , EDAC_MC_LABEL_LEN , "% s \ n ", dimm -> label ); <nl> + return snprintf ( data , sizeof ( dimm -> label ) + 1 , "% s \ n ", dimm -> label ); <nl> } <nl>  <nl> static ssize_t dimmdev_label_store ( struct device * dev ,
int mthca_init_db_tab ( struct mthca_dev * dev ) <nl>  <nl> init_MUTEX (& dev -> db_tab -> mutex ); <nl>  <nl> - dev -> db_tab -> npages = dev -> uar_table . uarc_size / PAGE_SIZE ; <nl> + dev -> db_tab -> npages = dev -> uar_table . uarc_size / 4096 ; <nl> dev -> db_tab -> max_group1 = 0 ; <nl> dev -> db_tab -> min_group2 = dev -> db_tab -> npages - 1 ; <nl> 
struct generic_pm_domain * of_genpd_get_from_provider ( <nl> struct generic_pm_domain * genpd = ERR_PTR (- ENOENT ); <nl> struct of_genpd_provider * provider ; <nl>  <nl> + if (! genpdspec ) <nl> + return ERR_PTR (- EINVAL ); <nl> + <nl> mutex_lock (& of_genpd_mutex ); <nl>  <nl> /* Check if we have such a provider in our array */
static int pmic8xxx_pwrkey_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> free_press_irq : <nl> - free_irq ( key_press_irq , NULL ); <nl> + free_irq ( key_press_irq , pwrkey ); <nl> unreg_input_dev : <nl> input_unregister_device ( pwr ); <nl> pwr = NULL ;
static int __init imx2_wdt_probe ( struct platform_device * pdev ) <nl> wdog -> max_timeout = IMX2_WDT_MAX_TIME ; <nl> wdog -> parent = & pdev -> dev ; <nl>  <nl> - clk_prepare_enable ( wdev -> clk ); <nl> + ret = clk_prepare_enable ( wdev -> clk ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> regmap_read ( wdev -> regmap , IMX2_WDT_WRSR , & val ); <nl> wdog -> bootstatus = val & IMX2_WDT_WRSR_TOUT ? WDIOF_CARDRESET : 0 ; <nl> static int imx2_wdt_resume ( struct device * dev ) <nl> { <nl> struct watchdog_device * wdog = dev_get_drvdata ( dev ); <nl> struct imx2_wdt_device * wdev = watchdog_get_drvdata ( wdog ); <nl> + int ret ; <nl>  <nl> - clk_prepare_enable ( wdev -> clk ); <nl> + ret = clk_prepare_enable ( wdev -> clk ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> if ( watchdog_active ( wdog ) && ! imx2_wdt_is_running ( wdev )) { <nl> /*
void laptop_mode_timer_fn ( unsigned long data ) <nl> if (! bdi_has_dirty_io (& q -> backing_dev_info )) <nl> return ; <nl>  <nl> + rcu_read_lock (); <nl> bdi_for_each_wb ( wb , & q -> backing_dev_info , & iter , 0 ) <nl> if ( wb_has_dirty_io ( wb )) <nl> wb_start_writeback ( wb , nr_pages , true , <nl> WB_REASON_LAPTOP_TIMER ); <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> /*
int __devinit av7110_ir_init ( struct av7110 * av7110 ) <nl> if ( av_cnt >= ARRAY_SIZE ( av_list )) <nl> return - ENOSPC ; <nl>  <nl> - av7110_check_ir_config ( av7110 , true ); <nl> av_list [ av_cnt ++] = av7110 ; <nl> + av7110_check_ir_config ( av7110 , true ); <nl>  <nl> init_timer (& av7110 -> ir . keyup_timer ); <nl> av7110 -> ir . keyup_timer . function = av7110_emit_keyup ;
asmlinkage long compat_sys_nfsservctl ( int cmd , struct compat_nfsctl_arg __user * <nl>  <nl> default : <nl> err = - EINVAL ; <nl> - goto done ; <nl> + break ; <nl> } <nl>  <nl> + if ( err ) <nl> + goto done ; <nl> + <nl> oldfs = get_fs (); <nl> set_fs ( KERNEL_DS ); <nl> /* The __user pointer casts are valid because of the set_fs () */
static ssize_t <nl> v9fs_file_read_iter ( struct kiocb * iocb , struct iov_iter * to ) <nl> { <nl> struct p9_fid * fid = iocb -> ki_filp -> private_data ; <nl> - int ret , err ; <nl> + int ret , err = 0 ; <nl>  <nl> p9_debug ( P9_DEBUG_VFS , " count % zu offset % lld \ n ", <nl> iov_iter_count ( to ), iocb -> ki_pos );
static int add_new_gdb ( handle_t * handle , struct inode * inode , <nl> return err ; <nl>  <nl> exit_inode : <nl> + kfree ( n_group_desc ); <nl> /* ext4_handle_release_buffer ( handle , iloc . bh ); */ <nl> brelse ( iloc . bh ); <nl> exit_dindj :
static netdev_tx_t rtl8139_start_xmit ( struct sk_buff * skb , <nl> if ( len < ETH_ZLEN ) <nl> memset ( tp -> tx_buf [ entry ], 0 , ETH_ZLEN ); <nl> skb_copy_and_csum_dev ( skb , tp -> tx_buf [ entry ]); <nl> - dev_kfree_skb ( skb ); <nl> + dev_kfree_skb_any ( skb ); <nl> } else { <nl> - dev_kfree_skb ( skb ); <nl> + dev_kfree_skb_any ( skb ); <nl> dev -> stats . tx_dropped ++; <nl> return NETDEV_TX_OK ; <nl> }
static int ibmvfc_get_err_result ( struct ibmvfc_cmd * vfc_cmd ) <nl> int fc_rsp_len = rsp -> fcp_rsp_len ; <nl>  <nl> if (( rsp -> flags & FCP_RSP_LEN_VALID ) && <nl> - ((! fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> + (( fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> rsp -> data . info . rsp_code )) <nl> return DID_ERROR << 16 ; <nl> 
__acquires ( musb -> lock ) <nl> musb -> g . a_alt_hnp_support = 1 ; <nl> break ; <nl> # endif <nl> + case USB_DEVICE_DEBUG_MODE : <nl> + handled = 0 ; <nl> + break ; <nl> stall : <nl> default : <nl> handled = - EINVAL ;
static DEFINE_MUTEX ( nb_smu_ind_mutex ); <nl> * Control ] <nl> */ <nl> # define F15H_M60H_REPORTED_TEMP_CTRL_OFFSET 0xd8200ca4 <nl> -# define PCI_DEVICE_ID_AMD_15H_M60H_NB_F3 0x1573 <nl>  <nl> static void amd_nb_smu_index_read ( struct pci_dev * pdev , unsigned int devfn , <nl> int offset , u32 * val )
static int lpc18xx_create_group_func_map ( struct device * dev , <nl> u16 pins [ ARRAY_SIZE ( lpc18xx_pins )]; <nl> int func , ngroups , i ; <nl>  <nl> - for ( func = 0 ; func < FUNC_MAX ; ngroups = 0 , func ++) { <nl> - <nl> - for ( i = 0 ; i < ARRAY_SIZE ( lpc18xx_pins ); i ++) { <nl> + for ( func = 0 ; func < FUNC_MAX ; func ++) { <nl> + for ( ngroups = 0 , i = 0 ; i < ARRAY_SIZE ( lpc18xx_pins ); i ++) { <nl> if ( lpc18xx_valid_pin_function ( i , func )) <nl> pins [ ngroups ++] = i ; <nl> }
int esas2r_ioctl_handler ( void * hostdata , int cmd , void __user * arg ) <nl>  <nl> rq = esas2r_alloc_request ( a ); <nl> if ( rq == NULL ) { <nl> - up (& a -> nvram_semaphore ); <nl> - ioctl -> data . prw . code = 0 ; <nl> - break ; <nl> + kfree ( ioctl ); <nl> + esas2r_log ( ESAS2R_LOG_WARN , <nl> + " could not allocate an internal request "); <nl> + return - ENOMEM ; <nl> } <nl>  <nl> code = esas2r_write_params ( a , rq ,
static int arizona_hpdet_read ( struct arizona_extcon_info * info ) <nl> >> ARIZONA_HP_IMPEDANCE_RANGE_SHIFT ; <nl>  <nl> if ( range < ARRAY_SIZE ( arizona_hpdet_b_ranges ) - 1 && <nl> - ( val < 100 || val > 0x3fb )) { <nl> + ( val < 100 || val >= 0x3fb )) { <nl> range ++; <nl> dev_dbg ( arizona -> dev , " Moving to HPDET range % d \ n ", <nl> range ); <nl> static int arizona_hpdet_read ( struct arizona_extcon_info * info ) <nl> } <nl>  <nl> /* If we go out of range report top of range */ <nl> - if ( val < 100 || val > 0x3fb ) { <nl> + if ( val < 100 || val >= 0x3fb ) { <nl> dev_dbg ( arizona -> dev , " Measurement out of range \ n "); <nl> return ARIZONA_HPDET_MAX ; <nl> }
static int receive ( struct sk_buff * skb , struct net_device * dev , <nl> { <nl> struct cfpkt * pkt ; <nl> struct caif_device_entry * caifd ; <nl> + int err ; <nl>  <nl> pkt = cfpkt_fromnative ( CAIF_DIR_IN , skb ); <nl>  <nl> static int receive ( struct sk_buff * skb , struct net_device * dev , <nl> caifd_hold ( caifd ); <nl> rcu_read_unlock (); <nl>  <nl> - caifd -> layer . up -> receive ( caifd -> layer . up , pkt ); <nl> + err = caifd -> layer . up -> receive ( caifd -> layer . up , pkt ); <nl> + <nl> + /* For - EILSEQ the packet is not freed so so it now */ <nl> + if ( err == - EILSEQ ) <nl> + cfpkt_destroy ( pkt ); <nl>  <nl> /* Release reference to stack upwards */ <nl> caifd_put ( caifd );
static void handle_reg_beacon ( struct wiphy * wiphy , <nl> if ( likely ( chan -> center_freq != reg_beacon -> chan . center_freq )) <nl> return ; <nl>  <nl> - if ( chan -> flags & IEEE80211_CHAN_PASSIVE_SCAN ) { <nl> + if (( chan -> flags & IEEE80211_CHAN_PASSIVE_SCAN ) && <nl> + !( chan -> orig_flags & IEEE80211_CHAN_PASSIVE_SCAN )) { <nl> chan -> flags &= ~ IEEE80211_CHAN_PASSIVE_SCAN ; <nl> REG_DEBUG_BEACON_FLAG (" active scanning "); <nl> } <nl>  <nl> - if ( chan -> flags & IEEE80211_CHAN_NO_IBSS ) { <nl> + if (( chan -> flags & IEEE80211_CHAN_NO_IBSS ) && <nl> + !( chan -> orig_flags & IEEE80211_CHAN_NO_IBSS )) { <nl> chan -> flags &= ~ IEEE80211_CHAN_NO_IBSS ; <nl> REG_DEBUG_BEACON_FLAG (" beaconing "); <nl> }
static int dn_shutdown ( struct socket * sock , int how ) <nl> if ( scp -> state == DN_O ) <nl> goto out ; <nl>  <nl> - if ( how != SHUTDOWN_MASK ) <nl> + if ( how != SHUT_RDWR ) <nl> goto out ; <nl>  <nl> - sk -> sk_shutdown = how ; <nl> + sk -> sk_shutdown = SHUTDOWN_MASK ; <nl> dn_destroy_sock ( sk ); <nl> err = 0 ; <nl> 
int i915_gem_init_stolen ( struct drm_device * dev ) <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> int bios_reserved = 0 ; <nl>  <nl> + if ( dev_priv -> gtt . stolen_size == 0 ) <nl> + return 0 ; <nl> + <nl> dev_priv -> mm . stolen_base = i915_stolen_to_physical ( dev ); <nl> if ( dev_priv -> mm . stolen_base == 0 ) <nl> return 0 ;
static int __devinit cobalt_raq_led_probe ( struct platform_device * pdev ) <nl> if (! res ) <nl> return - EBUSY ; <nl>  <nl> - led_port = ioremap ( res -> start , res -> end - res -> start + 1 ); <nl> + led_port = ioremap ( res -> start , resource_size ( res )); <nl> if (! led_port ) <nl> return - ENOMEM ; <nl> 
void flush_thread ( void ) <nl>  <nl> void start_thread ( struct pt_regs * regs , unsigned long eip , unsigned long esp ) <nl> { <nl> + get_safe_registers ( regs -> regs . gp , regs -> regs . fp ); <nl> PT_REGS_IP ( regs ) = eip ; <nl> PT_REGS_SP ( regs ) = esp ; <nl> current -> ptrace &= ~ PT_DTRACE ;
int bnx2i_send_iscsi_nopout ( struct bnx2i_conn * bnx2i_conn , <nl> bnx2i_cmd = ( struct bnx2i_cmd *) task -> dd_data ; <nl> nopout_hdr = ( struct iscsi_nopout *) task -> hdr ; <nl> nopout_wqe = ( struct bnx2i_nop_out_request *) ep -> qp . sq_prod_qe ; <nl> + <nl> + memset ( nopout_wqe , 0x00 , sizeof ( struct bnx2i_nop_out_request )); <nl> + <nl> nopout_wqe -> op_code = nopout_hdr -> opcode ; <nl> nopout_wqe -> op_attr = ISCSI_FLAG_CMD_FINAL ; <nl> memcpy ( nopout_wqe -> lun , nopout_hdr -> lun , 8 );
int sensor_hub_set_feature ( struct hid_sensor_hub_device * hsdev , u32 report_id , <nl> if ( buffer_size ) { <nl> for ( i = 0 ; i < buffer_size ; ++ i ) { <nl> hid_set_field ( report -> field [ field_index ], i , <nl> - cpu_to_le32 (* buf32 )); <nl> + ( __force __s32 ) cpu_to_le32 (* buf32 )); <nl> ++ buf32 ; <nl> } <nl> } <nl> int sensor_hub_set_feature ( struct hid_sensor_hub_device * hsdev , u32 report_id , <nl> value = 0 ; <nl> memcpy (& value , ( u8 *) buf32 , remaining_bytes ); <nl> hid_set_field ( report -> field [ field_index ], i , <nl> - cpu_to_le32 ( value )); <nl> + ( __force __s32 ) cpu_to_le32 ( value )); <nl> } <nl> hid_hw_request ( hsdev -> hdev , report , HID_REQ_SET_REPORT ); <nl> hid_hw_wait ( hsdev -> hdev );
EXPORT_SYMBOL_GPL ( irq_of_parse_and_map ); <nl>  <nl> void irq_dispose_mapping ( unsigned int virq ) <nl> { <nl> - struct irq_host * host = irq_map [ virq ]. host ; <nl> + struct irq_host * host ; <nl> irq_hw_number_t hwirq ; <nl> unsigned long flags ; <nl>  <nl> + if ( virq == NO_IRQ ) <nl> + return ; <nl> + <nl> + host = irq_map [ virq ]. host ; <nl> WARN_ON ( host == NULL ); <nl> if ( host == NULL ) <nl> return ;
int r100_cs_parse ( struct radeon_cs_parser * p ) <nl> int r ; <nl>  <nl> track = kzalloc ( sizeof (* track ), GFP_KERNEL ); <nl> + if (! track ) <nl> + return - ENOMEM ; <nl> r100_cs_track_clear ( p -> rdev , track ); <nl> p -> track = track ; <nl> do {
void reset_vma_resv_huge_pages ( struct vm_area_struct * vma ) <nl> /* Returns true if the VMA has associated reserve pages */ <nl> static int vma_has_reserves ( struct vm_area_struct * vma ) <nl> { <nl> + if ( vma -> vm_flags & VM_NORESERVE ) <nl> + return 0 ; <nl> if ( vma -> vm_flags & VM_MAYSHARE ) <nl> return 1 ; <nl> if ( is_vma_resv_set ( vma , HPAGE_RESV_OWNER ))
static int readable ( struct pcmcia_socket * s , struct resource * res , <nl> destroy_cis_cache ( s ); <nl> } <nl> s -> cis_mem . res = NULL ; <nl> - if (( ret != 0 ) || ( count == 0 )) <nl> + if (( ret != 0 ) || (* count == 0 )) <nl> return 0 ; <nl> return 1 ; <nl> }
static void iwlagn_tx_status ( struct iwl_priv * priv , struct sk_buff * skb ) <nl> struct ieee80211_sta * sta ; <nl> struct iwl_station_priv * sta_priv ; <nl>  <nl> + rcu_read_lock (); <nl> sta = ieee80211_find_sta ( priv -> vif , hdr -> addr1 ); <nl> if ( sta ) { <nl> sta_priv = ( void *) sta -> drv_priv ; <nl> static void iwlagn_tx_status ( struct iwl_priv * priv , struct sk_buff * skb ) <nl> atomic_dec_return (& sta_priv -> pending_frames ) == 0 ) <nl> ieee80211_sta_block_awake ( priv -> hw , sta , false ); <nl> } <nl> + rcu_read_unlock (); <nl>  <nl> ieee80211_tx_status_irqsafe ( priv -> hw , skb ); <nl> }
void __init init_bsp_APIC ( void ) <nl> /** <nl> * setup_local_APIC - setup the local APIC <nl> */ <nl> - void __devinit setup_local_APIC ( void ) <nl> + void __cpuinit setup_local_APIC ( void ) <nl> { <nl> unsigned long oldvalue , value , maxlvt , integrated ; <nl> int i , j ;
static struct hw_breakpoint { <nl> unsigned long addr ; <nl> int len ; <nl> int type ; <nl> - struct perf_event ** pev ; <nl> + struct perf_event * __percpu * pev ; <nl> } breakinfo [ HBP_NUM ]; <nl>  <nl> static unsigned long early_dr7 ;
static void i915_gem_dmabuf_kunmap ( struct dma_buf * dma_buf , unsigned long page_n <nl>  <nl> } <nl>  <nl> + static int i915_gem_dmabuf_mmap ( struct dma_buf * dma_buf , struct vm_area_struct * vma ) <nl> +{ <nl> + return - EINVAL ; <nl> +} <nl> + <nl> static const struct dma_buf_ops i915_dmabuf_ops = { <nl> . map_dma_buf = i915_gem_map_dma_buf , <nl> . unmap_dma_buf = i915_gem_unmap_dma_buf , <nl> static const struct dma_buf_ops i915_dmabuf_ops = { <nl> . kmap_atomic = i915_gem_dmabuf_kmap_atomic , <nl> . kunmap = i915_gem_dmabuf_kunmap , <nl> . kunmap_atomic = i915_gem_dmabuf_kunmap_atomic , <nl> + . mmap = i915_gem_dmabuf_mmap , <nl> }; <nl>  <nl> struct dma_buf * i915_gem_prime_export ( struct drm_device * dev ,
static int gfar_start_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> kfree_skb ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl> + <nl> + /* Steal sock reference for processing TX time stamps */ <nl> + swap ( skb_new -> sk , skb -> sk ); <nl> + swap ( skb_new -> destructor , skb -> destructor ); <nl> kfree_skb ( skb ); <nl> skb = skb_new ; <nl> }
struct extent_buffer * alloc_extent_buffer ( struct extent_io_tree * tree , <nl> spin_unlock (& tree -> buffer_lock ); <nl> goto free_eb ; <nl> } <nl> - spin_unlock (& tree -> buffer_lock ); <nl> - <nl> /* add one reference for the tree */ <nl> atomic_inc (& eb -> refs ); <nl> + spin_unlock (& tree -> buffer_lock ); <nl> return eb ; <nl>  <nl> free_eb :
static struct qdisc_size_table * qdisc_get_stab ( struct nlattr * opt ) <nl> tsize = nla_len ( tb [ TCA_STAB_DATA ]) / sizeof ( u16 ); <nl> } <nl>  <nl> - if (! s || tsize != s -> tsize || (! tab && tsize > 0 )) <nl> + if ( tsize != s -> tsize || (! tab && tsize > 0 )) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> spin_lock (& qdisc_stab_lock );
int ide_device_add ( u8 idx [ 4 ]) <nl>  <nl> hwif = & ide_hwifs [ idx [ i ]]; <nl>  <nl> - if ( hwif -> present ) <nl> + if ( hwif -> present ) { <nl> + if ( hwif -> chipset == ide_unknown || <nl> + hwif -> chipset == ide_forced ) <nl> + hwif -> chipset = ide_generic ; <nl> hwif_register_devices ( hwif ); <nl> + } <nl> } <nl>  <nl> for ( i = 0 ; i < 4 ; i ++) {
static void __devinit check_probe_mask ( struct azx * chip , int dev ) <nl> * white / black - list for enable_msi <nl> */ <nl> static struct snd_pci_quirk msi_black_list [] __devinitdata = { <nl> + SND_PCI_QUIRK ( 0x1043 , 0x81f2 , " ASUS ", 0 ), /* Athlon64 X2 + nvidia */ <nl> {} <nl> }; <nl> 
static void xlr_make_tx_desc ( struct nlm_fmn_msg * msg , unsigned long addr , <nl> (( u64 ) fr_stn_id << 54 ) | /* Free back id */ <nl> ( u64 ) 0 << 40 | /* Set len to 0 */ <nl> (( u64 ) physkb & 0xffffffff )); /* 32bit address */ <nl> - msg -> msg2 = msg -> msg3 = 0 ; <nl> + msg -> msg2 = 0 ; <nl> + msg -> msg3 = 0 ; <nl> } <nl>  <nl> static void __maybe_unused xlr_wakeup_queue ( unsigned long dev )
static void wlcore_op_stop_locked ( struct wl1271 * wl ) <nl>  <nl> /* <nl> * FW channels must be re - calibrated after recovery , <nl> - * clear the last Reg - Domain channel configuration . <nl> + * save current Reg - Domain channel configuration and clear it . <nl> */ <nl> + memcpy ( wl -> reg_ch_conf_pending , wl -> reg_ch_conf_last , <nl> + sizeof ( wl -> reg_ch_conf_pending )); <nl> memset ( wl -> reg_ch_conf_last , 0 , sizeof ( wl -> reg_ch_conf_last )); <nl> } <nl> 
void __init clocksource_of_init ( void ) <nl> clocksource_of_init_fn init_func ; <nl>  <nl> for_each_matching_node_and_match ( np , __clksrc_of_table , & match ) { <nl> + if (! of_device_is_available ( np )) <nl> + continue ; <nl> + <nl> init_func = match -> data ; <nl> init_func ( np ); <nl> }
int __pci_read_base ( struct pci_dev * dev , enum pci_bar_type type , <nl> /* Address above 32 - bit boundary ; disable the BAR */ <nl> pci_write_config_dword ( dev , pos , 0 ); <nl> pci_write_config_dword ( dev , pos + 4 , 0 ); <nl> + res -> flags |= IORESOURCE_UNSET ; <nl> region . start = 0 ; <nl> region . end = sz64 ; <nl> bar_disabled = true ;
struct vm_struct * alloc_vm_area ( size_t size ) <nl> return NULL ; <nl> } <nl>  <nl> - /* Make sure the pagetables are constructed in process kernel <nl> - mappings */ <nl> - vmalloc_sync_all (); <nl> - <nl> return area ; <nl> } <nl> EXPORT_SYMBOL_GPL ( alloc_vm_area );
int xfrm_init_replay ( struct xfrm_state * x ) <nl> replay_esn -> bmp_len * sizeof ( __u32 ) * 8 ) <nl> return - EINVAL ; <nl>  <nl> + if (( x -> props . flags & XFRM_STATE_ESN ) && replay_esn -> replay_window == 0 ) <nl> + return - EINVAL ; <nl> + <nl> if (( x -> props . flags & XFRM_STATE_ESN ) && x -> replay_esn ) <nl> x -> repl = & xfrm_replay_esn ; <nl> else
static int xfrm_get_policy ( struct sk_buff * skb , struct nlmsghdr * nlh , void ** xfr <nl> MSG_DONTWAIT ); <nl> } <nl> } else { <nl> + c . data . byid = p -> index ; <nl> c . event = XFRM_SAP_DELETED ; <nl> c . seq = nlh -> nlmsg_seq ; <nl> c . pid = nlh -> nlmsg_pid ;
struct vpif_disp_buffer { <nl> }; <nl>  <nl> struct common_obj { <nl> - /* Buffer specific parameters */ <nl> - u8 * fbuffers [ VIDEO_MAX_FRAME ]; /* List of buffer pointers for <nl> - * storing frames */ <nl> u32 numbuffers ; /* number of buffers */ <nl> struct vpif_disp_buffer * cur_frm ; /* Pointer pointing to current <nl> * vb2_buffer */
static int rt2800_init_registers ( struct rt2x00_dev * rt2x00dev ) <nl>  <nl> rt2800_register_read ( rt2x00dev , MM40_PROT_CFG , & reg ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_RATE , 0x4084 ); <nl> - rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_CTRL , <nl> - ! rt2x00_is_usb ( rt2x00dev )); <nl> + rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_CTRL , 0 ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_NAV , 1 ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_TX_OP_ALLOW_CCK , 1 ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_TX_OP_ALLOW_OFDM , 1 );
static inline u32 ext4_chksum ( struct ext4_sb_info * sbi , u32 crc , <nl> { <nl> struct { <nl> struct shash_desc shash ; <nl> - char ctx [ crypto_shash_descsize ( sbi -> s_chksum_driver )]; <nl> + char ctx [ 4 ]; <nl> } desc ; <nl> int err ; <nl>  <nl> + BUG_ON ( crypto_shash_descsize ( sbi -> s_chksum_driver )!= sizeof ( desc . ctx )); <nl> + <nl> desc . shash . tfm = sbi -> s_chksum_driver ; <nl> desc . shash . flags = 0 ; <nl> *( u32 *) desc . ctx = crc ;
ULTRA_check_channel_client ( void __iomem * pChannel , <nl> if ( uuid_le_cmp ( expectedTypeGuid , NULL_UUID_LE ) != 0 ) { <nl> uuid_le guid ; <nl>  <nl> - ioread8_rep (&(( CHANNEL_HEADER __iomem *)( pChannel ))-> Type , <nl> - & guid , sizeof ( guid )); <nl> + memcpy_fromio (& guid , <nl> + &(( CHANNEL_HEADER __iomem *)( pChannel ))-> Type , <nl> + sizeof ( guid )); <nl> /* caller wants us to verify type GUID */ <nl> if ( uuid_le_cmp ( guid , expectedTypeGuid ) != 0 ) { <nl> CHANNEL_GUID_MISMATCH ( expectedTypeGuid , channelName , <nl> " type ", expectedTypeGuid , <nl> - (( CHANNEL_HEADER __iomem *) <nl> - ( pChannel ))-> Type , fileName , <nl> + guid , fileName , <nl> lineNumber , logCtx ); <nl> return 0 ; <nl> }
static int b43_try_request_fw ( struct b43_request_fw_context * ctx ) <nl> int err ; <nl>  <nl> /* Get microcode */ <nl> - tmshigh = ssb_read32 ( dev -> sdev , SSB_TMSHIGH ); <nl> if (( rev >= 5 ) && ( rev <= 10 )) <nl> filename = " ucode5 "; <nl> else if (( rev >= 11 ) && ( rev <= 12 )) <nl> static int b43_try_request_fw ( struct b43_request_fw_context * ctx ) <nl> switch ( dev -> phy . type ) { <nl> case B43_PHYTYPE_A : <nl> if (( rev >= 5 ) && ( rev <= 10 )) { <nl> + tmshigh = ssb_read32 ( dev -> sdev , SSB_TMSHIGH ); <nl> if ( tmshigh & B43_TMSHIGH_HAVE_2GHZ_PHY ) <nl> filename = " a0g1initvals5 "; <nl> else <nl> static int b43_try_request_fw ( struct b43_request_fw_context * ctx ) <nl> switch ( dev -> phy . type ) { <nl> case B43_PHYTYPE_A : <nl> if (( rev >= 5 ) && ( rev <= 10 )) { <nl> + tmshigh = ssb_read32 ( dev -> sdev , SSB_TMSHIGH ); <nl> if ( tmshigh & B43_TMSHIGH_HAVE_2GHZ_PHY ) <nl> filename = " a0g1bsinitvals5 "; <nl> else
static int nfs4_proc_lookup_common ( struct rpc_clnt ** clnt , struct inode * dir , <nl> err = - EPERM ; <nl> if ( client != * clnt ) <nl> goto out ; <nl> - <nl> + /* No security negotiation if the user specified ' sec =' */ <nl> + if ( NFS_SERVER ( dir )-> flags & NFS_MOUNT_SECFLAVOUR ) <nl> + goto out ; <nl> client = nfs4_create_sec_client ( client , dir , name ); <nl> if ( IS_ERR ( client )) <nl> return PTR_ERR ( client );
DT_MACHINE_START ( R8A7794_DT , " Generic R8A7794 ( Flattened Device Tree )") <nl> . init_early = shmobile_init_delay , <nl> . init_late = shmobile_init_late , <nl> . init_time = rcar_gen2_timer_init , <nl> + . reserve = rcar_gen2_reserve , <nl> . dt_compat = r8a7794_boards_compat_dt , <nl> MACHINE_END
void intel_disable_gt_powersave ( struct drm_device * dev ) <nl> { <nl> if ( IS_IRONLAKE_M ( dev )) <nl> ironlake_disable_drps ( dev ); <nl> - if ( INTEL_INFO ( dev )-> gen >= 6 && ! IS_VALLEYVIEW ( dev )) <nl> + else if ( INTEL_INFO ( dev )-> gen >= 6 && ! IS_VALLEYVIEW ( dev )) <nl> gen6_disable_rps ( dev ); <nl> } <nl>  <nl> void intel_enable_gt_powersave ( struct drm_device * dev ) <nl> ironlake_enable_drps ( dev ); <nl> ironlake_enable_rc6 ( dev ); <nl> intel_init_emon ( dev ); <nl> - } <nl> - <nl> - if (( IS_GEN6 ( dev ) || IS_GEN7 ( dev )) && ! IS_VALLEYVIEW ( dev )) { <nl> + } else if (( IS_GEN6 ( dev ) || IS_GEN7 ( dev )) && ! IS_VALLEYVIEW ( dev )) { <nl> gen6_enable_rps ( dev ); <nl> gen6_update_ring_freq ( dev ); <nl> }
static int serdes_init_niu_1g_serdes ( struct niu * np ) <nl> struct niu_link_config * lp = & np -> link_config ; <nl> u16 pll_cfg , pll_sts ; <nl> int max_retry = 100 ; <nl> - u64 sig , mask , val ; <nl> + u64 uninitialized_var ( sig ), mask , val ; <nl> u32 tx_cfg , rx_cfg ; <nl> unsigned long i ; <nl> int err ; <nl> static int serdes_init_niu_10g_serdes ( struct niu * np ) <nl> struct niu_link_config * lp = & np -> link_config ; <nl> u32 tx_cfg , rx_cfg , pll_cfg , pll_sts ; <nl> int max_retry = 100 ; <nl> - u64 sig , mask , val ; <nl> + u64 uninitialized_var ( sig ), mask , val ; <nl> unsigned long i ; <nl> int err ; <nl> 
int n_tty_ioctl ( struct tty_struct * tty , struct file * file , <nl> ld = tty_ldisc_ref ( tty ); <nl> switch ( arg ) { <nl> case TCIFLUSH : <nl> - if ( ld -> flush_buffer ) <nl> + if ( ld && ld -> flush_buffer ) <nl> ld -> flush_buffer ( tty ); <nl> break ; <nl> case TCIOFLUSH : <nl> - if ( ld -> flush_buffer ) <nl> + if ( ld && ld -> flush_buffer ) <nl> ld -> flush_buffer ( tty ); <nl> /* fall through */ <nl> case TCOFLUSH :
static int sixpack_ioctl ( struct tty_struct * tty , struct file * file , <nl> unsigned int cmd , unsigned long arg ) <nl> { <nl> struct sixpack * sp = sp_get ( tty ); <nl> - struct net_device * dev = sp -> dev ; <nl> + struct net_device * dev ; <nl> unsigned int tmp , err ; <nl>  <nl> if (! sp ) <nl> return - ENXIO ; <nl> + dev = sp -> dev ; <nl>  <nl> switch ( cmd ) { <nl> case SIOCGIFNAME :
int ixgbe_ndo_set_vf_vlan ( struct net_device * netdev , int vf , u16 vlan , u8 qos ) <nl> if (( vf >= adapter -> num_vfs ) || ( vlan > 4095 ) || ( qos > 7 )) <nl> return - EINVAL ; <nl> if ( vlan || qos ) { <nl> + if ( adapter -> vfinfo [ vf ]. pf_vlan ) <nl> + err = ixgbe_set_vf_vlan ( adapter , false , <nl> + adapter -> vfinfo [ vf ]. pf_vlan , <nl> + vf ); <nl> + if ( err ) <nl> + goto out ; <nl> err = ixgbe_set_vf_vlan ( adapter , true , vlan , vf ); <nl> if ( err ) <nl> goto out ;
static int do_dev_config ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> if (! devs ) { <nl> dev_err ( dev -> class_dev , <nl> " Could not allocate memory . Out of memory ?\ n "); <nl> + kfree ( bdev ); <nl> return - ENOMEM ; <nl> } <nl> devpriv -> devs = devs ;
static void * i915_gem_dmabuf_vmap ( struct dma_buf * dma_buf ) <nl> goto error ; <nl>  <nl> i = 0 ; <nl> - for_each_sg_page ( obj -> pages -> sgl , & sg_iter , obj -> pages -> nents , 0 ); <nl> + for_each_sg_page ( obj -> pages -> sgl , & sg_iter , obj -> pages -> nents , 0 ) <nl> pages [ i ++] = sg_page_iter_page (& sg_iter ); <nl>  <nl> obj -> dma_buf_vmapping = vmap ( pages , i , 0 , PAGE_KERNEL );
void pvr2_hdw_v4l_store_minor_number ( struct pvr2_hdw * hdw , <nl> enum pvr2_v4l_type index , int v ) <nl> { <nl> switch ( index ) { <nl> - case pvr2_v4l_type_video : hdw -> v4l_minor_number_video = v ; <nl> - case pvr2_v4l_type_vbi : hdw -> v4l_minor_number_vbi = v ; <nl> - case pvr2_v4l_type_radio : hdw -> v4l_minor_number_radio = v ; <nl> + case pvr2_v4l_type_video : hdw -> v4l_minor_number_video = v ; break ; <nl> + case pvr2_v4l_type_vbi : hdw -> v4l_minor_number_vbi = v ; break ; <nl> + case pvr2_v4l_type_radio : hdw -> v4l_minor_number_radio = v ; break ; <nl> default : break ; <nl> } <nl> }
static struct sctp_endpoint * sctp_endpoint_init ( struct sctp_endpoint * ep , <nl> if ( sctp_addip_enable ) { <nl> auth_chunks -> chunks [ 0 ] = SCTP_CID_ASCONF ; <nl> auth_chunks -> chunks [ 1 ] = SCTP_CID_ASCONF_ACK ; <nl> - auth_chunks -> param_hdr . length += htons ( 2 ); <nl> + auth_chunks -> param_hdr . length = <nl> + htons ( sizeof ( sctp_paramhdr_t ) + 2 ); <nl> } <nl> } <nl> 
static unsigned int br_nf_pre_routing ( unsigned int hook , struct sk_buff * skb , <nl>  <nl> pskb_trim_rcsum ( skb , len ); <nl>  <nl> + /* BUG : Should really parse the IP options here . */ <nl> + memset ( IPCB ( skb ), 0 , sizeof ( struct inet_skb_parm )); <nl> + <nl> nf_bridge_put ( skb -> nf_bridge ); <nl> if (! nf_bridge_alloc ( skb )) <nl> return NF_DROP ;
static struct security_operations tomoyo_security_ops = { <nl> }; <nl>  <nl> /* Lock for GC . */ <nl> - struct srcu_struct tomoyo_ss ; <nl> + DEFINE_SRCU ( tomoyo_ss ); <nl>  <nl> /** <nl> * tomoyo_init - Register TOMOYO Linux as a LSM module . <nl> static int __init tomoyo_init ( void ) <nl> if (! security_module_enable (& tomoyo_security_ops )) <nl> return 0 ; <nl> /* register ourselves with the security framework */ <nl> - if ( register_security (& tomoyo_security_ops ) || <nl> - init_srcu_struct (& tomoyo_ss )) <nl> + if ( register_security (& tomoyo_security_ops )) <nl> panic (" Failure registering TOMOYO Linux "); <nl> printk ( KERN_INFO " TOMOYO Linux initialized \ n "); <nl> cred -> security = & tomoyo_kernel_domain ;
static void ipath_7220_put_tid ( struct ipath_devdata * dd , u64 __iomem * tidptr , <nl> " not 2KB aligned !\ n ", pa ); <nl> return ; <nl> } <nl> - if ( pa >= ( 1UL << IBA7220_TID_SZ_SHIFT )) { <nl> + if ( chippa >= ( 1UL << IBA7220_TID_SZ_SHIFT )) { <nl> ipath_dev_err ( dd , <nl> " BUG : Physical page address 0x % lx " <nl> " larger than supported \ n ", pa );
int setup_arg_pages ( struct linux_binprm * bprm , <nl> # else <nl> stack_top = arch_align_stack ( stack_top ); <nl> stack_top = PAGE_ALIGN ( stack_top ); <nl> + <nl> + if ( unlikely ( stack_top < mmap_min_addr ) || <nl> + unlikely ( vma -> vm_end - vma -> vm_start >= stack_top - mmap_min_addr )) <nl> + return - ENOMEM ; <nl> + <nl> stack_shift = vma -> vm_end - stack_top ; <nl>  <nl> bprm -> p -= stack_shift ;
static int xgene_dma_create_ring_one ( struct xgene_dma_chan * chan , <nl> struct xgene_dma_ring * ring , <nl> enum xgene_dma_ring_cfgsize cfgsize ) <nl> { <nl> + int ret ; <nl> + <nl> /* Setup DMA ring descriptor variables */ <nl> ring -> pdma = chan -> pdma ; <nl> ring -> cfgsize = cfgsize ; <nl> ring -> num = chan -> pdma -> ring_num ++; <nl> ring -> id = XGENE_DMA_RING_ID_GET ( ring -> owner , ring -> buf_num ); <nl>  <nl> - ring -> size = xgene_dma_get_ring_size ( chan , cfgsize ); <nl> - if ( ring -> size <= 0 ) <nl> - return ring -> size ; <nl> + ret = xgene_dma_get_ring_size ( chan , cfgsize ); <nl> + if ( ret <= 0 ) <nl> + return ret ; <nl> + ring -> size = ret ; <nl>  <nl> /* Allocate memory for DMA ring descriptor */ <nl> ring -> desc_vaddr = dma_zalloc_coherent ( chan -> dev , ring -> size ,
static int cciss_add_disk ( ctlr_info_t * h , struct gendisk * disk , <nl> int drv_index ) <nl> { <nl> disk -> queue = blk_init_queue ( do_cciss_request , & h -> lock ); <nl> + if (! disk -> queue ) <nl> + goto init_queue_failure ; <nl> sprintf ( disk -> disk_name , " cciss / c % dd % d ", h -> ctlr , drv_index ); <nl> disk -> major = h -> major ; <nl> disk -> first_minor = drv_index << NWD_SHIFT ; <nl> static int cciss_add_disk ( ctlr_info_t * h , struct gendisk * disk , <nl> cleanup_queue : <nl> blk_cleanup_queue ( disk -> queue ); <nl> disk -> queue = NULL ; <nl> + init_queue_failure : <nl> return - 1 ; <nl> } <nl> 
static int psbfb_create ( struct psb_fbdev * fbdev , <nl> mode_cmd . width = sizes -> surface_width ; <nl> mode_cmd . height = sizes -> surface_height ; <nl> bpp = sizes -> surface_bpp ; <nl> + depth = sizes -> surface_depth ; <nl>  <nl> /* No 24bit packed */ <nl> if ( bpp == 24 ) <nl> static int psbfb_create ( struct psb_fbdev * fbdev , <nl> * is ok with some fonts <nl> */ <nl> mode_cmd . pitches [ 0 ] = ALIGN ( mode_cmd . width * (( bpp + 7 ) / 8 ), 4096 >> pitch_lines ); <nl> - depth = sizes -> surface_depth ; <nl>  <nl> size = mode_cmd . pitches [ 0 ] * mode_cmd . height ; <nl> size = ALIGN ( size , PAGE_SIZE );
int btrfs_sync_log ( struct btrfs_trans_handle * trans , <nl> /* wait for previous tree log sync to complete */ <nl> if ( atomic_read (& root -> log_commit [( index1 + 1 ) % 2 ])) <nl> wait_log_commit ( trans , root , root -> log_transid - 1 ); <nl> - <nl> while ( 1 ) { <nl> unsigned long batch = root -> log_batch ; <nl> - if ( root -> log_multiple_pids ) { <nl> + /* when we ' re on an ssd , just kick the log commit out */ <nl> + if (! btrfs_test_opt ( root , SSD ) && root -> log_multiple_pids ) { <nl> mutex_unlock (& root -> log_mutex ); <nl> schedule_timeout_uninterruptible ( 1 ); <nl> mutex_lock (& root -> log_mutex );
static void sock_rmem_free ( struct sk_buff * skb ) <nl> */ <nl> int sock_queue_err_skb ( struct sock * sk , struct sk_buff * skb ) <nl> { <nl> + int len = skb -> len ; <nl> + <nl> if ( atomic_read (& sk -> sk_rmem_alloc ) + skb -> truesize >= <nl> ( unsigned ) sk -> sk_rcvbuf ) <nl> return - ENOMEM ; <nl> int sock_queue_err_skb ( struct sock * sk , struct sk_buff * skb ) <nl>  <nl> skb_queue_tail (& sk -> sk_error_queue , skb ); <nl> if (! sock_flag ( sk , SOCK_DEAD )) <nl> - sk -> sk_data_ready ( sk , skb -> len ); <nl> + sk -> sk_data_ready ( sk , len ); <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL ( sock_queue_err_skb );
static int show_numa_map ( struct seq_file * m , void * v , int is_pid ) <nl> walk . mm = mm ; <nl>  <nl> pol = get_vma_policy ( task , vma , vma -> vm_start ); <nl> - mpol_to_str ( buffer , sizeof ( buffer ), pol ); <nl> + n = mpol_to_str ( buffer , sizeof ( buffer ), pol ); <nl> mpol_cond_put ( pol ); <nl> + if ( n < 0 ) <nl> + return n ; <nl>  <nl> seq_printf ( m , "% 08lx % s ", vma -> vm_start , buffer ); <nl> 
int pci_scan_bridge ( struct pci_bus * bus , struct pci_dev * dev , int max , int pass ) <nl> goto out ; <nl> } <nl>  <nl> + if ( max >= bus -> busn_res . end ) { <nl> + dev_warn (& dev -> dev , " can ' t allocate child bus % 02x from % pR \ n ", <nl> + max , & bus -> busn_res ); <nl> + goto out ; <nl> + } <nl> + <nl> /* Clear errors */ <nl> pci_write_config_word ( dev , PCI_STATUS , 0xffff ); <nl>  <nl> - /* Prevent assigning a bus number that already exists . <nl> - * This can happen when a bridge is hot - plugged , so in <nl> - * this case we only re - scan this bus . */ <nl> + /* The bus will already exist if we are rescanning */ <nl> child = pci_find_bus ( pci_domain_nr ( bus ), max + 1 ); <nl> if (! child ) { <nl> child = pci_add_new_bus ( bus , dev , max + 1 );
static void kernel_pio ( struct kvm_io_device * pio_dev , <nl> { <nl> /* TODO : String I / O for in kernel device */ <nl>  <nl> + mutex_lock (& vcpu -> kvm -> lock ); <nl> if ( vcpu -> pio . in ) <nl> kvm_iodevice_read ( pio_dev , vcpu -> pio . port , <nl> vcpu -> pio . size , <nl> static void kernel_pio ( struct kvm_io_device * pio_dev , <nl> kvm_iodevice_write ( pio_dev , vcpu -> pio . port , <nl> vcpu -> pio . size , <nl> pd ); <nl> + mutex_unlock (& vcpu -> kvm -> lock ); <nl> } <nl>  <nl> static void pio_string_write ( struct kvm_io_device * pio_dev , <nl> static void pio_string_write ( struct kvm_io_device * pio_dev , <nl> void * pd = vcpu -> pio_data ; <nl> int i ; <nl>  <nl> + mutex_lock (& vcpu -> kvm -> lock ); <nl> for ( i = 0 ; i < io -> cur_count ; i ++) { <nl> kvm_iodevice_write ( pio_dev , io -> port , <nl> io -> size , <nl> pd ); <nl> pd += io -> size ; <nl> } <nl> + mutex_unlock (& vcpu -> kvm -> lock ); <nl> } <nl>  <nl> int kvm_emulate_pio ( struct kvm_vcpu * vcpu , struct kvm_run * run , int in , <nl> static long kvm_vm_ioctl ( struct file * filp , <nl> if ( copy_from_user (& irq_event , argp , sizeof irq_event )) <nl> goto out ; <nl> if ( irqchip_in_kernel ( kvm )) { <nl> + mutex_lock (& kvm -> lock ); <nl> if ( irq_event . irq < 16 ) <nl> kvm_pic_set_irq ( pic_irqchip ( kvm ), <nl> irq_event . irq , <nl> static long kvm_vm_ioctl ( struct file * filp , <nl> kvm_ioapic_set_irq ( kvm -> vioapic , <nl> irq_event . irq , <nl> irq_event . level ); <nl> + mutex_unlock (& kvm -> lock ); <nl> r = 0 ; <nl> } <nl> break ;
static int ov772x_mask_set ( struct i2c_client * client , u8 command , u8 mask , <nl>  <nl> static int ov772x_reset ( struct i2c_client * client ) <nl> { <nl> - int ret = ov772x_write ( client , COM7 , SCCB_RESET ); <nl> + int ret ; <nl> + <nl> + ret = ov772x_write ( client , COM7 , SCCB_RESET ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + <nl> msleep ( 1 ); <nl> - return ret ; <nl> + <nl> + return ov772x_mask_set ( client , COM2 , SOFT_SLEEP_MODE , SOFT_SLEEP_MODE ); <nl> } <nl>  <nl> /*
static int add_recvbuf_small ( struct virtnet_info * vi , struct receive_queue * rq , <nl> skb_put ( skb , GOOD_PACKET_LEN ); <nl>  <nl> hdr = skb_vnet_hdr ( skb ); <nl> - sg_init_table ( rq -> sg , MAX_SKB_FRAGS + 2 ); <nl> + sg_init_table ( rq -> sg , 2 ); <nl> sg_set_buf ( rq -> sg , hdr , vi -> hdr_len ); <nl> skb_to_sgvec ( skb , rq -> sg + 1 , 0 , skb -> len ); <nl>  <nl> static int xmit_skb ( struct send_queue * sq , struct sk_buff * skb ) <nl> if ( vi -> mergeable_rx_bufs ) <nl> hdr -> num_buffers = 0 ; <nl>  <nl> - sg_init_table ( sq -> sg , MAX_SKB_FRAGS + 2 ); <nl> + sg_init_table ( sq -> sg , skb_shinfo ( skb )-> nr_frags + ( can_push ? 1 : 2 )); <nl> if ( can_push ) { <nl> __skb_push ( skb , hdr_len ); <nl> num_sg = skb_to_sgvec ( skb , sq -> sg , 0 , skb -> len );
struct twl6040_data { <nl> }; <nl>  <nl> /* set of rates for each pll : low - power and high - performance */ <nl> - static unsigned int lp_rates [] = { <nl> + static const unsigned int lp_rates [] = { <nl> 8000 , <nl> 11250 , <nl> 16000 , <nl> static unsigned int lp_rates [] = { <nl> 96000 , <nl> }; <nl>  <nl> - static unsigned int hp_rates [] = { <nl> + static const unsigned int hp_rates [] = { <nl> 8000 , <nl> 16000 , <nl> 32000 , <nl> static unsigned int hp_rates [] = { <nl> 96000 , <nl> }; <nl>  <nl> - static struct snd_pcm_hw_constraint_list sysclk_constraints [] = { <nl> + static const struct snd_pcm_hw_constraint_list sysclk_constraints [] = { <nl> { . count = ARRAY_SIZE ( lp_rates ), . list = lp_rates , }, <nl> { . count = ARRAY_SIZE ( hp_rates ), . list = hp_rates , }, <nl> };
dma_error : <nl> buffer_info -> length = 0 ; <nl> buffer_info -> next_to_watch = 0 ; <nl> buffer_info -> mapped_as_page = false ; <nl> - count --; <nl>  <nl> /* clear timestamp and dma mappings for remaining portion of packet */ <nl> - while ( count >= 0 ) { <nl> - count --; <nl> + while ( count --) { <nl> + if ( i == 0 ) <nl> + i = tx_ring -> count ; <nl> i --; <nl> - if ( i < 0 ) <nl> - i += tx_ring -> count ; <nl> buffer_info = & tx_ring -> buffer_info [ i ]; <nl> igb_unmap_and_free_tx_resource ( tx_ring , buffer_info ); <nl> }
int isci_task_lu_reset ( struct domain_device * dev , u8 * lun ) <nl> int ret ; <nl>  <nl> spin_lock_irqsave (& ihost -> scic_lock , flags ); <nl> - idev = isci_lookup_device ( dev ); <nl> + idev = isci_get_device ( dev -> lldd_dev ); <nl> spin_unlock_irqrestore (& ihost -> scic_lock , flags ); <nl>  <nl> dev_dbg (& ihost -> pdev -> dev , <nl> int isci_task_abort_task ( struct sas_task * task ) <nl> if (!( task -> task_state_flags & SAS_TASK_STATE_DONE ) && <nl> ( task -> task_state_flags & SAS_TASK_AT_INITIATOR ) && <nl> old_request ) <nl> - idev = isci_lookup_device ( task -> dev ); <nl> + idev = isci_get_device ( task -> dev -> lldd_dev ); <nl>  <nl> spin_unlock (& task -> task_state_lock ); <nl> spin_unlock_irqrestore (& ihost -> scic_lock , flags ); <nl> int isci_task_abort_task ( struct sas_task * task ) <nl> ISCI_ABORT_TASK_TIMEOUT_MS ); <nl> } <nl> out : <nl> + dev_warn (& ihost -> pdev -> dev , <nl> + "% s : Done ; dev = % p , task = % p , old_request == % p \ n ", <nl> + __func__ , idev , task , old_request ); <nl> isci_put_device ( idev ); <nl> return ret ; <nl> }
UNUSUAL_DEV ( 0x0421 , 0x0495 , 0x0370 , 0x0370 , <nl> USB_SC_DEVICE , USB_PR_DEVICE , NULL , <nl> US_FL_MAX_SECTORS_64 ), <nl>  <nl> +/* Patch submitted by Victor A . Santos < victoraur . santos @ gmail . com > */ <nl> + UNUSUAL_DEV ( 0x0421 , 0x05af , 0x0742 , 0x0742 , <nl> + " Nokia ", <nl> + " 305 ", <nl> + USB_SC_DEVICE , USB_PR_DEVICE , NULL , <nl> + US_FL_MAX_SECTORS_64 ), <nl> + <nl> /* Patch submitted by Mikhail Zolotaryov < lebon @ lebon . org . ua > */ <nl> UNUSUAL_DEV ( 0x0421 , 0x06aa , 0x1110 , 0x1110 , <nl> " Nokia ",
static int __einj_error_trigger ( u64 trigger_paddr , u32 type , <nl> * This will cause resource conflict with regular memory . So <nl> * remove it from trigger table resources . <nl> */ <nl> - if ( param_extension && ( type & 0x0038 ) && param2 ) { <nl> + if (( param_extension || acpi5 ) && ( type & 0x0038 ) && param2 ) { <nl> struct apei_resources addr_resources ; <nl> apei_resources_init (& addr_resources ); <nl> trigger_param_region = einj_get_trigger_parameter_region (
static void imon_incoming_packet ( struct imon_context * ictx , <nl> /* Only panel type events left to process now */ <nl> spin_lock_irqsave (& ictx -> kc_lock , flags ); <nl>  <nl> + do_gettimeofday (& t ); <nl> /* KEY_MUTE repeats from knob need to be suppressed */ <nl> if ( ictx -> kc == KEY_MUTE && ictx -> kc == ictx -> last_keycode ) { <nl> - do_gettimeofday (& t ); <nl> msec = tv2int (& t , & prev_time ); <nl> - prev_time = t ; <nl> if ( msec < ictx -> idev -> rep [ REP_DELAY ]) { <nl> spin_unlock_irqrestore (& ictx -> kc_lock , flags ); <nl> return ; <nl> } <nl> } <nl> + prev_time = t ; <nl> kc = ictx -> kc ; <nl>  <nl> spin_unlock_irqrestore (& ictx -> kc_lock , flags ); <nl> static void imon_incoming_packet ( struct imon_context * ictx , <nl> input_report_key ( ictx -> idev , kc , 0 ); <nl> input_sync ( ictx -> idev ); <nl>  <nl> + spin_lock_irqsave (& ictx -> kc_lock , flags ); <nl> ictx -> last_keycode = kc ; <nl> + spin_unlock_irqrestore (& ictx -> kc_lock , flags ); <nl>  <nl> return ; <nl> 
static void handle_callback ( struct gfs2_glock * gl , unsigned int state , int remot <nl> } <nl> return ; <nl> } <nl> - } else if ( gl -> gl_demote_state != LM_ST_UNLOCKED ) { <nl> - gl -> gl_demote_state = state ; <nl> + } else if ( gl -> gl_demote_state != LM_ST_UNLOCKED && <nl> + gl -> gl_demote_state != state ) { <nl> + gl -> gl_demote_state = LM_ST_UNLOCKED ; <nl> } <nl> spin_unlock (& gl -> gl_spin ); <nl> }
int x509_process_extension ( void * context , size_t hdrlen , <nl>  <nl> ctx -> cert -> raw_skid_size = vlen ; <nl> ctx -> cert -> raw_skid = v ; <nl> - kid = asymmetric_key_generate_id ( ctx -> cert -> raw_subject , <nl> - ctx -> cert -> raw_subject_size , <nl> - v , vlen ); <nl> + kid = asymmetric_key_generate_id ( v , vlen , "", 0 ); <nl> if ( IS_ERR ( kid )) <nl> return PTR_ERR ( kid ); <nl> ctx -> cert -> skid = kid ; <nl> int x509_akid_note_kid ( void * context , size_t hdrlen , <nl> if ( ctx -> cert -> akid_skid ) <nl> return 0 ; <nl>  <nl> - kid = asymmetric_key_generate_id ( ctx -> cert -> raw_issuer , <nl> - ctx -> cert -> raw_issuer_size , <nl> - value , vlen ); <nl> + kid = asymmetric_key_generate_id ( value , vlen , "", 0 ); <nl> if ( IS_ERR ( kid )) <nl> return PTR_ERR ( kid ); <nl> pr_debug (" authkeyid %* phN \ n ", kid -> len , kid -> data );
static int __init lart_flash_init ( void ) <nl> mtd . name = module_name ; <nl> mtd . type = MTD_NORFLASH ; <nl> mtd . writesize = 1 ; <nl> + mtd . writebufsize = 4 ; <nl> mtd . flags = MTD_CAP_NORFLASH ; <nl> mtd . size = FLASH_BLOCKSIZE_PARAM * FLASH_NUMBLOCKS_16m_PARAM + FLASH_BLOCKSIZE_MAIN * FLASH_NUMBLOCKS_16m_MAIN ; <nl> mtd . erasesize = FLASH_BLOCKSIZE_MAIN ;
__append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> void append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> struct symbol ** syms ) <nl> { <nl> + if (! chain -> nr ) <nl> + return ; <nl> __append_chain_children ( root , chain , syms , 0 ); <nl> }
static long __video_do_ioctl ( struct file * file , <nl> struct v4l2_streamparm * p = arg ; <nl>  <nl> if ( ops -> vidioc_g_parm ) { <nl> + ret = check_fmt ( ops , p -> type ); <nl> + if ( ret ) <nl> + break ; <nl> ret = ops -> vidioc_g_parm ( file , fh , p ); <nl> } else { <nl> if ( p -> type != V4L2_BUF_TYPE_VIDEO_CAPTURE ) <nl> static long __video_do_ioctl ( struct file * file , <nl>  <nl> if (! ops -> vidioc_s_parm ) <nl> break ; <nl> + ret = check_fmt ( ops , p -> type ); <nl> + if ( ret ) <nl> + break ; <nl> + <nl> dbgarg ( cmd , " type =% d \ n ", p -> type ); <nl> ret = ops -> vidioc_s_parm ( file , fh , p ); <nl> break ;
DEFINE_MUTEX ( fuse_mutex ); <nl>  <nl> # define FUSE_SUPER_MAGIC 0x65735546 <nl>  <nl> +# define FUSE_DEFAULT_BLKSIZE 512 <nl> + <nl> struct fuse_mount_data { <nl> int fd ; <nl> unsigned rootmode ; <nl> static int parse_fuse_opt ( char * opt , struct fuse_mount_data * d , int is_bdev ) <nl> char * p ; <nl> memset ( d , 0 , sizeof ( struct fuse_mount_data )); <nl> d -> max_read = ~ 0 ; <nl> - d -> blksize = 512 ; <nl> + d -> blksize = FUSE_DEFAULT_BLKSIZE ; <nl>  <nl> while (( p = strsep (& opt , ",")) != NULL ) { <nl> int token ; <nl> static int fuse_show_options ( struct seq_file * m , struct vfsmount * mnt ) <nl> seq_puts ( m , ", allow_other "); <nl> if ( fc -> max_read != ~ 0 ) <nl> seq_printf ( m , ", max_read =% u ", fc -> max_read ); <nl> + if ( mnt -> mnt_sb -> s_bdev && <nl> + mnt -> mnt_sb -> s_blocksize != FUSE_DEFAULT_BLKSIZE ) <nl> + seq_printf ( m , ", blksize =% lu ", mnt -> mnt_sb -> s_blocksize ); <nl> return 0 ; <nl> } <nl> 
static int si470x_fops_release ( struct file * file ) <nl> video_unregister_device ( radio -> videodev ); <nl> kfree ( radio -> buffer ); <nl> kfree ( radio ); <nl> - goto done ; <nl> + goto unlock ; <nl> } <nl>  <nl> /* stop rds reception */ <nl> static int si470x_fops_release ( struct file * file ) <nl> retval = si470x_stop ( radio ); <nl> usb_autopm_put_interface ( radio -> intf ); <nl> } <nl> - <nl> + unlock : <nl> mutex_unlock (& radio -> disconnect_lock ); <nl> - <nl> done : <nl> return retval ; <nl> }
static bool __zone_watermark_ok ( struct zone * z , int order , unsigned long mark , <nl> long min = mark ; <nl> long lowmem_reserve = z -> lowmem_reserve [ classzone_idx ]; <nl> int o ; <nl> + long free_cma = 0 ; <nl>  <nl> free_pages -= ( 1 << order ) - 1 ; <nl> if ( alloc_flags & ALLOC_HIGH ) <nl> static bool __zone_watermark_ok ( struct zone * z , int order , unsigned long mark , <nl> # ifdef CONFIG_CMA <nl> /* If allocation can ' t use CMA areas don ' t use free CMA pages */ <nl> if (!( alloc_flags & ALLOC_CMA )) <nl> - free_pages -= zone_page_state ( z , NR_FREE_CMA_PAGES ); <nl> + free_cma = zone_page_state ( z , NR_FREE_CMA_PAGES ); <nl> # endif <nl> - if ( free_pages <= min + lowmem_reserve ) <nl> + <nl> + if ( free_pages - free_cma <= min + lowmem_reserve ) <nl> return false ; <nl> for ( o = 0 ; o < order ; o ++) { <nl> /* At the next order , this order ' s pages become unavailable */
e1000_configure_tx ( struct e1000_adapter * adapter ) <nl> } <nl>  <nl> /* Set the default values for the Tx Inter Packet Gap timer */ <nl> - <nl> - if ( hw -> media_type == e1000_media_type_fiber || <nl> - hw -> media_type == e1000_media_type_internal_serdes ) <nl> + if ( adapter -> hw . mac_type <= e1000_82547_rev_2 && <nl> + ( hw -> media_type == e1000_media_type_fiber || <nl> + hw -> media_type == e1000_media_type_internal_serdes )) <nl> tipg = DEFAULT_82543_TIPG_IPGT_FIBER ; <nl> else <nl> tipg = DEFAULT_82543_TIPG_IPGT_COPPER ;
static int i915_dma_cleanup ( struct drm_device * dev ) <nl> if ( dev -> irq_enabled ) <nl> drm_irq_uninstall ( dev ); <nl>  <nl> + mutex_lock (& dev -> struct_mutex ); <nl> intel_cleanup_ring_buffer ( dev , & dev_priv -> render_ring ); <nl> if ( HAS_BSD ( dev )) <nl> intel_cleanup_ring_buffer ( dev , & dev_priv -> bsd_ring ); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl>  <nl> /* Clear the HWS virtual address at teardown */ <nl> if ( I915_NEED_GFX_HWS ( dev ))
void __init lpc32xx_serial_init ( void ) <nl>  <nl> /* This needs to be done after all UART clocks are setup */ <nl> __raw_writel ( clkmodes , LPC32XX_UARTCTL_CLKMODE ); <nl> - for ( i = 0 ; i < ARRAY_SIZE ( uartinit_data ) - 1 ; i ++) { <nl> + for ( i = 0 ; i < ARRAY_SIZE ( uartinit_data ); i ++) { <nl> /* Force a flush of the RX FIFOs to work around a HW bug */ <nl> puart = serial_std_platform_data [ i ]. mapbase ; <nl> __raw_writel ( 0xC1 , LPC32XX_UART_IIR_FCR ( puart ));
void odm_DIGInit ( struct odm_dm_struct * pDM_Odm ) <nl> struct adapter * adapter = pDM_Odm -> Adapter ; <nl> struct rtw_dig * pDM_DigTable = & pDM_Odm -> DM_DigTable ; <nl>  <nl> - pDM_DigTable -> CurIGValue = ( u8 ) PHY_QueryBBReg ( adapter , ODM_REG ( IGI_A , pDM_Odm ), ODM_BIT ( IGI , pDM_Odm )); <nl> + pDM_DigTable -> CurIGValue = ( u8 ) PHY_QueryBBReg ( adapter , ODM_REG_IGI_A_11N , ODM_BIT_IGI_11N ); <nl> pDM_DigTable -> RssiLowThresh = DM_DIG_THRESH_LOW ; <nl> pDM_DigTable -> RssiHighThresh = DM_DIG_THRESH_HIGH ; <nl> pDM_DigTable -> FALowThresh = DM_false_ALARM_THRESH_LOW ;
static ssize_t solos_param_show ( struct device * dev , struct device_attribute * att <nl>  <nl> buflen = strlen ( attr -> attr . name ) + 10 ; <nl>  <nl> - skb = alloc_skb ( buflen , GFP_KERNEL ); <nl> + skb = alloc_skb ( sizeof (* header ) + buflen , GFP_KERNEL ); <nl> if (! skb ) { <nl> dev_warn (& card -> dev -> dev , " Failed to allocate sk_buff in solos_param_show ()\ n "); <nl> return - ENOMEM ; <nl> static ssize_t solos_param_store ( struct device * dev , struct device_attribute * at <nl>  <nl> buflen = strlen ( attr -> attr . name ) + 11 + count ; <nl>  <nl> - skb = alloc_skb ( buflen , GFP_KERNEL ); <nl> + skb = alloc_skb ( sizeof (* header ) + buflen , GFP_KERNEL ); <nl> if (! skb ) { <nl> dev_warn (& card -> dev -> dev , " Failed to allocate sk_buff in solos_param_store ()\ n "); <nl> return - ENOMEM ;
ic_dhcp_init_options ( u8 * options ) <nl> memcpy ( e , ic_req_params , sizeof ( ic_req_params )); <nl> e += sizeof ( ic_req_params ); <nl>  <nl> + if ( ic_host_name_set ) { <nl> + * e ++ = 12 ; /* host - name */ <nl> + len = strlen ( utsname ()-> nodename ); <nl> + * e ++ = len ; <nl> + memcpy ( e , utsname ()-> nodename , len ); <nl> + e += len ; <nl> + } <nl> if (* vendor_class_identifier ) { <nl> printk ( KERN_INFO " DHCP : sending class identifier \"% s \"\ n ", <nl> vendor_class_identifier );
service_in_request ( struct musb * musb , const struct usb_ctrlrequest * ctrlrequest ) <nl> static void musb_g_ep0_giveback ( struct musb * musb , struct usb_request * req ) <nl> { <nl> musb_g_giveback (& musb -> endpoints [ 0 ]. ep_in , req , 0 ); <nl> - musb -> ep0_state = MUSB_EP0_STAGE_SETUP ; <nl> } <nl>  <nl> /*
static int parse_addr ( const struct nf_conn * ct , const char * cp , <nl> int family = ct -> tuplehash [ IP_CT_DIR_ORIGINAL ]. tuple . src . l3num ; <nl> int ret = 0 ; <nl>  <nl> + memset ( addr , 0 , sizeof (* addr )); <nl> switch ( family ) { <nl> case AF_INET : <nl> ret = in4_pton ( cp , limit - cp , ( u8 *)& addr -> ip , - 1 , & end );
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> memcpy ( sta -> sta . addr , addr , ETH_ALEN ); <nl> sta -> local = local ; <nl> sta -> sdata = sdata ; <nl> + sta -> last_rx = jiffies ; <nl>  <nl> ewma_init (& sta -> avg_signal , 1024 , 8 ); <nl> 
int cfg80211_mlme_mgmt_tx ( struct cfg80211_registered_device * rdev , <nl> if ( memcmp ( mgmt -> bssid , dev -> dev_addr , ETH_ALEN )) <nl> err = - EINVAL ; <nl> break ; <nl> + case NL80211_IFTYPE_MESH_POINT : <nl> + if ( memcmp ( mgmt -> sa , mgmt -> bssid , ETH_ALEN )) { <nl> + err = - EINVAL ; <nl> + break ; <nl> + } <nl> + /* <nl> + * check for mesh DA must be done by driver as <nl> + * cfg80211 doesn ' t track the stations <nl> + */ <nl> + break ; <nl> default : <nl> err = - EOPNOTSUPP ; <nl> break ;
p9_client_rpc ( struct p9_client * c , int8_t type , const char * fmt , ...) <nl> c -> status = Disconnected ; <nl> goto reterr ; <nl> } <nl> + again : <nl> /* Wait for the response */ <nl> err = wait_event_interruptible (* req -> wq , <nl> req -> status >= REQ_STATUS_RCVD ); <nl>  <nl> + if (( err == - ERESTARTSYS ) && ( c -> status == Connected ) <nl> + && ( type == P9_TFLUSH )) { <nl> + sigpending = 1 ; <nl> + clear_thread_flag ( TIF_SIGPENDING ); <nl> + goto again ; <nl> + } <nl> + <nl> if ( req -> status == REQ_STATUS_ERROR ) { <nl> p9_debug ( P9_DEBUG_ERROR , " req_status error % d \ n ", req -> t_err ); <nl> err = req -> t_err ;
static int emumousebtn_input_register ( void ) <nl> if (! emumousebtn ) <nl> return - ENOMEM ; <nl>  <nl> - lockdep_set_class ( emumousebtn -> event_lock , & emumousebtn_event_class ); <nl> - lockdep_set_class ( emumousebtn -> mutex , & emumousebtn_mutex_class ); <nl> + lockdep_set_class (& emumousebtn -> event_lock , & emumousebtn_event_class ); <nl> + lockdep_set_class (& emumousebtn -> mutex , & emumousebtn_mutex_class ); <nl>  <nl> emumousebtn -> name = " Macintosh mouse button emulation "; <nl> emumousebtn -> id . bustype = BUS_ADB ;
static inline unsigned int elapsed_jiffies_msecs ( unsigned long start ) <nl> if ( end >= start ) <nl> return jiffies_to_msecs ( end - start ); <nl>  <nl> - return jiffies_to_msecs ( end + ( MAX_JIFFY_OFFSET - start ) + 1 ); <nl> + return jiffies_to_msecs ( end + ( ULONG_MAX - start ) + 1 ); <nl> } <nl>  <nl> void
struct clk * imx_clk_fixup_mux ( const char * name , void __iomem * reg , <nl> init . ops = & clk_fixup_mux_ops ; <nl> init . parent_names = parents ; <nl> init . num_parents = num_parents ; <nl> + init . flags = 0 ; <nl>  <nl> fixup_mux -> mux . reg = reg ; <nl> fixup_mux -> mux . shift = shift ;
static ssize_t tun_get_user ( struct tun_struct * tun , struct tun_file * tfile , <nl> u32 rxhash ; <nl>  <nl> if (!( tun -> flags & TUN_NO_PI )) { <nl> - if (( len -= sizeof ( pi )) > total_len ) <nl> + if (( len -= sizeof ( pi )) < 0 ) <nl> return - EINVAL ; <nl>  <nl> if ( memcpy_fromiovecend (( void *)& pi , iv , 0 , sizeof ( pi ))) <nl> static ssize_t tun_get_user ( struct tun_struct * tun , struct tun_file * tfile , <nl> } <nl>  <nl> if ( tun -> flags & TUN_VNET_HDR ) { <nl> - if (( len -= tun -> vnet_hdr_sz ) > total_len ) <nl> + if (( len -= tun -> vnet_hdr_sz ) < 0 ) <nl> return - EINVAL ; <nl>  <nl> if ( memcpy_fromiovecend (( void *)& gso , iv , offset , sizeof ( gso )))
struct request { <nl>  <nl> unsigned short ioprio ; <nl>  <nl> + unsigned int timeout ; <nl> + <nl> void * special ; /* opaque pointer available for LLD use */ <nl>  <nl> int errors ; <nl> struct request { <nl>  <nl> unsigned long deadline ; <nl> struct list_head timeout_list ; <nl> - unsigned int timeout ; <nl>  <nl> /* <nl> * completion callback .
fld_rrb_scan ( struct lu_client_fld * fld , u64 seq ) <nl> else <nl> hash = 0 ; <nl>  <nl> + again : <nl> list_for_each_entry ( target , & fld -> lcf_targets , ft_chain ) { <nl> if ( target -> ft_idx == hash ) <nl> return target ; <nl> } <nl>  <nl> + if ( hash != 0 ) { <nl> + /* It is possible the remote target ( MDT ) are not connected to <nl> + * with client yet , so we will refer this to MDT0 , which should <nl> + * be connected during mount */ <nl> + hash = 0 ; <nl> + goto again ; <nl> + } <nl> + <nl> CERROR ("% s : Can ' t find target by hash % d ( seq %# llx ). Targets (% d ):\ n ", <nl> fld -> lcf_name , hash , seq , fld -> lcf_count ); <nl> 
static int tpm_binary_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 5th : delimiter */ <nl> seq_putc ( m , '\ 0 '); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl>  <nl> static int tpm_ascii_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 4th : eventname <= max + \' 0 ' delimiter */ <nl> seq_printf ( m , " % s \ n ", eventname ); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl> 
static struct drm_driver driver = { <nl> . minor = DRIVER_MINOR , <nl> . patchlevel = DRIVER_PATCHLEVEL , <nl>  <nl> - . gem_free_object = ast_gem_free_object , <nl> + . gem_free_object_unlocked = ast_gem_free_object , <nl> . dumb_create = ast_dumb_create , <nl> . dumb_map_offset = ast_dumb_mmap_offset , <nl> . dumb_destroy = drm_gem_dumb_destroy ,
static int pcf8563_rtc_read_alarm ( struct device * dev , struct rtc_wkalrm * tm ) <nl> __func__ , buf [ 0 ], buf [ 1 ], buf [ 2 ], buf [ 3 ]); <nl>  <nl> tm -> time . tm_min = bcd2bin ( buf [ 0 ] & 0x7F ); <nl> - tm -> time . tm_hour = bcd2bin ( buf [ 1 ] & 0x7F ); <nl> - tm -> time . tm_mday = bcd2bin ( buf [ 2 ] & 0x1F ); <nl> + tm -> time . tm_hour = bcd2bin ( buf [ 1 ] & 0x3F ); <nl> + tm -> time . tm_mday = bcd2bin ( buf [ 2 ] & 0x3F ); <nl> tm -> time . tm_wday = bcd2bin ( buf [ 3 ] & 0x7 ); <nl> tm -> time . tm_mon = - 1 ; <nl> tm -> time . tm_year = - 1 ;
static void hdspm_set_dds_value ( struct hdspm * hdspm , int rate ) <nl> { <nl> u64 n ; <nl>  <nl> + if ( snd_BUG_ON ( rate <= 0 )) <nl> + return ; <nl> + <nl> if ( rate >= 112000 ) <nl> rate /= 4 ; <nl> else if ( rate >= 56000 ) <nl> static int hdspm_get_system_sample_rate ( struct hdspm * hdspm ) <nl> } else { <nl> /* slave mode , return external sample rate */ <nl> rate = hdspm_external_sample_rate ( hdspm ); <nl> + if (! rate ) <nl> + rate = hdspm -> system_sample_rate ; <nl> } <nl> } <nl>  <nl> static int snd_hdspm_put_system_sample_rate ( struct snd_kcontrol * kcontrol , <nl> ucontrol ) <nl> { <nl> struct hdspm * hdspm = snd_kcontrol_chip ( kcontrol ); <nl> + int rate = ucontrol -> value . integer . value [ 0 ]; <nl>  <nl> + if ( rate < 27000 || rate > 207000 ) <nl> + return - EINVAL ; <nl> hdspm_set_dds_value ( hdspm , ucontrol -> value . integer . value [ 0 ]); <nl> return 0 ; <nl> }
int brcms_c_get_curband ( struct brcms_c_info * wlc ) <nl>  <nl> void brcms_c_wait_for_tx_completion ( struct brcms_c_info * wlc , bool drop ) <nl> { <nl> + int timeout = 20 ; <nl> + <nl> /* flush packet queue when requested */ <nl> if ( drop ) <nl> brcmu_pktq_flush (& wlc -> pkt_queue -> q , false , NULL , NULL ); <nl>  <nl> /* wait for queue and DMA fifos to run dry */ <nl> - while (! pktq_empty (& wlc -> pkt_queue -> q ) || brcms_txpktpendtot ( wlc ) > 0 ) <nl> + while (! pktq_empty (& wlc -> pkt_queue -> q ) || brcms_txpktpendtot ( wlc ) > 0 ) { <nl> brcms_msleep ( wlc -> wl , 1 ); <nl> + <nl> + if (-- timeout == 0 ) <nl> + break ; <nl> + } <nl> + <nl> + WARN_ON_ONCE ( timeout == 0 ); <nl> } <nl>  <nl> void brcms_c_set_beacon_listen_interval ( struct brcms_c_info * wlc , u8 interval )
parse_tag_3_packet ( struct ecryptfs_crypt_stat * crypt_stat , <nl> } <nl> (* new_auth_tok )-> session_key . encrypted_key_size = <nl> ( body_size - ( ECRYPTFS_SALT_SIZE + 5 )); <nl> + if ((* new_auth_tok )-> session_key . encrypted_key_size <nl> + > ECRYPTFS_MAX_ENCRYPTED_KEY_BYTES ) { <nl> + printk ( KERN_WARNING " Tag 3 packet contains key larger " <nl> + " than ECRYPTFS_MAX_ENCRYPTED_KEY_BYTES \ n "); <nl> + rc = - EINVAL ; <nl> + goto out_free ; <nl> + } <nl> if ( unlikely ( data [(* packet_size )++] != 0x04 )) { <nl> printk ( KERN_WARNING " Unknown version number [% d ]\ n ", <nl> data [(* packet_size ) - 1 ]);
enum { <nl> # define DM_DEV_SET_GEOMETRY _IOWR ( DM_IOCTL , DM_DEV_SET_GEOMETRY_CMD , struct dm_ioctl ) <nl>  <nl> # define DM_VERSION_MAJOR 4 <nl> -# define DM_VERSION_MINOR 31 <nl> +# define DM_VERSION_MINOR 32 <nl> # define DM_VERSION_PATCHLEVEL 0 <nl> -# define DM_VERSION_EXTRA "- ioctl ( 2015 - 3 - 12 )" <nl> +# define DM_VERSION_EXTRA "- ioctl ( 2015 - 6 - 26 )" <nl>  <nl> /* Status bits */ <nl> # define DM_READONLY_FLAG ( 1 << 0 ) /* In / Out */
u32 bond_xmit_hash ( struct bonding * bond , struct sk_buff * skb ) <nl> struct flow_keys flow ; <nl> u32 hash ; <nl>  <nl> + if ( bond -> params . xmit_policy == BOND_XMIT_POLICY_ENCAP34 && <nl> + skb -> l4_hash ) <nl> + return skb -> hash ; <nl> + <nl> if ( bond -> params . xmit_policy == BOND_XMIT_POLICY_LAYER2 || <nl> ! bond_flow_dissect ( bond , skb , & flow )) <nl> return bond_eth_hash ( skb );
struct drm_gem_object * msm_gem_new ( struct drm_device * dev , <nl>  <nl> fail : <nl> if ( obj ) <nl> - drm_gem_object_unreference_unlocked ( obj ); <nl> + drm_gem_object_unreference ( obj ); <nl>  <nl> return ERR_PTR ( ret ); <nl> }
# define RCAR_IRQ_ACK_RECV (~( MAT | MDR ) & 0xFF ) <nl>  <nl> # define ID_LAST_MSG ( 1 << 0 ) <nl> -# define ID_IOERROR ( 1 << 1 ) <nl> # define ID_DONE ( 1 << 2 ) <nl> # define ID_ARBLOST ( 1 << 3 ) <nl> # define ID_NACK ( 1 << 4 ) <nl> static int rcar_i2c_master_xfer ( struct i2c_adapter * adap , <nl> break ; <nl> } <nl>  <nl> - if ( rcar_i2c_flags_has ( priv , ID_IOERROR )) { <nl> - ret = - EIO ; <nl> - break ; <nl> - } <nl> - <nl> ret = i + 1 ; /* The number of transfer */ <nl> } <nl> out :
int sched_domain_level_max ; <nl>  <nl> static int __init setup_relax_domain_level ( char * str ) <nl> { <nl> - unsigned long val ; <nl> - <nl> - val = simple_strtoul ( str , NULL , 0 ); <nl> - if ( val < sched_domain_level_max ) <nl> - default_relax_domain_level = val ; <nl> + if ( kstrtoint ( str , 0 , & default_relax_domain_level )) <nl> + pr_warn (" Unable to set relax_domain_level \ n "); <nl>  <nl> return 1 ; <nl> } <nl> struct sched_domain * build_sched_domain ( struct sched_domain_topology_level * tl , <nl> if (! sd ) <nl> return child ; <nl>  <nl> - set_domain_attribute ( sd , attr ); <nl> cpumask_and ( sched_domain_span ( sd ), cpu_map , tl -> mask ( cpu )); <nl> if ( child ) { <nl> sd -> level = child -> level + 1 ; <nl> struct sched_domain * build_sched_domain ( struct sched_domain_topology_level * tl , <nl> child -> parent = sd ; <nl> } <nl> sd -> child = child ; <nl> + set_domain_attribute ( sd , attr ); <nl>  <nl> return sd ; <nl> }
static int snd_pcm_oss_open_file ( struct file * file , <nl> for ( idx = 0 ; idx < 2 ; idx ++) { <nl> if ( setup [ idx ]. disable ) <nl> continue ; <nl> + if (! pcm -> streams [ idx ]. substream_count ) <nl> + continue ; /* no matching substream */ <nl> if ( idx == SNDRV_PCM_STREAM_PLAYBACK ) { <nl> if (! ( f_mode & FMODE_WRITE )) <nl> continue ;
static struct eg20t_port * pch_uart_init_port ( struct pci_dev * pdev , <nl> priv -> port . line = num ++; <nl> priv -> trigger = PCH_UART_HAL_TRIGGER_M ; <nl>  <nl> + spin_lock_init (& priv -> port . lock ); <nl> + <nl> pci_set_drvdata ( pdev , priv ); <nl> pch_uart_hal_request ( pdev , fifosize , base_baud ); <nl> 
static int qedr_update_qp_state ( struct qedr_dev * dev , <nl> int status = 0 ; <nl>  <nl> if ( new_state == qp -> state ) <nl> - return 1 ; <nl> + return 0 ; <nl>  <nl> switch ( qp -> state ) { <nl> case QED_ROCE_QP_STATE_RESET :
befs_check_sb ( struct super_block * sb ) <nl> return BEFS_ERR ; <nl> } <nl>  <nl> + <nl> + /* ag_shift also encodes the same information as blocks_per_ag in a <nl> + * different way , non - fatal consistency check <nl> + */ <nl> + if (( 1 << befs_sb -> ag_shift ) != befs_sb -> blocks_per_ag ) <nl> + befs_error ( sb , " ag_shift disagrees with blocks_per_ag ."); <nl> + <nl> if ( befs_sb -> log_start != befs_sb -> log_end || befs_sb -> flags == BEFS_DIRTY ) { <nl> befs_error ( sb , " Filesystem not clean ! There are blocks in the " <nl> " journal . You must boot into BeOS and mount this volume "
i915_gem_wait_ioctl ( struct drm_device * dev , void * data , struct drm_file * file ) <nl> u32 seqno = 0 ; <nl> int ret = 0 ; <nl>  <nl> + if ( args -> flags != 0 ) <nl> + return - EINVAL ; <nl> + <nl> ret = i915_mutex_lock_interruptible ( dev ); <nl> if ( ret ) <nl> return ret ;
static bool new_idmap_permitted ( const struct file * file , <nl> u32 id = new_map -> extent [ 0 ]. lower_first ; <nl> if ( cap_setid == CAP_SETUID ) { <nl> kuid_t uid = make_kuid ( ns -> parent , id ); <nl> - if ( uid_eq ( uid , file -> f_cred -> fsuid )) <nl> + if ( uid_eq ( uid , file -> f_cred -> euid )) <nl> return true ; <nl> } <nl> }
static void hidinput_configure_usage ( struct hid_input * hidinput , struct hid_fiel <nl> break ; <nl>  <nl> case HID_UP_BUTTON : <nl> - code = (( usage -> hid - 1 ) & 0xf ); <nl> + code = (( usage -> hid - 1 ) & HID_USAGE ); <nl>  <nl> switch ( field -> application ) { <nl> case HID_GD_MOUSE :
done : <nl> skb_set_tail_pointer ( skb , len ); <nl> } <nl>  <nl> + if (! skb -> sk || skb -> destructor == sock_edemux ) <nl> + skb_condense ( skb ); <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL ( ___pskb_trim );
print_graph_entry ( struct ftrace_graph_ent_entry * field , struct trace_seq * s , <nl>  <nl> /* Proc */ <nl> if ( tracer_flags . val & TRACE_GRAPH_PRINT_PROC ) { <nl> - ret = print_graph_proc ( s , pid ); <nl> + ret = print_graph_proc ( s , ent -> pid ); <nl> if ( ret == TRACE_TYPE_PARTIAL_LINE ) <nl> return TRACE_TYPE_PARTIAL_LINE ; <nl> 
static void dlm_run_purge_list ( struct dlm_ctxt * dlm , <nl> /* This may drop and reacquire the dlm spinlock if it <nl> * has to do migration . */ <nl> mlog ( 0 , " calling dlm_purge_lockres !\ n "); <nl> + dlm_lockres_get ( lockres ); <nl> if ( dlm_purge_lockres ( dlm , lockres )) <nl> BUG (); <nl> + dlm_lockres_put ( lockres ); <nl> mlog ( 0 , " DONE calling dlm_purge_lockres !\ n "); <nl>  <nl> /* Avoid adding any scheduling latencies */
int do_fallocate ( struct file * file , int mode , loff_t offset , loff_t len ) <nl> SYSCALL_DEFINE ( fallocate )( int fd , int mode , loff_t offset , loff_t len ) <nl> { <nl> struct file * file ; <nl> - int error = - EBADF ; <nl> + int error = - EBADF , fput_needed ; <nl>  <nl> - file = fget ( fd ); <nl> + file = fget_light ( fd , & fput_needed ); <nl> if ( file ) { <nl> error = do_fallocate ( file , mode , offset , len ); <nl> - fput ( file ); <nl> + fput_light ( file , fput_needed ); <nl> } <nl>  <nl> return error ;
long v4l_compat_ioctl32 ( struct file * file , unsigned int cmd , unsigned long arg ) <nl> { <nl> int ret = - ENOIOCTLCMD ; <nl>  <nl> - if (! file -> f_op -> ioctl ) <nl> + if (! file -> f_op -> ioctl && ! file -> f_op -> unlocked_ioctl ) <nl> return ret ; <nl>  <nl> switch ( cmd ) {
static int zcache_cpu_notifier ( struct notifier_block * nb , <nl> kp -> objnodes [ kp -> nr - 1 ] = NULL ; <nl> kp -> nr --; <nl> } <nl> - kmem_cache_free ( zcache_obj_cache , kp -> obj ); <nl> - free_page (( unsigned long ) kp -> page ); <nl> + if ( kp -> obj ) { <nl> + kmem_cache_free ( zcache_obj_cache , kp -> obj ); <nl> + kp -> obj = NULL ; <nl> + } <nl> + if ( kp -> page ) { <nl> + free_page (( unsigned long ) kp -> page ); <nl> + kp -> page = NULL ; <nl> + } <nl> break ; <nl> default : <nl> break ;
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> spin_unlock_bh (& g_cdev . lock ); <nl> if ( pd == NULL ) <nl> return - ENXIO ; <nl> + if (!( pd -> state & OZ_PD_S_CONNECTED )) <nl> + return - EAGAIN ; <nl> eb = & pd -> elt_buff ; <nl> ei = oz_elt_info_alloc ( eb ); <nl> if ( ei == NULL ) {
int netvsc_send ( struct hv_device * device , <nl> packet -> send_buf_index = NETVSC_INVALID_INDEX ; <nl> packet -> cp_partial = false ; <nl>  <nl> + /* Send control message directly without accessing msd ( Multi - Send <nl> + * Data ) field which may be changed during data packet processing . <nl> + */ <nl> + if (! skb ) { <nl> + cur_send = packet ; <nl> + goto send_now ; <nl> + } <nl> + <nl> msdp = & net_device -> msd [ q_idx ]; <nl>  <nl> /* batch packets in send buffer if possible */ <nl> int netvsc_send ( struct hv_device * device , <nl> } <nl> } <nl>  <nl> + send_now : <nl> if ( cur_send ) <nl> ret = netvsc_send_pkt ( cur_send , net_device , pb , skb ); <nl> 
static int nl80211_new_interface ( struct sk_buff * skb , struct genl_info * info ) <nl> return err ; <nl> } <nl>  <nl> - msg = nlmsg_new ( NLMSG_DEFAULT_SIZE , GFP_KERNEL ); <nl> - if (! msg ) <nl> - return - ENOMEM ; <nl> - <nl> err = parse_monitor_flags ( type == NL80211_IFTYPE_MONITOR ? <nl> info -> attrs [ NL80211_ATTR_MNTR_FLAGS ] : NULL , <nl> & flags ); <nl> static int nl80211_new_interface ( struct sk_buff * skb , struct genl_info * info ) <nl> !( rdev -> wiphy . features & NL80211_FEATURE_ACTIVE_MONITOR )) <nl> return - EOPNOTSUPP ; <nl>  <nl> + msg = nlmsg_new ( NLMSG_DEFAULT_SIZE , GFP_KERNEL ); <nl> + if (! msg ) <nl> + return - ENOMEM ; <nl> + <nl> wdev = rdev_add_virtual_intf ( rdev , <nl> nla_data ( info -> attrs [ NL80211_ATTR_IFNAME ]), <nl> type , err ? NULL : & flags , & params );
void intel_setup_bios ( struct drm_device * dev ) <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl>  <nl> /* Set the Panel Power On / Off timings if uninitialized . */ <nl> - if (( I915_READ ( PP_ON_DELAYS ) == 0 ) && ( I915_READ ( PP_OFF_DELAYS ) == 0 )) { <nl> + if (! HAS_PCH_SPLIT ( dev ) && <nl> + I915_READ ( PP_ON_DELAYS ) == 0 && I915_READ ( PP_OFF_DELAYS ) == 0 ) { <nl> /* Set T2 to 40ms and T5 to 200ms */ <nl> I915_WRITE ( PP_ON_DELAYS , 0x019007d0 ); <nl> 
int xhci_hub_control ( struct usb_hcd * hcd , u16 typeReq , u16 wValue , <nl> if ( hcd -> speed != HCD_USB3 ) <nl> goto error ; <nl>  <nl> + /* Set the U1 and U2 exit latencies . */ <nl> memcpy ( buf , & usb_bos_descriptor , <nl> USB_DT_BOS_SIZE + USB_DT_USB_SS_CAP_SIZE ); <nl> temp = xhci_readl ( xhci , & xhci -> cap_regs -> hcs_params3 ); <nl> buf [ 12 ] = HCS_U1_LATENCY ( temp ); <nl> put_unaligned_le16 ( HCS_U2_LATENCY ( temp ), & buf [ 13 ]); <nl>  <nl> + /* Indicate whether the host has LTM support . */ <nl> + temp = xhci_readl ( xhci , & xhci -> cap_regs -> hcc_params ); <nl> + if ( HCC_LTC ( temp )) <nl> + buf [ 8 ] |= USB_LTM_SUPPORT ; <nl> + <nl> spin_unlock_irqrestore (& xhci -> lock , flags ); <nl> return USB_DT_BOS_SIZE + USB_DT_USB_SS_CAP_SIZE ; <nl> case GetPortStatus :
static unsigned int rss_cpus ; <nl> module_param ( rss_cpus , uint , 0444 ); <nl> MODULE_PARM_DESC ( rss_cpus , " Number of CPUs to use for Receive - Side Scaling "); <nl>  <nl> + static int phy_flash_cfg ; <nl> + module_param ( phy_flash_cfg , int , 0644 ); <nl> + MODULE_PARM_DESC ( phy_flash_cfg , " Set PHYs into reflash mode initially "); <nl> + <nl> /************************************************************************** <nl> * <nl> * Utility functions and prototypes <nl> static int efx_probe_port ( struct efx_nic * efx ) <nl> if ( rc ) <nl> goto err ; <nl>  <nl> + if ( phy_flash_cfg ) <nl> + efx -> phy_mode = PHY_MODE_SPECIAL ; <nl> + <nl> /* Sanity check MAC address */ <nl> if ( is_valid_ether_addr ( efx -> mac_address )) { <nl> memcpy ( efx -> net_dev -> dev_addr , efx -> mac_address , ETH_ALEN );
static int ftrace_save_ops_tramp_hash ( struct ftrace_ops * ops ) <nl> if ( ftrace_rec_count ( rec ) == 1 && <nl> ftrace_ops_test ( ops , rec -> ip , rec )) { <nl>  <nl> + /* <nl> + * If another ops adds to a rec , the rec will <nl> + * lose its trampoline and never get it back <nl> + * until all ops are off of it . <nl> + */ <nl> + if (!( rec -> flags & FTRACE_FL_TRAMP )) <nl> + continue ; <nl> + <nl> /* This record had better have a trampoline */ <nl> if ( FTRACE_WARN_ON (!( rec -> flags & FTRACE_FL_TRAMP_EN ))) <nl> return - 1 ;
static struct afs_server * afs_alloc_server ( struct afs_cell * cell , <nl>  <nl> memcpy (& server -> addr , addr , sizeof ( struct in_addr )); <nl> server -> addr . s_addr = addr -> s_addr ; <nl> + _leave (" = % p {% d }", server , atomic_read (& server -> usage )); <nl> + } else { <nl> + _leave (" = NULL [ nomem ]"); <nl> } <nl> - <nl> - _leave (" = % p {% d }", server , atomic_read (& server -> usage )); <nl> return server ; <nl> } <nl> 
int udf_CS0toUTF8 ( struct ustr * utf_o , const struct ustr * ocu_i ) <nl> if ( c < 0x80U ) <nl> utf_o -> u_name [ utf_o -> u_len ++] = ( uint8_t ) c ; <nl> else if ( c < 0x800U ) { <nl> + if ( utf_o -> u_len > ( UDF_NAME_LEN - 4 )) <nl> + break ; <nl> utf_o -> u_name [ utf_o -> u_len ++] = <nl> ( uint8_t )( 0xc0 | ( c >> 6 )); <nl> utf_o -> u_name [ utf_o -> u_len ++] = <nl> ( uint8_t )( 0x80 | ( c & 0x3f )); <nl> } else { <nl> + if ( utf_o -> u_len > ( UDF_NAME_LEN - 5 )) <nl> + break ; <nl> utf_o -> u_name [ utf_o -> u_len ++] = <nl> ( uint8_t )( 0xe0 | ( c >> 12 )); <nl> utf_o -> u_name [ utf_o -> u_len ++] = <nl> static int udf_CS0toNLS ( struct nls_table * nls , struct ustr * utf_o , <nl> c = ( c << 8 ) | ocu [ i ++]; <nl>  <nl> len = nls -> uni2char ( c , & utf_o -> u_name [ utf_o -> u_len ], <nl> - UDF_NAME_LEN - utf_o -> u_len ); <nl> + UDF_NAME_LEN - 2 - utf_o -> u_len ); <nl> /* Valid character ? */ <nl> if ( len >= 0 ) <nl> utf_o -> u_len += len ;
static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> while ( 1 ) { <nl> struct btrfs_ordered_extent * ordered ; <nl> lock_extent (& BTRFS_I ( src )-> io_tree , off , off + len , GFP_NOFS ); <nl> - ordered = btrfs_lookup_first_ordered_extent ( inode , off + len ); <nl> - if ( BTRFS_I ( src )-> delalloc_bytes == 0 && ! ordered ) <nl> + ordered = btrfs_lookup_first_ordered_extent ( src , off + len ); <nl> + if (! ordered && <nl> + ! test_range_bit (& BTRFS_I ( src )-> io_tree , off , off + len , <nl> + EXTENT_DELALLOC , 0 , NULL )) <nl> break ; <nl> unlock_extent (& BTRFS_I ( src )-> io_tree , off , off + len , GFP_NOFS ); <nl> if ( ordered ) <nl> btrfs_put_ordered_extent ( ordered ); <nl> - btrfs_wait_ordered_range ( src , off , off + len ); <nl> + btrfs_wait_ordered_range ( src , off , len ); <nl> } <nl>  <nl> /* clone data */
static int nfs_open_revalidate ( struct dentry * dentry , struct nameidata * nd ) <nl> struct inode * dir ; <nl> int openflags , ret = 0 ; <nl>  <nl> - if (! is_atomic_open ( nd )) <nl> + if (! is_atomic_open ( nd ) || d_mountpoint ( dentry )) <nl> goto no_open ; <nl> parent = dget_parent ( dentry ); <nl> dir = parent -> d_inode ;
static int conntrack_mt_check ( const struct xt_mtchk_param * par ) <nl> { <nl> int ret ; <nl>  <nl> + if ( strcmp ( par -> table , " raw ") == 0 ) { <nl> + pr_info (" state is undetermined at the time of raw table \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> ret = nf_ct_l3proto_try_module_get ( par -> family ); <nl> if ( ret < 0 ) <nl> pr_info (" cannot load conntrack support for proto =% u \ n ",
static int __init n810_soc_init ( void ) <nl> clk_set_parent ( sys_clkout2_src , func96m_clk ); <nl> clk_set_rate ( sys_clkout2 , 12000000 ); <nl>  <nl> - if ( gpio_request ( N810_HEADSET_AMP_GPIO , " hs_amp ") < 0 ) <nl> - BUG (); <nl> - if ( gpio_request ( N810_SPEAKER_AMP_GPIO , " spk_amp ") < 0 ) <nl> - BUG (); <nl> + BUG_ON (( gpio_request ( N810_HEADSET_AMP_GPIO , " hs_amp ") < 0 ) || <nl> + ( gpio_request ( N810_SPEAKER_AMP_GPIO , " spk_amp ") < 0 )); <nl> + <nl> gpio_direction_output ( N810_HEADSET_AMP_GPIO , 0 ); <nl> gpio_direction_output ( N810_SPEAKER_AMP_GPIO , 0 ); <nl> 
 <nl> bool rtw_IOL_applied ( struct adapter * adapter ) <nl> { <nl> - if ( 1 == adapter -> registrypriv . fw_iol ) <nl> + if ( adapter -> registrypriv . fw_iol == 1 ) <nl> return true ; <nl>  <nl> - if (( 2 == adapter -> registrypriv . fw_iol ) && (! adapter_to_dvobj ( adapter )-> ishighspeed )) <nl> + if (( adapter -> registrypriv . fw_iol == 2 ) && (! adapter_to_dvobj ( adapter )-> ishighspeed )) <nl> return true ; <nl> return false ; <nl> }
static struct pxamci_platform_data magician_mci_info = { <nl>  <nl> static struct pxaohci_platform_data magician_ohci_info = { <nl> . port_mode = PMM_PERPORT_MODE , <nl> - . flags = ENABLE_PORT1 | ENABLE_PORT3 | POWER_CONTROL_LOW , <nl> + /* port1 : CSR Bluetooth , port2 : OTG with UDC */ <nl> + . flags = ENABLE_PORT1 | ENABLE_PORT2 | POWER_CONTROL_LOW , <nl> . power_budget = 0 , <nl> + . power_on_delay = 100 , <nl> }; <nl>  <nl> /*
struct msgdma_extended_desc { <nl> /* Tx buffer control flags <nl> */ <nl> # define MSGDMA_DESC_CTL_TX_FIRST ( MSGDMA_DESC_CTL_GEN_SOP | \ <nl> - MSGDMA_DESC_CTL_TR_ERR_IRQ | \ <nl> MSGDMA_DESC_CTL_GO ) <nl>  <nl> -# define MSGDMA_DESC_CTL_TX_MIDDLE ( MSGDMA_DESC_CTL_TR_ERR_IRQ | \ <nl> - MSGDMA_DESC_CTL_GO ) <nl> +# define MSGDMA_DESC_CTL_TX_MIDDLE ( MSGDMA_DESC_CTL_GO ) <nl>  <nl> # define MSGDMA_DESC_CTL_TX_LAST ( MSGDMA_DESC_CTL_GEN_EOP | \ <nl> MSGDMA_DESC_CTL_TR_COMP_IRQ | \ <nl> - MSGDMA_DESC_CTL_TR_ERR_IRQ | \ <nl> MSGDMA_DESC_CTL_GO ) <nl>  <nl> # define MSGDMA_DESC_CTL_TX_SINGLE ( MSGDMA_DESC_CTL_GEN_SOP | \
static u32 __seccomp_phase1_filter ( int this_syscall , struct seccomp_data * sd ) <nl>  <nl> switch ( action ) { <nl> case SECCOMP_RET_ERRNO : <nl> - /* Set the low - order 16 - bits as a errno . */ <nl> + /* Set low - order bits as an errno , capped at MAX_ERRNO . */ <nl> + if ( data > MAX_ERRNO ) <nl> + data = MAX_ERRNO ; <nl> syscall_set_return_value ( current , task_pt_regs ( current ), <nl> - data , 0 ); <nl> goto skip ;
static size_t log_output ( int facility , int level , enum log_flags lflags , const c <nl> cont_flush (); <nl> } <nl>  <nl> + /* Skip empty continuation lines that couldn ' t be added - they just flush */ <nl> + if (! text_len && ( lflags & LOG_CONT )) <nl> + return 0 ; <nl> + <nl> /* If it doesn ' t end in a newline , try to buffer the current line */ <nl> if (!( lflags & LOG_NEWLINE )) { <nl> if ( cont_add ( facility , level , lflags , text , text_len ))
void jffs2_rtime_exit ( void ); <nl> int jffs2_zlib_init ( void ); <nl> void jffs2_zlib_exit ( void ); <nl> # endif <nl> +# ifdef CONFIG_JFFS2_LZO <nl> + int jffs2_lzo_init ( void ); <nl> + void jffs2_lzo_exit ( void ); <nl> +# endif <nl>  <nl> # endif /* __JFFS2_COMPR_H__ */
static const char * const cw1200_debug_link_id [] = { <nl> " REQ ", <nl> " SOFT ", <nl> " HARD ", <nl> + " RESET ", <nl> + " RESET_REMAP ", <nl> }; <nl>  <nl> static const char * cw1200_debug_mode ( int mode )
int bond_enslave ( struct net_device * bond_dev , struct net_device * slave_dev ) <nl> * anyway ( it holds no special properties of the bond device ), <nl> * so we can change it without calling change_active_interface () <nl> */ <nl> - if (! bond -> curr_active_slave ) <nl> + if (! bond -> curr_active_slave && new_slave -> link == BOND_LINK_UP ) <nl> bond -> curr_active_slave = new_slave ; <nl>  <nl> break ;
u32 hci_inquiry_cache_update ( struct hci_dev * hdev , struct inquiry_data * data , <nl> } <nl>  <nl> /* Entry not in the cache . Add new one . */ <nl> - ie = kzalloc ( sizeof ( struct inquiry_entry ), GFP_ATOMIC ); <nl> + ie = kzalloc ( sizeof ( struct inquiry_entry ), GFP_KERNEL ); <nl> if (! ie ) { <nl> flags |= MGMT_DEV_FOUND_CONFIRM_NAME ; <nl> goto done ;
int pcmcia_modify_configuration ( struct pcmcia_device * p_dev , <nl> int i ; <nl>  <nl> io_on . speed = io_speed ; <nl> + mutex_lock (& s -> ops_mutex ); <nl> for ( i = 0 ; i < MAX_IO_WIN ; i ++) { <nl> if (! s -> io [ i ]. res ) <nl> continue ; <nl> int pcmcia_modify_configuration ( struct pcmcia_device * p_dev , <nl> mdelay ( 40 ); <nl> s -> ops -> set_io_map ( s , & io_on ); <nl> } <nl> + mutex_unlock (& s -> ops_mutex ); <nl> } <nl>  <nl> return 0 ; <nl> int pcmcia_release_configuration ( struct pcmcia_device * p_dev ) <nl> } <nl> if ( c -> state & CONFIG_LOCKED ) { <nl> c -> state &= ~ CONFIG_LOCKED ; <nl> + mutex_lock (& s -> ops_mutex ); <nl> if ( c -> state & CONFIG_IO_REQ ) <nl> for ( i = 0 ; i < MAX_IO_WIN ; i ++) { <nl> if (! s -> io [ i ]. res ) <nl> int pcmcia_release_configuration ( struct pcmcia_device * p_dev ) <nl> io . map = i ; <nl> s -> ops -> set_io_map ( s , & io ); <nl> } <nl> + mutex_unlock (& s -> ops_mutex ); <nl> } <nl>  <nl> return 0 ; <nl> int pcmcia_request_configuration ( struct pcmcia_device * p_dev , <nl>  <nl> /* Configure I / O windows */ <nl> if ( c -> state & CONFIG_IO_REQ ) { <nl> + mutex_lock (& s -> ops_mutex ); <nl> iomap . speed = io_speed ; <nl> for ( i = 0 ; i < MAX_IO_WIN ; i ++) <nl> if ( s -> io [ i ]. res ) { <nl> int pcmcia_request_configuration ( struct pcmcia_device * p_dev , <nl> s -> ops -> set_io_map ( s , & iomap ); <nl> s -> io [ i ]. Config ++; <nl> } <nl> + mutex_unlock (& s -> ops_mutex ); <nl> } <nl>  <nl> c -> state |= CONFIG_LOCKED ; <nl> int pcmcia_request_io ( struct pcmcia_device * p_dev , io_req_t * req ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + mutex_lock (& s -> ops_mutex ); <nl> dev_dbg (& s -> dev , " trying to allocate resource 1 \ n "); <nl> if ( alloc_io_space ( s , req -> Attributes1 , & req -> BasePort1 , <nl> req -> NumPorts1 , req -> IOAddrLines )) { <nl> dev_dbg (& s -> dev , " allocation of resource 1 failed \ n "); <nl> + mutex_unlock (& s -> ops_mutex ); <nl> return - EBUSY ; <nl> } <nl>  <nl> int pcmcia_request_io ( struct pcmcia_device * p_dev , io_req_t * req ) <nl> req -> NumPorts2 , req -> IOAddrLines )) { <nl> dev_dbg (& s -> dev , " allocation of resource 2 failed \ n "); <nl> release_io_space ( s , req -> BasePort1 , req -> NumPorts1 ); <nl> + mutex_unlock (& s -> ops_mutex ); <nl> return - EBUSY ; <nl> } <nl> } <nl> + mutex_unlock (& s -> ops_mutex ); <nl>  <nl> c -> io = * req ; <nl> c -> state |= CONFIG_IO_REQ ;
nfsd4_create_session ( struct svc_rqst * rqstp , <nl> if ( status ) { <nl> /* an unconfirmed replay returns misordered */ <nl> status = nfserr_seq_misordered ; <nl> - goto out_cache ; <nl> + goto out ; <nl> } <nl>  <nl> cs_slot -> sl_seqid ++; /* from 0 to 1 */ <nl> nfsd4_create_session ( struct svc_rqst * rqstp , <nl> NFS4_MAX_SESSIONID_LEN ); <nl> cr_ses -> seqid = cs_slot -> sl_seqid ; <nl>  <nl> - out_cache : <nl> /* cache solo and embedded create sessions under the state lock */ <nl> nfsd4_cache_create_session ( cr_ses , cs_slot , status ); <nl> out :
static int bnx2x_set_pauseparam ( struct net_device * dev , <nl> bp -> link_params . req_flow_ctrl [ cfg_idx ] = <nl> BNX2X_FLOW_CTRL_AUTO ; <nl> } <nl> + bp -> link_params . req_fc_auto_adv = BNX2X_FLOW_CTRL_NONE ; <nl> + if ( epause -> rx_pause ) <nl> + bp -> link_params . req_fc_auto_adv |= BNX2X_FLOW_CTRL_RX ; <nl> + <nl> + if ( epause -> tx_pause ) <nl> + bp -> link_params . req_fc_auto_adv |= BNX2X_FLOW_CTRL_TX ; <nl> } <nl>  <nl> DP ( BNX2X_MSG_ETHTOOL ,
static struct tty_struct * receive_chars ( struct uart_port * port , struct pt_regs * <nl> break ; <nl>  <nl> if ( c == CON_BREAK ) { <nl> + if ( uart_handle_break ( port )) <nl> + continue ; <nl> saw_console_brk = 1 ; <nl> c = 0 ; <nl> }
static int get_log_header ( struct gfs2_jdesc * jd , unsigned int blk , <nl> { <nl> struct buffer_head * bh ; <nl> struct gfs2_log_header_host lh ; <nl> + static const u32 nothing = 0 ; <nl> u32 hash ; <nl> int error ; <nl>  <nl> static int get_log_header ( struct gfs2_jdesc * jd , unsigned int blk , <nl> if ( error ) <nl> return error ; <nl>  <nl> - memcpy (& lh , bh -> b_data , sizeof ( struct gfs2_log_header )); /* XXX */ <nl> - lh . lh_hash = 0 ; <nl> - hash = gfs2_disk_hash (( char *)& lh , sizeof ( struct gfs2_log_header )); <nl> + hash = crc32_le (( u32 )~ 0 , bh -> b_data , sizeof ( struct gfs2_log_header ) - <nl> + sizeof ( u32 )); <nl> + hash = crc32_le ( hash , ( unsigned char const *)& nothing , sizeof ( nothing )); <nl> + hash ^= ( u32 )~ 0 ; <nl> gfs2_log_header_in (& lh , bh -> b_data ); <nl> - <nl> brelse ( bh ); <nl>  <nl> if ( lh . lh_header . mh_magic != GFS2_MAGIC ||
__cmd_probe ( int argc , const char ** argv , const char * prefix __maybe_unused ) <nl> OPT_CALLBACK (' x ', " exec ", NULL , " executable | path ", <nl> " target executable name or path ", opt_set_target ), <nl> OPT_BOOLEAN ( 0 , " demangle ", & symbol_conf . demangle , <nl> - " Disable symbol demangling "), <nl> + " Enable symbol demangling "), <nl> OPT_BOOLEAN ( 0 , " demangle - kernel ", & symbol_conf . demangle_kernel , <nl> " Enable kernel symbol demangling "), <nl> OPT_END ()
lpfc_config_port_post ( struct lpfc_hba * phba ) <nl>  <nl> lpfc_init_link ( phba , pmb , phba -> cfg_topology , phba -> cfg_link_speed ); <nl> pmb -> mbox_cmpl = lpfc_sli_def_mbox_cmpl ; <nl> - if ( lpfc_sli_issue_mbox ( phba , pmb , MBX_NOWAIT ) != MBX_SUCCESS ) { <nl> + rc = lpfc_sli_issue_mbox ( phba , pmb , MBX_NOWAIT ); <nl> + if ( rc != MBX_SUCCESS ) { <nl> lpfc_printf_log ( phba , <nl> KERN_ERR , <nl> LOG_INIT , <nl> lpfc_config_port_post ( struct lpfc_hba * phba ) <nl> readl ( phba -> HAregaddr ); /* flush */ <nl>  <nl> phba -> hba_state = LPFC_HBA_ERROR ; <nl> - mempool_free ( pmb , phba -> mbox_mem_pool ); <nl> + if ( rc != MBX_BUSY ) <nl> + mempool_free ( pmb , phba -> mbox_mem_pool ); <nl> return - EIO ; <nl> } <nl> /* MBOX buffer will be freed in mbox compl */
struct net_device * alloc_rtllib ( int sizeof_priv ) <nl> rtllib_softmac_init ( ieee ); <nl>  <nl> ieee -> pHTInfo = kzalloc ( sizeof ( struct rt_hi_throughput ), GFP_KERNEL ); <nl> - if ( ieee -> pHTInfo == NULL ) <nl> + if (! ieee -> pHTInfo ) <nl> return NULL ; <nl>  <nl> HTUpdateDefaultSetting ( ieee );
static int l2cap_segment_sdu ( struct l2cap_chan * chan , <nl> struct sk_buff * skb ; <nl> u16 sdu_len ; <nl> size_t pdu_len ; <nl> - int err = 0 ; <nl> u8 sar ; <nl>  <nl> BT_DBG (" chan % p , msg % p , len % d ", chan , msg , ( int ) len ); <nl> static int l2cap_segment_sdu ( struct l2cap_chan * chan , <nl> } <nl> } <nl>  <nl> - return err ; <nl> + return 0 ; <nl> } <nl>  <nl> int l2cap_chan_send ( struct l2cap_chan * chan , struct msghdr * msg , size_t len ,
static int sd_init ( struct gspca_dev * gspca_dev ) <nl> struct sd * sd = ( struct sd *) gspca_dev ; <nl> int i ; <nl> u16 sensor_id ; <nl> - u8 test_byte ; <nl> + u8 test_byte = 0 ; <nl> u16 reg80 , reg8e ; <nl>  <nl> static const u8 read_indexs [] =
int bench_futex_requeue ( int argc , const char ** argv , <nl> gettimeofday (& end , NULL ); <nl> timersub (& end , & start , & runtime ); <nl>  <nl> + if ( nrequeued > nthreads ) <nl> + nrequeued = nthreads ; <nl> + <nl> update_stats (& requeued_stats , nrequeued ); <nl> update_stats (& requeuetime_stats , runtime . tv_usec ); <nl>  <nl> int bench_futex_requeue ( int argc , const char ** argv , <nl> if ( ret ) <nl> err ( EXIT_FAILURE , " pthread_join "); <nl> } <nl> - <nl> } <nl>  <nl> /* cleanup & report results */
int kernfs_rename_ns ( struct kernfs_node * kn , struct kernfs_node * new_parent , <nl> if (! new_name ) <nl> goto out ; <nl>  <nl> - kfree ( kn -> name ); <nl> + if ( kn -> flags & KERNFS_STATIC_NAME ) <nl> + kn -> flags &= ~ KERNFS_STATIC_NAME ; <nl> + else <nl> + kfree ( kn -> name ); <nl> + <nl> kn -> name = new_name ; <nl> } <nl> 
static int __devinit xencons_probe ( struct xenbus_device * dev , <nl> if ( devid == 0 ) <nl> return - ENODEV ; <nl>  <nl> - info = kzalloc ( sizeof ( struct xencons_info ), GFP_KERNEL | __GFP_ZERO ); <nl> + info = kzalloc ( sizeof ( struct xencons_info ), GFP_KERNEL ); <nl> if (! info ) <nl> - goto error_nomem ; <nl> + return - ENOMEM ; <nl> dev_set_drvdata (& dev -> dev , info ); <nl> info -> xbdev = dev ; <nl> info -> vtermno = xenbus_devid_to_vtermno ( devid );
static long privcmd_ioctl ( struct file * file , <nl> # ifndef HAVE_ARCH_PRIVCMD_MMAP <nl> static int privcmd_fault ( struct vm_area_struct * vma , struct vm_fault * vmf ) <nl> { <nl> + printk ( KERN_DEBUG " privcmd_fault : vma =% p % lx -% lx , pgoff =% lx , uv =% p \ n ", <nl> + vma , vma -> vm_start , vma -> vm_end , <nl> + vmf -> pgoff , vmf -> virtual_address ); <nl> + <nl> return VM_FAULT_SIGBUS ; <nl> } <nl> 
struct gb_sdio_get_caps_response { <nl>  <nl> /* see possible values below at vdd */ <nl> __le32 ocr ; <nl> - __le16 max_blk_count ; <nl> - __le16 max_blk_size ; <nl> __le32 f_min ; <nl> __le32 f_max ; <nl> + __le16 max_blk_count ; <nl> + __le16 max_blk_size ; <nl> } __packed ; <nl>  <nl> /* set ios request : response has no payload */
int rds_tcp_accept_one ( struct socket * sock ) <nl> * so we must quiesce any send threads before resetting <nl> * c_transport_data . <nl> */ <nl> - wait_event ( conn -> c_waitq , <nl> - ! test_bit ( RDS_IN_XMIT , & conn -> c_flags )); <nl> - if ( ntohl ( inet -> inet_saddr ) < ntohl ( inet -> inet_daddr )) { <nl> + if ( ntohl ( inet -> inet_saddr ) < ntohl ( inet -> inet_daddr ) || <nl> + ! conn -> c_outgoing ) { <nl> goto rst_nsk ; <nl> - } else if ( rs_tcp -> t_sock ) { <nl> + } else { <nl> + atomic_set (& conn -> c_state , RDS_CONN_CONNECTING ); <nl> + wait_event ( conn -> c_waitq , <nl> + ! test_bit ( RDS_IN_XMIT , & conn -> c_flags )); <nl> rds_tcp_restore_callbacks ( rs_tcp -> t_sock , rs_tcp ); <nl> conn -> c_outgoing = 0 ; <nl> }
static const struct net_device_ops ipgre_netdev_ops = { <nl> static void ipgre_tunnel_setup ( struct net_device * dev ) <nl> { <nl> dev -> netdev_ops = & ipgre_netdev_ops ; <nl> + dev -> type = ARPHRD_IPGRE ; <nl> ip_tunnel_setup ( dev , ipgre_net_id ); <nl> } <nl>  <nl> static int ipgre_tunnel_init ( struct net_device * dev ) <nl> memcpy ( dev -> dev_addr , & iph -> saddr , 4 ); <nl> memcpy ( dev -> broadcast , & iph -> daddr , 4 ); <nl>  <nl> - dev -> type = ARPHRD_IPGRE ; <nl> dev -> flags = IFF_NOARP ; <nl> dev -> priv_flags &= ~ IFF_XMIT_DST_RELEASE ; <nl> dev -> addr_len = 4 ;
static ssize_t ati_remote2_store_channel_mask ( struct device * dev , <nl>  <nl> mutex_lock (& ati_remote2_mutex ); <nl>  <nl> - if ( mask != ar2 -> channel_mask && ! ati_remote2_setup ( ar2 , mask )) <nl> - ar2 -> channel_mask = mask ; <nl> + if ( mask != ar2 -> channel_mask ) { <nl> + r = ati_remote2_setup ( ar2 , mask ); <nl> + if (! r ) <nl> + ar2 -> channel_mask = mask ; <nl> + } <nl>  <nl> mutex_unlock (& ati_remote2_mutex ); <nl>  <nl> usb_autopm_put_interface ( ar2 -> intf [ 0 ]); <nl>  <nl> - return count ; <nl> + return r ? r : count ; <nl> } <nl>  <nl> static ssize_t ati_remote2_show_mode_mask ( struct device * dev ,
static int __init amd64_edac_init ( void ) <nl> mcis = kzalloc ( amd_nb_num () * sizeof ( mcis [ 0 ]), GFP_KERNEL ); <nl> ecc_stngs = kzalloc ( amd_nb_num () * sizeof ( ecc_stngs [ 0 ]), GFP_KERNEL ); <nl> if (!( mcis && ecc_stngs )) <nl> - goto err_ret ; <nl> + goto err_free ; <nl>  <nl> msrs = msrs_alloc (); <nl> if (! msrs )
static int nf_tables_delchain ( struct sock * nlsk , struct sk_buff * skb , <nl> if ( IS_ERR ( chain )) <nl> return PTR_ERR ( chain ); <nl>  <nl> - if (! list_empty (& chain -> rules )) <nl> + if (! list_empty (& chain -> rules ) || chain -> use > 0 ) <nl> return - EBUSY ; <nl>  <nl> list_del (& chain -> list );
static int fsl_pcie_check_link ( struct pci_controller * hose ) <nl> if ( hose -> indirect_type & PPC_INDIRECT_TYPE_FSL_CFG_REG_LINK ) { <nl> if ( hose -> ops -> read == fsl_indirect_read_config ) { <nl> struct pci_bus bus ; <nl> - bus . number = 0 ; <nl> + bus . number = hose -> first_busno ; <nl> bus . sysdata = hose ; <nl> bus . ops = hose -> ops ; <nl> indirect_read_config (& bus , 0 , PCIE_LTSSM , 4 , & val );
struct sctp_datamsg * sctp_datamsg_from_user ( struct sctp_association * asoc , <nl> goto errout ; <nl> err = sctp_user_addto_chunk ( chunk , offset , len , msgh -> msg_iov ); <nl> if ( err < 0 ) <nl> - goto errout ; <nl> + goto errout_chunk_free ; <nl>  <nl> offset += len ; <nl>  <nl> struct sctp_datamsg * sctp_datamsg_from_user ( struct sctp_association * asoc , <nl> __skb_pull ( chunk -> skb , ( __u8 *) chunk -> chunk_hdr <nl> - ( __u8 *) chunk -> skb -> data ); <nl> if ( err < 0 ) <nl> - goto errout ; <nl> + goto errout_chunk_free ; <nl>  <nl> sctp_datamsg_assign ( msg , chunk ); <nl> list_add_tail (& chunk -> frag_list , & msg -> chunks ); <nl> struct sctp_datamsg * sctp_datamsg_from_user ( struct sctp_association * asoc , <nl>  <nl> return msg ; <nl>  <nl> + errout_chunk_free : <nl> + sctp_chunk_free ( chunk ); <nl> + <nl> errout : <nl> list_for_each_safe ( pos , temp , & msg -> chunks ) { <nl> list_del_init ( pos );
static ssize_t efivarfs_file_read ( struct file * file , char __user * userbuf , <nl> unsigned long datasize = 0 ; <nl> u32 attributes ; <nl> void * data ; <nl> - ssize_t size ; <nl> + ssize_t size = 0 ; <nl>  <nl> status = efivars -> ops -> get_variable ( var -> var . VariableName , <nl> & var -> var . VendorGuid , <nl> static ssize_t efivarfs_file_read ( struct file * file , char __user * userbuf , <nl> & var -> var . VendorGuid , <nl> & attributes , & datasize , <nl> ( data + 4 )); <nl> - <nl> if ( status != EFI_SUCCESS ) <nl> - return 0 ; <nl> + goto out_free ; <nl>  <nl> memcpy ( data , & attributes , 4 ); <nl> size = simple_read_from_buffer ( userbuf , count , ppos , <nl> data , datasize + 4 ); <nl> + out_free : <nl> kfree ( data ); <nl>  <nl> return size ;
struct cyttsp4 * cyttsp4_probe ( const struct cyttsp4_bus_ops * ops , <nl> cd -> irq = gpio_to_irq ( cd -> cpdata -> irq_gpio ); <nl> if ( cd -> irq < 0 ) { <nl> rc = - EINVAL ; <nl> - goto error_free_cd ; <nl> + goto error_free_xfer ; <nl> } <nl>  <nl> dev_set_drvdata ( dev , cd ); <nl> error_request_irq : <nl> if ( cd -> cpdata -> init ) <nl> cd -> cpdata -> init ( cd -> cpdata , 0 , dev ); <nl> dev_set_drvdata ( dev , NULL ); <nl> + error_free_xfer : <nl> + kfree ( cd -> xfer_buf ); <nl> error_free_cd : <nl> kfree ( cd ); <nl> error_alloc_data :
static int fman_init ( struct fman * fman ) <nl> /* allocate MURAM for FIFO according to total size */ <nl> fman -> fifo_offset = fman_muram_alloc ( fman -> muram , <nl> fman -> state -> total_fifo_size ); <nl> - if ( IS_ERR_VALUE ( fman -> cam_offset )) { <nl> + if ( IS_ERR_VALUE ( fman -> fifo_offset )) { <nl> free_init_resources ( fman ); <nl> dev_err ( fman -> dev , "% s : MURAM alloc for BMI FIFO failed \ n ", <nl> __func__ );
MODULE_PARM_DESC ( ql2xshiftctondsd , <nl> " Set to control shifting of command type processing " <nl> " based on total number of SG elements ."); <nl>  <nl> - static void qla2x00_free_device ( scsi_qla_host_t *); <nl> - <nl> int ql2xfdmienable = 1 ; <nl> module_param ( ql2xfdmienable , int , S_IRUGO ); <nl> MODULE_PARM_DESC ( ql2xfdmienable , <nl> static int qla2xxx_eh_host_reset ( struct scsi_cmnd *); <nl>  <nl> static int qla2x00_change_queue_depth ( struct scsi_device *, int , int ); <nl> static int qla2x00_change_queue_type ( struct scsi_device *, int ); <nl> + static void qla2x00_free_device ( scsi_qla_host_t *); <nl>  <nl> struct scsi_host_template qla2xxx_driver_template = { <nl> . module = THIS_MODULE ,
static int l2cap_sar_reassembly_sdu ( struct sock * sk , struct sk_buff * skb , u16 co <nl> pi -> sdu_len = get_unaligned_le16 ( skb -> data ); <nl> skb_pull ( skb , 2 ); <nl>  <nl> + if ( pi -> sdu_len > pi -> imtu ) { <nl> + err = - EMSGSIZE ; <nl> + break ; <nl> + } <nl> + <nl> pi -> sdu = bt_skb_alloc ( pi -> sdu_len , GFP_ATOMIC ); <nl> if (! pi -> sdu ) { <nl> err = - ENOMEM ;
nfsd_rename ( struct svc_rqst * rqstp , struct svc_fh * ffhp , char * fname , int flen , <nl> tdentry = tfhp -> fh_dentry ; <nl> tdir = tdentry -> d_inode ; <nl>  <nl> - err = ( rqstp -> rq_vers == 2 ) ? nfserr_acces : nfserr_xdev ; <nl> - if ( ffhp -> fh_export != tfhp -> fh_export ) <nl> - goto out ; <nl> - <nl> err = nfserr_perm ; <nl> if (! flen || isdotent ( fname , flen ) || ! tlen || isdotent ( tname , tlen )) <nl> goto out ; <nl> nfsd_rename ( struct svc_rqst * rqstp , struct svc_fh * ffhp , char * fname , int flen , <nl> host_err = - EXDEV ; <nl> if ( ffhp -> fh_export -> ex_path . mnt != tfhp -> fh_export -> ex_path . mnt ) <nl> goto out_dput_new ; <nl> + if ( ffhp -> fh_export -> ex_path . dentry != tfhp -> fh_export -> ex_path . dentry ) <nl> + goto out_dput_new ; <nl>  <nl> host_err = nfsd_break_lease ( odentry -> d_inode ); <nl> if ( host_err )
static int tilcdc_irq_postinstall ( struct drm_device * dev ) <nl> struct tilcdc_drm_private * priv = dev -> dev_private ; <nl>  <nl> /* enable FIFO underflow irq : */ <nl> - if ( priv -> rev == 1 ) { <nl> + if ( priv -> rev == 1 ) <nl> tilcdc_set ( dev , LCDC_RASTER_CTRL_REG , LCDC_V1_UNDERFLOW_INT_ENA ); <nl> - } else { <nl> + else <nl> tilcdc_set ( dev , LCDC_INT_ENABLE_SET_REG , LCDC_V2_UNDERFLOW_INT_ENA ); <nl> - } <nl>  <nl> return 0 ; <nl> }
struct ib_mad_agent * ib_register_mad_agent ( struct ib_device * device , <nl> goto error1 ; <nl> } <nl>  <nl> + /* Verify the QP requested is supported . For example , Ethernet devices <nl> + * will not have QP0 */ <nl> + if (! port_priv -> qp_info [ qpn ]. qp ) { <nl> + ret = ERR_PTR (- EPROTONOSUPPORT ); <nl> + goto error1 ; <nl> + } <nl> + <nl> /* Allocate structures */ <nl> mad_agent_priv = kzalloc ( sizeof * mad_agent_priv , GFP_KERNEL ); <nl> if (! mad_agent_priv ) {
static int qat_hal_init_esram ( struct icp_qat_fw_loader_handle * handle ) <nl> unsigned int csr_val ; <nl> int times = 30 ; <nl>  <nl> + if ( handle -> pci_dev -> device == ADF_C3XXX_PCI_DEVICE_ID ) <nl> + return 0 ; <nl> + <nl> csr_val = ADF_CSR_RD ( csr_addr , 0 ); <nl> if (( csr_val & ESRAM_AUTO_TINIT ) && ( csr_val & ESRAM_AUTO_TINIT_DONE )) <nl> return 0 ;
static int parport_PS2_supported ( struct parport * pb ) <nl> } <nl>  <nl> # ifdef CONFIG_PARPORT_PC_FIFO <nl> - static int __devinit parport_ECP_supported ( struct parport * pb ) <nl> + static int parport_ECP_supported ( struct parport * pb ) <nl> { <nl> int i ; <nl> int config , configb ; <nl> static int parport_ECPEPP_supported ( struct parport * pb ) <nl> /* Don ' t bother probing for modes we know we won ' t use . */ <nl> static int __devinit parport_PS2_supported ( struct parport * pb ) { return 0 ; } <nl> # ifdef CONFIG_PARPORT_PC_FIFO <nl> - static int __devinit parport_ECP_supported ( struct parport * pb ) { return 0 ; } <nl> + static int parport_ECP_supported ( struct parport * pb ) { return 0 ; } <nl> # endif <nl> static int __devinit parport_EPP_supported ( struct parport * pb ) { return 0 ; } <nl> static int __devinit parport_ECPEPP_supported ( struct parport * pb ){ return 0 ;}
static int iwl4965_load_bsm ( struct iwl_priv * priv ) <nl>  <nl> /* Tell bootstrap uCode where to find the " Initialize " uCode <nl> * in host DRAM ... host DRAM physical address bits 35 : 4 for 4965 . <nl> - * NOTE : iwl4965_initialize_alive_start () will replace these values , <nl> + * NOTE : iwl_init_alive_start () will replace these values , <nl> * after the " initialize " uCode has run , to point to <nl> - * runtime / protocol instructions and backup data cache . */ <nl> + * runtime / protocol instructions and backup data cache . <nl> + */ <nl> pinst = priv -> ucode_init . p_addr >> 4 ; <nl> pdata = priv -> ucode_init_data . p_addr >> 4 ; <nl> inst_len = priv -> ucode_init . len ; <nl> static int iwl4965_load_bsm ( struct iwl_priv * priv ) <nl>  <nl> iwl_release_nic_access ( priv ); <nl>  <nl> + priv -> ucode_type = UCODE_INIT ; <nl> + <nl> return 0 ; <nl> } <nl>  <nl> static int iwl4965_set_ucode_ptrs ( struct iwl_priv * priv ) <nl>  <nl> IWL_DEBUG_INFO (" Runtime uCode pointers are set .\ n "); <nl>  <nl> + priv -> ucode_type = UCODE_RT ; <nl> + <nl> return ret ; <nl> } <nl> 
hv_get_next_readlocation_withoffset ( const struct hv_ring_buffer_info * ring_info , <nl> u32 next = ring_info -> ring_buffer -> read_index ; <nl>  <nl> next += offset ; <nl> - next %= ring_info -> ring_datasize ; <nl> + if ( next >= ring_info -> ring_datasize ) <nl> + next -= ring_info -> ring_datasize ; <nl>  <nl> return next ; <nl> } <nl> static u32 hv_copyfrom_ringbuffer ( <nl> memcpy ( dest , ring_buffer + start_read_offset , destlen ); <nl>  <nl> start_read_offset += destlen ; <nl> - start_read_offset %= ring_buffer_size ; <nl> + if ( start_read_offset >= ring_buffer_size ) <nl> + start_read_offset -= ring_buffer_size ; <nl>  <nl> return start_read_offset ; <nl> } <nl> static u32 hv_copyto_ringbuffer ( <nl> memcpy ( ring_buffer + start_write_offset , src , srclen ); <nl>  <nl> start_write_offset += srclen ; <nl> - start_write_offset %= ring_buffer_size ; <nl> + if ( start_write_offset >= ring_buffer_size ) <nl> + start_write_offset -= ring_buffer_size ; <nl>  <nl> return start_write_offset ; <nl> }
static int imx21_hc_urb_enqueue_isoc ( struct usb_hcd * hcd , <nl> i = DIV_ROUND_UP ( wrap_frame ( <nl> cur_frame - urb -> start_frame ), <nl> urb -> interval ); <nl> - if ( urb -> transfer_flags & URB_ISO_ASAP ) { <nl> + <nl> + /* Treat underruns as if URB_ISO_ASAP was set */ <nl> + if (( urb -> transfer_flags & URB_ISO_ASAP ) || <nl> + i >= urb -> number_of_packets ) { <nl> urb -> start_frame = wrap_frame ( urb -> start_frame <nl> + i * urb -> interval ); <nl> i = 0 ; <nl> - } else if ( i >= urb -> number_of_packets ) { <nl> - ret = - EXDEV ; <nl> - goto alloc_dmem_failed ; <nl> } <nl> } <nl> }
struct ib_srq * mlx5_ib_create_srq ( struct ib_pd * pd , <nl> mlx5_vfree ( in ); <nl> if ( err ) { <nl> mlx5_ib_dbg ( dev , " create SRQ failed , err % d \ n ", err ); <nl> - goto err_srq ; <nl> + goto err_usr_kern_srq ; <nl> } <nl>  <nl> mlx5_ib_dbg ( dev , " create SRQ with srqn 0x % x \ n ", srq -> msrq . srqn ); <nl> struct ib_srq * mlx5_ib_create_srq ( struct ib_pd * pd , <nl>  <nl> err_core : <nl> mlx5_core_destroy_srq (& dev -> mdev , & srq -> msrq ); <nl> + <nl> + err_usr_kern_srq : <nl> if ( pd -> uobject ) <nl> destroy_srq_user ( pd , srq ); <nl> else
void ieee80211_check_fast_xmit ( struct sta_info * sta ) <nl> goto out ; <nl>  <nl> /* fast - xmit doesn ' t handle fragmentation at all */ <nl> - if ( local -> hw . wiphy -> frag_threshold != ( u32 )- 1 ) <nl> + if ( local -> hw . wiphy -> frag_threshold != ( u32 )- 1 && <nl> + ! local -> ops -> set_frag_threshold ) <nl> goto out ; <nl>  <nl> rcu_read_lock ();
static int gfx_v8_0_cp_gfx_start ( struct amdgpu_device * adev ) <nl> amdgpu_ring_write ( ring , 0x3a00161a ); <nl> amdgpu_ring_write ( ring , 0x0000002e ); <nl> break ; <nl> - case CHIP_TOPAZ : <nl> case CHIP_CARRIZO : <nl> amdgpu_ring_write ( ring , 0x00000002 ); <nl> amdgpu_ring_write ( ring , 0x00000000 ); <nl> break ; <nl> + case CHIP_TOPAZ : <nl> + amdgpu_ring_write ( ring , adev -> gfx . config . num_rbs == 1 ? <nl> + 0x00000000 : 0x00000002 ); <nl> + amdgpu_ring_write ( ring , 0x00000000 ); <nl> + break ; <nl> case CHIP_STONEY : <nl> amdgpu_ring_write ( ring , 0x00000000 ); <nl> amdgpu_ring_write ( ring , 0x00000000 );
static struct rtllib_qos_parameters def_qos_parameters = { <nl> { 0 , 0 , 0 , 0 } <nl> }; <nl>  <nl> - static void rtl8192_update_beacon ( void * data ) <nl> + static void _rtl92e_update_beacon ( void * data ) <nl> { <nl> struct r8192_priv * priv = container_of_work_rsl ( data , struct r8192_priv , <nl> update_beacon_wq . work ); <nl> static void _rtl92e_init_priv_task ( struct net_device * dev ) <nl> INIT_DELAYED_WORK_RSL (& priv -> rfpath_check_wq , <nl> ( void *) rtl92e_dm_rf_pathcheck_wq , dev ); <nl> INIT_DELAYED_WORK_RSL (& priv -> update_beacon_wq , <nl> - ( void *) rtl8192_update_beacon , dev ); <nl> + ( void *) _rtl92e_update_beacon , dev ); <nl> INIT_WORK_RSL (& priv -> qos_activate , ( void *) _rtl92e_qos_activate , dev ); <nl> INIT_DELAYED_WORK_RSL (& priv -> rtllib -> hw_wakeup_wq , <nl> ( void *) rtl92e_hw_wakeup_wq , dev );
int mei_cl_notify_get ( struct mei_cl * cl , bool block , bool * notify_ev ) <nl>  <nl> dev = cl -> dev ; <nl>  <nl> + if (! dev -> hbm_f_ev_supported ) { <nl> + cl_dbg ( dev , cl , " notifications not supported \ n "); <nl> + return - EOPNOTSUPP ; <nl> + } <nl> + <nl> if (! mei_cl_is_connected ( cl )) <nl> return - ENODEV ; <nl> 
static void iwl3945_init_hw_rates ( struct iwl_priv * priv , <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < IWL_RATE_COUNT ; i ++) { <nl> + for ( i = 0 ; i < IWL_RATE_COUNT_LEGACY ; i ++) { <nl> rates [ i ]. bitrate = iwl3945_rates [ i ]. ieee * 5 ; <nl> rates [ i ]. hw_value = i ; /* Rate scaling will work on indexes */ <nl> rates [ i ]. hw_value_short = i ;
static int mtk_poll_rx ( struct napi_struct * napi , int budget , <nl> /* receive data */ <nl> skb = build_skb ( data , ring -> frag_size ); <nl> if ( unlikely (! skb )) { <nl> - put_page ( virt_to_head_page ( new_data )); <nl> + skb_free_frag ( new_data ); <nl> netdev -> stats . rx_dropped ++; <nl> goto release_desc ; <nl> }
static void dma_ops_domain_free ( struct dma_ops_domain * dom ) <nl>  <nl> free_pagetable (& dom -> domain ); <nl>  <nl> + if ( dom -> domain . id ) <nl> + domain_id_free ( dom -> domain . id ); <nl> + <nl> kfree ( dom ); <nl> } <nl> 
struct sock * sk_clone_lock ( const struct sock * sk , const gfp_t priority ) <nl> is_charged = sk_filter_charge ( newsk , filter ); <nl>  <nl> if ( unlikely (! is_charged || xfrm_sk_clone_policy ( newsk , sk ))) { <nl> + /* We need to make sure that we don ' t uncharge the new <nl> + * socket if we couldn ' t charge it in the first place <nl> + * as otherwise we uncharge the parent ' s filter . <nl> + */ <nl> + if (! is_charged ) <nl> + RCU_INIT_POINTER ( newsk -> sk_filter , NULL ); <nl> sk_free_unlock_clone ( newsk ); <nl> newsk = NULL ; <nl> goto out ;
static ssize_t cifs_write ( struct file * file , const char * write_data , <nl> if ( rc != 0 ) <nl> break ; <nl> } <nl> - if ( experimEnabled || ( pTcon -> ses -> server -> secMode & <nl> - ( SECMODE_SIGN_REQUIRED | SECMODE_SIGN_ENABLED )) == 0 ) { <nl> + if ( experimEnabled || ( pTcon -> ses -> server && <nl> + ( pTcon -> ses -> server -> secMode & <nl> + ( SECMODE_SIGN_REQUIRED | SECMODE_SIGN_ENABLED ) <nl> + == 0 ))) { <nl> struct kvec iov [ 2 ]; <nl> unsigned int len ; <nl> 
static int ibmvscsis_drop_nexus ( struct ibmvscsis_tport * tport ) <nl> /* <nl> * Release the SCSI I_T Nexus to the emulated ibmvscsis Target Port <nl> */ <nl> + target_wait_for_sess_cmds ( se_sess ); <nl> + transport_deregister_session_configfs ( se_sess ); <nl> transport_deregister_session ( se_sess ); <nl> tport -> ibmv_nexus = NULL ; <nl> kfree ( nexus );
static void __init kexec_enter_virtual_mode ( void ) <nl> */ <nl> if (! efi_is_native ()) { <nl> efi_unmap_memmap (); <nl> + clear_bit ( EFI_RUNTIME_SERVICES , & efi . flags ); <nl> return ; <nl> } <nl>  <nl> static void __init __efi_enter_virtual_mode ( void ) <nl> new_memmap = efi_map_regions (& count , & pg_shift ); <nl> if (! new_memmap ) { <nl> pr_err (" Error reallocating memory , EFI runtime non - functional !\ n "); <nl> + clear_bit ( EFI_RUNTIME_SERVICES , & efi . flags ); <nl> return ; <nl> } <nl>  <nl> static void __init __efi_enter_virtual_mode ( void ) <nl>  <nl> BUG_ON (! efi . systab ); <nl>  <nl> - if ( efi_setup_page_tables ( __pa ( new_memmap ), 1 << pg_shift )) <nl> + if ( efi_setup_page_tables ( __pa ( new_memmap ), 1 << pg_shift )) { <nl> + clear_bit ( EFI_RUNTIME_SERVICES , & efi . flags ); <nl> return ; <nl> + } <nl>  <nl> efi_sync_low_kernel_mappings (); <nl> efi_dump_pagetable ();
static struct of_device * __init scan_one_device ( struct device_node * dp , <nl> if (! parent ) <nl> strcpy ( op -> dev . bus_id , " root "); <nl> else <nl> - strcpy ( op -> dev . bus_id , dp -> path_component_name ); <nl> + sprintf ( op -> dev . bus_id , "% s @% 08x ", dp -> name , dp -> node ); <nl>  <nl> if ( of_device_register ( op )) { <nl> printk ("% s : Could not register of device .\ n ",
# define LSS_PWS_BITS 2 /* power state width */ <nl>  <nl> /* Supported device IDs */ <nl> +# define PCI_DEVICE_ID_PENWELL 0x0828 <nl> # define PCI_DEVICE_ID_TANGIER 0x11a1 <nl>  <nl> struct mid_pwr_dev { <nl> static int mid_pwr_probe ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl> return 0 ; <nl> } <nl>  <nl> - static int tng_set_initial_state ( struct mid_pwr * pwr ) <nl> + static int mid_set_initial_state ( struct mid_pwr * pwr ) <nl> { <nl> unsigned int i , j ; <nl> int ret ; <nl> static int tng_set_initial_state ( struct mid_pwr * pwr ) <nl> return 0 ; <nl> } <nl>  <nl> - static const struct mid_pwr_device_info tng_info = { <nl> - . set_initial_state = tng_set_initial_state , <nl> + static const struct mid_pwr_device_info mid_info = { <nl> + . set_initial_state = mid_set_initial_state , <nl> }; <nl>  <nl> static const struct pci_device_id mid_pwr_pci_ids [] = { <nl> - { PCI_VDEVICE ( INTEL , PCI_DEVICE_ID_TANGIER ), ( kernel_ulong_t )& tng_info }, <nl> + { PCI_VDEVICE ( INTEL , PCI_DEVICE_ID_PENWELL ), ( kernel_ulong_t )& mid_info }, <nl> + { PCI_VDEVICE ( INTEL , PCI_DEVICE_ID_TANGIER ), ( kernel_ulong_t )& mid_info }, <nl> {} <nl> }; <nl> MODULE_DEVICE_TABLE ( pci , mid_pwr_pci_ids );
static void * __init early_node_mem ( int nodeid , unsigned long start , <nl> unsigned long mem = find_e820_area ( start , end , size , align ); <nl> void * ptr ; <nl>  <nl> - if ( mem != - 1L ) { <nl> - mem = round_up ( mem , align ); <nl> + if ( mem != - 1L ) <nl> return __va ( mem ); <nl> - } <nl> + <nl> ptr = __alloc_bootmem_nopanic ( size , align , __pa ( MAX_DMA_ADDRESS )); <nl> if ( ptr == NULL ) { <nl> printk ( KERN_ERR " Cannot find % lu bytes in node % d \ n ",
static int asoc_simple_card_hw_params ( struct snd_pcm_substream * substream , <nl> { <nl> struct snd_soc_pcm_runtime * rtd = substream -> private_data ; <nl> struct snd_soc_dai * codec_dai = rtd -> codec_dai ; <nl> + struct snd_soc_dai * cpu_dai = rtd -> cpu_dai ; <nl> struct simple_card_data * priv = snd_soc_card_get_drvdata ( rtd -> card ); <nl> struct simple_dai_props * dai_props = <nl> & priv -> dai_props [ rtd - rtd -> card -> rtd ]; <nl> static int asoc_simple_card_hw_params ( struct snd_pcm_substream * substream , <nl> mclk = params_rate ( params ) * mclk_fs ; <nl> ret = snd_soc_dai_set_sysclk ( codec_dai , 0 , mclk , <nl> SND_SOC_CLOCK_IN ); <nl> + if ( ret && ret != - ENOTSUPP ) <nl> + goto err ; <nl> + <nl> + ret = snd_soc_dai_set_sysclk ( cpu_dai , 0 , mclk , <nl> + SND_SOC_CLOCK_OUT ); <nl> + if ( ret && ret != - ENOTSUPP ) <nl> + goto err ; <nl> } <nl>  <nl> + err : <nl> return ret ; <nl> } <nl> 
nfsd_open ( struct svc_rqst * rqstp , struct svc_fh * fhp , umode_t type , <nl> * If we get here , then the client has already done an " open ", <nl> * and ( hopefully ) checked permission - so allow OWNER_OVERRIDE <nl> * in case a chmod has now revoked permission . <nl> + * <nl> + * Arguably we should also allow the owner override for <nl> + * directories , but we never have and it doesn ' t seem to have <nl> + * caused anyone a problem . If we were to change this , note <nl> + * also that our filldir callbacks would need a variant of <nl> + * lookup_one_len that doesn ' t check permissions . <nl> */ <nl> - err = fh_verify ( rqstp , fhp , type , may_flags | NFSD_MAY_OWNER_OVERRIDE ); <nl> + if ( type == S_IFREG ) <nl> + may_flags |= NFSD_MAY_OWNER_OVERRIDE ; <nl> + err = fh_verify ( rqstp , fhp , type , may_flags ); <nl> if ( err ) <nl> goto out ; <nl> 
acpi_ex_load_op ( union acpi_operand_object * obj_desc , <nl> */ <nl> status = acpi_tb_add_table ( table_ptr , & table_index ); <nl> if ( ACPI_FAILURE ( status )) { <nl> - return_ACPI_STATUS ( status ); <nl> + goto cleanup ; <nl> } <nl>  <nl> status =
static void ddebug_change ( const struct ddebug_query * query , <nl>  <nl> if (! newflags ) <nl> dt -> num_enabled --; <nl> - else if (! dp - flags ) <nl> + else if (! dp -> flags ) <nl> dt -> num_enabled ++; <nl> dp -> flags = newflags ; <nl> if ( newflags ) {
svc_tcp_accept ( struct svc_sock * svsk ) <nl> serv -> sv_name ); <nl> printk ( KERN_NOTICE <nl> "% s : last TCP connect from % s \ n ", <nl> - serv -> sv_name , buf ); <nl> + serv -> sv_name , __svc_print_addr ( sin , <nl> + buf , sizeof ( buf ))); <nl> } <nl> /* <nl> * Always select the oldest socket . It ' s not fair ,
static const struct of_device_id usb_extcon_dt_match [] = { <nl> }; <nl> MODULE_DEVICE_TABLE ( of , usb_extcon_dt_match ); <nl>  <nl> + static const struct platform_device_id usb_extcon_platform_ids [] = { <nl> + { . name = " extcon - usb - gpio ", }, <nl> + { /* sentinel */ } <nl> +}; <nl> + MODULE_DEVICE_TABLE ( platform , usb_extcon_platform_ids ); <nl> + <nl> static struct platform_driver usb_extcon_driver = { <nl> . probe = usb_extcon_probe , <nl> . remove = usb_extcon_remove , <nl> static struct platform_driver usb_extcon_driver = { <nl> . pm = & usb_extcon_pm_ops , <nl> . of_match_table = usb_extcon_dt_match , <nl> }, <nl> + . id_table = usb_extcon_platform_ids , <nl> }; <nl>  <nl> module_platform_driver ( usb_extcon_driver );
static int p54_tx ( struct ieee80211_hw * dev , struct sk_buff * skb ) <nl> struct p54_common * priv = dev -> priv ; <nl> struct p54_hdr * hdr ; <nl> struct p54_tx_data * txhdr ; <nl> - size_t padding , len , tim_len ; <nl> + size_t padding , len , tim_len = 0 ; <nl> int i , j , ridx ; <nl> u16 hdr_flags = 0 , aid = 0 ; <nl> u8 rate , queue ;
intelfbhw_program_mode ( struct intelfb_info * dinfo , <nl> /* Wait for vblank . For now , just wait for a 50Hz cycle ( 20ms )) */ <nl> mdelay ( 20 ); <nl>  <nl> + OUTREG ( DVOB , INREG ( DVOB ) & ~ PORT_ENABLE ); <nl> + OUTREG ( DVOC , INREG ( DVOC ) & ~ PORT_ENABLE ); <nl> + OUTREG ( ADPA , INREG ( ADPA ) & ~ ADPA_DAC_ENABLE ); <nl> + <nl> /* Disable Sync */ <nl> tmp = INREG ( ADPA ); <nl> tmp &= ~ ADPA_DPMS_CONTROL_MASK ; <nl> intelfbhw_program_mode ( struct intelfb_info * dinfo , <nl> OUTREG ( dpll_reg , tmp ); <nl>  <nl> /* Set PLL parameters */ <nl> - OUTREG ( dpll_reg , * dpll & ~ DPLL_VCO_ENABLE ); <nl> OUTREG ( fp0_reg , * fp0 ); <nl> OUTREG ( fp1_reg , * fp1 ); <nl>  <nl> /* Enable PLL */ <nl> - tmp = INREG ( dpll_reg ); <nl> - tmp |= DPLL_VCO_ENABLE ; <nl> - OUTREG ( dpll_reg , tmp ); <nl> + OUTREG ( dpll_reg , * dpll ); <nl>  <nl> /* Set DVOs B / C */ <nl> OUTREG ( DVOB , hw -> dvob );
static int ds2760_battery_remove ( struct platform_device * pdev ) <nl> & di -> set_charged_work ); <nl> destroy_workqueue ( di -> monitor_wqueue ); <nl> power_supply_unregister (& di -> bat ); <nl> + kfree ( di ); <nl>  <nl> return 0 ; <nl> }
static const struct cap_s { <nl> static void reg_r ( struct gspca_dev * gspca_dev , <nl> u16 value , int len ) <nl> { <nl> - usb_control_msg ( gspca_dev -> dev , <nl> + int ret ; <nl> + <nl> + if ( gspca_dev -> usb_err < 0 ) <nl> + return ; <nl> + ret = usb_control_msg ( gspca_dev -> dev , <nl> usb_rcvctrlpipe ( gspca_dev -> dev , 0 ), <nl> 0x0c , <nl> USB_DIR_IN | USB_TYPE_VENDOR | USB_RECIP_DEVICE , <nl> value , 0 , gspca_dev -> usb_buf , len , <nl> 500 ); <nl> + if ( ret < 0 ) { <nl> + PDEBUG ( D_ERR , " reg_r % 04x failed % d ", value , ret ); <nl> + gspca_dev -> usb_err = ret ; <nl> + } <nl> } <nl>  <nl> static void reg_w ( struct gspca_dev * gspca_dev , u16 value , u16 index )
# define CYBERJACK_PRODUCT_ID 0x0100 <nl>  <nl> /* Function prototypes */ <nl> + static int cyberjack_attach ( struct usb_serial * serial ); <nl> static int cyberjack_port_probe ( struct usb_serial_port * port ); <nl> static int cyberjack_port_remove ( struct usb_serial_port * port ); <nl> static int cyberjack_open ( struct tty_struct * tty , <nl> static struct usb_serial_driver cyberjack_device = { <nl> . description = " Reiner SCT Cyberjack USB card reader ", <nl> . id_table = id_table , <nl> . num_ports = 1 , <nl> + . attach = cyberjack_attach , <nl> . port_probe = cyberjack_port_probe , <nl> . port_remove = cyberjack_port_remove , <nl> . open = cyberjack_open , <nl> struct cyberjack_private { <nl> short wrsent ; /* Data already sent */ <nl> }; <nl>  <nl> + static int cyberjack_attach ( struct usb_serial * serial ) <nl> +{ <nl> + if ( serial -> num_bulk_out < serial -> num_ports ) <nl> + return - ENODEV ; <nl> + <nl> + return 0 ; <nl> +} <nl> + <nl> static int cyberjack_port_probe ( struct usb_serial_port * port ) <nl> { <nl> struct cyberjack_private * priv ;
static void uvc_set_le_value ( struct uvc_control_mapping * mapping , <nl> int offset = mapping -> offset ; <nl> __u8 mask ; <nl>  <nl> + /* According to the v4l2 spec , writing any value to a button control <nl> + * should result in the action belonging to the button control being <nl> + * triggered . UVC devices however want to see a 1 written -> override <nl> + * value . <nl> + */ <nl> + if ( mapping -> v4l2_type == V4L2_CTRL_TYPE_BUTTON ) <nl> + value = - 1 ; <nl> + <nl> data += offset / 8 ; <nl> offset &= 7 ; <nl> 
static void mxs_auart_reset ( struct uart_port * u ) <nl>  <nl> static int mxs_auart_startup ( struct uart_port * u ) <nl> { <nl> + int ret ; <nl> struct mxs_auart_port * s = to_auart_port ( u ); <nl>  <nl> - clk_prepare_enable ( s -> clk ); <nl> + ret = clk_prepare_enable ( s -> clk ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> writel ( AUART_CTRL0_CLKGATE , u -> membase + AUART_CTRL0_CLR ); <nl>  <nl> auart_console_setup ( struct console * co , char * options ) <nl> if (! s ) <nl> return - ENODEV ; <nl>  <nl> - clk_prepare_enable ( s -> clk ); <nl> + ret = clk_prepare_enable ( s -> clk ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> if ( options ) <nl> uart_parse_options ( options , & baud , & parity , & bits , & flow );
static void bm_work ( struct work_struct * work ) <nl> * root , and thus , IRM . <nl> */ <nl> new_root_id = local_id ; <nl> - fw_notice ( card , "% s , making local node (% 02x ) root \ n ", <nl> - " BM lock failed ", new_root_id ); <nl> + fw_notice ( card , " BM lock failed (% s ), making local node (% 02x ) root \ n ", <nl> + fw_rcode_string ( rcode ), new_root_id ); <nl> goto pick_me ; <nl> } <nl> } else if ( card -> bm_generation != generation ) {
static int vxlan_dev_configure ( struct net * src_net , struct net_device * dev , <nl> dst -> remote_ip . sa . sa_family = AF_INET ; <nl>  <nl> if ( dst -> remote_ip . sa . sa_family == AF_INET6 || <nl> - vxlan -> cfg . saddr . sa . sa_family == AF_INET6 ) <nl> + vxlan -> cfg . saddr . sa . sa_family == AF_INET6 ) { <nl> + if (! IS_ENABLED ( CONFIG_IPV6 )) <nl> + return - EPFNOSUPPORT ; <nl> use_ipv6 = true ; <nl> + } <nl>  <nl> if ( conf -> remote_ifindex ) { <nl> struct net_device * lowerdev
static int ahash_prepare_alg ( struct ahash_alg * alg ) <nl> struct crypto_alg * base = & alg -> halg . base ; <nl>  <nl> if ( alg -> halg . digestsize > PAGE_SIZE / 8 || <nl> - alg -> halg . statesize > PAGE_SIZE / 8 ) <nl> + alg -> halg . statesize > PAGE_SIZE / 8 || <nl> + alg -> halg . statesize == 0 ) <nl> return - EINVAL ; <nl>  <nl> base -> cra_type = & crypto_ahash_type ;
static void __gdm_wimax_event_send ( struct work_struct * work ) <nl> e = list_entry ( wm_event . evtq . next , struct evt_entry , list ); <nl> spin_unlock_irqrestore (& wm_event . evt_lock , flags ); <nl>  <nl> - sscanf ( e -> dev -> name , " wm % d ", & idx ); <nl> - netlink_send ( wm_event . sock , idx , 0 , e -> evt_data , e -> size ); <nl> + if ( sscanf ( e -> dev -> name , " wm % d ", & idx ) == 1 ) <nl> + netlink_send ( wm_event . sock , idx , 0 , e -> evt_data , <nl> + e -> size ); <nl>  <nl> spin_lock_irqsave (& wm_event . evt_lock , flags ); <nl> list_del (& e -> list );
int ext4_punch_hole ( struct inode * inode , loff_t offset , loff_t length ) <nl> up_write (& EXT4_I ( inode )-> i_data_sem ); <nl> if ( IS_SYNC ( inode )) <nl> ext4_handle_sync ( handle ); <nl> + <nl> + /* Now release the pages again to reduce race window */ <nl> + if ( last_block_offset > first_block_offset ) <nl> + truncate_pagecache_range ( inode , first_block_offset , <nl> + last_block_offset ); <nl> + <nl> inode -> i_mtime = inode -> i_ctime = ext4_current_time ( inode ); <nl> ext4_mark_inode_dirty ( handle , inode ); <nl> out_stop :
static int get_strength ( struct drxk_state * state , u64 * strength ) <nl> return status ; <nl>  <nl> /* SCU c . o . c . */ <nl> - read16 ( state , SCU_RAM_AGC_RF_IACCU_HI_CO__A , & scu_coc ); <nl> + status = read16 ( state , SCU_RAM_AGC_RF_IACCU_HI_CO__A , & scu_coc ); <nl> if ( status < 0 ) <nl> return status ; <nl> 
static int sbp2scsi_slave_configure ( struct scsi_device * sdev ) <nl> blk_queue_dma_alignment ( sdev -> request_queue , ( 512 - 1 )); <nl> sdev -> use_10_for_rw = 1 ; <nl>  <nl> + if ( sdev -> type == TYPE_ROM ) <nl> + sdev -> use_10_for_ms = 1 ; <nl> if ( sdev -> type == TYPE_DISK && <nl> lu -> workarounds & SBP2_WORKAROUND_MODE_SENSE_8 ) <nl> sdev -> skip_ms_page_8 = 1 ;
static int cpsw_probe_dt ( struct cpsw_platform_data * data , <nl> } <nl> snprintf ( slave_data -> phy_id , sizeof ( slave_data -> phy_id ), <nl> PHY_ID_FMT , mdio -> name , phyid ); <nl> + put_device (& mdio -> dev ); <nl> } else { <nl> dev_err (& pdev -> dev , <nl> " No slave [% d ] phy_id , phy - handle , or fixed - link property \ n ",
static void vmbus_process_offer ( struct work_struct * work ) <nl> spin_lock_irqsave (& vmbus_connection . channel_lock , flags ); <nl> list_del (& newchannel -> listentry ); <nl> spin_unlock_irqrestore (& vmbus_connection . channel_lock , flags ); <nl> + kfree ( newchannel -> device_obj ); <nl>  <nl> free_channel ( newchannel ); <nl> } else {
static char * cplb_print_entry ( char * buf , int type ) <nl> int entry = 0 , used_cplb = 0 ; <nl>  <nl> if ( type == CPLB_I ) { <nl> - buf += sprintf ( buf , " Instrction CPLB entry :\ n "); <nl> + buf += sprintf ( buf , " Instruction CPLB entry :\ n "); <nl> p_addr = ipdt_table ; <nl> p_data = ipdt_table + 1 ; <nl> p_icount = ipdt_swapcount_table ;
xfs_uuid_mount ( <nl> uuid_t * uuid = & mp -> m_sb . sb_uuid ; <nl> int hole , i ; <nl>  <nl> + /* Publish UUID in struct super_block */ <nl> + BUILD_BUG_ON ( sizeof ( mp -> m_super -> s_uuid ) != sizeof ( uuid_t )); <nl> + memcpy (& mp -> m_super -> s_uuid , uuid , sizeof ( uuid_t )); <nl> + <nl> if ( mp -> m_flags & XFS_MOUNT_NOUUID ) <nl> return 0 ; <nl> 
static void ath_beacon_config_sta ( struct ath_softc * sc , <nl> u64 tsf ; <nl> int num_beacons , offset , dtim_dec_count , cfp_dec_count ; <nl>  <nl> + /* No need to configure beacon if we are not associated */ <nl> + if (! common -> curaid ) { <nl> + ath_print ( common , ATH_DBG_BEACON , <nl> + " STA is not yet associated .. skipping beacon config \ n "); <nl> + return ; <nl> + } <nl> + <nl> memset (& bs , 0 , sizeof ( bs )); <nl> intval = conf -> beacon_interval & ATH9K_BEACON_PERIOD ; <nl>  <nl> void ath_beacon_config ( struct ath_softc * sc , struct ieee80211_vif * vif ) <nl> enum nl80211_iftype iftype ; <nl>  <nl> /* Setup the beacon configuration parameters */ <nl> - <nl> if ( vif ) { <nl> struct ieee80211_bss_conf * bss_conf = & vif -> bss_conf ; <nl> 
static int inet6_dump_addr ( struct sk_buff * skb , struct netlink_callback * cb , <nl> cb -> nlh -> nlmsg_seq , <nl> RTM_NEWADDR , <nl> NLM_F_MULTI ); <nl> + if ( err <= 0 ) <nl> + break ; <nl> } <nl> break ; <nl> case MULTICAST_ADDR : <nl> static int inet6_dump_addr ( struct sk_buff * skb , struct netlink_callback * cb , <nl> cb -> nlh -> nlmsg_seq , <nl> RTM_GETMULTICAST , <nl> NLM_F_MULTI ); <nl> + if ( err <= 0 ) <nl> + break ; <nl> } <nl> break ; <nl> case ANYCAST_ADDR : <nl> static int inet6_dump_addr ( struct sk_buff * skb , struct netlink_callback * cb , <nl> cb -> nlh -> nlmsg_seq , <nl> RTM_GETANYCAST , <nl> NLM_F_MULTI ); <nl> + if ( err <= 0 ) <nl> + break ; <nl> } <nl> break ; <nl> default :
static int marvell_pre_reset ( struct ata_port * ap ) <nl> switch ( ap -> port_no ) <nl> { <nl> case 0 : <nl> - /* Might be backward , docs unclear */ <nl> if ( inb ( ap -> ioaddr . bmdma_addr + 1 ) & 1 ) <nl> - ap -> cbl = ATA_CBL_PATA80 ; <nl> - else <nl> ap -> cbl = ATA_CBL_PATA40 ; <nl> + else <nl> + ap -> cbl = ATA_CBL_PATA80 ; <nl> + break ; <nl>  <nl> case 1 : /* Legacy SATA port */ <nl> ap -> cbl = ATA_CBL_SATA ;
static int lm3630_backlight_register ( struct lm3630_chip_data * pchip , <nl> backlight_device_register ( name , pchip -> dev , pchip , <nl> & lm3630_bank_a_ops , & props ); <nl> if ( IS_ERR ( pchip -> bled1 )) <nl> - return - EIO ; <nl> + return PTR_ERR ( pchip -> bled1 ); <nl> break ; <nl> case BLED_2 : <nl> props . brightness = pdata -> init_brt_led2 ; <nl> static int lm3630_backlight_register ( struct lm3630_chip_data * pchip , <nl> backlight_device_register ( name , pchip -> dev , pchip , <nl> & lm3630_bank_b_ops , & props ); <nl> if ( IS_ERR ( pchip -> bled2 )) <nl> - return - EIO ; <nl> + return PTR_ERR ( pchip -> bled2 ); <nl> break ; <nl> } <nl> return 0 ;
new_slab : <nl> stat ( s , ALLOC_SLAB ); <nl> c -> node = page_to_nid ( page ); <nl> c -> page = page ; <nl> + <nl> + if ( kmem_cache_debug ( s )) <nl> + goto debug ; <nl> goto load_freelist ; <nl> } <nl> if (!( gfpflags & __GFP_NOWARN ) && printk_ratelimit ())
static int max7301_probe ( struct spi_device * spi ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> - ts = kzalloc ( sizeof ( struct max7301 ), GFP_KERNEL ); <nl> + ts = devm_kzalloc (& spi -> dev , sizeof ( struct max7301 ), GFP_KERNEL ); <nl> if (! ts ) <nl> return - ENOMEM ; <nl>  <nl> static int max7301_probe ( struct spi_device * spi ) <nl> ts -> dev = & spi -> dev ; <nl>  <nl> ret = __max730x_probe ( ts ); <nl> - if ( ret ) <nl> - kfree ( ts ); <nl> return ret ; <nl> } <nl> 
retry : <nl>  <nl> blkcg = bio_blkcg ( bio ); <nl> cfqg = cfq_lookup_create_cfqg ( cfqd , blkcg ); <nl> + if (! cfqg ) { <nl> + cfqq = & cfqd -> oom_cfqq ; <nl> + goto out ; <nl> + } <nl> + <nl> cfqq = cic_to_cfqq ( cic , is_sync ); <nl>  <nl> /* <nl> retry : <nl> } else <nl> cfqq = & cfqd -> oom_cfqq ; <nl> } <nl> - <nl> + out : <nl> if ( new_cfqq ) <nl> kmem_cache_free ( cfq_pool , new_cfqq ); <nl> 
EXPORT_SYMBOL ( smp_num_siblings ); <nl>  <nl> /* Last level cache ID of each logical CPU */ <nl> u8 cpu_llc_id [ NR_CPUS ] __cpuinitdata = {[ 0 ... NR_CPUS - 1 ] = BAD_APICID }; <nl> - EXPORT_SYMBOL ( cpu_llc_id ); <nl>  <nl> /* Bitmask of currently online CPUs */ <nl> cpumask_t cpu_online_map __read_mostly ;
static void sk_prot_free ( struct proto * prot , struct sock * sk ) <nl> # ifdef CONFIG_CGROUPS <nl> void sock_update_classid ( struct sock * sk ) <nl> { <nl> - u32 classid = task_cls_classid ( current ); <nl> + u32 classid ; <nl>  <nl> + rcu_read_lock (); /* doing current task , which cannot vanish . */ <nl> + classid = task_cls_classid ( current ); <nl> + rcu_read_unlock (); <nl> if ( classid && classid != sk -> sk_classid ) <nl> sk -> sk_classid = classid ; <nl> }
static int lstats_open ( struct inode * inode , struct file * file ) <nl> struct seq_file * m ; <nl> struct task_struct * task = get_proc_task ( inode ); <nl>  <nl> + if (! task ) <nl> + return - ENOENT ; <nl> ret = single_open ( file , lstats_show_proc , NULL ); <nl> if (! ret ) { <nl> m = file -> private_data ;
static int acpi_fujitsu_add ( struct acpi_device * device ) <nl>  <nl> err_unregister_input_dev : <nl> input_unregister_device ( input ); <nl> + input = NULL ; <nl> err_free_input_dev : <nl> input_free_device ( input ); <nl> err_stop : <nl> static int acpi_fujitsu_remove ( struct acpi_device * device , int type ) <nl>  <nl> input_unregister_device ( input ); <nl>  <nl> - input_free_device ( input ); <nl> - <nl> fujitsu -> acpi_handle = NULL ; <nl>  <nl> return 0 ; <nl> static int acpi_fujitsu_hotkey_add ( struct acpi_device * device ) <nl>  <nl> err_unregister_input_dev : <nl> input_unregister_device ( input ); <nl> + input = NULL ; <nl> err_free_input_dev : <nl> input_free_device ( input ); <nl> err_free_fifo : <nl> static int acpi_fujitsu_hotkey_remove ( struct acpi_device * device , int type ) <nl>  <nl> input_unregister_device ( input ); <nl>  <nl> - input_free_device ( input ); <nl> - <nl> kfifo_free (& fujitsu_hotkey -> fifo ); <nl>  <nl> fujitsu_hotkey -> acpi_handle = NULL ;
EXPORT_SYMBOL ( skb_try_coalesce ); <nl> */ <nl> void skb_scrub_packet ( struct sk_buff * skb , bool xnet ) <nl> { <nl> - if ( xnet ) <nl> - skb_orphan ( skb ); <nl> skb -> tstamp . tv64 = 0 ; <nl> skb -> pkt_type = PACKET_HOST ; <nl> skb -> skb_iif = 0 ; <nl> skb -> ignore_df = 0 ; <nl> skb_dst_drop ( skb ); <nl> - skb -> mark = 0 ; <nl> skb_sender_cpu_clear ( skb ); <nl> secpath_reset ( skb ); <nl> nf_reset ( skb ); <nl> nf_reset_trace ( skb ); <nl> + <nl> + if (! xnet ) <nl> + return ; <nl> + <nl> + skb_orphan ( skb ); <nl> + skb -> mark = 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( skb_scrub_packet ); <nl> 
int __devinit snd_emu10k1_mixer ( struct snd_emu10k1 * emu , <nl> if ( emu -> ac97 -> id == AC97_ID_STAC9758 ) { <nl> emu -> rear_ac97 = 1 ; <nl> snd_emu10k1_ptr_write ( emu , AC97SLOT , 0 , AC97SLOT_CNTR | AC97SLOT_LFE | AC97SLOT_REAR_LEFT | AC97SLOT_REAR_RIGHT ); <nl> + snd_ac97_write_cache ( emu -> ac97 , AC97_HEADPHONE , 0x0202 ); <nl> } <nl> /* remove unused AC97 controls */ <nl> snd_ac97_write_cache ( emu -> ac97 , AC97_SURROUND_MASTER , 0x0202 );
int radeon_device_init ( struct radeon_device * rdev , <nl>  <nl> /* set DMA mask + need_dma32 flags . <nl> * PCIE - can handle 40 - bits . <nl> - * IGP - can handle 40 - bits ( in theory ) <nl> + * IGP - can handle 40 - bits <nl> * AGP - generally dma32 is safest <nl> - * PCI - only dma32 <nl> + * PCI - dma32 for legacy pci gart , 40 bits on newer asics <nl> */ <nl> rdev -> need_dma32 = false ; <nl> if ( rdev -> flags & RADEON_IS_AGP ) <nl> rdev -> need_dma32 = true ; <nl> - if ( rdev -> flags & RADEON_IS_PCI ) <nl> + if (( rdev -> flags & RADEON_IS_PCI ) && <nl> + ( rdev -> family < CHIP_RS400 )) <nl> rdev -> need_dma32 = true ; <nl>  <nl> dma_bits = rdev -> need_dma32 ? 32 : 40 ;
void tcp_fastopen_add_skb ( struct sock * sk , struct sk_buff * skb ) <nl> tp -> segs_in = 0 ; <nl> tcp_segs_in ( tp , skb ); <nl> __skb_pull ( skb , tcp_hdrlen ( skb )); <nl> + sk_forced_mem_schedule ( sk , skb -> truesize ); <nl> skb_set_owner_r ( skb , sk ); <nl>  <nl> TCP_SKB_CB ( skb )-> seq ++;
static void __show_regs ( const struct pt_regs * regs ) <nl> */ <nl> printk (" epc : % 0 * lx % pS \ n ", field , regs -> cp0_epc , <nl> ( void *) regs -> cp0_epc ); <nl> - printk (" % s \ n ", print_tainted ()); <nl> printk (" ra : % 0 * lx % pS \ n ", field , regs -> regs [ 31 ], <nl> ( void *) regs -> regs [ 31 ]); <nl> 
static int __devinit rtl8139_init_one ( struct pci_dev * pdev , <nl> for ( i = 0 ; i < 3 ; i ++) <nl> (( u16 *) ( dev -> dev_addr ))[ i ] = <nl> le16_to_cpu ( read_eeprom ( ioaddr , i + 7 , addr_len )); <nl> + memcpy ( dev -> perm_addr , dev -> dev_addr , dev -> addr_len ); <nl>  <nl> /* The Rtl8139 - specific entries in the device structure . */ <nl> dev -> open = rtl8139_open ; <nl> static struct ethtool_ops rtl8139_ethtool_ops = { <nl> . get_strings = rtl8139_get_strings , <nl> . get_stats_count = rtl8139_get_stats_count , <nl> . get_ethtool_stats = rtl8139_get_ethtool_stats , <nl> + . get_perm_addr = ethtool_op_get_perm_addr , <nl> }; <nl>  <nl> static int netdev_ioctl ( struct net_device * dev , struct ifreq * rq , int cmd )
static irqreturn_t xilinx_pcie_intr_handler ( int irq , void * data ) <nl> /* Check whether interrupt valid */ <nl> if (!( val & XILINX_PCIE_RPIFR1_INTR_VALID )) { <nl> dev_warn ( port -> dev , " RP Intr FIFO1 read error \ n "); <nl> - return IRQ_HANDLED ; <nl> + goto error ; <nl> } <nl>  <nl> if (!( val & XILINX_PCIE_RPIFR1_MSI_INTR )) { <nl> static irqreturn_t xilinx_pcie_intr_handler ( int irq , void * data ) <nl>  <nl> if (!( val & XILINX_PCIE_RPIFR1_INTR_VALID )) { <nl> dev_warn ( port -> dev , " RP Intr FIFO1 read error \ n "); <nl> - return IRQ_HANDLED ; <nl> + goto error ; <nl> } <nl>  <nl> if ( val & XILINX_PCIE_RPIFR1_MSI_INTR ) { <nl> static irqreturn_t xilinx_pcie_intr_handler ( int irq , void * data ) <nl> if ( status & XILINX_PCIE_INTR_MST_ERRP ) <nl> dev_warn ( port -> dev , " Master error poison \ n "); <nl>  <nl> + error : <nl> /* Clear the Interrupt Decode register */ <nl> pcie_write ( port , status , XILINX_PCIE_REG_IDR ); <nl> 
out : <nl> static int si476x_codec_probe ( struct snd_soc_codec * codec ) <nl> { <nl> codec -> control_data = dev_get_regmap ( codec -> dev -> parent , NULL ); <nl> - return 0 ; <nl> + return snd_soc_codec_set_cache_io ( codec , 0 , 0 , SND_SOC_REGMAP ); <nl> } <nl>  <nl> static struct snd_soc_dai_ops si476x_dai_ops = {
static int __init s3c24xxfb_probe ( struct platform_device * pdev , <nl> } <nl>  <nl> /* create device files */ <nl> - device_create_file (& pdev -> dev , & dev_attr_debug ); <nl> + ret = device_create_file (& pdev -> dev , & dev_attr_debug ); <nl> + if ( ret ) { <nl> + printk ( KERN_ERR " failed to add debug attribute \ n "); <nl> + } <nl>  <nl> printk ( KERN_INFO " fb % d : % s frame buffer device \ n ", <nl> fbinfo -> node , fbinfo -> fix . id );
static void c_can_stop ( struct net_device * dev ) <nl>  <nl> c_can_irq_control ( priv , false ); <nl>  <nl> + /* put ctrl to init on stop to end ongoing transmission */ <nl> + priv -> write_reg ( priv , C_CAN_CTRL_REG , CONTROL_INIT ); <nl> + <nl> /* deactivate pins */ <nl> pinctrl_pm_select_sleep_state ( dev -> dev . parent ); <nl> priv -> can . state = CAN_STATE_STOPPED ;
static int i40e_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> u16 wol_nvm_bits ; <nl> u16 link_status ; <nl> int err ; <nl> - u32 len ; <nl> u32 val ; <nl> u32 i ; <nl> u8 set_fc_aq_fail ; <nl> static int i40e_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> pf -> num_alloc_vsi = pf -> hw . func_caps . num_vsis ; <nl>  <nl> /* Set up the * vsi struct and our local tracking of the MAIN PF vsi . */ <nl> - len = sizeof ( struct i40e_vsi *) * pf -> num_alloc_vsi ; <nl> - pf -> vsi = kzalloc ( len , GFP_KERNEL ); <nl> + pf -> vsi = kcalloc ( pf -> num_alloc_vsi , sizeof ( struct i40e_vsi *), <nl> + GFP_KERNEL ); <nl> if (! pf -> vsi ) { <nl> err = - ENOMEM ; <nl> goto err_switch_setup ;
static int __init bm2835_mmal_init ( void ) <nl> num_cameras = MAX_BCM2835_CAMERAS ; <nl>  <nl> for ( camera = 0 ; camera < num_cameras ; camera ++) { <nl> - dev = kzalloc ( sizeof ( struct bm2835_mmal_dev ), GFP_KERNEL ); <nl> + dev = kzalloc ( sizeof (* dev ), GFP_KERNEL ); <nl> if (! dev ) { <nl> ret = - ENOMEM ; <nl> goto cleanup_gdev ;
static u32 slic_card_locate ( struct adapter * adapter ) <nl> if (! physcard ) { <nl> /* no structure allocated for this physical card yet */ <nl> physcard = kzalloc ( sizeof ( struct physcard ), GFP_ATOMIC ); <nl> - if (! physcard ) <nl> + if (! physcard ) { <nl> + if ( card_hostid == SLIC_HOSTID_DEFAULT ) <nl> + kfree ( card ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> physcard -> next = slic_global . phys_card ; <nl> slic_global . phys_card = physcard ;
static const struct net_device_ops device_netdev_ops = { <nl>  <nl> static void device_print_info ( struct vnt_private * pDevice ) <nl> { <nl> - struct net_device * dev = pDevice -> dev ; <nl> + dev_info (& pDevice -> pcid -> dev , "% s \ n ", get_chip_name ( pDevice -> chip_id )); <nl>  <nl> - pr_info ("% s : % s \ n ", dev -> name , get_chip_name ( pDevice -> chip_id )); <nl> - pr_info ("% s : MAC =% pM IO = 0x % lx Mem = 0x % lx IRQ =% d \ n ", <nl> - dev -> name , dev -> dev_addr , ( unsigned long ) pDevice -> ioaddr , <nl> - ( unsigned long ) pDevice -> PortOffset , pDevice -> dev -> irq ); <nl> + dev_info (& pDevice -> pcid -> dev , " MAC =% pM IO = 0x % lx Mem = 0x % lx IRQ =% d \ n ", <nl> + pDevice -> abyCurrentNetAddr , ( unsigned long ) pDevice -> ioaddr , <nl> + ( unsigned long ) pDevice -> PortOffset , pDevice -> pcid -> irq ); <nl> } <nl>  <nl> static void vt6655_init_info ( struct pci_dev * pcid ,
static struct usb_host_endpoint * alt_xfer ( struct usb_host_interface * alt , <nl> for ( i = 0 ; i < alt -> desc . bNumEndpoints ; i ++) { <nl> ep = & alt -> endpoint [ i ]; <nl> attr = ep -> desc . bmAttributes & USB_ENDPOINT_XFERTYPE_MASK ; <nl> - if ( attr == xfer ) <nl> + if ( attr == xfer <nl> + && ep -> desc . wMaxPacketSize != 0 ) <nl> return ep ; <nl> } <nl> return NULL ;
static int edge_startup ( struct usb_serial * serial ) <nl> int status ; <nl> u16 product_id ; <nl>  <nl> + /* Make sure we have the required endpoints when in download mode . */ <nl> + if ( serial -> interface -> cur_altsetting -> desc . bNumEndpoints > 1 ) { <nl> + if ( serial -> num_bulk_in < serial -> num_ports || <nl> + serial -> num_bulk_out < serial -> num_ports ) <nl> + return - ENODEV ; <nl> + } <nl> + <nl> /* create our private serial structure */ <nl> edge_serial = kzalloc ( sizeof ( struct edgeport_serial ), GFP_KERNEL ); <nl> if (! edge_serial )
void disable_lapic_nmi_watchdog ( void ) <nl> return ; <nl>  <nl> on_each_cpu ( stop_apic_nmi_watchdog , NULL , 0 , 1 ); <nl> - wd_ops -> unreserve (); <nl> + <nl> + if ( wd_ops ) <nl> + wd_ops -> unreserve (); <nl>  <nl> BUG_ON ( atomic_read (& nmi_active ) != 0 ); <nl> }
static int __mlxsw_sp_port_fid_join ( struct mlxsw_sp_port * mlxsw_sp_port , <nl> { <nl> struct mlxsw_sp_fid * f ; <nl>  <nl> + if ( test_bit ( fid , mlxsw_sp_port -> active_vlans )) <nl> + return 0 ; <nl> + <nl> f = mlxsw_sp_fid_find ( mlxsw_sp_port -> mlxsw_sp , fid ); <nl> if (! f ) { <nl> f = mlxsw_sp_fid_create ( mlxsw_sp_port -> mlxsw_sp , fid );
static void ad1884_fixup_thinkpad ( struct hda_codec * codec , <nl> { <nl> struct ad198x_spec * spec = codec -> spec ; <nl>  <nl> - if ( action == HDA_FIXUP_ACT_PRE_PROBE ) <nl> + if ( action == HDA_FIXUP_ACT_PRE_PROBE ) { <nl> spec -> gen . keep_eapd_on = 1 ; <nl> + spec -> gen . vmaster_mute . hook = ad_vmaster_eapd_hook ; <nl> + spec -> eapd_nid = 0x12 ; <nl> + } <nl> } <nl>  <nl> /* set magic COEFs for dmic */
static struct notifier_block fib_netdev_notifier = { <nl>  <nl> static int __net_init ip_fib_net_init ( struct net * net ) <nl> { <nl> + int err ; <nl> unsigned int i ; <nl>  <nl> net -> ipv4 . fib_table_hash = kzalloc ( <nl> static int __net_init ip_fib_net_init ( struct net * net ) <nl> for ( i = 0 ; i < FIB_TABLE_HASHSZ ; i ++) <nl> INIT_HLIST_HEAD (& net -> ipv4 . fib_table_hash [ i ]); <nl>  <nl> - return fib4_rules_init ( net ); <nl> + err = fib4_rules_init ( net ); <nl> + if ( err < 0 ) <nl> + goto fail ; <nl> + return 0 ; <nl> + <nl> + fail : <nl> + kfree ( net -> ipv4 . fib_table_hash ); <nl> + return err ; <nl> } <nl>  <nl> static void __net_exit ip_fib_net_exit ( struct net * net )
static int pdc_wdt_probe ( struct platform_device * pdev ) <nl> pdc_wdt -> wdt_dev . ops = & pdc_wdt_ops ; <nl> pdc_wdt -> wdt_dev . max_timeout = 1 << PDC_WDT_CONFIG_DELAY_MASK ; <nl> pdc_wdt -> wdt_dev . parent = & pdev -> dev ; <nl> + watchdog_set_drvdata (& pdc_wdt -> wdt_dev , pdc_wdt ); <nl>  <nl> ret = watchdog_init_timeout (& pdc_wdt -> wdt_dev , heartbeat , & pdev -> dev ); <nl> if ( ret < 0 ) { <nl> static int pdc_wdt_probe ( struct platform_device * pdev ) <nl> watchdog_set_nowayout (& pdc_wdt -> wdt_dev , nowayout ); <nl>  <nl> platform_set_drvdata ( pdev , pdc_wdt ); <nl> - watchdog_set_drvdata (& pdc_wdt -> wdt_dev , pdc_wdt ); <nl>  <nl> ret = watchdog_register_device (& pdc_wdt -> wdt_dev ); <nl> if ( ret )
static int lov_set_info_async ( const struct lu_env * env , struct obd_export * exp , <nl> } <nl>  <nl> void lov_stripe_lock ( struct lov_stripe_md * md ) <nl> + __acquires (& md -> lsm_lock ) <nl> { <nl> LASSERT ( md -> lsm_lock_owner != current_pid ()); <nl> spin_lock (& md -> lsm_lock ); <nl> void lov_stripe_lock ( struct lov_stripe_md * md ) <nl> EXPORT_SYMBOL ( lov_stripe_lock ); <nl>  <nl> void lov_stripe_unlock ( struct lov_stripe_md * md ) <nl> + __releases (& md -> lsm_lock ) <nl> { <nl> LASSERT ( md -> lsm_lock_owner == current_pid ()); <nl> md -> lsm_lock_owner = 0 ;
static void b43_supported_bands ( struct b43_wldev * dev , bool * have_2ghz_phy , <nl> * have_5ghz_phy = true ; <nl> return ; <nl> case 0x4321 : /* BCM4306 */ <nl> + /* There are 14e4 : 4321 PCI devs with 2 . 4 GHz BCM4321 ( N - PHY ) */ <nl> + if ( dev -> phy . type != B43_PHYTYPE_G ) <nl> + break ; <nl> + /* fall through */ <nl> case 0x4313 : /* BCM4311 */ <nl> case 0x431a : /* BCM4318 */ <nl> case 0x432a : /* BCM4321 */
static int snd_card_asihpi_pcm_hw_params ( struct snd_pcm_substream * substream , <nl> struct snd_card_asihpi * card = snd_pcm_substream_chip ( substream ); <nl> int err ; <nl> u16 format ; <nl> + int width ; <nl> unsigned int bytes_per_sec ; <nl>  <nl> print_hwparams ( params ); <nl> static int snd_card_asihpi_pcm_hw_params ( struct snd_pcm_substream * substream , <nl> dpcm -> hpi_buffer_attached ); <nl> } <nl> bytes_per_sec = params_rate ( params ) * params_channels ( params ); <nl> - bytes_per_sec *= snd_pcm_format_width ( params_format ( params )); <nl> + width = snd_pcm_format_width ( params_format ( params )); <nl> + bytes_per_sec *= width ; <nl> bytes_per_sec /= 8 ; <nl> - if ( bytes_per_sec <= 0 ) <nl> + if ( width < 0 || bytes_per_sec == 0 ) <nl> return - EINVAL ; <nl>  <nl> dpcm -> bytes_per_sec = bytes_per_sec ;
static const struct comedi_lrange * dac_range_table [] = { <nl>  <nl> static const struct comedi_lrange * dac_range_lkup ( int opt ) <nl> { <nl> - if ( opt < 0 || opt > 5 ) <nl> + if ( opt < 0 || opt >= 5 ) <nl> return & range_unknown ; <nl> return dac_range_table [ opt ]; <nl> }
static int intel_pstate_init_cpu ( unsigned int cpunum ) <nl>  <nl> cpu = all_cpu_data [ cpunum ]; <nl>  <nl> - intel_pstate_get_cpu_pstates ( cpu ); <nl> - <nl> cpu -> cpu = cpunum ; <nl> + intel_pstate_get_cpu_pstates ( cpu ); <nl>  <nl> init_timer_deferrable (& cpu -> timer ); <nl> cpu -> timer . function = intel_pstate_timer_func ;
static struct dgrp_proc_entry dgrp_dpa_table [] = { <nl>  <nl> void dgrp_unregister_proc ( void ) <nl> { <nl> - unregister_proc_table ( dgrp_table , dgrp_proc_dir_entry ); <nl> net_entry_pointer = NULL ; <nl> mon_entry_pointer = NULL ; <nl> dpa_entry_pointer = NULL ; <nl> ports_entry_pointer = NULL ; <nl>  <nl> if ( dgrp_proc_dir_entry ) { <nl> + unregister_proc_table ( dgrp_table , dgrp_proc_dir_entry ); <nl> remove_proc_entry ( dgrp_proc_dir_entry -> name , <nl> dgrp_proc_dir_entry -> parent ); <nl> dgrp_proc_dir_entry = NULL ; <nl> static void register_proc_table ( struct dgrp_proc_entry * table , <nl>  <nl> if ( table == NULL ) <nl> return ; <nl> + if ( root == NULL ) <nl> + return ; <nl>  <nl> for (; table -> id ; table ++) { <nl> /* Can ' t do anything without a proc name . */
static int bq24257_power_supply_init ( struct bq24257_device * bq ) <nl> & bq24257_power_supply_desc , <nl> & psy_cfg ); <nl>  <nl> - if ( IS_ERR ( bq -> charger )) <nl> - return PTR_ERR ( bq -> charger ); <nl> - <nl> - return 0 ; <nl> + return PTR_ERR_OR_ZERO ( bq -> charger ); <nl> } <nl>  <nl> static int bq24257_pg_gpio_probe ( struct bq24257_device * bq )
int drm_mode_create_dumb_ioctl ( struct drm_device * dev , <nl> return - EINVAL ; <nl>  <nl> /* overflow checks for 32bit size calculations */ <nl> + /* NOTE : DIV_ROUND_UP () can overflow */ <nl> cpp = DIV_ROUND_UP ( args -> bpp , 8 ); <nl> - if ( cpp > 0xffffffffU / args -> width ) <nl> + if (! cpp || cpp > 0xffffffffU / args -> width ) <nl> return - EINVAL ; <nl> stride = cpp * args -> width ; <nl> if ( args -> height > 0xffffffffU / stride )
static void flow_free ( struct sw_flow * flow ) <nl> { <nl> int node ; <nl>  <nl> - kfree (( struct sf_flow_acts __force *) flow -> sf_acts ); <nl> + kfree (( struct sw_flow_actions __force *) flow -> sf_acts ); <nl> for_each_node ( node ) <nl> if ( flow -> stats [ node ]) <nl> kmem_cache_free ( flow_stats_cache ,
static inline int gfar_has_errata ( struct gfar_private * priv , <nl> return priv -> errata & err ; <nl> } <nl>  <nl> - static inline u32 gfar_read ( volatile unsigned __iomem * addr ) <nl> + static inline u32 gfar_read ( unsigned __iomem * addr ) <nl> { <nl> u32 val ; <nl> - val = in_be32 ( addr ); <nl> + val = ioread32be ( addr ); <nl> return val ; <nl> } <nl>  <nl> - static inline void gfar_write ( volatile unsigned __iomem * addr , u32 val ) <nl> + static inline void gfar_write ( unsigned __iomem * addr , u32 val ) <nl> { <nl> - out_be32 ( addr , val ); <nl> + iowrite32be ( val , addr ); <nl> } <nl>  <nl> static inline void gfar_write_filer ( struct gfar_private * priv ,
int add_extent_mapping ( struct extent_map_tree * tree , <nl> rb = tree_insert (& tree -> map , em -> start , & em -> rb_node ); <nl> if ( rb ) { <nl> ret = - EEXIST ; <nl> - free_extent_map ( merge ); <nl> goto out ; <nl> } <nl> atomic_inc (& em -> refs );
static int sh_eth_ring_init ( struct net_device * ndev ) <nl> mdp -> rx_buf_sz += NET_IP_ALIGN ; <nl>  <nl> /* Allocate RX and TX skb rings */ <nl> - mdp -> rx_skbuff = kmalloc_array ( mdp -> num_rx_ring , <nl> - sizeof (* mdp -> rx_skbuff ), GFP_KERNEL ); <nl> + mdp -> rx_skbuff = kcalloc ( mdp -> num_rx_ring , sizeof (* mdp -> rx_skbuff ), <nl> + GFP_KERNEL ); <nl> if (! mdp -> rx_skbuff ) { <nl> ret = - ENOMEM ; <nl> return ret ; <nl> } <nl>  <nl> - mdp -> tx_skbuff = kmalloc_array ( mdp -> num_tx_ring , <nl> - sizeof (* mdp -> tx_skbuff ), GFP_KERNEL ); <nl> + mdp -> tx_skbuff = kcalloc ( mdp -> num_tx_ring , sizeof (* mdp -> tx_skbuff ), <nl> + GFP_KERNEL ); <nl> if (! mdp -> tx_skbuff ) { <nl> ret = - ENOMEM ; <nl> goto skb_ring_free ;
int btrfs_qgroup_inherit ( struct btrfs_trans_handle * trans , <nl>  <nl> if ( srcid ) { <nl> srcgroup = find_qgroup_rb ( fs_info , srcid ); <nl> - if (! srcgroup ) <nl> + if (! srcgroup ) { <nl> + ret = - EINVAL ; <nl> goto unlock ; <nl> + } <nl> dstgroup -> rfer = srcgroup -> rfer - level_size ; <nl> dstgroup -> rfer_cmpr = srcgroup -> rfer_cmpr - level_size ; <nl> srcgroup -> excl = level_size ; <nl> int btrfs_qgroup_inherit ( struct btrfs_trans_handle * trans , <nl> qgroup_dirty ( fs_info , srcgroup ); <nl> } <nl>  <nl> - if (! inherit ) <nl> + if (! inherit ) { <nl> + ret = - EINVAL ; <nl> goto unlock ; <nl> + } <nl>  <nl> i_qgroups = ( u64 *)( inherit + 1 ); <nl> for ( i = 0 ; i < inherit -> num_qgroups ; ++ i ) {
void ceph_handle_snap ( struct ceph_mds_client * mdsc , <nl> * queued ( again ) by ceph_update_snap_trace () <nl> * below . Queue it _now_ , under the old context . <nl> */ <nl> + spin_lock (& realm -> inodes_with_caps_lock ); <nl> list_del_init (& ci -> i_snap_realm_item ); <nl> + spin_unlock (& realm -> inodes_with_caps_lock ); <nl> spin_unlock (& inode -> i_lock ); <nl>  <nl> ceph_queue_cap_snap ( ci ,
long drm_ioctl ( struct file * filp , <nl> usize = asize = _IOC_SIZE ( cmd ); <nl> if ( drv_size > asize ) <nl> asize = drv_size ; <nl> + cmd = ioctl -> cmd_drv ; <nl> } <nl> else if (( nr >= DRM_COMMAND_END ) || ( nr < DRM_COMMAND_BASE )) { <nl> ioctl = & drm_ioctls [ nr ];
static int rds_rdma_pages ( struct rds_rdma_args * args ) <nl> return - EINVAL ; <nl>  <nl> tot_pages += nr_pages ; <nl> + <nl> + /* <nl> + * nr_pages for one entry is limited to ( UINT_MAX >> PAGE_SHIFT )+ 1 , <nl> + * so tot_pages cannot overflow without first going negative . <nl> + */ <nl> + if (( int ) tot_pages < 0 ) <nl> + return - EINVAL ; <nl> } <nl>  <nl> return tot_pages ;
card_probe_error : <nl> if ( card -> remove ) <nl> card -> remove ( card ); <nl>  <nl> + snd_soc_dapm_free (& card -> dapm ); <nl> soc_cleanup_card_debugfs ( card ); <nl> snd_card_free ( card -> snd_card ); <nl> 
int snd_hda_create_spdif_out_ctls ( struct hda_codec * codec , hda_nid_t nid ) <nl> } <nl> for ( dig_mix = dig_mixes ; dig_mix -> name ; dig_mix ++) { <nl> kctl = snd_ctl_new1 ( dig_mix , codec ); <nl> + if (! kctl ) <nl> + return - ENOMEM ; <nl> kctl -> id . index = idx ; <nl> kctl -> private_value = nid ; <nl> err = snd_hda_ctl_add ( codec , kctl ); <nl> snd_hda_attach_pcm ( struct hda_codec * codec , struct hda_pcm * pcm ) <nl> struct hda_pcm_stream * info ; <nl> int stream , err ; <nl>  <nl> - if (! pcm -> name ) <nl> + if ( snd_BUG_ON (! pcm -> name )) <nl> return - EINVAL ; <nl> for ( stream = 0 ; stream < 2 ; stream ++) { <nl> info = & pcm -> stream [ stream ];
ieee80211_tx_h_rate_ctrl ( struct ieee80211_tx_data * tx ) <nl> IEEE80211_TX_RC_USE_RTS_CTS ; <nl>  <nl> /* RC is busted */ <nl> - if ( WARN_ON ( info -> control . rates [ i ]. idx >= <nl> - sband -> n_bitrates )) { <nl> + if ( WARN_ON_ONCE ( info -> control . rates [ i ]. idx >= <nl> + sband -> n_bitrates )) { <nl> info -> control . rates [ i ]. idx = - 1 ; <nl> continue ; <nl> }
static int vmw_fb_kms_detach ( struct vmw_fb_par * par , <nl> set . y = 0 ; <nl> set . mode = NULL ; <nl> set . fb = NULL ; <nl> - set . num_connectors = 1 ; <nl> + set . num_connectors = 0 ; <nl> set . connectors = & par -> con ; <nl> ret = drm_mode_set_config_internal (& set ); <nl> if ( ret ) { <nl> int vmw_fb_off ( struct vmw_private * vmw_priv ) <nl> flush_delayed_work (& par -> local_work ); <nl>  <nl> mutex_lock (& par -> bo_mutex ); <nl> + drm_modeset_lock_all ( vmw_priv -> dev ); <nl> ( void ) vmw_fb_kms_detach ( par , true , false ); <nl> + drm_modeset_unlock_all ( vmw_priv -> dev ); <nl> mutex_unlock (& par -> bo_mutex ); <nl>  <nl> return 0 ;
static inline int netif_set_real_num_rx_queues ( struct net_device * dev , <nl> static inline int netif_copy_real_num_queues ( struct net_device * to_dev , <nl> const struct net_device * from_dev ) <nl> { <nl> - netif_set_real_num_tx_queues ( to_dev , from_dev -> real_num_tx_queues ); <nl> + int err ; <nl> + <nl> + err = netif_set_real_num_tx_queues ( to_dev , <nl> + from_dev -> real_num_tx_queues ); <nl> + if ( err ) <nl> + return err ; <nl> # ifdef CONFIG_RPS <nl> return netif_set_real_num_rx_queues ( to_dev , <nl> from_dev -> real_num_rx_queues );
static int patch_cs420x ( struct hda_codec * codec ) <nl> if (! spec ) <nl> return - ENOMEM ; <nl>  <nl> + spec -> gen . automute_hook = cs_automute ; <nl> + <nl> snd_hda_pick_fixup ( codec , cs420x_models , cs420x_fixup_tbl , <nl> cs420x_fixups ); <nl> snd_hda_apply_fixup ( codec , HDA_FIXUP_ACT_PRE_PROBE ); <nl> static int patch_cs4210 ( struct hda_codec * codec ) <nl> if (! spec ) <nl> return - ENOMEM ; <nl>  <nl> + spec -> gen . automute_hook = cs_automute ; <nl> + <nl> snd_hda_pick_fixup ( codec , cs421x_models , cs421x_fixup_tbl , <nl> cs421x_fixups ); <nl> snd_hda_apply_fixup ( codec , HDA_FIXUP_ACT_PRE_PROBE );
static int __devinit isp1761_pci_probe ( struct pci_dev * dev , <nl> hcd = isp1760_register ( pci_mem_phy0 , length , dev -> irq , <nl> IRQF_SHARED | IRQF_DISABLED , & dev -> dev , dev_name (& dev -> dev ), <nl> devflags ); <nl> - pci_set_drvdata ( dev , hcd ); <nl> - if (! hcd ) <nl> + if (! IS_ERR ( hcd )) { <nl> + pci_set_drvdata ( dev , hcd ); <nl> return 0 ; <nl> + } <nl> clean : <nl> status = - ENODEV ; <nl> iounmap ( iobase );
int mccic_irq ( struct mcam_camera * cam , unsigned int irqs ) <nl> if ( irqs & ( IRQ_EOF0 << frame )) { <nl> mcam_frame_complete ( cam , frame ); <nl> handled = 1 ; <nl> + if ( cam -> buffer_mode == B_DMA_sg ) <nl> + break ; <nl> } <nl> /* <nl> * If a frame starts , note that we have DMA active . This
static u32 mop500_sdi0_vdd_handler ( struct device * dev , unsigned int vdd , <nl> unsigned char power_mode ) <nl> { <nl> if ( power_mode == MMC_POWER_UP ) <nl> - gpio_set_value ( GPIO_SDMMC_EN , 1 ); <nl> + gpio_set_value_cansleep ( GPIO_SDMMC_EN , 1 ); <nl> else if ( power_mode == MMC_POWER_OFF ) <nl> - gpio_set_value ( GPIO_SDMMC_EN , 0 ); <nl> + gpio_set_value_cansleep ( GPIO_SDMMC_EN , 0 ); <nl>  <nl> return MCI_FBCLKEN | MCI_CMDDIREN | MCI_DATA0DIREN | <nl> MCI_DATA2DIREN | MCI_DATA31DIREN ;
static int ocfs2_rename ( struct inode * old_dir , <nl> * <nl> * And that ' s why , just like the VFS , we need a file system <nl> * rename lock . */ <nl> - if ( old_dentry != new_dentry ) { <nl> + if ( old_dir != new_dir && S_ISDIR ( old_inode -> i_mode )) { <nl> status = ocfs2_rename_lock ( osb ); <nl> if ( status < 0 ) { <nl> mlog_errno ( status );
struct nes_cqp_request * nes_get_cqp_request ( struct nes_device * nesdev ) <nl>  <nl> if (! list_empty (& nesdev -> cqp_avail_reqs )) { <nl> spin_lock_irqsave (& nesdev -> cqp . lock , flags ); <nl> - cqp_request = list_entry ( nesdev -> cqp_avail_reqs . next , <nl> + if (! list_empty (& nesdev -> cqp_avail_reqs )) { <nl> + cqp_request = list_entry ( nesdev -> cqp_avail_reqs . next , <nl> struct nes_cqp_request , list ); <nl> - list_del_init (& cqp_request -> list ); <nl> + list_del_init (& cqp_request -> list ); <nl> + } <nl> spin_unlock_irqrestore (& nesdev -> cqp . lock , flags ); <nl> - } else { <nl> + } <nl> + if ( cqp_request == NULL ) { <nl> cqp_request = kzalloc ( sizeof ( struct nes_cqp_request ), GFP_KERNEL ); <nl> if ( cqp_request ) { <nl> cqp_request -> dynamic = 1 ;
static int srp_create_ch_ib ( struct srp_rdma_ch * ch ) <nl> struct ib_qp * qp ; <nl> struct ib_fmr_pool * fmr_pool = NULL ; <nl> struct srp_fr_pool * fr_pool = NULL ; <nl> - const int m = 1 + dev -> use_fast_reg ; <nl> + const int m = dev -> use_fast_reg ? 3 : 1 ; <nl> struct ib_cq_init_attr cq_attr = {}; <nl> int ret ; <nl> 
static void xmit_common ( struct sk_buff * skb , struct ehea_swqe * swqe ) <nl> { <nl> swqe -> tx_control |= EHEA_SWQE_IMM_DATA_PRESENT | EHEA_SWQE_CRC ; <nl>  <nl> - if ( skb -> protocol != htons ( ETH_P_IP )) <nl> + if ( vlan_get_protocol ( skb ) != htons ( ETH_P_IP )) <nl> return ; <nl>  <nl> if ( skb -> ip_summed == CHECKSUM_PARTIAL )
static inline __be16 x25_type_trans ( struct sk_buff * skb , struct net_device * dev ) <nl> { <nl> skb -> mac . raw = skb -> data ; <nl> + skb -> dev = dev ; <nl> skb -> pkt_type = PACKET_HOST ; <nl>  <nl> return htons ( ETH_P_X25 );
static struct resource dove_sdio0_resources [] = { <nl> }; <nl>  <nl> static struct platform_device dove_sdio0 = { <nl> - . name = " sdhci - mv ", <nl> + . name = " sdhci - dove ", <nl> . id = 0 , <nl> . dev = { <nl> . dma_mask = & sdio_dmamask , <nl> static struct resource dove_sdio1_resources [] = { <nl> }; <nl>  <nl> static struct platform_device dove_sdio1 = { <nl> - . name = " sdhci - mv ", <nl> + . name = " sdhci - dove ", <nl> . id = 1 , <nl> . dev = { <nl> . dma_mask = & sdio_dmamask ,
out_problem : <nl>  <nl> int event__process_task ( event_t * self , struct perf_session * session ) <nl> { <nl> - struct thread * thread = perf_session__findnew ( session , self -> fork . pid ); <nl> - struct thread * parent = perf_session__findnew ( session , self -> fork . ppid ); <nl> + struct thread * thread = perf_session__findnew ( session , self -> fork . tid ); <nl> + struct thread * parent = perf_session__findnew ( session , self -> fork . ptid ); <nl>  <nl> dump_printf ("(% d :% d ):(% d :% d )\ n ", self -> fork . pid , self -> fork . tid , <nl> self -> fork . ppid , self -> fork . ptid ); <nl> - /* <nl> - * A thread clone will have the same PID for both parent and child . <nl> - */ <nl> - if ( thread == parent ) <nl> - return 0 ; <nl>  <nl> if ( self -> header . type == PERF_RECORD_EXIT ) <nl> return 0 ;
int dns_query ( const char * type , const char * name , size_t namelen , <nl> if (!* _result ) <nl> goto put ; <nl>  <nl> - memcpy (* _result , upayload -> data , len + 1 ); <nl> + memcpy (* _result , upayload -> data , len ); <nl> + * _result [ len ] = '\ 0 '; <nl> + <nl> if ( _expiry ) <nl> * _expiry = rkey -> expiry ; <nl> 
retry : <nl> return UBI_IO_BITFLIPS ; <nl> } <nl>  <nl> - if ( read != len && retries ++ < UBI_IO_RETRIES ) { <nl> + if ( retries ++ < UBI_IO_RETRIES ) { <nl> dbg_io (" error % d % s while reading % d bytes from PEB % d :% d ," <nl> " read only % zd bytes , retry ", <nl> err , errstr , len , pnum , offset , read );
prio_dequeue ( struct Qdisc * sch ) <nl> * pulling an skb . This way we avoid excessive requeues <nl> * for slower queues . <nl> */ <nl> - if (! netif_subqueue_stopped ( sch -> dev , ( q -> mq ? prio : 0 ))) { <nl> + if (! __netif_subqueue_stopped ( sch -> dev , ( q -> mq ? prio : 0 ))) { <nl> qdisc = q -> queues [ prio ]; <nl> skb = qdisc -> dequeue ( qdisc ); <nl> if ( skb ) { <nl> static struct sk_buff * rr_dequeue ( struct Qdisc * sch ) <nl> * for slower queues . If the queue is stopped , try the <nl> * next queue . <nl> */ <nl> - if (! netif_subqueue_stopped ( sch -> dev , <nl> + if (! __netif_subqueue_stopped ( sch -> dev , <nl> ( q -> mq ? q -> curband : 0 ))) { <nl> qdisc = q -> queues [ q -> curband ]; <nl> skb = qdisc -> dequeue ( qdisc );
static void frontend_init ( struct dvb_bt8xx_card * card , u32 type ) <nl> /* DST is not a frontend , attaching the ASIC */ <nl> if ( dvb_attach ( dst_attach , state , & card -> dvb_adapter ) == NULL ) { <nl> pr_err ("% s : Could not find a Twinhan DST \ n ", __func__ ); <nl> + kfree ( state ); <nl> break ; <nl> } <nl> /* Attach other DST peripherals if any */
unsigned int ata_sff_qc_issue ( struct ata_queued_cmd * qc ) <nl> break ; <nl>  <nl> default : <nl> - WARN_ON_ONCE ( 1 ); <nl> return AC_ERR_SYSTEM ; <nl> } <nl> 
static int brcmf_fws_fifocreditback_indicate ( struct brcmf_fws_info * fws , <nl>  <nl> static int brcmf_fws_txstatus_indicate ( struct brcmf_fws_info * fws , u8 * data ) <nl> { <nl> + __le32 status_le ; <nl> u32 status ; <nl> u32 hslot ; <nl> u32 genbit ; <nl> u8 flags ; <nl>  <nl> fws -> stats . txs_indicate ++; <nl> - status = le32_to_cpu (*( __le32 *) data ); <nl> + memcpy (& status_le , data , sizeof ( status_le )); <nl> + status = le32_to_cpu ( status_le ); <nl> flags = brcmf_txstatus_get_field ( status , FLAGS ); <nl> hslot = brcmf_txstatus_get_field ( status , HSLOT ); <nl> genbit = brcmf_txstatus_get_field ( status , GENERATION );
static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> if (!( src_file -> f_mode & FMODE_READ )) <nl> goto out_fput ; <nl>  <nl> + /* don ' t make the dst file partly checksummed */ <nl> + if (( BTRFS_I ( src )-> flags & BTRFS_INODE_NODATASUM ) != <nl> + ( BTRFS_I ( inode )-> flags & BTRFS_INODE_NODATASUM )) <nl> + goto out_fput ; <nl> + <nl> ret = - EISDIR ; <nl> if ( S_ISDIR ( src -> i_mode ) || S_ISDIR ( inode -> i_mode )) <nl> goto out_fput ;
static int get_cac_tdp_table ( <nl>  <nl> hwmgr -> dyn_state . cac_dtp_table = kzalloc ( table_size , GFP_KERNEL ); <nl>  <nl> - if ( NULL == hwmgr -> dyn_state . cac_dtp_table ) <nl> + if ( NULL == hwmgr -> dyn_state . cac_dtp_table ) { <nl> + kfree ( tdp_table ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> memset ( hwmgr -> dyn_state . cac_dtp_table , 0x00 , table_size ); <nl> 
* pata_pdc202xx_old . c - Promise PDC202xx PATA for new ATA layer <nl> * ( C ) 2005 Red Hat Inc <nl> * Alan Cox < alan @ lxorguk . ukuu . org . uk > <nl> - * ( C ) 2007 Bartlomiej Zolnierkiewicz <nl> + * ( C ) 2007 , 2009 Bartlomiej Zolnierkiewicz <nl> * <nl> * Based in part on linux / drivers / ide / pci / pdc202xx_old . c <nl> * <nl> static void pdc2026x_bmdma_start ( struct ata_queued_cmd * qc ) <nl> u32 len ; <nl>  <nl> /* Check we keep host level locking here */ <nl> - if ( adev -> dma_mode >= XFER_UDMA_2 ) <nl> + if ( adev -> dma_mode > XFER_UDMA_2 ) <nl> iowrite8 ( ioread8 ( clock ) | sel66 , clock ); <nl> else <nl> iowrite8 ( ioread8 ( clock ) & ~ sel66 , clock ); <nl> static void pdc2026x_bmdma_stop ( struct ata_queued_cmd * qc ) <nl> iowrite8 ( ioread8 ( clock ) & ~ sel66 , clock ); <nl> } <nl> /* Flip back to 33Mhz for PIO */ <nl> - if ( adev -> dma_mode >= XFER_UDMA_2 ) <nl> + if ( adev -> dma_mode > XFER_UDMA_2 ) <nl> iowrite8 ( ioread8 ( clock ) & ~ sel66 , clock ); <nl> ata_bmdma_stop ( qc ); <nl> pdc202xx_set_piomode ( ap , adev );
# define DOC_ECCCONF1 0x1042 <nl> # define DOC_ECCPRESET 0x1044 <nl> # define DOC_HAMMINGPARITY 0x1046 <nl> -# define DOC_BCH_SYNDROM ( idx ) ( 0x1048 + ( idx << 1 )) <nl> +# define DOC_BCH_SYNDROM ( idx ) ( 0x1048 + ( idx << 0 )) <nl>  <nl> # define DOC_PROTECTION 0x1056 <nl> # define DOC_DPS0_ADDRLOW 0x1060
tcp_sacktag_write_queue ( struct sock * sk , struct sk_buff * ack_skb , u32 prior_snd_ <nl> if ( before ( TCP_SKB_CB ( ack_skb )-> ack_seq , prior_snd_una - tp -> max_window )) <nl> return 0 ; <nl>  <nl> + if (! tp -> packets_out ) <nl> + goto out ; <nl> + <nl> /* SACK fastpath : <nl> * if the only SACK change is the increase of the end_seq of <nl> * the first block then only apply that SACK block <nl> tcp_sacktag_write_queue ( struct sock * sk , struct sk_buff * ack_skb , u32 prior_snd_ <nl> (! tp -> frto_highmark || after ( tp -> snd_una , tp -> frto_highmark ))) <nl> tcp_update_reordering ( sk , tp -> fackets_out - reord , 0 ); <nl>  <nl> + out : <nl> + <nl> # if FASTRETRANS_DEBUG > 0 <nl> BUG_TRAP (( int ) tp -> sacked_out >= 0 ); <nl> BUG_TRAP (( int ) tp -> lost_out >= 0 );
union acpi_parse_object ; <nl>  <nl> static char * acpi_gbl_mutex_names [ ACPI_NUM_MUTEX ] = { <nl> " ACPI_MTX_Interpreter ", <nl> - " ACPI_MTX_Tables ", <nl> " ACPI_MTX_Namespace ", <nl> + " ACPI_MTX_Tables ", <nl> " ACPI_MTX_Events ", <nl> " ACPI_MTX_Caches ", <nl> " ACPI_MTX_Memory ",
restore_sigcontext ( struct sigcontext __user * sc , struct pt_regs * regs , <nl> unsigned long usp ; <nl> long i , err = __get_user ( regs -> pc , & sc -> sc_pc ); <nl>  <nl> + current_thread_info ()-> restart_block . fn = do_no_restart_syscall ; <nl> + <nl> sw -> r26 = ( unsigned long ) ret_from_sys_call ; <nl>  <nl> err |= __get_user ( regs -> r0 , sc -> sc_regs + 0 ); <nl> syscall_restart ( unsigned long r0 , unsigned long r19 , <nl> regs -> pc -= 4 ; <nl> break ; <nl> case ERESTART_RESTARTBLOCK : <nl> - current_thread_info ()-> restart_block . fn = do_no_restart_syscall ; <nl> regs -> r0 = EINTR ; <nl> break ; <nl> }
int of_pci_range_to_resource ( struct of_pci_range * range , <nl> } <nl> res -> start = port ; <nl> } else { <nl> + if (( sizeof ( resource_size_t ) < 8 ) && <nl> + upper_32_bits ( range -> cpu_addr )) { <nl> + err = - EINVAL ; <nl> + goto invalid_range ; <nl> + } <nl> + <nl> res -> start = range -> cpu_addr ; <nl> } <nl> res -> end = res -> start + range -> size - 1 ;
static void flush_tlb_others_ipi ( const struct cpumask * cpumask , <nl> * We have to send the IPI only to <nl> * CPUs affected . <nl> */ <nl> - send_IPI_mask ( cpumask , INVALIDATE_TLB_VECTOR_START + sender ); <nl> + send_IPI_mask ( f -> flush_cpumask , INVALIDATE_TLB_VECTOR_START + sender ); <nl>  <nl> while (! cpumask_empty ( to_cpumask ( f -> flush_cpumask ))) <nl> cpu_relax ();
static int mwifiex_cfg80211_resume ( struct wiphy * wiphy ) <nl> } <nl> } <nl>  <nl> + if (! wiphy -> wowlan_config ) <nl> + goto done ; <nl> + <nl> priv = mwifiex_get_priv ( adapter , MWIFIEX_BSS_ROLE_STA ); <nl> mwifiex_get_wakeup_reason ( priv , HostCmd_ACT_GEN_GET , MWIFIEX_SYNC_CMD , <nl> & wakeup_reason ); <nl> static int mwifiex_cfg80211_resume ( struct wiphy * wiphy ) <nl> cfg80211_report_wowlan_wakeup (& priv -> wdev , & wakeup_report , <nl> GFP_KERNEL ); <nl>  <nl> + done : <nl> if ( adapter -> nd_info ) { <nl> for ( i = 0 ; i < adapter -> nd_info -> n_matches ; i ++) <nl> kfree ( adapter -> nd_info -> matches [ i ]);
int drm_mode_getconnector ( struct drm_device * dev , void * data , <nl>  <nl> props_count = connector -> properties . count ; <nl>  <nl> - for ( i = 0 ; i < DRM_CONNECTOR_MAX_ENCODER ; i ++) { <nl> - if ( connector -> encoder_ids [ i ] != 0 ) { <nl> + for ( i = 0 ; i < DRM_CONNECTOR_MAX_ENCODER ; i ++) <nl> + if ( connector -> encoder_ids [ i ] != 0 ) <nl> encoders_count ++; <nl> - } <nl> - } <nl>  <nl> if ( out_resp -> count_modes == 0 ) { <nl> connector -> funcs -> fill_modes ( connector ,
# define AR9300_OTP_BASE \ <nl> (( AR_SREV_9340 ( ah ) || AR_SREV_9550 ( ah )) ? 0x30000 : 0x14000 ) <nl> # define AR9300_OTP_STATUS \ <nl> - (( AR_SREV_9340 ( ah ) || AR_SREV_9550 ( ah )) ? 0x30018 : 0x15f18 ) <nl> + (( AR_SREV_9340 ( ah ) || AR_SREV_9550 ( ah )) ? 0x31018 : 0x15f18 ) <nl> # define AR9300_OTP_STATUS_TYPE 0x7 <nl> # define AR9300_OTP_STATUS_VALID 0x4 <nl> # define AR9300_OTP_STATUS_ACCESS_BUSY 0x2 <nl> # define AR9300_OTP_STATUS_SM_BUSY 0x1 <nl> # define AR9300_OTP_READ_DATA \ <nl> - (( AR_SREV_9340 ( ah ) || AR_SREV_9550 ( ah )) ? 0x3001c : 0x15f1c ) <nl> + (( AR_SREV_9340 ( ah ) || AR_SREV_9550 ( ah )) ? 0x3101c : 0x15f1c ) <nl>  <nl> enum targetPowerHTRates { <nl> HT_TARGET_RATE_0_8_16 ,
static int fdp1_remove ( struct platform_device * pdev ) <nl> return 0 ; <nl> } <nl>  <nl> - static int fdp1_pm_runtime_suspend ( struct device * dev ) <nl> + static int __maybe_unused fdp1_pm_runtime_suspend ( struct device * dev ) <nl> { <nl> struct fdp1_dev * fdp1 = dev_get_drvdata ( dev ); <nl>  <nl> static int fdp1_pm_runtime_suspend ( struct device * dev ) <nl> return 0 ; <nl> } <nl>  <nl> - static int fdp1_pm_runtime_resume ( struct device * dev ) <nl> + static int __maybe_unused fdp1_pm_runtime_resume ( struct device * dev ) <nl> { <nl> struct fdp1_dev * fdp1 = dev_get_drvdata ( dev ); <nl> 
struct ssb_sprom_core_pwr_info { <nl>  <nl> struct ssb_sprom { <nl> u8 revision ; <nl> - u8 il0mac [ 6 ]; /* MAC address for 802 . 11b / g */ <nl> - u8 et0mac [ 6 ]; /* MAC address for Ethernet */ <nl> - u8 et1mac [ 6 ]; /* MAC address for 802 . 11a */ <nl> + u8 il0mac [ 6 ] __aligned ( sizeof ( u16 )); /* MAC address for 802 . 11b / g */ <nl> + u8 et0mac [ 6 ] __aligned ( sizeof ( u16 )); /* MAC address for Ethernet */ <nl> + u8 et1mac [ 6 ] __aligned ( sizeof ( u16 )); /* MAC address for 802 . 11a */ <nl> u8 et0phyaddr ; /* MII address for enet0 */ <nl> u8 et1phyaddr ; /* MII address for enet1 */ <nl> u8 et0mdcport ; /* MDIO for enet0 */
qla2x00_probe_one ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl>  <nl> sht = & qla2x00_driver_template ; <nl> if ( pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP2422 || <nl> - pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP2432 ) <nl> + pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP2432 || <nl> + pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP5422 || <nl> + pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP5432 ) <nl> sht = & qla24xx_driver_template ; <nl> host = scsi_host_alloc ( sht , sizeof ( scsi_qla_host_t )); <nl> if ( host == NULL ) {
static void intel_hdmi_mode_set ( struct intel_encoder * encoder ) <nl> else <nl> hdmi_val |= SDVO_COLOR_FORMAT_8bpc ; <nl>  <nl> - /* Required on CPT */ <nl> - if ( intel_hdmi -> has_hdmi_sink && HAS_PCH_CPT ( dev )) <nl> + if ( intel_hdmi -> has_hdmi_sink && <nl> + ( HAS_PCH_CPT ( dev ) || IS_VALLEYVIEW ( dev ))) <nl> hdmi_val |= HDMI_MODE_SELECT_HDMI ; <nl>  <nl> if ( intel_hdmi -> has_audio ) {
again : <nl> add_dev ( numdevs ++, port , - 1 ); <nl>  <nl> /* Find out the legacy device ' s IEEE 1284 device ID . */ <nl> - deviceid = kmalloc ( 1000 , GFP_KERNEL ); <nl> + deviceid = kmalloc ( 1024 , GFP_KERNEL ); <nl> if ( deviceid ) { <nl> - if ( parport_device_id ( numdevs - 1 , deviceid , 1000 ) > 2 ) <nl> + if ( parport_device_id ( numdevs - 1 , deviceid , 1024 ) > 2 ) <nl> detected ++; <nl>  <nl> kfree ( deviceid ); <nl> static int assign_addrs ( struct parport * port ) <nl> detected ); <nl>  <nl> /* Ask the new devices to introduce themselves . */ <nl> - deviceid = kmalloc ( 1000 , GFP_KERNEL ); <nl> + deviceid = kmalloc ( 1024 , GFP_KERNEL ); <nl> if (! deviceid ) return 0 ; <nl>  <nl> for ( daisy = 0 ; thisdev < numdevs ; thisdev ++, daisy ++) <nl> - parport_device_id ( thisdev , deviceid , 1000 ); <nl> + parport_device_id ( thisdev , deviceid , 1024 ); <nl>  <nl> kfree ( deviceid ); <nl> return detected ;
ldebugfs_fid_write_common ( const char __user * buffer , size_t count , <nl> rc = sscanf ( kernbuf , "[% llx - % llx ]\ n ", <nl> ( unsigned long long *)& tmp . lsr_start , <nl> ( unsigned long long *)& tmp . lsr_end ); <nl> + if ( rc != 2 ) <nl> + return - EINVAL ; <nl> if (! range_is_sane (& tmp ) || range_is_zero (& tmp ) || <nl> tmp . lsr_start < range -> lsr_start || tmp . lsr_end > range -> lsr_end ) <nl> return - EINVAL ;
static int hdlcdrv_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> case HDLCDRVCTL_CALIBRATE : <nl> if (! capable ( CAP_SYS_RAWIO )) <nl> return - EPERM ; <nl> + if ( bi . data . calibrate > INT_MAX / s -> par . bitrate ) <nl> + return - EINVAL ; <nl> s -> hdlctx . calibrate = bi . data . calibrate * s -> par . bitrate / 16 ; <nl> return 0 ; <nl> 
int btrfs_check_data_free_space ( struct inode * inode , u64 start , u64 len ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> - /* <nl> - * Use new btrfs_qgroup_reserve_data to reserve precious data space <nl> - * <nl> - * TODO : Find a good method to avoid reserve data space for NOCOW <nl> - * range , but don ' t impact performance on quota disable case . <nl> - */ <nl> + /* Use new btrfs_qgroup_reserve_data to reserve precious data space . */ <nl> ret = btrfs_qgroup_reserve_data ( inode , start , len ); <nl> + if ( ret ) <nl> + btrfs_free_reserved_data_space_noquota ( inode , start , len ); <nl> return ret ; <nl> } <nl> 
relookup : <nl> secure_ipv6_id ( daddr -> addr . a6 )); <nl> p -> metrics [ RTAX_LOCK - 1 ] = INETPEER_METRICS_NEW ; <nl> p -> rate_tokens = 0 ; <nl> - p -> rate_last = 0 ; <nl> + /* 60 * HZ is arbitrary , but chosen enough high so that the first <nl> + * calculation of tokens is at its maximum . <nl> + */ <nl> + p -> rate_last = jiffies - 60 * HZ ; <nl> INIT_LIST_HEAD (& p -> gc_list ); <nl>  <nl> /* Link the node . */
static void blade_image_blit ( struct tridentfb_par * par , const char * data , <nl> writemmr ( par , DST1 , point ( x , y )); <nl> writemmr ( par , DST2 , point ( x + w - 1 , y + h - 1 )); <nl>  <nl> - memcpy ( par -> io_virt + 0x10000 , data , 4 * size ); <nl> + iowrite32_rep ( par -> io_virt + 0x10000 , data , size ); <nl> } <nl>  <nl> static void blade_copy_rect ( struct tridentfb_par * par ,
err_exit : <nl>  <nl> int aq_ring_rx_fill ( struct aq_ring_s * self ) <nl> { <nl> + unsigned int pages_order = fls ( AQ_CFG_RX_FRAME_MAX / PAGE_SIZE + <nl> + ( AQ_CFG_RX_FRAME_MAX % PAGE_SIZE ? 1 : 0 )) - 1 ; <nl> struct aq_ring_buff_s * buff = NULL ; <nl> int err = 0 ; <nl> int i = 0 ; <nl> int aq_ring_rx_fill ( struct aq_ring_s * self ) <nl> buff -> len = AQ_CFG_RX_FRAME_MAX ; <nl>  <nl> buff -> page = alloc_pages ( GFP_ATOMIC | __GFP_COLD | <nl> - __GFP_COMP , 0 ); <nl> + __GFP_COMP , pages_order ); <nl> if (! buff -> page ) { <nl> err = - ENOMEM ; <nl> goto err_exit ;
static inline void prefetch_dst ( const void * addr ) <nl> /* Copy from a not - aligned src to an aligned dst , using shifts . Handles 4 words <nl> * per loop . This code is derived from glibc . <nl> */ <nl> - static inline unsigned long copy_dstaligned ( unsigned long dst , <nl> + static noinline unsigned long copy_dstaligned ( unsigned long dst , <nl> unsigned long src , unsigned long len ) <nl> { <nl> /* gcc complains that a2 and a3 may be uninitialized , but actually <nl> handle_store_error : <nl> /* Returns PA_MEMCPY_OK , PA_MEMCPY_LOAD_ERROR or PA_MEMCPY_STORE_ERROR . <nl> * In case of an access fault the faulty address can be read from the per_cpu <nl> * exception data struct . */ <nl> - static unsigned long pa_memcpy_internal ( void * dstp , const void * srcp , <nl> + static noinline unsigned long pa_memcpy_internal ( void * dstp , const void * srcp , <nl> unsigned long len ) <nl> { <nl> register unsigned long src , dst , t1 , t2 , t3 ;
static int dmx_section_feed_release_filter ( struct dmx_section_feed * feed , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( feed -> is_filtering ) <nl> + if ( feed -> is_filtering ) { <nl> + /* release dvbdmx -> mutex as far as <nl> + it is acquired by stop_filtering () itself */ <nl> + mutex_unlock (& dvbdmx -> mutex ); <nl> feed -> stop_filtering ( feed ); <nl> + mutex_lock (& dvbdmx -> mutex ); <nl> + } <nl>  <nl> spin_lock_irq (& dvbdmx -> lock ); <nl> f = dvbdmxfeed -> filter ;
static void alc283_shutup ( struct hda_codec * codec ) <nl>  <nl> alc_write_coef_idx ( codec , 0x43 , 0x9004 ); <nl>  <nl> + /* depop hp during suspend */ <nl> + alc_write_coef_idx ( codec , 0x06 , 0x2100 ); <nl> + <nl> snd_hda_codec_write ( codec , hp_pin , 0 , <nl> AC_VERB_SET_AMP_GAIN_MUTE , AMP_OUT_MUTE ); <nl> 
static int ata_bus_probe ( struct ata_port * ap ) <nl>  <nl> /* reset */ <nl> if ( ap -> ops -> probe_reset ) { <nl> + for ( i = 0 ; i < ATA_MAX_DEVICES ; i ++) <nl> + classes [ i ] = ATA_DEV_UNKNOWN ; <nl> + <nl> rc = ap -> ops -> probe_reset ( ap , classes ); <nl> if ( rc ) { <nl> printk (" ata % u : reset failed ( errno =% d )\ n ", ap -> id , rc );
static int __init ic_open_devs ( void ) <nl> } <nl> } <nl>  <nl> + /* no point in waiting if we could not bring up at least one device */ <nl> + if (! ic_first_dev ) <nl> + goto have_carrier ; <nl> + <nl> /* wait for a carrier on at least one device */ <nl> start = jiffies ; <nl> while ( jiffies - start < msecs_to_jiffies ( CONF_CARRIER_TIMEOUT )) {
unmap_intr_base : <nl> iounmap ( priv -> avs_intr_base ); <nl> unmap_base : <nl> iounmap ( priv -> base ); <nl> - platform_set_drvdata ( pdev , NULL ); <nl>  <nl> return ret ; <nl> } <nl> static int brcm_avs_cpufreq_remove ( struct platform_device * pdev ) <nl> priv = platform_get_drvdata ( pdev ); <nl> iounmap ( priv -> base ); <nl> iounmap ( priv -> avs_intr_base ); <nl> - platform_set_drvdata ( pdev , NULL ); <nl>  <nl> return 0 ; <nl> }
static void rt6_free_pcpu ( struct rt6_info * non_pcpu_rt ) <nl> } <nl> } <nl>  <nl> + free_percpu ( non_pcpu_rt -> rt6i_pcpu ); <nl> non_pcpu_rt -> rt6i_pcpu = NULL ; <nl> } <nl> 
asmlinkage long compat_sys_recvmmsg ( int fd , struct compat_mmsghdr __user * mmsg , <nl> return __sys_recvmmsg ( fd , ( struct mmsghdr __user *) mmsg , vlen , <nl> flags | MSG_CMSG_COMPAT , NULL ); <nl>  <nl> - if ( get_user ( ktspec . tv_sec , & timeout -> tv_sec ) || <nl> - get_user ( ktspec . tv_nsec , & timeout -> tv_nsec )) <nl> + if ( get_compat_timespec (& ktspec , timeout )) <nl> return - EFAULT ; <nl>  <nl> datagrams = __sys_recvmmsg ( fd , ( struct mmsghdr __user *) mmsg , vlen , <nl> flags | MSG_CMSG_COMPAT , & ktspec ); <nl> - if ( datagrams > 0 && <nl> - ( put_user ( ktspec . tv_sec , & timeout -> tv_sec ) || <nl> - put_user ( ktspec . tv_nsec , & timeout -> tv_nsec ))) <nl> + if ( datagrams > 0 && put_compat_timespec (& ktspec , timeout )) <nl> datagrams = - EFAULT ; <nl>  <nl> return datagrams ;
static int be_close ( struct net_device * netdev ) <nl>  <nl> static int be_rx_qs_create ( struct be_adapter * adapter ) <nl> { <nl> + struct rss_info * rss = & adapter -> rss_info ; <nl> + u8 rss_key [ RSS_HASH_KEY_LEN ]; <nl> struct be_rx_obj * rxo ; <nl> int rc , i , j ; <nl> - u8 rss_hkey [ RSS_HASH_KEY_LEN ]; <nl> - struct rss_info * rss = & adapter -> rss_info ; <nl>  <nl> for_all_rx_queues ( adapter , rxo , i ) { <nl> rc = be_queue_alloc ( adapter , & rxo -> q , RX_Q_LEN , <nl> static int be_rx_qs_create ( struct be_adapter * adapter ) <nl> rss -> rss_flags = RSS_ENABLE_NONE ; <nl> } <nl>  <nl> - get_random_bytes ( rss_hkey , RSS_HASH_KEY_LEN ); <nl> + netdev_rss_key_fill ( rss_key , RSS_HASH_KEY_LEN ); <nl> rc = be_cmd_rss_config ( adapter , rss -> rsstable , rss -> rss_flags , <nl> - 128 , rss_hkey ); <nl> + 128 , rss_key ); <nl> if ( rc ) { <nl> rss -> rss_flags = RSS_ENABLE_NONE ; <nl> return rc ; <nl> } <nl>  <nl> - memcpy ( rss -> rss_hkey , rss_hkey , RSS_HASH_KEY_LEN ); <nl> + memcpy ( rss -> rss_hkey , rss_key , RSS_HASH_KEY_LEN ); <nl>  <nl> /* First time posting */ <nl> for_all_rx_queues ( adapter , rxo , i )
static int __inode_security_revalidate ( struct inode * inode , <nl>  <nl> might_sleep_if ( may_sleep ); <nl>  <nl> - if ( isec -> initialized != LABEL_INITIALIZED ) { <nl> + if ( ss_initialized && isec -> initialized != LABEL_INITIALIZED ) { <nl> if (! may_sleep ) <nl> return - ECHILD ; <nl> 
* This file initializes the trap entry points <nl> */ <nl>  <nl> +# include < linux / jiffies . h > <nl> # include < linux / mm . h > <nl> # include < linux / sched . h > <nl> # include < linux / tty . h > <nl> do_entUnaUser ( void __user * va , unsigned long opcode , <nl> unsigned long reg , struct pt_regs * regs ) <nl> { <nl> static int cnt = 0 ; <nl> - static long last_time = 0 ; <nl> + static unsigned long last_time ; <nl>  <nl> unsigned long tmp1 , tmp2 , tmp3 , tmp4 ; <nl> unsigned long fake_reg , * reg_addr = & fake_reg ; <nl> do_entUnaUser ( void __user * va , unsigned long opcode , <nl> with the unaliged access . */ <nl>  <nl> if (! test_thread_flag ( TIF_UAC_NOPRINT )) { <nl> - if ( cnt >= 5 && jiffies - last_time > 5 * HZ ) { <nl> + if ( cnt >= 5 && time_after ( jiffies , last_time + 5 * HZ )) { <nl> cnt = 0 ; <nl> } <nl> if (++ cnt < 5 ) {
do_kern_mount ( const char * fstype , int flags , const char * name , void * data ) <nl> mnt -> mnt_parent = mnt ; <nl> mnt -> mnt_namespace = current -> namespace ; <nl> up_write (& sb -> s_umount ); <nl> + free_secdata ( secdata ); <nl> put_filesystem ( type ); <nl> return mnt ; <nl> out_sb :
struct avtab_node * <nl> avtab_insert_nonunique ( struct avtab * h , struct avtab_key * key , struct avtab_datum * datum ) <nl> { <nl> int hvalue ; <nl> - struct avtab_node * prev , * cur , * newnode ; <nl> + struct avtab_node * prev , * cur ; <nl> u16 specified = key -> specified & ~( AVTAB_ENABLED | AVTAB_ENABLED_OLD ); <nl>  <nl> if (! h || ! h -> htable ) <nl> avtab_insert_nonunique ( struct avtab * h , struct avtab_key * key , struct avtab_datu <nl> key -> target_class < cur -> key . target_class ) <nl> break ; <nl> } <nl> - newnode = avtab_insert_node ( h , hvalue , prev , cur , key , datum ); <nl> - <nl> - return newnode ; <nl> + return avtab_insert_node ( h , hvalue , prev , cur , key , datum ); <nl> } <nl>  <nl> struct avtab_datum * avtab_search ( struct avtab * h , struct avtab_key * key )
* <nl> * For licensing information , see the file ' LICENCE ' in this directory . <nl> * <nl> - * $ Id : nodelist . c , v 1 . 90 2004 / 12 / 08 17 : 59 : 20 dwmw2 Exp $ <nl> + * $ Id : nodelist . c , v 1 . 92 2005 / 01 / 19 19 : 22 : 00 tpoynor Exp $ <nl> * <nl> */ <nl>  <nl> int jffs2_get_inode_nodes ( struct jffs2_sb_info * c , struct jffs2_inode_info * f , <nl>  <nl> valid_ref = jffs2_first_valid_node ( f -> inocache -> nodes ); <nl>  <nl> - if (! valid_ref ) <nl> + if (! valid_ref && ( f -> inocache -> ino != 1 )) <nl> printk ( KERN_WARNING " Eep . No valid nodes for ino #% u \ n ", f -> inocache -> ino ); <nl>  <nl> while ( valid_ref ) {
void __init smp_prepare_boot_cpu ( void ) <nl>  <nl> static void send_ipi_message ( const struct cpumask * mask , enum ipi_msg_type msg ) <nl> { <nl> - unsigned long flags ; <nl> - <nl> - local_irq_save ( flags ); <nl> - <nl> /* <nl> * Call the platform specific cross - CPU call function . <nl> */ <nl> smp_cross_call ( mask , msg ); <nl> - <nl> - local_irq_restore ( flags ); <nl> } <nl>  <nl> void arch_send_call_function_ipi_mask ( const struct cpumask * mask )
int usb_stor_CB_transport ( struct scsi_cmnd * srb , struct us_data * us ) <nl>  <nl> /* COMMAND STAGE */ <nl> /* let ' s send the command via the control pipe */ <nl> + /* <nl> + * Command is sometime ( f . e . after scsi_eh_prep_cmnd ) on the stack . <nl> + * Stack may be vmallocated . So no DMA for us . Make a copy . <nl> + */ <nl> + memcpy ( us -> iobuf , srb -> cmnd , srb -> cmd_len ); <nl> result = usb_stor_ctrl_transfer ( us , us -> send_ctrl_pipe , <nl> US_CBI_ADSC , <nl> USB_TYPE_CLASS | USB_RECIP_INTERFACE , 0 , <nl> - us -> ifnum , srb -> cmnd , srb -> cmd_len ); <nl> + us -> ifnum , us -> iobuf , srb -> cmd_len ); <nl>  <nl> /* check the return code for the command */ <nl> usb_stor_dbg ( us , " Call to usb_stor_ctrl_transfer () returned % d \ n ",
static void tcp_init_metrics ( struct sock * sk ) <nl> } <nl> if ( dst_metric ( dst , RTAX_RTTVAR ) > tp -> mdev ) { <nl> tp -> mdev = dst_metric ( dst , RTAX_RTTVAR ); <nl> - tp -> mdev_max = tp -> rttvar = max ( tp -> mdev , TCP_RTO_MIN ); <nl> + tp -> mdev_max = tp -> rttvar = max ( tp -> mdev , tcp_rto_min ( sk )); <nl> } <nl> tcp_set_rto ( sk ); <nl> tcp_bound_rto ( sk );
static void ms_lib_free_writebuf ( struct us_data * us ) <nl> ms_lib_clear_pagemap ( info ); /* ( pdx )-> MS_Lib . pagemap memset 0 in ms . h */ <nl>  <nl> if ( info -> MS_Lib . blkpag ) { <nl> - kfree (( u8 *)( info -> MS_Lib . blkpag )); /* Arnold test ... */ <nl> + kfree ( info -> MS_Lib . blkpag ); /* Arnold test ... */ <nl> info -> MS_Lib . blkpag = NULL ; <nl> } <nl>  <nl> if ( info -> MS_Lib . blkext ) { <nl> - kfree (( u8 *)( info -> MS_Lib . blkext )); /* Arnold test ... */ <nl> + kfree ( info -> MS_Lib . blkext ); /* Arnold test ... */ <nl> info -> MS_Lib . blkext = NULL ; <nl> } <nl> }
static int selinux_setprocattr ( struct task_struct * p , <nl> return error ; <nl>  <nl> /* Obtain a SID for the context , if one was specified . */ <nl> - if ( size && str [ 1 ] && str [ 1 ] != '\ n ') { <nl> + if ( size && str [ 0 ] && str [ 0 ] != '\ n ') { <nl> if ( str [ size - 1 ] == '\ n ') { <nl> str [ size - 1 ] = 0 ; <nl> size --;
static unsigned long ramfs_nommu_get_unmapped_area ( struct file * file , <nl>  <nl> /* gang - find the pages */ <nl> ret = - ENOMEM ; <nl> - pages = kzalloc ( lpages * sizeof ( struct page *), GFP_KERNEL ); <nl> + pages = kcalloc ( lpages , sizeof ( struct page *), GFP_KERNEL ); <nl> if (! pages ) <nl> goto out_free ; <nl> 
void dlm_lowcomms_stop ( void ) <nl> con = __nodeid2con ( i , 0 ); <nl> if ( con ) { <nl> close_connection ( con , true ); <nl> + if ( con -> othercon ) <nl> + kmem_cache_free ( con_cache , con -> othercon ); <nl> kmem_cache_free ( con_cache , con ); <nl> } <nl> }
static inline int h4_check_data_len ( struct h4_struct * h4 , int len ) <nl> /* Recv data */ <nl> static int h4_recv ( struct hci_uart * hu , void * data , int count ) <nl> { <nl> - if ( hci_recv_stream_fragment ( hu -> hdev , data , count ) < 0 ) <nl> + int ret ; <nl> + <nl> + ret = hci_recv_stream_fragment ( hu -> hdev , data , count ); <nl> + if ( ret < 0 ) { <nl> BT_ERR (" Frame Reassembly Failed "); <nl> + return ret ; <nl> + } <nl>  <nl> return count ; <nl> }
static int ti_abb_probe ( struct platform_device * pdev ) <nl>  <nl> pname = " ldo - address "; <nl> res = platform_get_resource_byname ( pdev , IORESOURCE_MEM , pname ); <nl> + if (! res ) { <nl> + dev_dbg ( dev , " Missing '% s ' IO resource \ n ", pname ); <nl> + ret = - ENODEV ; <nl> + goto skip_opt ; <nl> + } <nl> abb -> ldo_base = devm_ioremap_resource ( dev , res ); <nl> if ( IS_ERR ( abb -> ldo_base )) { <nl> ret = PTR_ERR ( abb -> ldo_base );
static int mmc_blk_issue_rw_rq ( struct mmc_queue * mq , struct request * rqc ) <nl>  <nl> start_new_req : <nl> if ( rqc ) { <nl> - mmc_blk_rw_rq_prep ( mq -> mqrq_cur , card , 0 , mq ); <nl> - mmc_start_req ( card -> host , & mq -> mqrq_cur -> mmc_active , NULL ); <nl> + if ( mmc_card_removed ( card )) { <nl> + rqc -> cmd_flags |= REQ_QUIET ; <nl> + blk_end_request_all ( rqc , - EIO ); <nl> + } else { <nl> + mmc_blk_rw_rq_prep ( mq -> mqrq_cur , card , 0 , mq ); <nl> + mmc_start_req ( card -> host , <nl> + & mq -> mqrq_cur -> mmc_active , NULL ); <nl> + } <nl> } <nl>  <nl> return 0 ;
void bpf_jit_compile ( struct bpf_prog * fp ) <nl>  <nl> memset (& ctx , 0 , sizeof ( ctx )); <nl>  <nl> - ctx . offsets = kcalloc ( fp -> len , sizeof (* ctx . offsets ), GFP_KERNEL ); <nl> + ctx . offsets = kcalloc ( fp -> len + 1 , sizeof (* ctx . offsets ), GFP_KERNEL ); <nl> if ( ctx . offsets == NULL ) <nl> return ; <nl> 
static int __devinit snd_ca0106_probe ( struct pci_dev * pci , <nl> snd_ca0106_proc_init ( chip ); <nl> # endif <nl>  <nl> + snd_card_set_dev ( card , & pci -> dev ); <nl> + <nl> if (( err = snd_card_register ( card )) < 0 ) { <nl> snd_card_free ( card ); <nl> return err ;
static int p54u_probe ( struct usb_interface * intf , <nl> priv -> upload_fw = p54u_upload_firmware_net2280 ; <nl> } <nl> err = p54u_load_firmware ( dev , intf ); <nl> + if ( err ) { <nl> + usb_put_dev ( udev ); <nl> + p54_free_common ( dev ); <nl> + } <nl> return err ; <nl> } <nl> 
static int wl1271_op_add_interface ( struct ieee80211_hw * hw , <nl> struct ieee80211_vif * vif ) <nl> { <nl> struct wl1271 * wl = hw -> priv ; <nl> + struct wiphy * wiphy = hw -> wiphy ; <nl> int retries = WL1271_BOOT_RETRIES ; <nl> int ret = 0 ; <nl>  <nl> static int wl1271_op_add_interface ( struct ieee80211_hw * hw , <nl>  <nl> wl -> state = WL1271_STATE_ON ; <nl> wl1271_info (" firmware booted (% s )", wl -> chip . fw_ver ); <nl> + <nl> + /* update hw / fw version info in wiphy struct */ <nl> + wiphy -> hw_version = wl -> chip . id ; <nl> + strncpy ( wiphy -> fw_version , wl -> chip . fw_ver , <nl> + sizeof ( wiphy -> fw_version )); <nl> + <nl> goto out ; <nl>  <nl> irq_disable :
static int ismt_access ( struct i2c_adapter * adap , u16 addr , <nl> desc -> wr_len_cmd = dma_size ; <nl> desc -> control |= ISMT_DESC_BLK ; <nl> priv -> dma_buffer [ 0 ] = command ; <nl> - memcpy (& priv -> dma_buffer [ 1 ], & data -> block [ 1 ], dma_size ); <nl> + memcpy (& priv -> dma_buffer [ 1 ], & data -> block [ 1 ], dma_size - 1 ); <nl> } else { <nl> /* Block Read */ <nl> dev_dbg ( dev , " I2C_SMBUS_BLOCK_DATA : READ \ n "); <nl> static int ismt_access ( struct i2c_adapter * adap , u16 addr , <nl> desc -> wr_len_cmd = dma_size ; <nl> desc -> control |= ISMT_DESC_I2C ; <nl> priv -> dma_buffer [ 0 ] = command ; <nl> - memcpy (& priv -> dma_buffer [ 1 ], & data -> block [ 1 ], dma_size ); <nl> + memcpy (& priv -> dma_buffer [ 1 ], & data -> block [ 1 ], dma_size - 1 ); <nl> } else { <nl> /* i2c Block Read */ <nl> dev_dbg ( dev , " I2C_SMBUS_I2C_BLOCK_DATA : READ \ n ");
static void do_checkpoint ( struct f2fs_sb_info * sbi , bool is_umount ) <nl> /* Here , we only have one bio having CP pack */ <nl> sync_meta_pages ( sbi , META_FLUSH , LONG_MAX ); <nl>  <nl> - if ( unlikely (! is_set_ckpt_flags ( ckpt , CP_ERROR_FLAG ))) { <nl> + if (! is_set_ckpt_flags ( ckpt , CP_ERROR_FLAG )) { <nl> clear_prefree_segments ( sbi ); <nl> release_dirty_inode ( sbi ); <nl> F2FS_RESET_SB_DIRT ( sbi );
static void vss_on_reset ( void ) <nl> int <nl> hv_vss_init ( struct hv_util_service * srv ) <nl> { <nl> + if ( vmbus_proto_version < VERSION_WIN8_1 ) { <nl> + pr_warn (" Integration service ' Backup ( volume snapshot )'" <nl> + " not supported on this host version .\ n "); <nl> + return - ENOTSUPP ; <nl> + } <nl> recv_buffer = srv -> recv_buffer ; <nl>  <nl> /*
static int ceph_con_in_msg_alloc ( struct ceph_connection * con , int * skip ) <nl> msg = con -> ops -> alloc_msg ( con , hdr , skip ); <nl> mutex_lock (& con -> mutex ); <nl> if ( con -> state != CON_STATE_OPEN ) { <nl> - ceph_msg_put ( msg ); <nl> + if ( msg ) <nl> + ceph_msg_put ( msg ); <nl> return - EAGAIN ; <nl> } <nl> con -> in_msg = msg ;
int ssb_bus_scan ( struct ssb_bus * bus , <nl> /* Ignore PCI cores on PCI - E cards . <nl> * Ignore PCI - E cores on PCI cards . */ <nl> if ( dev -> id . coreid == SSB_DEV_PCI ) { <nl> - if ( bus -> host_pci -> is_pcie ) <nl> + if ( pci_is_pcie ( bus -> host_pci )) <nl> continue ; <nl> } else { <nl> - if (! bus -> host_pci -> is_pcie ) <nl> + if (! pci_is_pcie ( bus -> host_pci )) <nl> continue ; <nl> } <nl> }
static int __pppoe_xmit ( struct sock * sk , struct sk_buff * skb ) <nl> * give dev_queue_xmit something it can free . <nl> */ <nl> skb2 = skb_clone ( skb , GFP_ATOMIC ); <nl> + <nl> + if ( skb2 == NULL ) <nl> + goto abort ; <nl> } <nl>  <nl> ph = ( struct pppoe_hdr *) skb_push ( skb2 , sizeof ( struct pppoe_hdr ));
static int snd_asihpi_sampleclock_add ( struct snd_card_asihpi * asihpi , <nl>  <nl> static int snd_card_asihpi_mixer_new ( struct snd_card_asihpi * asihpi ) <nl> { <nl> - struct snd_card * card = asihpi -> card ; <nl> + struct snd_card * card ; <nl> unsigned int idx = 0 ; <nl> unsigned int subindex = 0 ; <nl> int err ; <nl> static int snd_card_asihpi_mixer_new ( struct snd_card_asihpi * asihpi ) <nl>  <nl> if ( snd_BUG_ON (! asihpi )) <nl> return - EINVAL ; <nl> + card = asihpi -> card ; <nl> strcpy ( card -> mixername , " Asihpi Mixer "); <nl>  <nl> err =
radeon_atom_encoder_dpms_dig ( struct drm_encoder * encoder , int mode ) <nl> * does the same thing and more . <nl> */ <nl> if (( rdev -> family != CHIP_RV710 ) && ( rdev -> family != CHIP_RV730 ) && <nl> - ( rdev -> family != CHIP_RS880 )) <nl> + ( rdev -> family != CHIP_RS780 ) && ( rdev -> family != CHIP_RS880 )) <nl> atombios_dig_transmitter_setup ( encoder , ATOM_TRANSMITTER_ACTION_ENABLE_OUTPUT , 0 , 0 ); <nl> } <nl> if ( ENCODER_MODE_IS_DP ( atombios_get_encoder_mode ( encoder )) && connector ) {
static int gfar_probe ( struct platform_device * pdev ) <nl> dev -> hard_start_xmit = gfar_start_xmit ; <nl> dev -> tx_timeout = gfar_timeout ; <nl> dev -> watchdog_timeo = TX_TIMEOUT ; <nl> +# ifdef CONFIG_GFAR_NAPI <nl> netif_napi_add ( dev , & priv -> napi , gfar_poll , GFAR_DEV_WEIGHT ); <nl> +# endif <nl> # ifdef CONFIG_NET_POLL_CONTROLLER <nl> dev -> poll_controller = gfar_netpoll ; <nl> # endif <nl> tx_skb_fail : <nl> /* Returns 0 for success . */ <nl> static int gfar_enet_open ( struct net_device * dev ) <nl> { <nl> + struct gfar_private * priv = netdev_priv ( dev ); <nl> int err ; <nl>  <nl> napi_enable (& priv -> napi );
void mgmt_device_disconnected ( struct hci_dev * hdev , bdaddr_t * bdaddr , <nl> struct mgmt_ev_device_disconnected ev ; <nl> struct sock * sk = NULL ; <nl>  <nl> + if ( link_type != ACL_LINK && link_type != LE_LINK ) <nl> + return ; <nl> + <nl> mgmt_pending_foreach ( MGMT_OP_DISCONNECT , hdev , disconnect_rsp , & sk ); <nl>  <nl> bacpy (& ev . addr . bdaddr , bdaddr );
static int set_msr_mce ( struct kvm_vcpu * vcpu , u32 msr , u64 data ) <nl> break ; <nl> default : <nl> if ( msr >= MSR_IA32_MC0_CTL && <nl> - msr < MSR_IA32_MC0_CTL + 4 * bank_num ) { <nl> + msr < MSR_IA32_MCx_CTL ( bank_num )) { <nl> u32 offset = msr - MSR_IA32_MC0_CTL ; <nl> /* only 0 or all 1s can be written to IA32_MCi_CTL <nl> * some Linux kernels though clear bit 10 in bank 4 to <nl> int kvm_set_msr_common ( struct kvm_vcpu * vcpu , struct msr_data * msr_info ) <nl>  <nl> case MSR_IA32_MCG_CTL : <nl> case MSR_IA32_MCG_STATUS : <nl> - case MSR_IA32_MC0_CTL ... MSR_IA32_MC0_CTL + 4 * KVM_MAX_MCE_BANKS - 1 : <nl> + case MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL ( KVM_MAX_MCE_BANKS ) - 1 : <nl> return set_msr_mce ( vcpu , msr , data ); <nl>  <nl> /* Performance counters are not protected by a CPUID bit , <nl> static int get_msr_mce ( struct kvm_vcpu * vcpu , u32 msr , u64 * pdata ) <nl> break ; <nl> default : <nl> if ( msr >= MSR_IA32_MC0_CTL && <nl> - msr < MSR_IA32_MC0_CTL + 4 * bank_num ) { <nl> + msr < MSR_IA32_MCx_CTL ( bank_num )) { <nl> u32 offset = msr - MSR_IA32_MC0_CTL ; <nl> data = vcpu -> arch . mce_banks [ offset ]; <nl> break ; <nl> int kvm_get_msr_common ( struct kvm_vcpu * vcpu , u32 msr , u64 * pdata ) <nl> case MSR_IA32_MCG_CAP : <nl> case MSR_IA32_MCG_CTL : <nl> case MSR_IA32_MCG_STATUS : <nl> - case MSR_IA32_MC0_CTL ... MSR_IA32_MC0_CTL + 4 * KVM_MAX_MCE_BANKS - 1 : <nl> + case MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL ( KVM_MAX_MCE_BANKS ) - 1 : <nl> return get_msr_mce ( vcpu , msr , pdata ); <nl> case MSR_K7_CLK_CTL : <nl> /*
static int atalk_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> return - ENOBUFS ; <nl>  <nl> * uaddr_len = sizeof ( struct sockaddr_at ); <nl> + memset (& sat . sat_zero , 0 , sizeof ( sat . sat_zero )); <nl>  <nl> if ( peer ) { <nl> if ( sk -> sk_state != TCP_ESTABLISHED )
static int remove_uuid ( struct sock * sk , struct hci_dev * hdev , void * data , <nl> continue ; <nl>  <nl> list_del (& match -> list ); <nl> + kfree ( match ); <nl> found ++; <nl> } <nl> 
ssize_t hdmi_avi_infoframe_pack ( struct hdmi_avi_infoframe * frame , void * buffer , <nl> if ( size < length ) <nl> return - ENOSPC ; <nl>  <nl> - memset ( buffer , 0 , length ); <nl> + memset ( buffer , 0 , size ); <nl>  <nl> ptr [ 0 ] = frame -> type ; <nl> ptr [ 1 ] = frame -> version ; <nl> ssize_t hdmi_spd_infoframe_pack ( struct hdmi_spd_infoframe * frame , void * buffer , <nl> if ( size < length ) <nl> return - ENOSPC ; <nl>  <nl> - memset ( buffer , 0 , length ); <nl> + memset ( buffer , 0 , size ); <nl>  <nl> ptr [ 0 ] = frame -> type ; <nl> ptr [ 1 ] = frame -> version ; <nl> ssize_t hdmi_audio_infoframe_pack ( struct hdmi_audio_infoframe * frame , <nl> if ( size < length ) <nl> return - ENOSPC ; <nl>  <nl> - memset ( buffer , 0 , length ); <nl> + memset ( buffer , 0 , size ); <nl>  <nl> if ( frame -> channels >= 2 ) <nl> channels = frame -> channels - 1 ; <nl> ssize_t hdmi_vendor_infoframe_pack ( struct hdmi_vendor_infoframe * frame , <nl> if ( size < length ) <nl> return - ENOSPC ; <nl>  <nl> - memset ( buffer , 0 , length ); <nl> + memset ( buffer , 0 , size ); <nl>  <nl> ptr [ 0 ] = frame -> type ; <nl> ptr [ 1 ] = frame -> version ;
static int multipath_ctr ( struct dm_target * ti , unsigned int argc , <nl> } <nl>  <nl> ti -> num_flush_requests = 1 ; <nl> + ti -> num_discard_requests = 1 ; <nl>  <nl> return 0 ; <nl>  <nl> static int do_end_io ( struct multipath * m , struct request * clone , <nl> if ( error == - EOPNOTSUPP ) <nl> return error ; <nl>  <nl> + if ( clone -> cmd_flags & REQ_DISCARD ) <nl> + /* <nl> + * Pass all discard request failures up . <nl> + * FIXME : only fail_path if the discard failed due to a <nl> + * transport problem . This requires precise understanding <nl> + * of the underlying failure ( e . g . the SCSI sense ). <nl> + */ <nl> + return error ; <nl> + <nl> if ( mpio -> pgpath ) <nl> fail_path ( mpio -> pgpath ); <nl> 
int rtllib_rx ( struct rtllib_device * ieee , struct sk_buff * skb , <nl> return ret ; <nl>  <nl> rx_dropped : <nl> - ieee -> stats . rx_dropped ++; <nl> + if ( ieee ) <nl> + ieee -> stats . rx_dropped ++; <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL ( rtllib_rx );
static bool _is_valid_div ( struct clk_divider * divider , unsigned int div ) <nl> static int _round_up_table ( const struct clk_div_table * table , int div ) <nl> { <nl> const struct clk_div_table * clkt ; <nl> - int up = _get_table_maxdiv ( table ); <nl> + int up = INT_MAX ; <nl>  <nl> for ( clkt = table ; clkt -> div ; clkt ++) { <nl> if ( clkt -> div == div )
static ssize_t i40e_dbg_command_write ( struct file * filp , <nl> if (! cmd_buf ) <nl> return count ; <nl> bytes_not_copied = copy_from_user ( cmd_buf , buffer , count ); <nl> - if ( bytes_not_copied < 0 ) <nl> + if ( bytes_not_copied < 0 ) { <nl> + kfree ( cmd_buf ); <nl> return bytes_not_copied ; <nl> + } <nl> if ( bytes_not_copied > 0 ) <nl> count -= bytes_not_copied ; <nl> cmd_buf [ count ] = '\ 0 ';
void drm_mm_remove_node ( struct drm_mm_node * node ) <nl> struct drm_mm * mm = node -> mm ; <nl> struct drm_mm_node * prev_node ; <nl>  <nl> + if ( WARN_ON (! node -> allocated )) <nl> + return ; <nl> + <nl> BUG_ON ( node -> scanned_block || node -> scanned_prev_free <nl> || node -> scanned_next_free ); <nl> 
static int __devinit rtl8192_pci_probe ( struct pci_dev * pdev , <nl> u8 unit = 0 ; <nl> int ret = - ENODEV ; <nl> unsigned long pmem_start , pmem_len , pmem_flags ; <nl> + u8 revisionid ; <nl>  <nl> RT_TRACE ( COMP_INIT ," Configuring chip resources \ n "); <nl>  <nl> static int __devinit rtl8192_pci_probe ( struct pci_dev * pdev , <nl> pci_write_config_byte ( pdev , 0x41 , 0x00 ); <nl>  <nl>  <nl> + pci_read_config_byte ( pdev , 0x08 , & revisionid ); <nl> + /* If the revisionid is 0x10 , the device uses rtl8192se . */ <nl> + if ( pdev -> device == 0x8192 && revisionid == 0x10 ) <nl> + goto fail1 ; <nl> + <nl> pci_read_config_byte ( pdev , 0x05 , & unit ); <nl> pci_write_config_byte ( pdev , 0x05 , unit & (~ 0x04 )); <nl> 
__ieee80211_tx_prepare ( struct ieee80211_tx_data * tx , <nl> tx -> local = local ; <nl> tx -> sdata = IEEE80211_DEV_TO_SUB_IF ( dev ); <nl> tx -> channel = local -> hw . conf . channel ; <nl> + tx -> rate_idx = - 1 ; <nl> + tx -> last_frag_rate_idx = - 1 ; <nl> /* <nl> * Set this flag ( used below to indicate " automatic fragmentation "), <nl> * it will be cleared / left by radiotap as desired .
# include < linux / slab . h > <nl> # include < linux / of_address . h > <nl> # include < linux / of_irq . h > <nl> +# include < linux / of_platform . h > <nl>  <nl> MODULE_DESCRIPTION (" Broadcom ' s specific AMBA driver "); <nl> MODULE_LICENSE (" GPL "); <nl> int bcma_bus_register ( struct bcma_bus * bus ) <nl> bcma_core_pci_early_init (& bus -> drv_pci [ 0 ]); <nl> } <nl>  <nl> + if ( bus -> host_pdev ) { <nl> + struct device * dev = & bus -> host_pdev -> dev ; <nl> + <nl> + of_platform_populate ( dev -> of_node , of_default_bus_match_table , <nl> + NULL , dev ); <nl> + } <nl> + <nl> /* Cores providing flash access go before SPROM init */ <nl> list_for_each_entry ( core , & bus -> cores , list ) { <nl> if ( bcma_is_core_needed_early ( core -> id . id ))
err_hci : <nl> free_irq ( client -> irq , phy ); <nl>  <nl> err_rti : <nl> - if ( pdata -> free_resources != NULL ) <nl> + if (! pdata ) { <nl> + gpio_free ( phy -> gpio_en ); <nl> + gpio_free ( phy -> gpio_fw ); <nl> + } else if ( pdata -> free_resources ) { <nl> pdata -> free_resources (); <nl> + } <nl>  <nl> return r ; <nl> }
static int virtnet_probe ( struct virtio_device * vdev ) <nl> dev -> mtu = mtu ; <nl> dev -> max_mtu = mtu ; <nl> } <nl> + <nl> + /* TODO : size buffers correctly in this case . */ <nl> + if ( dev -> mtu > ETH_DATA_LEN ) <nl> + vi -> big_packets = true ; <nl> } <nl>  <nl> if ( vi -> any_header_sg )
# include < linux / linkage . h > <nl> # include < linux / printk . h > <nl> # include < linux / workqueue . h > <nl> +# include < linux / sched . h > <nl>  <nl> # include < asm / cacheflush . h > <nl>  <nl> void bpf_jit_free ( struct bpf_prog * fp ); <nl> static inline void bpf_jit_dump ( unsigned int flen , unsigned int proglen , <nl> u32 pass , void * image ) <nl> { <nl> - pr_err (" flen =% u proglen =% u pass =% u image =% pK \ n ", <nl> - flen , proglen , pass , image ); <nl> + pr_err (" flen =% u proglen =% u pass =% u image =% pK from =% s pid =% d \ n ", flen , <nl> + proglen , pass , image , current -> comm , task_pid_nr ( current )); <nl> + <nl> if ( image ) <nl> print_hex_dump ( KERN_ERR , " JIT code : ", DUMP_PREFIX_OFFSET , <nl> 16 , 1 , image , proglen , false );
void ieee80211_offchannel_return ( struct ieee80211_local * local , <nl>  <nl> mutex_lock (& local -> iflist_mtx ); <nl> list_for_each_entry ( sdata , & local -> interfaces , list ) { <nl> + if ( sdata -> vif . type != NL80211_IFTYPE_MONITOR ) <nl> + clear_bit ( SDATA_STATE_OFFCHANNEL , & sdata -> state ); <nl> + <nl> if (! ieee80211_sdata_running ( sdata )) <nl> continue ; <nl>  <nl> void ieee80211_offchannel_return ( struct ieee80211_local * local , <nl> } <nl>  <nl> if ( sdata -> vif . type != NL80211_IFTYPE_MONITOR ) { <nl> - clear_bit ( SDATA_STATE_OFFCHANNEL , & sdata -> state ); <nl> /* <nl> * This may wake up queues even though the driver <nl> * currently has them stopped . This is not very
static int nvme_rdma_route_resolved ( struct nvme_rdma_queue * queue ) <nl> { <nl> struct nvme_rdma_ctrl * ctrl = queue -> ctrl ; <nl> struct rdma_conn_param param = { }; <nl> - struct nvme_rdma_cm_req priv ; <nl> + struct nvme_rdma_cm_req priv = { }; <nl> int ret ; <nl>  <nl> param . qp_num = queue -> qp -> qp_num ;
static void ieee80211_do_stop ( struct ieee80211_sub_if_data * sdata , <nl> if ( sdata -> vif . txq ) { <nl> struct txq_info * txqi = to_txq_info ( sdata -> vif . txq ); <nl>  <nl> + spin_lock_bh (& txqi -> queue . lock ); <nl> ieee80211_purge_tx_queue (& local -> hw , & txqi -> queue ); <nl> + spin_unlock_bh (& txqi -> queue . lock ); <nl> + <nl> atomic_set (& sdata -> txqs_len [ txqi -> txq . ac ], 0 ); <nl> } <nl> 
struct v4l2_subdev_audio_ops { <nl> s_std_output : set v4l2_std_id for video OUTPUT devices . This is ignored by <nl> video input devices . <nl>  <nl> + g_std_output : get current standard for video OUTPUT devices . This is ignored <nl> + by video input devices . <nl> + <nl> g_tvnorms_output : get v4l2_std_id with all standards supported by video <nl> OUTPUT device . This is ignored by video input devices . <nl>  <nl> struct v4l2_subdev_video_ops { <nl> int (* s_routing )( struct v4l2_subdev * sd , u32 input , u32 output , u32 config ); <nl> int (* s_crystal_freq )( struct v4l2_subdev * sd , u32 freq , u32 flags ); <nl> int (* s_std_output )( struct v4l2_subdev * sd , v4l2_std_id std ); <nl> + int (* g_std_output )( struct v4l2_subdev * sd , v4l2_std_id * std ); <nl> int (* querystd )( struct v4l2_subdev * sd , v4l2_std_id * std ); <nl> int (* g_tvnorms_output )( struct v4l2_subdev * sd , v4l2_std_id * std ); <nl> int (* g_input_status )( struct v4l2_subdev * sd , u32 * status );
static void mwifiex_unregister_dev ( struct mwifiex_adapter * adapter ) <nl> { <nl> struct pcie_service_card * card = adapter -> card ; <nl> const struct mwifiex_pcie_card_reg * reg ; <nl> - struct pci_dev * pdev = card -> dev ; <nl> + struct pci_dev * pdev ; <nl> int i ; <nl>  <nl> if ( card ) { <nl> + pdev = card -> dev ; <nl> if ( card -> msix_enable ) { <nl> for ( i = 0 ; i < MWIFIEX_NUM_MSIX_VECTORS ; i ++) <nl> synchronize_irq ( card -> msix_entries [ i ]. vector );
drm_atomic_helper_wait_for_vblanks ( struct drm_device * dev , <nl> for_each_crtc_in_state ( old_state , crtc , old_crtc_state , i ) { <nl> struct drm_crtc_state * new_crtc_state = crtc -> state ; <nl>  <nl> - if (! new_crtc_state -> active ) <nl> - continue ; <nl> - <nl> - if (! drm_atomic_helper_framebuffer_changed ( dev , <nl> - old_state , crtc )) <nl> + if (! new_crtc_state -> active || ! new_crtc_state -> planes_changed ) <nl> continue ; <nl>  <nl> ret = drm_crtc_vblank_get ( crtc );
int qed_resc_alloc ( struct qed_dev * cdev ) <nl> DP_ERR ( p_hwfn , <nl> " Cannot allocate 0x % x EQ elements . The maximum of a u16 chain is 0x % x \ n ", <nl> n_eqes , 0xFFFF ); <nl> + rc = - EINVAL ; <nl> goto alloc_err ; <nl> } <nl> 
static void taal_esd_work ( struct work_struct * work ) <nl> } <nl> /* Self - diagnostics result is also shown on TE GPIO line . We need <nl> * to re - enable TE after self diagnostics */ <nl> - if ( td -> use_ext_te && td -> te_enabled ) <nl> - taal_enable_te ( dssdev , true ); <nl> + if ( td -> use_ext_te && td -> te_enabled ) { <nl> + r = taal_dcs_write_1 ( DCS_TEAR_ON , 0 ); <nl> + if ( r ) <nl> + goto err ; <nl> + } <nl>  <nl> dsi_bus_unlock (); <nl> 
static bool ixgbevf_clean_tx_irq ( struct ixgbevf_adapter * adapter , <nl> ( count < tx_ring -> work_limit )) { <nl> bool cleaned = false ; <nl> rmb (); /* read buffer_info after eop_desc */ <nl> + /* eop could change between read and DD - check */ <nl> + if ( unlikely ( eop != tx_ring -> tx_buffer_info [ i ]. next_to_watch )) <nl> + goto cont_loop ; <nl> for ( ; ! cleaned ; count ++) { <nl> struct sk_buff * skb ; <nl> tx_desc = IXGBE_TX_DESC_ADV (* tx_ring , i ); <nl> static bool ixgbevf_clean_tx_irq ( struct ixgbevf_adapter * adapter , <nl> i = 0 ; <nl> } <nl>  <nl> + cont_loop : <nl> eop = tx_ring -> tx_buffer_info [ i ]. next_to_watch ; <nl> eop_desc = IXGBE_TX_DESC_ADV (* tx_ring , eop ); <nl> }
xfs_ialloc_ag_alloc ( <nl> int version ; /* inode version number to use */ <nl> int isaligned = 0 ; /* inode allocation at stripe unit */ <nl> /* boundary */ <nl> + unsigned int gen ; <nl>  <nl> args . tp = tp ; <nl> args . mp = tp -> t_mountp ; <nl> xfs_ialloc_ag_alloc ( <nl> else <nl> version = XFS_DINODE_VERSION_1 ; <nl>  <nl> + /* <nl> + * Seed the new inode cluster with a random generation number . This <nl> + * prevents short - term reuse of generation numbers if a chunk is <nl> + * freed and then immediately reallocated . We use random numbers <nl> + * rather than a linear progression to prevent the next generation <nl> + * number from being easily guessable . <nl> + */ <nl> + gen = random32 (); <nl> for ( j = 0 ; j < nbufs ; j ++) { <nl> /* <nl> * Get the block . <nl> xfs_ialloc_ag_alloc ( <nl> free = XFS_MAKE_IPTR ( args . mp , fbuf , i ); <nl> free -> di_core . di_magic = cpu_to_be16 ( XFS_DINODE_MAGIC ); <nl> free -> di_core . di_version = version ; <nl> + free -> di_core . di_gen = cpu_to_be32 ( gen ); <nl> free -> di_next_unlinked = cpu_to_be32 ( NULLAGINO ); <nl> xfs_ialloc_log_di ( tp , fbuf , i , <nl> XFS_DI_CORE_BITS | XFS_DI_NEXT_UNLINKED );
static ssize_t boottotool_store ( struct device * dev , <nl> struct device_attribute * attr , <nl> const char * buf , size_t count ) <nl> { <nl> - int val , ret ; <nl> + int val , err ; <nl> struct efi_spar_indication efi_spar_indication ; <nl>  <nl> if ( kstrtoint ( buf , 10 , & val )) <nl> return - EINVAL ; <nl>  <nl> efi_spar_indication . boot_to_tool = val ; <nl> - ret = visorchannel_write <nl> + err = visorchannel_write <nl> ( chipset_dev -> controlvm_channel , <nl> offsetof ( struct spar_controlvm_channel_protocol , <nl> efi_spar_ind ), &( efi_spar_indication ), <nl> sizeof ( struct efi_spar_indication )); <nl>  <nl> - if ( ret ) <nl> - return ret ; <nl> + if ( err ) <nl> + return err ; <nl> return count ; <nl> } <nl> static DEVICE_ATTR_RW ( boottotool );
int dlm_posix_lock ( dlm_lockspace_t * lockspace , u64 number , struct file * file , <nl> send_op ( op ); <nl>  <nl> if ( xop -> callback == NULL ) { <nl> - rv = wait_event_killable ( recv_wq , ( op -> done != 0 )); <nl> + rv = wait_event_interruptible ( recv_wq , ( op -> done != 0 )); <nl> if ( rv == - ERESTARTSYS ) { <nl> log_debug ( ls , " dlm_posix_lock : wait killed % llx ", <nl> ( unsigned long long ) number );
static void tipc_purge_publications ( struct name_seq * seq ) <nl> struct sub_seq * sseq ; <nl> struct name_info * info ; <nl>  <nl> - if (! seq -> sseqs ) { <nl> - nameseq_delete_empty ( seq ); <nl> - return ; <nl> - } <nl> sseq = seq -> sseqs ; <nl> info = sseq -> info ; <nl> list_for_each_entry_safe ( publ , safe , & info -> zone_list , zone_list ) { <nl> static void tipc_purge_publications ( struct name_seq * seq ) <nl> publ -> ref , publ -> key ); <nl> kfree ( publ ); <nl> } <nl> + hlist_del_init (& seq -> ns_list ); <nl> + kfree ( seq -> sseqs ); <nl> + kfree ( seq ); <nl> } <nl>  <nl> void tipc_nametbl_stop ( void )
struct nfs_server * nfs4_create_referral_server ( struct nfs_clone_mount * data , <nl> parent_server -> client -> cl_xprt -> prot , <nl> parent_client -> retrans_timeo , <nl> parent_client -> retrans_count ); <nl> + if ( error < 0 ) <nl> + goto error ; <nl>  <nl> /* Initialise the client representation from the parent server */ <nl> nfs_server_copy_userdata ( server , parent_server );
static int atmel_pdmic_cpu_dai_startup ( struct snd_pcm_substream * substream , <nl> return ret ; <nl>  <nl> ret = clk_prepare_enable ( dd -> pclk ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + clk_disable_unprepare ( dd -> gclk ); <nl> return ret ; <nl> + } <nl>  <nl> /* Clear all bits in the Control Register ( PDMIC_CR ) */ <nl> regmap_write ( dd -> regmap , PDMIC_CR , 0 );
void __init early_trap_init ( void * vectors_base ) <nl> extern char __vectors_start [], __vectors_end []; <nl> extern char __kuser_helper_start [], __kuser_helper_end []; <nl> int kuser_sz = __kuser_helper_end - __kuser_helper_start ; <nl> + unsigned i ; <nl>  <nl> vectors_page = vectors_base ; <nl>  <nl> + /* <nl> + * Poison the vectors page with an undefined instruction . This <nl> + * instruction is chosen to be undefined for both ARM and Thumb <nl> + * ISAs . The Thumb version is an undefined instruction with a <nl> + * branch back to the undefined instruction . <nl> + */ <nl> + for ( i = 0 ; i < PAGE_SIZE / sizeof ( u32 ); i ++) <nl> + (( u32 *) vectors_base )[ i ] = 0xe7fddef1 ; <nl> + <nl> /* <nl> * Copy the vectors , stubs and kuser helpers ( in entry - armv . S ) <nl> * into the vector page , mapped at 0xffff0000 , and ensure these
static inline void rtsx_exclusive_enter_ss ( struct rtsx_chip * chip ) <nl> { <nl> struct rtsx_dev * dev = chip -> rtsx ; <nl>  <nl> - spin_lock (&( dev -> reg_lock )); <nl> + spin_lock (& dev -> reg_lock ); <nl> rtsx_enter_ss ( chip ); <nl> - spin_unlock (&( dev -> reg_lock )); <nl> + spin_unlock (& dev -> reg_lock ); <nl> } <nl>  <nl> static inline void rtsx_reset_detected_cards ( struct rtsx_chip * chip , int flag )
skl_ddi_pll_select ( struct intel_crtc * intel_crtc , <nl> DPLL_CFGCR2_KDIV ( wrpll_params . kdiv ) | <nl> DPLL_CFGCR2_PDIV ( wrpll_params . pdiv ) | <nl> wrpll_params . central_freq ; <nl> - } else if ( intel_encoder -> type == INTEL_OUTPUT_DISPLAYPORT ) { <nl> + } else if ( intel_encoder -> type == INTEL_OUTPUT_DISPLAYPORT || <nl> + intel_encoder -> type == INTEL_OUTPUT_DP_MST ) { <nl> switch ( crtc_state -> port_clock / 2 ) { <nl> case 81000 : <nl> ctrl1 |= DPLL_CTRL1_LINK_RATE ( DPLL_CTRL1_LINK_RATE_810 , 0 );
vma_address ( struct page * page , struct vm_area_struct * vma ) <nl> unsigned long page_address_in_vma ( struct page * page , struct vm_area_struct * vma ) <nl> { <nl> if ( PageAnon ( page )) { <nl> - if ( vma -> anon_vma -> root != page_anon_vma ( page )-> root ) <nl> + struct anon_vma * page__anon_vma = page_anon_vma ( page ); <nl> + /* <nl> + * Note : swapoff ' s unuse_vma () is more efficient with this <nl> + * check , and needs it to match anon_vma when KSM is active . <nl> + */ <nl> + if (! vma -> anon_vma || ! page__anon_vma || <nl> + vma -> anon_vma -> root != page__anon_vma -> root ) <nl> return - EFAULT ; <nl> } else if ( page -> mapping && !( vma -> vm_flags & VM_NONLINEAR )) { <nl> if (! vma -> vm_file ||
int fscrypt_ioctl_set_policy ( struct file * filp , const void __user * arg ) <nl> printk ( KERN_WARNING <nl> "% s : Policy inconsistent with encryption context \ n ", <nl> __func__ ); <nl> - ret = - EINVAL ; <nl> + ret = - EEXIST ; <nl> } <nl>  <nl> inode_unlock ( inode );
fail : <nl> insert_pt = & gh2 -> gh_list ; <nl> } <nl> set_bit ( GLF_QUEUED , & gl -> gl_flags ); <nl> + trace_gfs2_glock_queue ( gh , 1 ); <nl> if ( likely ( insert_pt == NULL )) { <nl> list_add_tail (& gh -> gh_list , & gl -> gl_holders ); <nl> if ( unlikely ( gh -> gh_flags & LM_FLAG_PRIORITY )) <nl> goto do_cancel ; <nl> return ; <nl> } <nl> - trace_gfs2_glock_queue ( gh , 1 ); <nl> list_add_tail (& gh -> gh_list , insert_pt ); <nl> do_cancel : <nl> gh = list_entry ( gl -> gl_holders . next , struct gfs2_holder , gh_list );
sys_epoll_ctl ( int epfd , int op , int fd , struct epoll_event __user * event ) <nl> switch ( op ) { <nl> case EPOLL_CTL_ADD : <nl> if (! epi ) { <nl> - epds . events |= POLLERR | POLLHUP | POLLRDHUP ; <nl> + epds . events |= POLLERR | POLLHUP ; <nl>  <nl> error = ep_insert ( ep , & epds , tfile , fd ); <nl> } else <nl> sys_epoll_ctl ( int epfd , int op , int fd , struct epoll_event __user * event ) <nl> break ; <nl> case EPOLL_CTL_MOD : <nl> if ( epi ) { <nl> - epds . events |= POLLERR | POLLHUP | POLLRDHUP ; <nl> + epds . events |= POLLERR | POLLHUP ; <nl> error = ep_modify ( ep , epi , & epds ); <nl> } else <nl> error = - ENOENT ;
static int acpi_cpufreq_cpu_init ( struct cpufreq_policy * policy ) <nl>  <nl> switch ( perf -> control_register . space_id ) { <nl> case ACPI_ADR_SPACE_SYSTEM_IO : <nl> + if ( boot_cpu_data . x86_vendor == X86_VENDOR_AMD && <nl> + boot_cpu_data . x86 == 0xf ) { <nl> + pr_debug (" AMD K8 systems must use native drivers .\ n "); <nl> + result = - ENODEV ; <nl> + goto err_unreg ; <nl> + } <nl> pr_debug (" SYSTEM IO addr space \ n "); <nl> data -> cpu_feature = SYSTEM_IO_CAPABLE ; <nl> break ;
static int dax_pmem_probe ( struct device * dev ) <nl> nsio = to_nd_namespace_io (& ndns -> dev ); <nl>  <nl> /* parse the ' pfn ' info block via -> rw_bytes */ <nl> - devm_nsio_enable ( dev , nsio ); <nl> + rc = devm_nsio_enable ( dev , nsio ); <nl> + if ( rc ) <nl> + return rc ; <nl> altmap = nvdimm_setup_pfn ( nd_pfn , & res , & __altmap ); <nl> if ( IS_ERR ( altmap )) <nl> return PTR_ERR ( altmap );
relookup : <nl> p -> rate_last = 0 ; <nl> p -> pmtu_expires = 0 ; <nl> p -> pmtu_orig = 0 ; <nl> + p -> redirect_genid = 0 ; <nl> memset (& p -> redirect_learned , 0 , sizeof ( p -> redirect_learned )); <nl>  <nl> 
cpu_dev_register ( amd_cpu_dev ); <nl> const int amd_erratum_400 [] = <nl> AMD_OSVW_ERRATUM ( 1 , AMD_MODEL_RANGE ( 0xf , 0x41 , 0x2 , 0xff , 0xf ), <nl> AMD_MODEL_RANGE ( 0x10 , 0x2 , 0x1 , 0xff , 0xf )); <nl> + EXPORT_SYMBOL_GPL ( amd_erratum_400 ); <nl>  <nl> const int amd_erratum_383 [] = <nl> AMD_OSVW_ERRATUM ( 3 , AMD_MODEL_RANGE ( 0x10 , 0 , 0 , 0xff , 0xf )); <nl> + EXPORT_SYMBOL_GPL ( amd_erratum_383 ); <nl>  <nl> bool cpu_has_amd_erratum ( const int * erratum ) <nl> { <nl> bool cpu_has_amd_erratum ( const int * erratum ) <nl>  <nl> return false ; <nl> } <nl> + <nl> + EXPORT_SYMBOL_GPL ( cpu_has_amd_erratum );
struct linux_xfrm_mib { <nl> */ <nl> # define SNMP_UPD_PO_STATS ( mib , basefield , addend ) \ <nl> do { \ <nl> - this_cpu_inc ( mib [ 0 ]-> mibs [ basefield ## PKTS ]); \ <nl> - this_cpu_add ( mib [ 0 ]-> mibs [ basefield ## OCTETS ], addend ); \ <nl> + __typeof__ (* mib [ 0 ]-> mibs ) * ptr = mib [ 0 ]-> mibs ; \ <nl> + this_cpu_inc ( ptr [ basefield ## PKTS ]); \ <nl> + this_cpu_add ( ptr [ basefield ## OCTETS ], addend ); \ <nl> } while ( 0 ) <nl> # define SNMP_UPD_PO_STATS_BH ( mib , basefield , addend ) \ <nl> do { \ <nl> - __this_cpu_inc ( mib [ 0 ]-> mibs [ basefield ## PKTS ]); \ <nl> - __this_cpu_add ( mib [ 0 ]-> mibs [ basefield ## OCTETS ], addend ); \ <nl> + __typeof__ (* mib [ 0 ]-> mibs ) * ptr = mib [ 0 ]-> mibs ; \ <nl> + __this_cpu_inc ( ptr [ basefield ## PKTS ]); \ <nl> + __this_cpu_add ( ptr [ basefield ## OCTETS ], addend ); \ <nl> } while ( 0 ) <nl>  <nl> 
mwifiex_wmm_get_highest_priolist_ptr ( struct mwifiex_adapter * adapter , <nl> list_for_each_entry ( ptr , & tid_ptr -> ra_list , <nl> list ) { <nl>  <nl> - if (! skb_queue_empty (& ptr -> skb_head )) <nl> + if (! ptr -> tx_paused && <nl> + ! skb_queue_empty (& ptr -> skb_head )) <nl> /* holds both locks */ <nl> goto found ; <nl> }
pte_t * __page_check_address ( struct page * page , struct mm_struct * mm , <nl> spinlock_t * ptl ; <nl>  <nl> if ( unlikely ( PageHuge ( page ))) { <nl> + /* when pud is not present , pte will be NULL */ <nl> pte = huge_pte_offset ( mm , address ); <nl> + if (! pte ) <nl> + return NULL ; <nl> + <nl> ptl = huge_pte_lockptr ( page_hstate ( page ), mm , pte ); <nl> goto check ; <nl> }
static void sta_ps_start ( struct sta_info * sta ) <nl> for ( tid = 0 ; tid < ARRAY_SIZE ( sta -> sta . txq ); tid ++) { <nl> struct txq_info * txqi = to_txq_info ( sta -> sta . txq [ tid ]); <nl>  <nl> - if (! txqi -> tin . backlog_packets ) <nl> + if ( txqi -> tin . backlog_packets ) <nl> set_bit ( tid , & sta -> txq_buffered_tids ); <nl> else <nl> clear_bit ( tid , & sta -> txq_buffered_tids );
static int nfs_idmap_legacy_upcall ( struct key_construction * cons , <nl> msg = & data -> pipe_msg ; <nl> im = & data -> idmap_msg ; <nl> data -> idmap = idmap ; <nl> + data -> key_cons = cons ; <nl>  <nl> ret = nfs_idmap_prepare_message ( key -> description , idmap , im , msg ); <nl> if ( ret < 0 )
 <nl> /* <nl> * Divide positive or negative dividend by positive divisor and round <nl> - * to closest integer . Result is undefined for negative divisors . <nl> + * to closest integer . Result is undefined for negative divisors and <nl> + * for negative dividends if the divisor variable type is unsigned . <nl> */ <nl> # define DIV_ROUND_CLOSEST ( x , divisor )( \ <nl> { \ <nl> typeof ( x ) __x = x ; \ <nl> typeof ( divisor ) __d = divisor ; \ <nl> - ((( typeof ( x ))- 1 ) > 0 || ( __x ) > 0 ) ? \ <nl> + ((( typeof ( x ))- 1 ) > 0 || \ <nl> + (( typeof ( divisor ))- 1 ) > 0 || ( __x ) > 0 ) ? \ <nl> ((( __x ) + (( __d ) / 2 )) / ( __d )) : \ <nl> ((( __x ) - (( __d ) / 2 )) / ( __d )); \ <nl> } \
static void intel_find_plane_obj ( struct intel_crtc * intel_crtc , <nl> return ; <nl>  <nl> kfree ( intel_crtc -> base . fb ); <nl> + intel_crtc -> base . fb = NULL ; <nl>  <nl> /* <nl> * Failed to alloc the obj , check to see if we should share
static void alloc_mem ( void ** dst , void ** src , size_t length ) <nl> * src = zalloc ( length ); <nl> if (!* src ) <nl> die (" memory allocation failed - maybe length is too large ?\ n "); <nl> + /* Make sure to always replace the zero pages even if MMAP_THRESH is crossed */ <nl> + memset (* src , 0 , length ); <nl> } <nl>  <nl> static u64 do_memcpy_cycle ( memcpy_t fn , size_t len , bool prefault )
static int cciss_bigpassthru ( ctlr_info_t * h , void __user * argp ) <nl> return - EINVAL ; <nl> if (! capable ( CAP_SYS_RAWIO )) <nl> return - EPERM ; <nl> - ioc = ( BIG_IOCTL_Command_struct *) <nl> - kmalloc ( sizeof (* ioc ), GFP_KERNEL ); <nl> + ioc = kmalloc ( sizeof (* ioc ), GFP_KERNEL ); <nl> if (! ioc ) { <nl> status = - ENOMEM ; <nl> goto cleanup1 ;
# include < linux / module . h > <nl> # include < linux / if_vlan . h > <nl> # include < linux / inet_lro . h > <nl> +# include < net / checksum . h > <nl>  <nl> MODULE_LICENSE (" GPL "); <nl> MODULE_AUTHOR (" Jan - Bernd Themann < themann @ de . ibm . com >"); <nl> static void lro_update_tcp_ip_header ( struct net_lro_desc * lro_desc ) <nl> *( p + 2 ) = lro_desc -> tcp_rcv_tsecr ; <nl> } <nl>  <nl> + csum_replace2 (& iph -> check , iph -> tot_len , htons ( lro_desc -> ip_tot_len )); <nl> iph -> tot_len = htons ( lro_desc -> ip_tot_len ); <nl>  <nl> - iph -> check = 0 ; <nl> - iph -> check = ip_fast_csum (( u8 *) lro_desc -> iph , iph -> ihl ); <nl> - <nl> tcph -> check = 0 ; <nl> tcp_hdr_csum = csum_partial ( tcph , TCP_HDR_LEN ( tcph ), 0 ); <nl> lro_desc -> data_csum = csum_add ( lro_desc -> data_csum , tcp_hdr_csum );
static void acm_tty_flush_chars ( struct tty_struct * tty ) <nl> int err ; <nl> unsigned long flags ; <nl>  <nl> + if (! cur ) /* nothing to do */ <nl> + return ; <nl> + <nl> acm -> putbuffer = NULL ; <nl> err = usb_autopm_get_interface_async ( acm -> control ); <nl> spin_lock_irqsave (& acm -> write_lock , flags ); <nl> if ( err < 0 ) { <nl> cur -> use = 0 ; <nl> + acm -> putbuffer = cur ; <nl> goto out ; <nl> } <nl> 
static void scrub_print_warning ( const char * errstr , struct scrub_block * sblock ) <nl> u64 flags = 0 ; <nl> u64 ref_root ; <nl> u32 item_size ; <nl> - u8 ref_level ; <nl> + u8 ref_level = 0 ; <nl> int ret ; <nl>  <nl> WARN_ON ( sblock -> page_count < 1 );
fill_write_buffer ( struct sysfs_buffer * buffer , const char __user * buf , size_t <nl> count = PAGE_SIZE - 1 ; <nl> error = copy_from_user ( buffer -> page , buf , count ); <nl> buffer -> needs_read_fill = 1 ; <nl> + /* if buf is assumed to contain a string , terminate it by \ 0 , <nl> + so e . g . sscanf () can scan the string easily */ <nl> + buffer -> page [ count ] = 0 ; <nl> return error ? - EFAULT : count ; <nl> } <nl> 
static void rockchip_drm_unbind ( struct device * dev ) <nl> rockchip_drm_fbdev_fini ( drm_dev ); <nl> drm_kms_helper_poll_fini ( drm_dev ); <nl>  <nl> + drm_atomic_helper_shutdown ( drm_dev ); <nl> drm_vblank_cleanup ( drm_dev ); <nl> component_unbind_all ( dev , drm_dev ); <nl> drm_mode_config_cleanup ( drm_dev );
void sync_timeline_signal ( struct sync_timeline * obj ) <nl> list_for_each_entry_safe ( pt , next , & obj -> active_list_head , <nl> active_list ) { <nl> if ( fence_is_signaled_locked (& pt -> base )) <nl> - list_del (& pt -> active_list ); <nl> + list_del_init (& pt -> active_list ); <nl> } <nl>  <nl> spin_unlock_irqrestore (& obj -> child_list_lock , flags );
static void vblank_disable_and_save ( struct drm_device * dev , int crtc ) <nl> * has been ticking all along until this time . This makes the <nl> * count account for the entire time between drm_vblank_on () and <nl> * drm_vblank_off (). <nl> + * <nl> + * But only do this if precise vblank timestamps are available . <nl> + * Otherwise we might read a totally bogus timestamp since drivers <nl> + * lacking precise timestamp support rely upon sampling the system clock <nl> + * at vblank interrupt time . Which obviously won ' t work out well if the <nl> + * vblank interrupt is disabled . <nl> */ <nl> - if (! vblank -> enabled ) { <nl> + if (! vblank -> enabled && <nl> + drm_get_last_vbltimestamp ( dev , crtc , & tvblank , 0 ) > 0 ) { <nl> drm_update_vblank_count ( dev , crtc ); <nl> spin_unlock_irqrestore (& dev -> vblank_time_lock , irqflags ); <nl> return ;
static void carl9170_ps_beacon ( struct ar9170 * ar , void * data , unsigned int len ) <nl> cam = ieee80211_check_tim ( tim_ie , tim_len , ar -> common . curaid ); <nl>  <nl> /* 2 . Maybe the AP wants to send multicast / broadcast data ? */ <nl> - cam = !!( tim_ie -> bitmap_ctrl & 0x01 ); <nl> + cam |= !!( tim_ie -> bitmap_ctrl & 0x01 ); <nl>  <nl> if (! cam ) { <nl> /* back to low - power land . */
static void end_bio_extent_writepage ( struct bio * bio , int err ) <nl> static void end_bio_extent_readpage ( struct bio * bio , int err ) <nl> { <nl> int uptodate = test_bit ( BIO_UPTODATE , & bio -> bi_flags ); <nl> - struct bio_vec * bvec = bio -> bi_io_vec + bio -> bi_vcnt - 1 ; <nl> + struct bio_vec * bvec_end = bio -> bi_io_vec + bio -> bi_vcnt - 1 ; <nl> + struct bio_vec * bvec = bio -> bi_io_vec ; <nl> struct extent_io_tree * tree ; <nl> u64 start ; <nl> u64 end ; <nl> static void end_bio_extent_readpage ( struct bio * bio , int err ) <nl> else <nl> whole_page = 0 ; <nl>  <nl> - if (-- bvec >= bio -> bi_io_vec ) <nl> + if (++ bvec <= bvec_end ) <nl> prefetchw (& bvec -> bv_page -> flags ); <nl>  <nl> if ( uptodate && tree -> ops && tree -> ops -> readpage_end_io_hook ) { <nl> static void end_bio_extent_readpage ( struct bio * bio , int err ) <nl> } <nl> check_page_locked ( tree , page ); <nl> } <nl> - } while ( bvec >= bio -> bi_io_vec ); <nl> + } while ( bvec <= bvec_end ); <nl>  <nl> bio_put ( bio ); <nl> }
copy_resp_to_buf ( struct snd_efw * efw , void * data , size_t length , int * rcode ) <nl> spin_lock_irq (& efw -> lock ); <nl>  <nl> t = ( struct snd_efw_transaction *) data ; <nl> - length = min_t ( size_t , t -> length * sizeof ( t -> length ), length ); <nl> + length = min_t ( size_t , be32_to_cpu ( t -> length ) * sizeof ( u32 ), length ); <nl>  <nl> if ( efw -> push_ptr < efw -> pull_ptr ) <nl> capacity = ( unsigned int )( efw -> pull_ptr - efw -> push_ptr );
ecryptfs_add_global_auth_tok ( struct ecryptfs_mount_crypt_stat * mount_crypt_stat , <nl> struct ecryptfs_global_auth_tok * new_auth_tok ; <nl> int rc = 0 ; <nl>  <nl> - new_auth_tok = kmem_cache_alloc ( ecryptfs_global_auth_tok_cache , <nl> + new_auth_tok = kmem_cache_zalloc ( ecryptfs_global_auth_tok_cache , <nl> GFP_KERNEL ); <nl> if (! new_auth_tok ) { <nl> rc = - ENOMEM ;
__flush_batch ( journal_t * journal , int * batch_count ) <nl>  <nl> for ( i = 0 ; i < * batch_count ; i ++) { <nl> struct buffer_head * bh = journal -> j_chkpt_bhs [ i ]; <nl> - clear_buffer_jwrite ( bh ); <nl> BUFFER_TRACE ( bh , " brelse "); <nl> __brelse ( bh ); <nl> } <nl> static int __process_buffer ( journal_t * journal , struct journal_head * jh , <nl> BUFFER_TRACE ( bh , " queue "); <nl> get_bh ( bh ); <nl> J_ASSERT_BH ( bh , ! buffer_jwrite ( bh )); <nl> - set_buffer_jwrite ( bh ); <nl> journal -> j_chkpt_bhs [* batch_count ] = bh ; <nl> __buffer_relink_io ( jh ); <nl> jbd_unlock_bh_state ( bh );
static int moxart_gpio_probe ( struct platform_device * pdev ) <nl> gc -> parent = dev ; <nl> gc -> owner = THIS_MODULE ; <nl>  <nl> - ret = gpiochip_add_data ( gc , NULL ); <nl> + ret = devm_gpiochip_add_data ( dev , gc , NULL ); <nl> if ( ret ) { <nl> dev_err ( dev , "% s : gpiochip_add failed \ n ", <nl> dev -> of_node -> full_name );
out : <nl> xfrm_state_put ( x ); <nl> } <nl>  <nl> + static void xfrm_replay_timer_handler ( unsigned long data ); <nl> + <nl> struct xfrm_state * xfrm_state_alloc ( void ) <nl> { <nl> struct xfrm_state * x ; <nl> void xfrm_replay_notify ( struct xfrm_state * x , int event ) <nl> c . data . aevent = event ; <nl> km_state_notify ( x , & c ); <nl>  <nl> - resched : <nl> if ( x -> replay_maxage && <nl> ! mod_timer (& x -> rtimer , jiffies + x -> replay_maxage )) <nl> xfrm_state_hold ( x ); <nl> - <nl> } <nl>  <nl> static void xfrm_replay_timer_handler ( unsigned long data )
static int hdsp_dds_offset ( struct hdsp * hdsp ) <nl> unsigned int dds_value = hdsp -> dds_value ; <nl> int system_sample_rate = hdsp -> system_sample_rate ; <nl>  <nl> + if (! dds_value ) <nl> + return 0 ; <nl> + <nl> n = DDS_NUMERATOR ; <nl> /* <nl> * dds_value = n / rate
static int ad1836_register ( struct ad1836_priv * ad1836 ) <nl>  <nl> if ( ad1836_codec ) { <nl> dev_err ( codec -> dev , " Another ad1836 is registered \ n "); <nl> + kfree ( ad1836 ); <nl> return - EINVAL ; <nl> } <nl> 
static int hostap_enable_hostapd ( PSDevice pDevice , int rtnl_locked ) <nl>  <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO "% s : Enabling hostapd mode \ n ", dev -> name ); <nl>  <nl> - pDevice -> apdev = kzalloc ( sizeof ( struct net_device ), GFP_KERNEL ); <nl> + pDevice -> apdev = alloc_etherdev ( sizeof (* apdev_priv )); <nl> if ( pDevice -> apdev == NULL ) <nl> return - ENOMEM ; <nl> 
*/ <nl> # define AT91_BASE_SYS 0xffffc000 <nl>  <nl> +# endif <nl> + <nl> /* <nl> * On sama5d4 there is no system controller , we map some needed peripherals <nl> */ <nl> # define AT91_ALT_BASE_SYS 0xfc069000 <nl> -# endif <nl>  <nl> /* <nl> * On all at91 have the Advanced Interrupt Controller starts at address <nl> */ <nl> # define AT91_IO_PHYS_BASE AT91_BASE_SYS <nl> # define AT91_IO_VIRT_BASE IOMEM ( AT91_IO_PHYS_BASE ) <nl> + <nl> +# define AT91_ALT_IO_PHYS_BASE AT91_ALT_BASE_SYS <nl> +# define AT91_ALT_IO_VIRT_BASE IOMEM ( AT91_ALT_BASE_SYS ) <nl> # endif <nl>  <nl> # define AT91_IO_SIZE ( 0xFFFFFFFF - AT91_IO_PHYS_BASE + 1 )
static void mib_counters_update ( struct mv643xx_eth_private * mp ) <nl> p -> rx_discard += rdlp ( mp , RX_DISCARD_FRAME_CNT ); <nl> p -> rx_overrun += rdlp ( mp , RX_OVERRUN_FRAME_CNT ); <nl> spin_unlock_bh (& mp -> mib_counters_lock ); <nl> - <nl> - mod_timer (& mp -> mib_counters_timer , jiffies + 30 * HZ ); <nl> } <nl>  <nl> static void mib_counters_timer_wrapper ( unsigned long _mp ) <nl> { <nl> struct mv643xx_eth_private * mp = ( void *) _mp ; <nl> - <nl> mib_counters_update ( mp ); <nl> + mod_timer (& mp -> mib_counters_timer , jiffies + 30 * HZ ); <nl> } <nl>  <nl> 
static int dw8250_probe_of ( struct uart_port * p , <nl> return 0 ; <nl> } <nl>  <nl> -# ifdef CONFIG_ACPI <nl> static int dw8250_probe_acpi ( struct uart_8250_port * up , <nl> struct dw8250_data * data ) <nl> { <nl> static int dw8250_probe_acpi ( struct uart_8250_port * up , <nl>  <nl> return 0 ; <nl> } <nl> -# else <nl> - static inline int dw8250_probe_acpi ( struct uart_8250_port * up , <nl> - struct dw8250_data * data ) <nl> -{ <nl> - return - ENODEV ; <nl> -} <nl> -# endif /* CONFIG_ACPI */ <nl>  <nl> static int dw8250_probe ( struct platform_device * pdev ) <nl> {
module_exit ( visornic_cleanup ); <nl>  <nl> MODULE_AUTHOR (" Unisys "); <nl> MODULE_LICENSE (" GPL "); <nl> - MODULE_DESCRIPTION (" sPAR nic driver for sparlinux : ver 1 . 0 . 0 . 0 "); <nl> - MODULE_VERSION (" 1 . 0 . 0 . 0 "); <nl> + MODULE_DESCRIPTION (" sPAR nic driver for sparlinux ");
static int _opp_add_static_v2 ( struct device * dev , struct device_node * np ) <nl> struct device_opp * dev_opp ; <nl> struct dev_pm_opp * new_opp ; <nl> u64 rate ; <nl> + u32 val ; <nl> int ret ; <nl>  <nl> /* Hold our list modification lock here */ <nl> static int _opp_add_static_v2 ( struct device * dev , struct device_node * np ) <nl> new_opp -> np = np ; <nl> new_opp -> dynamic = false ; <nl> new_opp -> available = true ; <nl> - of_property_read_u32 ( np , " clock - latency - ns ", <nl> - ( u32 *)& new_opp -> clock_latency_ns ); <nl> + <nl> + if (! of_property_read_u32 ( np , " clock - latency - ns ", & val )) <nl> + new_opp -> clock_latency_ns = val ; <nl>  <nl> ret = opp_get_microvolt ( new_opp , dev ); <nl> if ( ret ) <nl> goto free_opp ; <nl>  <nl> - of_property_read_u32 ( np , " opp - microamp ", ( u32 *)& new_opp -> u_amp ); <nl> + if (! of_property_read_u32 ( new_opp -> np , " opp - microamp ", & val )) <nl> + new_opp -> u_amp = val ; <nl>  <nl> ret = _opp_add ( dev , new_opp , dev_opp ); <nl> if ( ret )
static void hrtimer_force_reprogram ( struct hrtimer_cpu_base * cpu_base ) <nl> continue ; <nl> timer = rb_entry ( base -> first , struct hrtimer , node ); <nl> expires = ktime_sub ( hrtimer_get_expires ( timer ), base -> offset ); <nl> + /* <nl> + * clock_was_set () has changed base -> offset so the <nl> + * result might be negative . Fix it up to prevent a <nl> + * false positive in clockevents_program_event () <nl> + */ <nl> + if ( expires . tv64 < 0 ) <nl> + expires . tv64 = 0 ; <nl> if ( expires . tv64 < cpu_base -> expires_next . tv64 ) <nl> cpu_base -> expires_next = expires ; <nl> }
int ivtv_stop_v4l2_encode_stream ( struct ivtv_stream * s , int gop_end ) <nl> stopmode = 1 ; <nl> } <nl>  <nl> + /* ensure these actions are done only once */ <nl> + mutex_lock (& itv -> serialize_lock ); <nl> + <nl> /* end_capture */ <nl> /* when : 0 = end of GOP 1 = NOW !, type : 0 = mpeg , subtype : 3 = video + audio */ <nl> ivtv_vapi ( itv , CX2341X_ENC_STOP_CAPTURE , 3 , stopmode , cap_type , s -> subtype ); <nl> int ivtv_stop_v4l2_encode_stream ( struct ivtv_stream * s , int gop_end ) <nl> /* Clear capture and no - read bits */ <nl> clear_bit ( IVTV_F_S_STREAMING , & s -> s_flags ); <nl>  <nl> - /* ensure these global cleanup actions are done only once */ <nl> - mutex_lock (& itv -> serialize_lock ); <nl> - <nl> if ( s -> type == IVTV_ENC_STREAM_TYPE_VBI ) <nl> ivtv_set_irq_mask ( itv , IVTV_IRQ_ENC_VBI_CAP ); <nl> 
static int rcar_i2c_probe ( struct platform_device * pdev ) <nl> return ret ; <nl> } <nl>  <nl> + pm_runtime_enable ( dev ); <nl> + platform_set_drvdata ( pdev , priv ); <nl> + <nl> ret = i2c_add_numbered_adapter ( adap ); <nl> if ( ret < 0 ) { <nl> dev_err ( dev , " reg adap failed : % d \ n ", ret ); <nl> + pm_runtime_disable ( dev ); <nl> return ret ; <nl> } <nl>  <nl> - pm_runtime_enable ( dev ); <nl> - platform_set_drvdata ( pdev , priv ); <nl> - <nl> dev_info ( dev , " probed \ n "); <nl>  <nl> return 0 ;
static int pty_write ( struct tty_struct * tty , const unsigned char * buf , <nl>  <nl> static int pty_write_room ( struct tty_struct * tty ) <nl> { <nl> + if ( tty -> stopped ) <nl> + return 0 ; <nl> return pty_space ( tty -> link ); <nl> } <nl> 
sch_handle_egress ( struct sk_buff * skb , int * ret , struct net_device * dev ) <nl> case TC_ACT_SHOT : <nl> qdisc_qstats_cpu_drop ( cl -> q ); <nl> * ret = NET_XMIT_DROP ; <nl> - goto drop ; <nl> + kfree_skb ( skb ); <nl> + return NULL ; <nl> case TC_ACT_STOLEN : <nl> case TC_ACT_QUEUED : <nl> * ret = NET_XMIT_SUCCESS ; <nl> - drop : <nl> - kfree_skb ( skb ); <nl> + consume_skb ( skb ); <nl> return NULL ; <nl> case TC_ACT_REDIRECT : <nl> /* No need to push / pop skb ' s mac_header here on egress ! */
struct gpio_chip * gpiochip_find ( void * data , <nl>  <nl> spin_lock_irqsave (& gpio_lock , flags ); <nl> list_for_each_entry ( gdev , & gpio_devices , list ) <nl> - if ( match ( gdev -> chip , data )) <nl> + if ( gdev -> chip && match ( gdev -> chip , data )) <nl> break ; <nl>  <nl> /* No match ? */
static int polaris10_populate_ulv_level ( struct pp_hwmgr * hwmgr , <nl> struct smu7_hwmgr * data = ( struct smu7_hwmgr *)( hwmgr -> backend ); <nl> struct phm_ppt_v1_information * table_info = <nl> ( struct phm_ppt_v1_information *)( hwmgr -> pptable ); <nl> + struct pp_smumgr * smumgr = hwmgr -> smumgr ; <nl>  <nl> state -> CcPwrDynRm = 0 ; <nl> state -> CcPwrDynRm1 = 0 ; <nl> static int polaris10_populate_ulv_level ( struct pp_hwmgr * hwmgr , <nl> state -> VddcOffsetVid = ( uint8_t )( table_info -> us_ulv_voltage_offset * <nl> VOLTAGE_VID_OFFSET_SCALE2 / VOLTAGE_VID_OFFSET_SCALE1 ); <nl>  <nl> - state -> VddcPhase = ( data -> vddc_phase_shed_control ) ? 0 : 1 ; <nl> + if ( smumgr -> is_kicker ) <nl> + state -> VddcPhase = data -> vddc_phase_shed_control ^ 0x3 ; <nl> + else <nl> + state -> VddcPhase = ( data -> vddc_phase_shed_control ) ? 0 : 1 ; <nl>  <nl> CONVERT_FROM_HOST_TO_SMC_UL ( state -> CcPwrDynRm ); <nl> CONVERT_FROM_HOST_TO_SMC_UL ( state -> CcPwrDynRm1 );
int mlx4_en_activate_cq ( struct mlx4_en_priv * priv , struct mlx4_en_cq * cq , <nl> name ); <nl> } <nl>  <nl> - cq -> irq_desc = <nl> - irq_to_desc ( mlx4_eq_get_irq ( mdev -> dev , <nl> - cq -> vector )); <nl> } <nl> } else { <nl> cq -> vector = ( cq -> ring + 1 + priv -> port ) % <nl> mdev -> dev -> caps . num_comp_vectors ; <nl> } <nl> + <nl> + cq -> irq_desc = <nl> + irq_to_desc ( mlx4_eq_get_irq ( mdev -> dev , <nl> + cq -> vector )); <nl> } else { <nl> /* For TX we use the same irq per <nl> ring we assigned for the RX */
nouveau_therm_update_linear ( struct nouveau_therm * therm ) <nl> u8 temp = therm -> temp_get ( therm ); <nl> u16 duty ; <nl>  <nl> + /* handle the non - linear part first */ <nl> + if ( temp < linear_min_temp ) <nl> + return priv -> fan -> bios . min_duty ; <nl> + else if ( temp > linear_max_temp ) <nl> + return priv -> fan -> bios . max_duty ; <nl> + <nl> + /* we are in the linear zone */ <nl> duty = ( temp - linear_min_temp ); <nl> duty *= ( priv -> fan -> bios . max_duty - priv -> fan -> bios . min_duty ); <nl> duty /= ( linear_max_temp - linear_min_temp );
static int devfreq_update_status ( struct devfreq * devfreq , unsigned long freq ) <nl>  <nl> cur_time = jiffies ; <nl>  <nl> + /* Immediately exit if previous_freq is not initialized yet . */ <nl> + if (! devfreq -> previous_freq ) <nl> + goto out ; <nl> + <nl> prev_lev = devfreq_get_freq_level ( devfreq , devfreq -> previous_freq ); <nl> if ( prev_lev < 0 ) { <nl> ret = prev_lev ;
xfs_check_page_type ( <nl> if ( type == XFS_IO_UNWRITTEN ) <nl> return true ; <nl> } else if ( buffer_delay ( bh )) { <nl> - if ( type == XFS_IO_DELALLOC ); <nl> + if ( type == XFS_IO_DELALLOC ) <nl> return true ; <nl> } else if ( buffer_dirty ( bh ) && buffer_mapped ( bh )) { <nl> - if ( type == XFS_IO_OVERWRITE ); <nl> + if ( type == XFS_IO_OVERWRITE ) <nl> return true ; <nl> } <nl> 
static int __devinit sta2x11_mfd_probe ( struct pci_dev * pdev , <nl> sta2x11_mfd_setup ( pdev , setup_data ); <nl>  <nl> /* Record this pdev before mfd_add_devices : their probe looks for it */ <nl> - sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl> + if (! sta2x11_mfd_find ( pdev )) <nl> + sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl>  <nl> /* Just 2 bars for all mfd ' s at present */ <nl> for ( i = 0 ; i < 2 ; i ++) {
static int nokia_modem_probe ( struct device * dev ) <nl> dev_set_drvdata ( dev , modem ); <nl>  <nl> irq = irq_of_parse_and_map ( np , 0 ); <nl> - if ( irq < 0 ) { <nl> + if (! irq ) { <nl> dev_err ( dev , " Invalid rst_ind interrupt (% d )\ n ", irq ); <nl> - return irq ; <nl> + return - EINVAL ; <nl> } <nl> modem -> nokia_modem_rst_ind_irq = irq ; <nl> pflags = irq_get_trigger_type ( irq );
static inline void update_sd_lb_stats ( struct sched_domain * sd , int this_cpu , <nl> /* <nl> * In case the child domain prefers tasks go to siblings <nl> * first , lower the sg capacity to one so that we ' ll try <nl> - * and move all the excess tasks away . <nl> + * and move all the excess tasks away . We lower the capacity <nl> + * of a group only if the local group has the capacity to fit <nl> + * these excess tasks , i . e . nr_running < group_capacity . The <nl> + * extra check prevents the case where you always pull from the <nl> + * heaviest group when it is already under - utilized ( possible <nl> + * with a large weight task outweighs the tasks on the system ). <nl> */ <nl> - if ( prefer_sibling ) <nl> + if ( prefer_sibling && ! local_group && sds -> this_has_capacity ) <nl> sgs . group_capacity = min ( sgs . group_capacity , 1UL ); <nl>  <nl> if ( local_group ) {
process_filter ( struct event_format * event , struct filter_arg ** parg , <nl> * parg = current_op ; <nl> else <nl> * parg = current_exp ; <nl> + free ( token ); <nl> return PEVENT_ERRNO__UNBALANCED_PAREN ; <nl> } <nl> break ; <nl> process_filter ( struct event_format * event , struct filter_arg ** parg , <nl>  <nl> * parg = current_op ; <nl>  <nl> + free ( token ); <nl> return 0 ; <nl>  <nl> fail_alloc :
void lbs_mac_event_disconnected ( struct lbs_private * priv , <nl> bool locally_generated ) <nl> { <nl> + unsigned long flags ; <nl> + <nl> if ( priv -> connect_status != LBS_CONNECTED ) <nl> return ; <nl>  <nl> void lbs_mac_event_disconnected ( struct lbs_private * priv , <nl> netif_carrier_off ( priv -> dev ); <nl>  <nl> /* Free Tx and Rx packets */ <nl> + spin_lock_irqsave (& priv -> driver_lock , flags ); <nl> kfree_skb ( priv -> currenttxskb ); <nl> priv -> currenttxskb = NULL ; <nl> priv -> tx_pending_len = 0 ; <nl> + spin_unlock_irqrestore (& priv -> driver_lock , flags ); <nl>  <nl> priv -> connect_status = LBS_DISCONNECTED ; <nl> 
void kvm_slot_page_track_remove_page ( struct kvm * kvm , <nl> bool kvm_page_track_is_active ( struct kvm_vcpu * vcpu , gfn_t gfn , <nl> enum kvm_page_track_mode mode ) <nl> { <nl> - struct kvm_memory_slot * slot = kvm_vcpu_gfn_to_memslot ( vcpu , gfn ); <nl> - int index = gfn_to_index ( gfn , slot -> base_gfn , PT_PAGE_TABLE_LEVEL ); <nl> + struct kvm_memory_slot * slot ; <nl> + int index ; <nl>  <nl> if ( WARN_ON (! page_track_mode_is_valid ( mode ))) <nl> return false ; <nl>  <nl> + slot = kvm_vcpu_gfn_to_memslot ( vcpu , gfn ); <nl> + if (! slot ) <nl> + return false ; <nl> + <nl> + index = gfn_to_index ( gfn , slot -> base_gfn , PT_PAGE_TABLE_LEVEL ); <nl> return !! ACCESS_ONCE ( slot -> arch . gfn_track [ mode ][ index ]); <nl> } <nl> 
static int __vhost_add_used_n ( struct vhost_virtqueue * vq , <nl>  <nl> start = vq -> last_used_idx % vq -> num ; <nl> used = vq -> used -> ring + start ; <nl> - if ( copy_to_user ( used , heads , count * sizeof * used )) { <nl> + if ( __copy_to_user ( used , heads , count * sizeof * used )) { <nl> vq_err ( vq , " Failed to write used "); <nl> return - EFAULT ; <nl> }
int regmap_raw_read ( struct regmap * map , unsigned int reg , void * val , <nl> return - EINVAL ; <nl> if ( reg % map -> reg_stride ) <nl> return - EINVAL ; <nl> + if ( val_count == 0 ) <nl> + return - EINVAL ; <nl>  <nl> map -> lock ( map -> lock_arg ); <nl> 
struct device ; <nl>  <nl> enum led_brightness { <nl> LED_OFF = 0 , <nl> + LED_ON = 1 , <nl> LED_HALF = 127 , <nl> LED_FULL = 255 , <nl> };
int video_register_device_index ( struct video_device * vfd , int type , int nr , <nl> int ret ; <nl> char * name_base ; <nl>  <nl> + if ( vfd == NULL ) <nl> + return - EINVAL ; <nl> + <nl> switch ( type ) { <nl> case VFL_TYPE_GRABBER : <nl> base = MINOR_VFL_TYPE_GRABBER_MIN ;
__acquires (& pool -> lock ) <nl> * kernels , where a requeueing work item waiting for something to <nl> * happen could deadlock with stop_machine as such work item could <nl> * indefinitely requeue itself while all other CPUs are trapped in <nl> - * stop_machine . <nl> + * stop_machine . At the same time , report a quiescent RCU state so <nl> + * the same condition doesn ' t freeze RCU . <nl> */ <nl> + rcu_note_voluntary_context_switch ( current ); <nl> cond_resched (); <nl>  <nl> spin_lock_irq (& pool -> lock );
int test_range_bit ( struct extent_io_tree * tree , u64 start , u64 end , <nl> bitset = 0 ; <nl> break ; <nl> } <nl> + <nl> + if ( state -> end == ( u64 )- 1 ) <nl> + break ; <nl> + <nl> start = state -> end + 1 ; <nl> if ( start > end ) <nl> break ;
int switch_ssc_clock ( struct rtsx_chip * chip , int clk ) <nl> return STATUS_FAIL ; <nl> } <nl>  <nl> - mcu_cnt = ( u8 )( 125 / clk + 3 ); <nl> + mcu_cnt = ( u8 )( 125 / clk + 3 ); <nl> if ( mcu_cnt > 7 ) <nl> mcu_cnt = 7 ; <nl> 
static struct p9_trans_module p9_virtio_trans = { <nl> . close = p9_virtio_close , <nl> . request = p9_virtio_request , <nl> . cancel = p9_virtio_cancel , <nl> - . maxsize = PAGE_SIZE * 16 , <nl> + . maxsize = PAGE_SIZE * VIRTQUEUE_NUM , <nl> . pref = P9_TRANS_PREF_PAYLOAD_SEP , <nl> . def = 0 , <nl> . owner = THIS_MODULE ,
static int sysfs_get_sb ( struct file_system_type * fs_type , <nl> if ( IS_ERR ( sb ) || sb -> s_fs_info != info ) <nl> kfree ( info ); <nl> if ( IS_ERR ( sb )) { <nl> - kfree ( info ); <nl> error = PTR_ERR ( sb ); <nl> goto out ; <nl> }
nfqnl_recv_config ( struct sock * ctnl , struct sk_buff * skb , <nl>  <nl> if ( nfqa [ NFQA_CFG_PARAMS - 1 ]) { <nl> struct nfqnl_msg_config_params * params ; <nl> - params = NFA_DATA ( nfqa [ NFQA_CFG_PARAMS - 1 ]); <nl>  <nl> + if (! queue ) { <nl> + ret = - ENOENT ; <nl> + goto out_put ; <nl> + } <nl> + params = NFA_DATA ( nfqa [ NFQA_CFG_PARAMS - 1 ]); <nl> nfqnl_set_mode ( queue , params -> copy_mode , <nl> ntohl ( params -> copy_range )); <nl> }
static int emac_link_differs ( struct emac_instance * dev ) <nl> static void emac_link_timer ( struct work_struct * work ) <nl> { <nl> struct emac_instance * dev = <nl> - container_of (( struct delayed_work *) work , <nl> + container_of ( to_delayed_work ( work ), <nl> struct emac_instance , link_work ); <nl> int link_poll_interval ; <nl> 
out : <nl>  <nl> out_free_cache : <nl> memcg_free_cache_params ( s ); <nl> - kfree ( s ); <nl> + kmem_cache_free ( kmem_cache , s ); <nl> goto out ; <nl> } <nl> 
static struct vm_special_mapping vdso_vvar_mapping = { <nl> static void __init init_vdso_image ( struct mips_vdso_image * image ) <nl> { <nl> unsigned long num_pages , i ; <nl> + unsigned long data_pfn ; <nl>  <nl> BUG_ON (! PAGE_ALIGNED ( image -> data )); <nl> BUG_ON (! PAGE_ALIGNED ( image -> size )); <nl>  <nl> num_pages = image -> size / PAGE_SIZE ; <nl>  <nl> - for ( i = 0 ; i < num_pages ; i ++) { <nl> - image -> mapping . pages [ i ] = <nl> - virt_to_page ( image -> data + ( i * PAGE_SIZE )); <nl> - } <nl> + data_pfn = __phys_to_pfn ( __pa_symbol ( image -> data )); <nl> + for ( i = 0 ; i < num_pages ; i ++) <nl> + image -> mapping . pages [ i ] = pfn_to_page ( data_pfn + i ); <nl> } <nl>  <nl> static int __init init_vdso ( void )
static int do_garbage_collect ( struct f2fs_sb_info * sbi , <nl>  <nl> for ( segno = start_segno ; segno < end_segno ; segno ++) { <nl>  <nl> - if ( get_valid_blocks ( sbi , segno , 1 ) == 0 ) <nl> + if ( get_valid_blocks ( sbi , segno , 1 ) == 0 || <nl> + unlikely ( f2fs_cp_error ( sbi ))) <nl> goto next ; <nl>  <nl> /* find segment summary of victim */
static int log_one_block ( struct log_writes_c * lc , <nl> goto out ; <nl> sector ++; <nl>  <nl> + atomic_inc (& lc -> io_blocks ); <nl> bio = bio_alloc ( GFP_KERNEL , block -> vec_cnt ); <nl> if (! bio ) { <nl> DMERR (" Couldn ' t alloc log bio "); <nl> goto error ; <nl> } <nl> - atomic_inc (& lc -> io_blocks ); <nl> bio -> bi_iter . bi_size = 0 ; <nl> bio -> bi_iter . bi_sector = sector ; <nl> bio -> bi_bdev = lc -> logdev -> bdev ;
static ssize_t gt_max_freq_mhz_show ( struct device * kdev , struct device_attribute <nl> int ret ; <nl>  <nl> mutex_lock (& dev_priv -> rps . hw_lock ); <nl> - ret = dev_priv -> rps . hw_max * GT_FREQUENCY_MULTIPLIER ; <nl> + ret = dev_priv -> rps . max_delay * GT_FREQUENCY_MULTIPLIER ; <nl> mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> return snprintf ( buf , PAGE_SIZE , "% d \ n ", ret );
nfsd_setattr ( struct svc_rqst * rqstp , struct svc_fh * fhp , struct iattr * iap , <nl> fh_lock ( fhp ); <nl> host_err = notify_change ( dentry , iap , NULL ); <nl> fh_unlock ( fhp ); <nl> + err = nfserrno ( host_err ); <nl>  <nl> out_put_write_access : <nl> if ( size_change )
static void mv_host_intr ( struct ata_host_set * host_set , u32 relevant , <nl> handled ++; <nl> } <nl>  <nl> - if ( ap && <nl> - ( ap -> flags & ( ATA_FLAG_PORT_DISABLED | ATA_FLAG_NOINTR ))) <nl> + if ( ap && ( ap -> flags & ATA_FLAG_PORT_DISABLED )) <nl> continue ; <nl>  <nl> err_mask = ac_err_mask ( ata_status ); <nl> static void mv_host_intr ( struct ata_host_set * host_set , u32 relevant , <nl> VPRINTK (" port % u IRQ found for qc , " <nl> " ata_status 0x % x \ n ", port , ata_status ); <nl> /* mark qc status appropriately */ <nl> - if (!( qc -> tf . ctl & ATA_NIEN )) <nl> + if (!( qc -> tf . flags & ATA_TFLAG_POLLING )) <nl> ata_qc_complete ( qc , err_mask ); <nl> } <nl> }
static void source_sink_complete ( struct usb_ep * ep , struct usb_request * req ) <nl> switch ( status ) { <nl>  <nl> case 0 : /* normal completion ? */ <nl> - if ( ep == dev -> out_ep ) <nl> + if ( ep == dev -> out_ep ) { <nl> check_read_data ( dev , ep , req ); <nl> - else <nl> + memset ( req -> buf , 0x55 , req -> length ); <nl> + } else <nl> reinit_write_data ( dev , ep , req ); <nl> break ; <nl>  <nl> source_sink_start_ep ( struct usb_ep * ep , gfp_t gfp_flags ) <nl>  <nl> if ( strcmp ( ep -> name , EP_IN_NAME ) == 0 ) <nl> reinit_write_data ( ep -> driver_data , ep , req ); <nl> + else <nl> + memset ( req -> buf , 0x55 , req -> length ); <nl>  <nl> status = usb_ep_queue ( ep , req , gfp_flags ); <nl> if ( status ) {
static int __devinit r6040_init_one ( struct pci_dev * pdev , <nl> adrp [ 1 ] = ioread16 ( ioaddr + MID_0M ); <nl> adrp [ 2 ] = ioread16 ( ioaddr + MID_0H ); <nl>  <nl> + /* Some bootloader / BIOSes do not initialize <nl> + * MAC address , warn about that */ <nl> + if (!( adrp [ 0 ] || adrp [ 1 ] || adrp [ 2 ])) <nl> + printk ( KERN_WARNING DRV_NAME ": MAC address not initialized \ n "); <nl> + <nl> /* Link new device into r6040_root_dev */ <nl> lp -> pdev = pdev ; <nl> lp -> dev = dev ;
nfnl_acct_new ( struct sock * nfnl , struct sk_buff * skb , <nl> return - EINVAL ; <nl>  <nl> acct_name = nla_data ( tb [ NFACCT_NAME ]); <nl> + if ( strlen ( acct_name ) == 0 ) <nl> + return - EINVAL ; <nl>  <nl> list_for_each_entry ( nfacct , & nfnl_acct_list , head ) { <nl> if ( strncmp ( nfacct -> name , acct_name , NFACCT_NAME_MAX ) != 0 )
int blk_queue_enter ( struct request_queue * q , bool nowait ) <nl> if ( nowait ) <nl> return - EBUSY ; <nl>  <nl> + /* <nl> + * read pair of barrier in blk_mq_freeze_queue_start (), <nl> + * we need to order reading __PERCPU_REF_DEAD flag of <nl> + * . q_usage_counter and reading . mq_freeze_depth , <nl> + * otherwise the following wait may never return if the <nl> + * two reads are reordered . <nl> + */ <nl> + smp_rmb (); <nl> + <nl> ret = wait_event_interruptible ( q -> mq_freeze_wq , <nl> ! atomic_read (& q -> mq_freeze_depth ) || <nl> blk_queue_dying ( q ));
static void magicmouse_setup_input ( struct input_dev * input , struct hid_device * h <nl> __set_bit ( BTN_TOOL_TRIPLETAP , input -> keybit ); <nl> __set_bit ( BTN_TOOL_QUADTAP , input -> keybit ); <nl> __set_bit ( BTN_TOUCH , input -> keybit ); <nl> + __set_bit ( INPUT_PROP_POINTER , input -> propbit ); <nl> + __set_bit ( INPUT_PROP_BUTTONPAD , input -> propbit ); <nl> } <nl>  <nl> if ( report_touches ) {
static int __devinit dwc3_omap_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> /* enable all IRQs */ <nl> - dwc3_writel ( omap -> base , USBOTGSS_IRQENABLE_SET_0 , 0x01 ); <nl> + reg = USBOTGSS_IRQO_COREIRQ_ST ; <nl> + dwc3_writel ( omap -> base , USBOTGSS_IRQENABLE_SET_0 , reg ); <nl>  <nl> reg = ( USBOTGSS_IRQ1_OEVT | <nl> USBOTGSS_IRQ1_DRVVBUS_RISE |
static int oz_build_endpoints_for_interface ( struct usb_hcd * hcd , <nl> int request_heartbeat = 0 ; <nl>  <nl> oz_dbg ( ON , " interface [% d ] = % p \ n ", if_ix , intf ); <nl> + if ( if_ix >= port -> num_iface || port -> iface == NULL ) <nl> + return - ENOMEM ; <nl> for ( i = 0 ; i < intf -> desc . bNumEndpoints ; i ++) { <nl> struct usb_host_endpoint * hep = & intf -> endpoint [ i ]; <nl> u8 ep_addr = hep -> desc . bEndpointAddress ;
static struct xfrm_state * pfkey_msg2xfrm_state ( struct net * net , <nl> x -> aalg -> alg_key_len = key -> sadb_key_bits ; <nl> memcpy ( x -> aalg -> alg_key , key + 1 , keysize ); <nl> } <nl> + x -> aalg -> alg_trunc_len = a -> uinfo . auth . icv_truncbits ; <nl> x -> props . aalgo = sa -> sadb_sa_auth ; <nl> /* x -> algo . flags = sa -> sadb_sa_flags ; */ <nl> }
static void vp_del_vq ( struct virtqueue * vq ) <nl> { <nl> struct virtio_pci_device * vp_dev = to_vp_device ( vq -> vdev ); <nl> struct virtio_pci_vq_info * info = vq -> priv ; <nl> - unsigned long size ; <nl> + unsigned long flags , size ; <nl> + <nl> + spin_lock_irqsave (& vp_dev -> lock , flags ); <nl> + list_del (& info -> node ); <nl> + spin_unlock_irqrestore (& vp_dev -> lock , flags ); <nl>  <nl> iowrite16 ( info -> queue_index , vp_dev -> ioaddr + VIRTIO_PCI_QUEUE_SEL ); <nl> 
static int trusted_update ( struct key * key , const void * data , size_t datalen ) <nl> ret = datablob_parse ( datablob , new_p , new_o ); <nl> if ( ret != Opt_update ) { <nl> ret = - EINVAL ; <nl> + kfree ( new_p ); <nl> goto out ; <nl> } <nl> /* copy old key values , and reseal with new pcrs */
out : <nl> return ret ; <nl> } <nl>  <nl> - static void ocfs2_split_record ( struct inode * inode , <nl> + static void ocfs2_split_record ( struct ocfs2_extent_tree * et , <nl> struct ocfs2_path * left_path , <nl> struct ocfs2_path * right_path , <nl> struct ocfs2_extent_rec * split_rec , <nl> static void ocfs2_split_record ( struct inode * inode , <nl> } <nl>  <nl> rec = & el -> l_recs [ index ]; <nl> - ocfs2_subtract_from_rec ( inode -> i_sb , split , rec , split_rec ); <nl> + ocfs2_subtract_from_rec ( ocfs2_metadata_cache_get_super ( et -> et_ci ), <nl> + split , rec , split_rec ); <nl> ocfs2_rotate_leaf ( insert_el , split_rec ); <nl> } <nl>  <nl> static int ocfs2_insert_path ( struct inode * inode , <nl> * of splits , but it ' s easier to just let one separate <nl> * function sort it all out . <nl> */ <nl> - ocfs2_split_record ( inode , left_path , right_path , <nl> + ocfs2_split_record ( et , left_path , right_path , <nl> insert_rec , insert -> ins_split ); <nl>  <nl> /*
static int cache_create ( struct cache_args * ca , struct cache ** result ) <nl> atomic_set (& cache -> nr_migrations , 0 ); <nl> init_waitqueue_head (& cache -> migration_wait ); <nl>  <nl> + r = - ENOMEM ; <nl> cache -> nr_dirty = 0 ; <nl> cache -> dirty_bitset = alloc_bitset ( from_cblock ( cache -> cache_size )); <nl> if (! cache -> dirty_bitset ) {
static int cpufreq_init ( struct cpufreq_policy * policy ) <nl> priv = kzalloc ( sizeof (* priv ), GFP_KERNEL ); <nl> if (! priv ) { <nl> ret = - ENOMEM ; <nl> - goto out_put_node ; <nl> + goto out_free_opp ; <nl> } <nl>  <nl> of_property_read_u32 ( np , " voltage - tolerance ", & priv -> voltage_tolerance ); <nl> out_free_cpufreq_table : <nl> dev_pm_opp_free_cpufreq_table ( cpu_dev , & freq_table ); <nl> out_free_priv : <nl> kfree ( priv ); <nl> - out_put_node : <nl> + out_free_opp : <nl> + of_free_opp_table ( cpu_dev ); <nl> of_node_put ( np ); <nl> out_put_reg_clk : <nl> clk_put ( cpu_clk ); <nl> static int cpufreq_exit ( struct cpufreq_policy * policy ) <nl> if ( priv -> cdev ) <nl> cpufreq_cooling_unregister ( priv -> cdev ); <nl> dev_pm_opp_free_cpufreq_table ( priv -> cpu_dev , & policy -> freq_table ); <nl> + of_free_opp_table ( priv -> cpu_dev ); <nl> clk_put ( policy -> clk ); <nl> if (! IS_ERR ( priv -> cpu_reg )) <nl> regulator_put ( priv -> cpu_reg );
nf_nat_redirect_ipv4 ( struct sk_buff * skb , <nl>  <nl> rcu_read_lock (); <nl> indev = __in_dev_get_rcu ( skb -> dev ); <nl> - if ( indev != NULL ) { <nl> + if ( indev && indev -> ifa_list ) { <nl> ifa = indev -> ifa_list ; <nl> newdst = ifa -> ifa_local ; <nl> }
static void dma_ops_free_addresses ( struct dma_ops_domain * dom , <nl> { <nl> address >>= PAGE_SHIFT ; <nl> iommu_area_free ( dom -> bitmap , address , pages ); <nl> + <nl> + if ( address + pages >= dom -> next_bit ) <nl> + dom -> need_flush = true ; <nl> } <nl>  <nl> /**************************************************************************** <nl> static void __unmap_single ( struct amd_iommu * iommu , <nl>  <nl> dma_ops_free_addresses ( dma_dom , dma_addr , pages ); <nl>  <nl> - if ( amd_iommu_unmap_flush ) <nl> + if ( amd_iommu_unmap_flush || dma_dom -> need_flush ) { <nl> iommu_flush_pages ( iommu , dma_dom -> domain . id , dma_addr , size ); <nl> + dma_dom -> need_flush = false ; <nl> + } <nl> } <nl>  <nl> /*
static int vpfe_open ( struct file * file ) <nl> if (! vpfe_dev -> initialized ) { <nl> if ( vpfe_initialize_device ( vpfe_dev )) { <nl> mutex_unlock (& vpfe_dev -> lock ); <nl> + v4l2_fh_exit (& fh -> fh ); <nl> + kfree ( fh ); <nl> return - ENODEV ; <nl> } <nl> }
static int machine__process_kernel_mmap_event ( struct machine * machine , <nl> if ( __machine__create_kernel_maps ( machine , kernel ) < 0 ) <nl> goto out_problem ; <nl>  <nl> + if ( strstr ( dso -> long_name , " vmlinux ")) <nl> + dso__set_short_name ( dso , "[ kernel . vmlinux ]", false ); <nl> + <nl> machine__set_kernel_mmap_len ( machine , event ); <nl>  <nl> /*
static void rtl8225_rf_set_tx_power ( struct ieee80211_hw * dev , int channel ) <nl> u32 reg ; <nl> int i ; <nl>  <nl> - cck_power = priv -> channels [ channel - 1 ]. val & 0xFF ; <nl> - ofdm_power = priv -> channels [ channel - 1 ]. val >> 8 ; <nl> + cck_power = priv -> channels [ channel - 1 ]. val & 0xF ; <nl> + ofdm_power = priv -> channels [ channel - 1 ]. val >> 4 ; <nl>  <nl> cck_power = min ( cck_power , ( u8 ) 11 ); <nl> ofdm_power = min ( ofdm_power , ( u8 ) 35 ); <nl> static void rtl8225z2_rf_set_tx_power ( struct ieee80211_hw * dev , int channel ) <nl> u32 reg ; <nl> int i ; <nl>  <nl> - cck_power = priv -> channels [ channel - 1 ]. val & 0xFF ; <nl> - ofdm_power = priv -> channels [ channel - 1 ]. val >> 8 ; <nl> + cck_power = priv -> channels [ channel - 1 ]. val & 0xF ; <nl> + ofdm_power = priv -> channels [ channel - 1 ]. val >> 4 ; <nl>  <nl> cck_power = min ( cck_power , ( u8 ) 15 ); <nl> cck_power += priv -> txpwr_base & 0xF ;
static int pm8001_chip_sata_req ( struct pm8001_hba_info * pm8001_ha , <nl>  <nl> /* Check for read log for failed drive and return */ <nl> if ( sata_cmd . sata_fis . command == 0x2f ) { <nl> - if ( pm8001_ha_dev && (( pm8001_ha_dev -> id & NCQ_READ_LOG_FLAG ) || <nl> + if ((( pm8001_ha_dev -> id & NCQ_READ_LOG_FLAG ) || <nl> ( pm8001_ha_dev -> id & NCQ_ABORT_ALL_FLAG ) || <nl> ( pm8001_ha_dev -> id & NCQ_2ND_RLE_FLAG ))) { <nl> struct task_status_struct * ts ;
static int __devexit tegra_nvec_remove ( struct platform_device * pdev ) <nl> static int tegra_nvec_suspend ( struct platform_device * pdev , pm_message_t state ) <nl> { <nl> struct nvec_chip * nvec = platform_get_drvdata ( pdev ); <nl> + struct nvec_msg * msg ; <nl>  <nl> dev_dbg ( nvec -> dev , " suspending \ n "); <nl> - nvec_write_async ( nvec , EC_DISABLE_EVENT_REPORTING , 3 ); <nl> - nvec_write_async ( nvec , "\ x04 \ x02 ", 2 ); <nl> + <nl> + /* keep these sync or you ' ll break suspend */ <nl> + msg = nvec_write_sync ( nvec , EC_DISABLE_EVENT_REPORTING , 3 ); <nl> + nvec_msg_free ( nvec , msg ); <nl> + msg = nvec_write_sync ( nvec , "\ x04 \ x02 ", 2 ); <nl> + nvec_msg_free ( nvec , msg ); <nl> + <nl> nvec_disable_i2c_slave ( nvec ); <nl>  <nl> return 0 ;
static int __devinit xgifb_probe ( struct pci_dev * pdev , <nl> xgifb_info -> hasVB = HASVB_NONE ; <nl> } else if ( xgifb_info -> chip == XG21 ) { <nl> CR38 = xgifb_reg_get ( XGICR , 0x38 ); <nl> - if (( CR38 & 0xE0 ) == 0xC0 ) { <nl> + if (( CR38 & 0xE0 ) == 0xC0 ) <nl> xgifb_info -> display2 = XGIFB_DISP_LCD ; <nl> - } else if (( CR38 & 0xE0 ) == 0x60 ) { <nl> + else if (( CR38 & 0xE0 ) == 0x60 ) <nl> xgifb_info -> hasVB = HASVB_CHRONTEL ; <nl> - } else { <nl> + else <nl> xgifb_info -> hasVB = HASVB_NONE ; <nl> - } <nl> } else { <nl> XGIfb_get_VB_type ( xgifb_info ); <nl> } <nl> static int __devinit xgifb_probe ( struct pci_dev * pdev , <nl> if ( tmp & 0x20 ) { <nl> tmp = xgifb_reg_get ( <nl> XGIPART1 , 0x13 ); <nl> - if ( tmp & 0x04 ) { <nl> - /* XGI_Pr . XGI_UseLCDA = 1 ; */ <nl> - } <nl> } <nl> } <nl> }
static int read_bus_info_block ( struct fw_device * device , int generation ) <nl> return - ENOMEM ; <nl>  <nl> stack = & rom [ READ_BIB_ROM_SIZE ]; <nl> + memset ( rom , 0 , sizeof (* rom ) * READ_BIB_ROM_SIZE ); <nl>  <nl> device -> max_speed = SCODE_100 ; <nl> 
void omap_sram_idle ( void ) <nl> printk ( KERN_ERR " Invalid mpu state in sram_idle \ n "); <nl> return ; <nl> } <nl> - pwrdm_pre_transition (); <nl>  <nl> /* NEON control */ <nl> if ( pwrdm_read_pwrst ( neon_pwrdm ) == PWRDM_POWER_ON ) <nl> void omap_sram_idle ( void ) <nl> if (! console_trylock ()) <nl> goto console_still_active ; <nl>  <nl> + pwrdm_pre_transition (); <nl> + <nl> /* PER */ <nl> if ( per_next_state < PWRDM_POWER_ON ) { <nl> per_going_off = ( per_next_state == PWRDM_POWER_OFF ) ? 1 : 0 ; <nl> void omap_sram_idle ( void ) <nl> } <nl> omap3_intc_resume_idle (); <nl>  <nl> + pwrdm_post_transition (); <nl> + <nl> /* PER */ <nl> if ( per_next_state < PWRDM_POWER_ON ) { <nl> per_prev_state = pwrdm_read_prev_pwrst ( per_pwrdm ); <nl> console_still_active : <nl> omap3_disable_io_chain (); <nl> } <nl>  <nl> - pwrdm_post_transition (); <nl> - <nl> clkdm_allow_idle ( mpu_pwrdm -> pwrdm_clkdms [ 0 ]); <nl> } <nl> 
out_disable_phy : <nl> out_unregister_bus : <nl> phy_exit ( host -> generic_phy ); <nl> out_host_free : <nl> - devm_kfree ( dev , host ); <nl> ufshcd_set_variant ( hba , NULL ); <nl> out : <nl> return err ;
static const struct pci_device_id iwl_hw_card_ids [] = { <nl> /* 8000 Series */ <nl> { IWL_PCI_DEVICE ( 0x24F3 , 0x0010 , iwl8260_2ac_cfg )}, <nl> { IWL_PCI_DEVICE ( 0x24F3 , 0x1010 , iwl8260_2ac_cfg )}, <nl> + { IWL_PCI_DEVICE ( 0x24F3 , 0x0110 , iwl8260_2ac_cfg )}, <nl> + { IWL_PCI_DEVICE ( 0x24F3 , 0x1110 , iwl8260_2ac_cfg )}, <nl> { IWL_PCI_DEVICE ( 0x24F3 , 0x0050 , iwl8260_2ac_cfg )}, <nl> + { IWL_PCI_DEVICE ( 0x24F3 , 0x0250 , iwl8260_2ac_cfg )}, <nl> { IWL_PCI_DEVICE ( 0x24F3 , 0x1050 , iwl8260_2ac_cfg )}, <nl> + { IWL_PCI_DEVICE ( 0x24F3 , 0x0150 , iwl8260_2ac_cfg )}, <nl> { IWL_PCI_DEVICE ( 0x24F4 , 0x0030 , iwl8260_2ac_cfg )}, <nl> + { IWL_PCI_DEVICE ( 0x24F4 , 0x1130 , iwl8260_2ac_cfg )}, <nl> { IWL_PCI_DEVICE ( 0x24F4 , 0x1030 , iwl8260_2ac_cfg )}, <nl> { IWL_PCI_DEVICE ( 0x24F3 , 0xC010 , iwl8260_2ac_cfg )}, <nl> { IWL_PCI_DEVICE ( 0x24F3 , 0xD010 , iwl8260_2ac_cfg )}, <nl> static const struct pci_device_id iwl_hw_card_ids [] = { <nl> { IWL_PCI_DEVICE ( 0x24F5 , 0x0010 , iwl4165_2ac_cfg )}, <nl> { IWL_PCI_DEVICE ( 0x24F6 , 0x0030 , iwl4165_2ac_cfg )}, <nl> { IWL_PCI_DEVICE ( 0x24F3 , 0x0810 , iwl8260_2ac_cfg )}, <nl> + { IWL_PCI_DEVICE ( 0x24F3 , 0x0910 , iwl8260_2ac_cfg )}, <nl> + { IWL_PCI_DEVICE ( 0x24F3 , 0x0850 , iwl8260_2ac_cfg )}, <nl> + { IWL_PCI_DEVICE ( 0x24F3 , 0x0950 , iwl8260_2ac_cfg )}, <nl> # endif /* CONFIG_IWLMVM */ <nl>  <nl> { 0 }
static void bat_iv_ogm_iface_enable ( struct hard_iface * hard_iface ) <nl> { <nl> struct batman_ogm_packet * batman_ogm_packet ; <nl> + uint32_t random_seqno ; <nl> + <nl> + /* randomize initial seqno to avoid collision */ <nl> + get_random_bytes (& random_seqno , sizeof ( random_seqno )); <nl> + atomic_set (& hard_iface -> seqno , random_seqno ); <nl>  <nl> hard_iface -> packet_len = BATMAN_OGM_LEN ; <nl> hard_iface -> packet_buff = kmalloc ( hard_iface -> packet_len , GFP_ATOMIC );
SYSCALL_DEFINE2 ( delete_module , const char __user *, name_user , <nl> return - EFAULT ; <nl> name [ MODULE_NAME_LEN - 1 ] = '\ 0 '; <nl>  <nl> + audit_log_kern_module ( name ); <nl> + <nl> if ( mutex_lock_interruptible (& module_mutex ) != 0 ) <nl> return - EINTR ; <nl> 
struct rxrpc_call * rxrpc_kernel_begin_call ( struct socket * sock , <nl> struct rxrpc_transport * trans ; <nl> struct rxrpc_call * call ; <nl> struct rxrpc_sock * rx = rxrpc_sk ( sock -> sk ); <nl> + int ret ; <nl>  <nl> _enter (",,% x ,% lx ", key_serial ( key ), user_call_ID ); <nl>  <nl> + ret = rxrpc_validate_address ( rx , srx , sizeof (* srx )); <nl> + if ( ret < 0 ) <nl> + return ERR_PTR ( ret ); <nl> + <nl> lock_sock (& rx -> sk ); <nl>  <nl> if (! key )
static int wm8904_remove ( struct snd_soc_codec * codec ) <nl>  <nl> wm8904_set_bias_level ( codec , SND_SOC_BIAS_OFF ); <nl> regulator_bulk_free ( ARRAY_SIZE ( wm8904 -> supplies ), wm8904 -> supplies ); <nl> + kfree ( wm8904 -> retune_mobile_texts ); <nl> + kfree ( wm8904 -> drc_texts ); <nl>  <nl> return 0 ; <nl> }
int mwifiex_ret_wmm_get_status ( struct mwifiex_private * priv , <nl> " WMM Parameter Set Count : % d \ n ", <nl> wmm_param_ie -> qos_info_bitmap & mask ); <nl>  <nl> + if ( wmm_param_ie -> vend_hdr . len + 2 > <nl> + sizeof ( struct ieee_types_wmm_parameter )) <nl> + break ; <nl> + <nl> memcpy (( u8 *) & priv -> curr_bss_params . bss_descriptor . <nl> wmm_ie , wmm_param_ie , <nl> wmm_param_ie -> vend_hdr . len + 2 );
static int wl1271_prepare_tx_frame ( struct wl1271 * wl , struct wl12xx_vif * wlvif , <nl> is_wep = ( cipher == WLAN_CIPHER_SUITE_WEP40 ) || <nl> ( cipher == WLAN_CIPHER_SUITE_WEP104 ); <nl>  <nl> - if ( WARN_ON ( is_wep && wlvif -> default_key != idx )) { <nl> + if ( WARN_ON ( is_wep && wlvif && wlvif -> default_key != idx )) { <nl> ret = wl1271_set_default_wep_key ( wl , wlvif , idx ); <nl> if ( ret < 0 ) <nl> return ret ;
struct pwm_device * pwm_get ( struct device * dev , const char * con_id ) <nl> unsigned int best = 0 ; <nl> struct pwm_lookup * p ; <nl> unsigned int match ; <nl> + unsigned int period ; <nl> + enum pwm_polarity polarity ; <nl>  <nl> /* look up via DT first */ <nl> if ( IS_ENABLED ( CONFIG_OF ) && dev && dev -> of_node ) <nl> struct pwm_device * pwm_get ( struct device * dev , const char * con_id ) <nl> if ( match > best ) { <nl> chip = pwmchip_find_by_name ( p -> provider ); <nl> index = p -> index ; <nl> + period = p -> period ; <nl> + polarity = p -> polarity ; <nl>  <nl> if ( match != 3 ) <nl> best = match ; <nl> struct pwm_device * pwm_get ( struct device * dev , const char * con_id ) <nl> if ( IS_ERR ( pwm )) <nl> return pwm ; <nl>  <nl> - pwm_set_period ( pwm , p -> period ); <nl> - pwm_set_polarity ( pwm , p -> polarity ); <nl> + pwm_set_period ( pwm , period ); <nl> + pwm_set_polarity ( pwm , polarity ); <nl>  <nl>  <nl> return pwm ;
void __init paging_init ( void ) <nl>  <nl> mem_map = NODE_DATA ( 0 )-> node_mem_map ; <nl>  <nl> - memset ( zero_page , 0 , PAGE_SIZE ); <nl> empty_zero_page = virt_to_page ( zero_page ); <nl> flush_dcache_page ( empty_zero_page ); <nl> }
static void tw5864_handle_frame ( struct tw5864_h264_frame * frame ) <nl> input -> vb = NULL ; <nl> spin_unlock_irqrestore (& input -> slock , flags ); <nl>  <nl> - v4l2_buf = to_vb2_v4l2_buffer (& vb -> vb . vb2_buf ); <nl> - <nl> if (! vb ) { /* Gone because of disabling */ <nl> dev_dbg (& dev -> pci -> dev , " vb is empty , dropping frame \ n "); <nl> return ; <nl> } <nl>  <nl> + v4l2_buf = to_vb2_v4l2_buffer (& vb -> vb . vb2_buf ); <nl> + <nl> /* <nl> * Check for space . <nl> * Mind the overhead of startcode emulation prevention .
static inline int kvm_apic_id ( struct kvm_lapic * apic ) <nl> return ( kvm_apic_get_reg ( apic , APIC_ID ) >> 24 ) & 0xff ; <nl> } <nl>  <nl> +# define KVM_X2APIC_CID_BITS 0 <nl> + <nl> static void recalculate_apic_map ( struct kvm * kvm ) <nl> { <nl> struct kvm_apic_map * new , * old = NULL ; <nl> static void recalculate_apic_map ( struct kvm * kvm ) <nl> if ( apic_x2apic_mode ( apic )) { <nl> new -> ldr_bits = 32 ; <nl> new -> cid_shift = 16 ; <nl> - new -> cid_mask = new -> lid_mask = 0xffff ; <nl> + new -> cid_mask = ( 1 << KVM_X2APIC_CID_BITS ) - 1 ; <nl> + new -> lid_mask = 0xffff ; <nl> } else if ( kvm_apic_sw_enabled ( apic ) && <nl> ! new -> cid_mask /* flat mode */ && <nl> kvm_apic_get_reg ( apic , APIC_DFR ) == APIC_DFR_CLUSTER ) {
int BcmGetSectionValEndOffset ( struct bcm_mini_adapter * Adapter , enum bcm_flash2x <nl> case CONTROL_SECTION : <nl> /* Not Clear So Putting failure . confirm and fix it . */ <nl> SectEndOffset = STATUS_FAILURE ; <nl> + break ; <nl> case ISO_IMAGE1_PART2 : <nl> if ( Adapter -> psFlash2xCSInfo -> OffsetISOImage1Part2End != UNINIT_PTR_IN_CS ) <nl> SectEndOffset = ( Adapter -> psFlash2xCSInfo -> OffsetISOImage1Part2End );
static int snd_seq_ioctl_remove_events ( struct snd_seq_client * client , <nl> * No restrictions so for a user client we can clear <nl> * the whole fifo <nl> */ <nl> - if ( client -> type == USER_CLIENT ) <nl> + if ( client -> type == USER_CLIENT && client -> data . user . fifo ) <nl> snd_seq_fifo_clear ( client -> data . user . fifo ); <nl> } <nl> 
static int pn533_target_found ( struct pn533 * dev , <nl> if ( resp -> tg != 1 ) <nl> return - EPROTO ; <nl>  <nl> + memset (& nfc_tgt , 0 , sizeof ( struct nfc_target )); <nl> + <nl> target_data_len = resp_len - sizeof ( struct pn533_poll_response ); <nl>  <nl> switch ( dev -> poll_mod_curr ) {
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> spin_unlock_bh (& g_cdev . lock ); <nl> if ( pd == NULL ) <nl> return - ENXIO ; <nl> + if (!( pd -> state & OZ_PD_S_CONNECTED )) <nl> + return - EAGAIN ; <nl> eb = & pd -> elt_buff ; <nl> ei = oz_elt_info_alloc ( eb ); <nl> if ( ei == NULL ) {
ath5k_intr ( int irq , void * dev_id ) <nl> tasklet_schedule (& sc -> restq ); <nl> } else { <nl> if ( status & AR5K_INT_SWBA ) { <nl> - tasklet_schedule (& sc -> beacontq ); <nl> + tasklet_hi_schedule (& sc -> beacontq ); <nl> } <nl> if ( status & AR5K_INT_RXEOL ) { <nl> /*
static int __init memory_tier_init ( void ) <nl> * than default DRAM tier . <nl> */ <nl> default_dram_type = alloc_memory_type ( MEMTIER_ADISTANCE_DRAM ); <nl> - if (! default_dram_type ) <nl> + if ( IS_ERR ( default_dram_type )) <nl> panic ("% s () failed to allocate default DRAM tier \ n ", __func__ ); <nl>  <nl> /*
static int nl80211_get_ftm_responder_stats ( struct sk_buff * skb , <nl> hdr = nl80211hdr_put ( msg , info -> snd_portid , info -> snd_seq , 0 , <nl> NL80211_CMD_GET_FTM_RESPONDER_STATS ); <nl> if (! hdr ) <nl> - return - ENOBUFS ; <nl> + goto nla_put_failure ; <nl>  <nl> if ( nla_put_u32 ( msg , NL80211_ATTR_IFINDEX , dev -> ifindex )) <nl> goto nla_put_failure ;
batadv_iv_ogm_orig_get ( struct batadv_priv * bat_priv , const uint8_t * addr ) <nl> free_bcast_own : <nl> kfree ( orig_node -> bat_iv . bcast_own ); <nl> free_orig_node : <nl> + /* free twice , as batadv_orig_node_new sets refcount to 2 */ <nl> + batadv_orig_node_free_ref ( orig_node ); <nl> batadv_orig_node_free_ref ( orig_node ); <nl>  <nl> return NULL ;
static int xfrm_dump_sa_done ( struct netlink_callback * cb ) <nl> struct sock * sk = cb -> skb -> sk ; <nl> struct net * net = sock_net ( sk ); <nl>  <nl> - xfrm_state_walk_done ( walk , net ); <nl> + if ( cb -> args [ 0 ]) <nl> + xfrm_state_walk_done ( walk , net ); <nl> return 0 ; <nl> } <nl>  <nl> static int xfrm_dump_sa ( struct sk_buff * skb , struct netlink_callback * cb ) <nl> u8 proto = 0 ; <nl> int err ; <nl>  <nl> - cb -> args [ 0 ] = 1 ; <nl> - <nl> err = nlmsg_parse ( cb -> nlh , 0 , attrs , XFRMA_MAX , <nl> xfrma_policy ); <nl> if ( err < 0 ) <nl> static int xfrm_dump_sa ( struct sk_buff * skb , struct netlink_callback * cb ) <nl> proto = nla_get_u8 ( attrs [ XFRMA_PROTO ]); <nl>  <nl> xfrm_state_walk_init ( walk , proto , filter ); <nl> + cb -> args [ 0 ] = 1 ; <nl> } <nl>  <nl> ( void ) xfrm_state_walk ( net , walk , dump_one_state , & info );
static int pl08x_probe ( struct amba_device * adev , const struct amba_id * id ) <nl> if ( ret ) <nl> return ret ; <nl>  <nl> + /* Ensure that we can do DMA */ <nl> + ret = dma_set_mask_and_coherent (& adev -> dev , DMA_BIT_MASK ( 32 )); <nl> + if ( ret ) <nl> + goto out_no_pl08x ; <nl> + <nl> /* Create the driver state holder */ <nl> pl08x = kzalloc ( sizeof (* pl08x ), GFP_KERNEL ); <nl> if (! pl08x ) {
static inline unsigned long ppc_function_entry ( void * func ) <nl> * On PPC64 ABIv1 the function pointer actually points to the <nl> * function ' s descriptor . The first entry in the descriptor is the <nl> * address of the function text . <nl> + * <nl> + * However , we may also receive pointer to an assembly symbol . To <nl> + * detect that , we first check if the function pointer we receive <nl> + * already points to kernel / module text and we only dereference it <nl> + * if it doesn ' t . <nl> */ <nl> - return (( func_descr_t *) func )-> entry ; <nl> + if ( kernel_text_address (( unsigned long ) func )) <nl> + return ( unsigned long ) func ; <nl> + else <nl> + return (( func_descr_t *) func )-> entry ; <nl> # else <nl> return ( unsigned long ) func ; <nl> # endif
static int hci_sock_setsockopt ( struct socket * sock , int level , int optname , <nl>  <nl> BT_DBG (" sk % p , opt % d ", sk , optname ); <nl>  <nl> + if ( level != SOL_HCI ) <nl> + return - ENOPROTOOPT ; <nl> + <nl> lock_sock ( sk ); <nl>  <nl> if ( hci_pi ( sk )-> channel != HCI_CHANNEL_RAW ) { <nl> static int hci_sock_getsockopt ( struct socket * sock , int level , int optname , <nl>  <nl> BT_DBG (" sk % p , opt % d ", sk , optname ); <nl>  <nl> + if ( level != SOL_HCI ) <nl> + return - ENOPROTOOPT ; <nl> + <nl> if ( get_user ( len , optlen )) <nl> return - EFAULT ; <nl> 
static int ci_hdrc_usb2_probe ( struct platform_device * pdev ) <nl>  <nl> if (! ci_pdata ) { <nl> ci_pdata = devm_kmalloc ( dev , sizeof (* ci_pdata ), GFP_KERNEL ); <nl> + if (! ci_pdata ) <nl> + return - ENOMEM ; <nl> * ci_pdata = ci_default_pdata ; /* struct copy */ <nl> } <nl> 
static int i2c_hid_hwreset ( struct i2c_client * client ) <nl> static void i2c_hid_get_input ( struct i2c_hid * ihid ) <nl> { <nl> int ret , ret_size ; <nl> - int size = le16_to_cpu ( ihid -> hdesc . wMaxInputLength ); <nl> + int size = ihid -> bufsize ; <nl>  <nl> ret = i2c_master_recv ( ihid -> client , ihid -> inbuf , size ); <nl> if ( ret != size ) {
static int rbd_header_from_disk ( struct rbd_device * rbd_dev , <nl> /* Allocate this now to avoid having to handle failure below */ <nl>  <nl> if ( first_time ) { <nl> - size_t len ; <nl> - <nl> - len = strnlen ( ondisk -> object_prefix , <nl> - sizeof ( ondisk -> object_prefix )); <nl> - object_prefix = kmalloc ( len + 1 , GFP_KERNEL ); <nl> + object_prefix = kstrndup ( ondisk -> object_prefix , <nl> + sizeof ( ondisk -> object_prefix ), <nl> + GFP_KERNEL ); <nl> if (! object_prefix ) <nl> return - ENOMEM ; <nl> - memcpy ( object_prefix , ondisk -> object_prefix , len ); <nl> - object_prefix [ len ] = '\ 0 '; <nl> } <nl>  <nl> /* Allocate the snapshot context and fill it in */
static inline struct s3c_gpio_chip * s3c_gpiolib_getchip ( unsigned int pin ) <nl> return NULL ; <nl>  <nl> chip = & s3c24xx_gpios [ pin / 32 ]; <nl> - return ( S3C2410_GPIO_OFFSET ( pin ) > chip -> chip . ngpio ) ? chip : NULL ; <nl> + return ( S3C2410_GPIO_OFFSET ( pin ) < chip -> chip . ngpio ) ? chip : NULL ; <nl> } <nl>  <nl> # endif /* __ASM_ARCH_GPIO_CORE_H */
static struct super_block * v9fs_get_sb ( struct file_system_type <nl> v9fs_t_clunk ( v9ses , newfid ); <nl> } else { <nl> /* Setup the Root Inode */ <nl> - kfree ( fcall ); <nl> root_fid = v9fs_fid_create ( v9ses , newfid ); <nl> if ( root_fid == NULL ) { <nl> retval = - ENOMEM ; <nl> static struct super_block * v9fs_get_sb ( struct file_system_type <nl> } <nl>  <nl> retval = v9fs_fid_insert ( root_fid , root ); <nl> - if ( retval < 0 ) <nl> + if ( retval < 0 ) { <nl> + kfree ( fcall ); <nl> goto put_back_sb ; <nl> + } <nl>  <nl> root_fid -> qid = fcall -> params . rstat . stat . qid ; <nl> root -> d_inode -> i_ino = <nl> static struct super_block * v9fs_get_sb ( struct file_system_type <nl> v9fs_stat2inode (& fcall -> params . rstat . stat , root -> d_inode , sb ); <nl> } <nl>  <nl> + kfree ( fcall ); <nl> + <nl> if ( stat_result < 0 ) { <nl> retval = stat_result ; <nl> goto put_back_sb ;
int snd_usbmidi_create ( struct snd_card * card , <nl> else <nl> err = snd_usbmidi_create_endpoints ( umidi , endpoints ); <nl> if ( err < 0 ) { <nl> - snd_usbmidi_free ( umidi ); <nl> return err ; <nl> } <nl> 
static int snd_usb_cm106_boot_quirk ( struct usb_device * dev ) <nl> */ <nl> static int snd_usb_cm6206_boot_quirk ( struct usb_device * dev ) <nl> { <nl> - int err , reg ; <nl> + int err = 0 , reg ; <nl> int val [] = { 0x2004 , 0x3000 , 0xf800 , 0x143f , 0x0000 , 0x3000 }; <nl>  <nl> for ( reg = 0 ; reg < ARRAY_SIZE ( val ); reg ++) {
static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> . mc = gp100_mc_new , <nl> + . pci = gp100_pci_new , <nl> . top = gk104_top_new , <nl> }; <nl> 
static int goldfish_tty_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> err_tty_register_device_failed : <nl> - free_irq ( irq , pdev ); <nl> + free_irq ( irq , qtty ); <nl> err_request_irq_failed : <nl> goldfish_tty_current_line_count --; <nl> if ( goldfish_tty_current_line_count == 0 )
static int br_multicast_ipv4_rcv ( struct net_bridge * br , <nl> if ( unlikely ( ip_fast_csum (( u8 *) iph , iph -> ihl ))) <nl> return - EINVAL ; <nl>  <nl> - if ( iph -> protocol != IPPROTO_IGMP ) <nl> + if ( iph -> protocol != IPPROTO_IGMP ) { <nl> + if (( iph -> daddr & IGMP_LOCAL_GROUP_MASK ) != IGMP_LOCAL_GROUP ) <nl> + BR_INPUT_SKB_CB ( skb )-> mrouters_only = 1 ; <nl> return 0 ; <nl> + } <nl>  <nl> len = ntohs ( iph -> tot_len ); <nl> if ( skb -> len < len || len < ip_hdrlen ( skb ))
static void sba_process_received_request ( struct sba_device * sba , <nl>  <nl> WARN_ON ( tx -> cookie < 0 ); <nl> if ( tx -> cookie > 0 ) { <nl> + spin_lock_irqsave (& sba -> reqs_lock , flags ); <nl> dma_cookie_complete ( tx ); <nl> + spin_unlock_irqrestore (& sba -> reqs_lock , flags ); <nl> dmaengine_desc_get_callback_invoke ( tx , NULL ); <nl> dma_descriptor_unmap ( tx ); <nl> tx -> callback = NULL ;
static int goldfish_audio_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> err_misc_register_failed : <nl> + free_irq ( data -> irq , data ); <nl> err_request_irq_failed : <nl> dma_free_coherent (& pdev -> dev , COMBINED_BUFFER_SIZE , <nl> data -> buffer_virt , data -> buffer_phys );
static int ioat_pci_probe ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl>  <nl> device -> version = readb ( device -> reg_base + IOAT_VER_OFFSET ); <nl> if ( device -> version >= IOAT_VER_3_0 ) { <nl> + if ( is_skx_ioat ( pdev )) <nl> + device -> version = IOAT_VER_3_2 ; <nl> err = ioat3_dma_probe ( device , ioat_dca_enabled ); <nl>  <nl> if ( device -> version >= IOAT_VER_3_3 )
static struct map * kernel_get_module_map ( const char * module ) <nl> module = " kernel "; <nl>  <nl> for ( pos = maps__first ( maps ); pos ; pos = map__next ( pos )) { <nl> + /* short_name is "[ module ]" */ <nl> if ( strncmp ( pos -> dso -> short_name + 1 , module , <nl> - pos -> dso -> short_name_len - 2 ) == 0 ) { <nl> + pos -> dso -> short_name_len - 2 ) == 0 && <nl> + module [ pos -> dso -> short_name_len - 2 ] == '\ 0 ') { <nl> return pos ; <nl> } <nl> }
static void __devinit dfx_bus_init ( struct net_device * dev ) <nl> * Interrupts are disabled at the adapter bus - specific logic . <nl> */ <nl>  <nl> - static void __devinit dfx_bus_uninit ( struct net_device * dev ) <nl> + static void __devexit dfx_bus_uninit ( struct net_device * dev ) <nl> { <nl> DFX_board_t * bp = netdev_priv ( dev ); <nl> struct device * bdev = bp -> bus_dev ;
EXPORT_SYMBOL_GPL ( crypto_init_spawn2 ); <nl>  <nl> void crypto_drop_spawn ( struct crypto_spawn * spawn ) <nl> { <nl> + if (! spawn -> alg ) <nl> + return ; <nl> + <nl> down_write (& crypto_alg_sem ); <nl> list_del (& spawn -> list ); <nl> up_write (& crypto_alg_sem );
int xhci_queue_bulk_tx ( struct xhci_hcd * xhci , gfp_t mem_flags , <nl> send_addr = addr ; <nl>  <nl> /* Queue the TRBs , even if they are zero - length */ <nl> - for ( enqd_len = 0 ; enqd_len < full_len ; enqd_len += trb_buff_len ) { <nl> + for ( enqd_len = 0 ; first_trb || enqd_len < full_len ; <nl> + enqd_len += trb_buff_len ) { <nl> field = TRB_TYPE ( TRB_NORMAL ); <nl>  <nl> /* TRB buffer should not cross 64KB boundaries */
static int cdrom_ioctl_drive_status ( struct cdrom_device_info * cdi , <nl> if (! CDROM_CAN ( CDC_SELECT_DISC ) || <nl> ( arg == CDSL_CURRENT || arg == CDSL_NONE )) <nl> return cdi -> ops -> drive_status ( cdi , CDSL_CURRENT ); <nl> - if ((( int ) arg >= cdi -> capacity )) <nl> + if ( arg >= cdi -> capacity ) <nl> return - EINVAL ; <nl> return cdrom_slot_status ( cdi , arg ); <nl> }
int kobject_action_type ( const char * buf , size_t count , <nl> enum kobject_action action ; <nl> int ret = - EINVAL ; <nl>  <nl> - if ( count && buf [ count - 1 ] == '\ n ') <nl> + if ( count && ( buf [ count - 1 ] == '\ n ' || buf [ count - 1 ] == '\ 0 ')) <nl> count --; <nl>  <nl> if (! count )
static int verify_level_key ( struct btrfs_fs_info * fs_info , <nl> if (! first_key ) <nl> return 0 ; <nl>  <nl> + /* <nl> + * For live tree block ( new tree blocks in current transaction ), <nl> + * we need proper lock context to avoid race , which is impossible here . <nl> + * So we only checks tree blocks which is read from disk , whose <nl> + * generation <= fs_info -> last_trans_committed . <nl> + */ <nl> + if ( btrfs_header_generation ( eb ) > fs_info -> last_trans_committed ) <nl> + return 0 ; <nl> if ( found_level ) <nl> btrfs_node_key_to_cpu ( eb , & found_key , 0 ); <nl> else
static int fsl_diu_suspend ( struct platform_device * ofdev , pm_message_t state ) <nl> static int fsl_diu_resume ( struct platform_device * ofdev ) <nl> { <nl> struct fsl_diu_data * data ; <nl> + unsigned int i ; <nl>  <nl> data = dev_get_drvdata (& ofdev -> dev ); <nl> - enable_lcdc ( data -> fsl_diu_info ); <nl> + <nl> + fsl_diu_enable_interrupts ( data ); <nl> + update_lcdc ( data -> fsl_diu_info ); <nl> + for ( i = 0 ; i < NUM_AOIS ; i ++) { <nl> + if ( data -> mfb [ i ]. count ) <nl> + fsl_diu_enable_panel (& data -> fsl_diu_info [ i ]); <nl> + } <nl>  <nl> return 0 ; <nl> }
static void llc_cmsg_rcv ( struct msghdr * msg , struct sk_buff * skb ) <nl> if ( llc -> cmsg_flags & LLC_CMSG_PKTINFO ) { <nl> struct llc_pktinfo info ; <nl>  <nl> + memset (& info , 0 , sizeof ( info )); <nl> info . lpi_ifindex = llc_sk ( skb -> sk )-> dev -> ifindex ; <nl> llc_pdu_decode_dsap ( skb , & info . lpi_sap ); <nl> llc_pdu_decode_da ( skb , info . lpi_mac );
int btrfs_rm_device ( struct btrfs_fs_info * fs_info , const char * device_path , <nl>  <nl> cur_devices -> num_devices --; <nl> cur_devices -> total_devices --; <nl> + /* Update total_devices of the parent fs_devices if it ' s seed */ <nl> + if ( cur_devices != fs_devices ) <nl> + fs_devices -> total_devices --; <nl>  <nl> if ( test_bit ( BTRFS_DEV_STATE_MISSING , & device -> dev_state )) <nl> cur_devices -> missing_devices --;
static int apci3120_auto_attach ( struct comedi_device * dev , <nl> devpriv -> amcc = pci_resource_start ( pcidev , 0 ); <nl> devpriv -> addon = pci_resource_start ( pcidev , 2 ); <nl>  <nl> + apci3120_reset ( dev ); <nl> + <nl> if ( pcidev -> irq > 0 ) { <nl> ret = request_irq ( pcidev -> irq , apci3120_interrupt , IRQF_SHARED , <nl> dev -> board_name , dev ); <nl> static int apci3120_auto_attach ( struct comedi_device * dev , <nl> s -> insn_read = apci3120_read_insn_timer ; <nl> s -> insn_config = apci3120_config_insn_timer ; <nl>  <nl> - apci3120_reset ( dev ); <nl> return 0 ; <nl> } <nl> 
static int do_test ( int m ) <nl> speed_template_32_64 ); <nl> break ; <nl>  <nl> + case 208 : <nl> + test_cipher_speed (" ecb ( arc4 )", ENCRYPT , sec , NULL , 0 , <nl> + speed_template_8 ); <nl> + break ; <nl> + <nl> case 300 : <nl> /* fall through */ <nl>  <nl> static int do_test ( int m ) <nl> speed_template_32_48_64 ); <nl> break ; <nl>  <nl> + case 505 : <nl> + test_acipher_speed (" ecb ( arc4 )", ENCRYPT , sec , NULL , 0 , <nl> + speed_template_8 ); <nl> + break ; <nl> + <nl> case 1000 : <nl> test_available (); <nl> break ;
static int iwl_store_ucode_sec ( struct iwl_firmware_pieces * pieces , <nl>  <nl> sec -> offset = le32_to_cpu ( sec_parse -> offset ); <nl> sec -> data = sec_parse -> data ; <nl> + sec -> size = size - sizeof ( sec_parse -> offset ); <nl>  <nl> ++ img -> sec_counter ; <nl> 
static int hns_roce_v2_modify_qp ( struct ib_qp * ibqp , <nl> ( cur_state == IB_QPS_RTR && new_state == IB_QPS_ERR ) || <nl> ( cur_state == IB_QPS_RTS && new_state == IB_QPS_ERR ) || <nl> ( cur_state == IB_QPS_SQD && new_state == IB_QPS_ERR ) || <nl> - ( cur_state == IB_QPS_SQE && new_state == IB_QPS_ERR )) { <nl> + ( cur_state == IB_QPS_SQE && new_state == IB_QPS_ERR ) || <nl> + ( cur_state == IB_QPS_ERR && new_state == IB_QPS_ERR )) { <nl> /* Nothing */ <nl> ; <nl> } else {
int firmware_fallback_sysfs ( struct firmware * fw , const char * name , <nl> if (! fw_run_sysfs_fallback ( opt_flags )) <nl> return ret ; <nl>  <nl> - dev_warn ( device , " Falling back to user helper \ n "); <nl> + if (!( opt_flags & FW_OPT_NO_WARN )) <nl> + dev_warn ( device , " Falling back to syfs fallback for : % s \ n ", <nl> + name ); <nl> + else <nl> + dev_dbg ( device , " Falling back to sysfs fallback for : % s \ n ", <nl> + name ); <nl> return fw_load_from_user_helper ( fw , name , device , opt_flags ); <nl> }
static void gfx_v8_0_ring_emit_vm_flush ( struct amdgpu_ring * ring , <nl>  <nl> amdgpu_ring_write ( ring , PACKET3 ( PACKET3_WAIT_REG_MEM , 5 )); <nl> amdgpu_ring_write ( ring , ( WAIT_REG_MEM_MEM_SPACE ( 1 ) | /* memory */ <nl> - WAIT_REG_MEM_FUNCTION ( 3 ))); /* equal */ <nl> + WAIT_REG_MEM_FUNCTION ( 3 ) | /* equal */ <nl> + WAIT_REG_MEM_ENGINE ( usepfp ))); /* pfp or me */ <nl> amdgpu_ring_write ( ring , addr & 0xfffffffc ); <nl> amdgpu_ring_write ( ring , upper_32_bits ( addr ) & 0xffffffff ); <nl> amdgpu_ring_write ( ring , seq );
static void t3_io_resume ( struct pci_dev * pdev ) <nl> CH_ALERT ( adapter , " adapter recovering , PEX ERR 0x % x \ n ", <nl> t3_read_reg ( adapter , A_PCIE_PEX_ERR )); <nl>  <nl> + rtnl_lock (); <nl> t3_resume_ports ( adapter ); <nl> + rtnl_unlock (); <nl> } <nl>  <nl> static const struct pci_error_handlers t3_err_handler = {
int nfs4_proc_delegreturn ( struct inode * inode , struct rpc_cred * cred , const nfs4 <nl> static unsigned long <nl> nfs4_set_lock_task_retry ( unsigned long timeout ) <nl> { <nl> - freezable_schedule_timeout_killable_unsafe ( timeout ); <nl> + freezable_schedule_timeout_interruptible ( timeout ); <nl> timeout <<= 1 ; <nl> if ( timeout > NFS4_LOCK_MAXTIMEOUT ) <nl> return NFS4_LOCK_MAXTIMEOUT ;
static int journal_unmap_buffer ( journal_t * journal , struct buffer_head * bh ) <nl> clear_buffer_mapped ( bh ); <nl> clear_buffer_req ( bh ); <nl> clear_buffer_new ( bh ); <nl> + clear_buffer_delay ( bh ); <nl> + clear_buffer_unwritten ( bh ); <nl> bh -> b_bdev = NULL ; <nl> return may_free ; <nl> }
static int dvb_ca_ioctl ( struct file * file , unsigned int cmd , void * parg ) <nl> { <nl> ca_slot_info_t * info =( ca_slot_info_t *) parg ; <nl>  <nl> - if ( info -> num > 1 ) <nl> + if ( info -> num < 0 || info -> num > 1 ) <nl> return - EINVAL ; <nl> av7110 -> ci_slot [ info -> num ]. num = info -> num ; <nl> av7110 -> ci_slot [ info -> num ]. type = FW_CI_LL_SUPPORT ( av7110 -> arm_app ) ?
static int __init hp_wmi_input_setup ( void ) <nl> int err ; <nl>  <nl> hp_wmi_input_dev = input_allocate_device (); <nl> + if (! hp_wmi_input_dev ) <nl> + return - ENOMEM ; <nl>  <nl> hp_wmi_input_dev -> name = " HP WMI hotkeys "; <nl> hp_wmi_input_dev -> phys = " wmi / input0 ";
static int tcp_packet ( struct nf_conn * ct , <nl> nf_ct_kill_acct ( ct , ctinfo , skb ); <nl> return NF_ACCEPT ; <nl> } <nl> + /* ESTABLISHED without SEEN_REPLY , i . e . mid - connection <nl> + * pickup with loose = 1 . Avoid large ESTABLISHED timeout . <nl> + */ <nl> + if ( new_state == TCP_CONNTRACK_ESTABLISHED && <nl> + timeout > timeouts [ TCP_CONNTRACK_UNACK ]) <nl> + timeout = timeouts [ TCP_CONNTRACK_UNACK ]; <nl> } else if (! test_bit ( IPS_ASSURED_BIT , & ct -> status ) <nl> && ( old_state == TCP_CONNTRACK_SYN_RECV <nl> || old_state == TCP_CONNTRACK_ESTABLISHED )
void __init mem_init ( void ) <nl>  <nl> pci_iommu_alloc (); <nl>  <nl> - /* clear the zero - page */ <nl> - memset ( empty_zero_page , 0 , PAGE_SIZE ); <nl> + /* clear_bss () already clear the empty_zero_page */ <nl>  <nl> reservedpages = 0 ; <nl> 
int tipc_nl_publ_dump ( struct sk_buff * skb , struct netlink_callback * cb ) <nl> if ( err ) <nl> return err ; <nl>  <nl> + if (! attrs [ TIPC_NLA_SOCK ]) <nl> + return - EINVAL ; <nl> + <nl> err = nla_parse_nested ( sock , TIPC_NLA_SOCK_MAX , <nl> attrs [ TIPC_NLA_SOCK ], <nl> tipc_nl_sock_policy );
static void wdm_out_callback ( struct urb * urb ) <nl> spin_lock (& desc -> iuspin ); <nl> desc -> werr = urb -> status ; <nl> spin_unlock (& desc -> iuspin ); <nl> - clear_bit ( WDM_IN_USE , & desc -> flags ); <nl> kfree ( desc -> outbuf ); <nl> + desc -> outbuf = NULL ; <nl> + clear_bit ( WDM_IN_USE , & desc -> flags ); <nl> wake_up (& desc -> wait ); <nl> } <nl>  <nl> static ssize_t wdm_write <nl> if ( we < 0 ) <nl> return - EIO ; <nl>  <nl> - desc -> outbuf = buf = kmalloc ( count , GFP_KERNEL ); <nl> + buf = kmalloc ( count , GFP_KERNEL ); <nl> if (! buf ) { <nl> rv = - ENOMEM ; <nl> goto outnl ; <nl> static ssize_t wdm_write <nl> req -> wIndex = desc -> inum ; <nl> req -> wLength = cpu_to_le16 ( count ); <nl> set_bit ( WDM_IN_USE , & desc -> flags ); <nl> + desc -> outbuf = buf ; <nl>  <nl> rv = usb_submit_urb ( desc -> command , GFP_KERNEL ); <nl> if ( rv < 0 ) { <nl> kfree ( buf ); <nl> + desc -> outbuf = NULL ; <nl> clear_bit ( WDM_IN_USE , & desc -> flags ); <nl> dev_err (& desc -> intf -> dev , " Tx URB error : % d \ n ", rv ); <nl> } else {
static void * __alloc_from_pool ( size_t size , struct page ** ret_page , gfp_t flags ) <nl>  <nl> * ret_page = phys_to_page ( phys ); <nl> ptr = ( void *) val ; <nl> - if ( flags & __GFP_ZERO ) <nl> - memset ( ptr , 0 , size ); <nl> + memset ( ptr , 0 , size ); <nl> } <nl>  <nl> return ptr ; <nl> static void * __dma_alloc_coherent ( struct device * dev , size_t size , <nl>  <nl> * dma_handle = phys_to_dma ( dev , page_to_phys ( page )); <nl> addr = page_address ( page ); <nl> - if ( flags & __GFP_ZERO ) <nl> - memset ( addr , 0 , size ); <nl> + memset ( addr , 0 , size ); <nl> return addr ; <nl> } else { <nl> return swiotlb_alloc_coherent ( dev , size , dma_handle , flags );
struct map * map__new ( struct list_head * dsos__list , u64 start , u64 len , <nl> map -> ino = ino ; <nl> map -> ino_generation = ino_gen ; <nl>  <nl> - if ( anon ) { <nl> + if (( anon || no_dso ) && type == MAP__FUNCTION ) { <nl> snprintf ( newfilename , sizeof ( newfilename ), "/ tmp / perf -% d . map ", pid ); <nl> filename = newfilename ; <nl> } <nl> struct map * map__new ( struct list_head * dsos__list , u64 start , u64 len , <nl> * functions still return NULL , and we avoid the <nl> * unnecessary map__load warning . <nl> */ <nl> - if ( no_dso ) <nl> + if ( type != MAP__FUNCTION ) <nl> dso__set_loaded ( dso , map -> type ); <nl> } <nl> }
static int caif_seqpkt_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> if ( m -> msg_flags & MSG_OOB ) <nl> goto read_error ; <nl>  <nl> + m -> msg_namelen = 0 ; <nl> + <nl> skb = skb_recv_datagram ( sk , flags , 0 , & ret ); <nl> if (! skb ) <nl> goto read_error ;
# include < linux / seq_file . h > <nl> # include < linux / debugfs . h > <nl> # include < linux / ctype . h > <nl> +# include < linux / efi . h > <nl> # include < acpi / video . h > <nl>  <nl> /* <nl> static int __init samsung_init ( void ) <nl> struct samsung_laptop * samsung ; <nl> int ret ; <nl>  <nl> + if ( efi_enabled ( EFI_BOOT )) <nl> + return - ENODEV ; <nl> + <nl> quirks = & samsung_unknown ; <nl> if (! force && ! dmi_check_system ( samsung_dmi_table )) <nl> return - ENODEV ;
beiscsi_create_wrb_rings ( struct beiscsi_hba * phba , <nl> if ( status != 0 ) { <nl> shost_printk ( KERN_ERR , phba -> shost , <nl> " wrbq create failed ."); <nl> + kfree ( pwrb_arr ); <nl> return status ; <nl> } <nl> phwi_ctrlr -> wrb_context [ i * 2 ]. cid = phwi_context -> be_wrbq [ i ].
int nvdimm_namespace_attach_btt ( struct nd_namespace_common * ndns ) <nl> } <nl>  <nl> btt_sb = devm_kzalloc (& nd_btt -> dev , sizeof (* btt_sb ), GFP_KERNEL ); <nl> + if (! btt_sb ) <nl> + return - ENOMEM ; <nl>  <nl> /* <nl> * If this returns < 0 , that is ok as it just means there wasn ' t
static int fsmc_nand_probe_config_dt ( struct platform_device * pdev , <nl> { <nl> struct fsmc_nand_platform_data * pdata = dev_get_platdata (& pdev -> dev ); <nl> u32 val ; <nl> + int ret ; <nl>  <nl> /* Set default NAND width to 8 bits */ <nl> pdata -> width = 8 ; <nl> static int fsmc_nand_probe_config_dt ( struct platform_device * pdev , <nl> sizeof (* pdata -> nand_timings ), GFP_KERNEL ); <nl> if (! pdata -> nand_timings ) <nl> return - ENOMEM ; <nl> - of_property_read_u8_array ( np , " timings ", ( u8 *) pdata -> nand_timings , <nl> + ret = of_property_read_u8_array ( np , " timings ", ( u8 *) pdata -> nand_timings , <nl> sizeof (* pdata -> nand_timings )); <nl> + if ( ret ) { <nl> + dev_info (& pdev -> dev , " No timings in dts specified , using default timings !\ n "); <nl> + pdata -> nand_timings = NULL ; <nl> + } <nl>  <nl> /* Set default NAND bank to 0 */ <nl> pdata -> bank = 0 ;
static int pwm_setup_backlight ( struct intel_connector * connector , <nl> return - ENODEV ; <nl> } <nl>  <nl> + /* <nl> + * FIXME : pwm_apply_args () should be removed when switching to <nl> + * the atomic PWM API . <nl> + */ <nl> + pwm_apply_args ( panel -> backlight . pwm ); <nl> + <nl> retval = pwm_config ( panel -> backlight . pwm , CRC_PMIC_PWM_PERIOD_NS , <nl> CRC_PMIC_PWM_PERIOD_NS ); <nl> if ( retval < 0 ) {
static int threshold_create_bank ( unsigned int cpu , unsigned int bank ) <nl> const char * name = get_name ( bank , NULL ); <nl> int err = 0 ; <nl>  <nl> + if (! dev ) <nl> + return - ENODEV ; <nl> + <nl> if ( is_shared_bank ( bank )) { <nl> nb = node_to_amd_nb ( amd_get_nb_id ( cpu )); <nl> 
static ssize_t ath10k_write_pktlog_filter ( struct file * file , <nl> goto out ; <nl> } <nl>  <nl> - if ( filter && ( filter != ar -> debug . pktlog_filter )) { <nl> + if ( filter == ar -> debug . pktlog_filter ) { <nl> + ret = count ; <nl> + goto out ; <nl> + } <nl> + <nl> + if ( filter ) { <nl> ret = ath10k_wmi_pdev_pktlog_enable ( ar , filter ); <nl> if ( ret ) { <nl> ath10k_warn ( ar , " failed to enable pktlog filter % x : % d \ n ",
static void uart_set_termios ( struct tty_struct * tty , struct ktermios * old_termio <nl>  <nl> /* <nl> * These are the bits that are used to setup various <nl> - * flags in the low level driver . <nl> + * flags in the low level driver . We can ignore the Bfoo <nl> + * bits in c_cflag ; c_ [ io ] speed will always be set <nl> + * appropriately by set_termios () in tty_ioctl . c <nl> */ <nl> # define RELEVANT_IFLAG ( iflag ) (( iflag ) & ( IGNBRK | BRKINT | IGNPAR | PARMRK | INPCK )) <nl> - <nl> if (( cflag ^ old_termios -> c_cflag ) == 0 && <nl> + tty -> termios -> c_ospeed == old_termios -> c_ospeed && <nl> + tty -> termios -> c_ispeed == old_termios -> c_ispeed && <nl> RELEVANT_IFLAG ( tty -> termios -> c_iflag ^ old_termios -> c_iflag ) == 0 ) <nl> return ; <nl> 
static int __devinit pmic8xxx_pwrkey_probe ( struct platform_device * pdev ) <nl> unsigned int delay ; <nl> u8 pon_cntl ; <nl> struct pmic8xxx_pwrkey * pwrkey ; <nl> - const struct pm8xxx_pwrkey_platform_data * pdata = mfd_get_data ( pdev ); <nl> + const struct pm8xxx_pwrkey_platform_data * pdata = <nl> + dev_get_platdata (& pdev -> dev ); <nl>  <nl> if (! pdata ) { <nl> dev_err (& pdev -> dev , " power key platform data not supplied \ n ");
static int list_devices ( struct file * filp , struct dm_ioctl * param , size_t param_ <nl> * Grab our output buffer . <nl> */ <nl> nl = orig_nl = get_result_buffer ( param , param_size , & len ); <nl> - if ( len < needed ) { <nl> + if ( len < needed || len < sizeof ( nl -> dev )) { <nl> param -> flags |= DM_BUFFER_FULL_FLAG ; <nl> goto out ; <nl> }
SMB2_read ( const unsigned int xid , struct cifs_io_parms * io_parms , <nl> rqst . rq_nvec = 1 ; <nl>  <nl> rc = cifs_send_recv ( xid , ses , & rqst , & resp_buftype , flags , & rsp_iov ); <nl> - cifs_small_buf_release ( req ); <nl> - <nl> rsp = ( struct smb2_read_rsp *) rsp_iov . iov_base ; <nl>  <nl> if ( rc ) { <nl> SMB2_read ( const unsigned int xid , struct cifs_io_parms * io_parms , <nl> io_parms -> tcon -> tid , ses -> Suid , <nl> io_parms -> offset , io_parms -> length ); <nl>  <nl> + cifs_small_buf_release ( req ); <nl> + <nl> * nbytes = le32_to_cpu ( rsp -> DataLength ); <nl> if ((* nbytes > CIFS_MAX_MSGSIZE ) || <nl> (* nbytes > io_parms -> length )) {
static void __init early_vmalloc ( char ** arg ) <nl> " vmalloc area too small , limiting to % luMB \ n ", <nl> vmalloc_reserve >> 20 ); <nl> } <nl> + <nl> + if ( vmalloc_reserve > VMALLOC_END - ( PAGE_OFFSET + SZ_32M )) { <nl> + vmalloc_reserve = VMALLOC_END - ( PAGE_OFFSET + SZ_32M ); <nl> + printk ( KERN_WARNING <nl> + " vmalloc area is too big , limiting to % luMB \ n ", <nl> + vmalloc_reserve >> 20 ); <nl> + } <nl> } <nl> __early_param (" vmalloc =", early_vmalloc ); <nl> 
static int ath10k_sdio_mbox_rxmsg_pending_handler ( struct ath10k * ar , <nl> lookaheads [ 0 ] = msg_lookahead ; <nl>  <nl> timeout = jiffies + SDIO_MBOX_PROCESSING_TIMEOUT_HZ ; <nl> - while ( time_before ( jiffies , timeout )) { <nl> + do { <nl> /* Try to allocate as many HTC RX packets indicated by <nl> * n_lookaheads . <nl> */ <nl> static int ath10k_sdio_mbox_rxmsg_pending_handler ( struct ath10k * ar , <nl> * performance in high throughput situations . <nl> */ <nl> * done = false ; <nl> - } <nl> + } while ( time_before ( jiffies , timeout )); <nl>  <nl> if ( ret && ( ret != - ECANCELED )) <nl> ath10k_warn ( ar , " failed to get pending recv messages : % d \ n ", <nl> static void ath10k_sdio_irq_handler ( struct sdio_func * func ) <nl> sdio_release_host ( ar_sdio -> func ); <nl>  <nl> timeout = jiffies + ATH10K_SDIO_HIF_COMMUNICATION_TIMEOUT_HZ ; <nl> - while ( time_before ( jiffies , timeout ) && ! done ) { <nl> + do { <nl> ret = ath10k_sdio_mbox_proc_pending_irqs ( ar , & done ); <nl> if ( ret ) <nl> break ; <nl> - } <nl> + } while ( time_before ( jiffies , timeout ) && ! done ); <nl>  <nl> sdio_claim_host ( ar_sdio -> func ); <nl> 
static int dump_holder ( struct seq_file * seq , const struct gfs2_holder * gh ) <nl> struct task_struct * gh_owner = NULL ; <nl> char flags_buf [ 32 ]; <nl>  <nl> + rcu_read_lock (); <nl> if ( gh -> gh_owner_pid ) <nl> gh_owner = pid_task ( gh -> gh_owner_pid , PIDTYPE_PID ); <nl> gfs2_print_dbg ( seq , " H : s :% s f :% s e :% d p :% ld [% s ] % pS \ n ", <nl> static int dump_holder ( struct seq_file * seq , const struct gfs2_holder * gh ) <nl> gh -> gh_owner_pid ? ( long ) pid_nr ( gh -> gh_owner_pid ) : - 1 , <nl> gh_owner ? gh_owner -> comm : "( ended )", <nl> ( void *) gh -> gh_ip ); <nl> + rcu_read_unlock (); <nl> return 0 ; <nl> } <nl> 
static void solo_enc_free ( struct solo_enc_dev * solo_enc ) <nl> if ( solo_enc == NULL ) <nl> return ; <nl>  <nl> + pci_free_consistent ( solo_enc -> solo_dev -> pdev , <nl> + sizeof ( struct solo_p2m_desc ) * solo_enc -> desc_nelts , <nl> + solo_enc -> desc_items , solo_enc -> desc_dma ); <nl> video_unregister_device ( solo_enc -> vfd ); <nl> v4l2_ctrl_handler_free (& solo_enc -> hdl ); <nl> kfree ( solo_enc );
static int s3c24xx_eint_init ( struct samsung_pinctrl_drv_data * d ) <nl> irq = bank -> eint_offset ; <nl> mask = bank -> eint_mask ; <nl> for ( pin = 0 ; mask ; ++ pin , mask >>= 1 ) { <nl> - if ( irq > NUM_EINT ) <nl> + if ( irq >= NUM_EINT ) <nl> break ; <nl> if (!( mask & 1 )) <nl> continue ;
xfs_qm_freelist_destroy ( xfs_frlist_t * ql ) <nl> xfs_qm_dqdestroy ( dqp ); <nl> dqp = nextdqp ; <nl> } <nl> - /* <nl> - * Don ' t bother about unlocking . <nl> - */ <nl> + mutex_unlock (& ql -> qh_lock ); <nl> mutex_destroy (& ql -> qh_lock ); <nl>  <nl> ASSERT ( ql -> qh_nelems == 0 );
static int find_fsync_dnodes ( struct f2fs_sb_info * sbi , struct list_head * head ) <nl> if ( IS_ERR ( entry -> inode )) { <nl> err = PTR_ERR ( entry -> inode ); <nl> kmem_cache_free ( fsync_entry_slab , entry ); <nl> - if ( err == - ENOENT ) <nl> + if ( err == - ENOENT ) { <nl> + err = 0 ; <nl> goto next ; <nl> + } <nl> break ; <nl> } <nl> list_add_tail (& entry -> list , head );
static int watchdog_release ( struct inode * inode , struct file * file ) <nl> struct watchdog_core_data * wd_data = file -> private_data ; <nl> struct watchdog_device * wdd ; <nl> int err = - EBUSY ; <nl> + bool running ; <nl>  <nl> mutex_lock (& wd_data -> lock ); <nl>  <nl> static int watchdog_release ( struct inode * inode , struct file * file ) <nl> clear_bit ( _WDOG_DEV_OPEN , & wd_data -> status ); <nl>  <nl> done : <nl> + running = wdd && watchdog_hw_running ( wdd ); <nl> mutex_unlock (& wd_data -> lock ); <nl> /* <nl> * Allow the owner module to be unloaded again unless the watchdog <nl> * is still running . If the watchdog is still running , it can not <nl> * be stopped , and its driver must not be unloaded . <nl> */ <nl> - if (! watchdog_hw_running ( wdd )) { <nl> - module_put ( wdd -> ops -> owner ); <nl> + if (! running ) { <nl> + module_put ( wd_data -> cdev . owner ); <nl> kref_put (& wd_data -> kref , watchdog_core_data_release ); <nl> } <nl> return 0 ;
brcmf_notify_sched_scan_results ( struct brcmf_if * ifp , <nl> } <nl>  <nl> set_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status ); <nl> + cfg -> escan_info . run = brcmf_run_escan ; <nl> err = brcmf_do_escan ( cfg , wiphy , ifp , request ); <nl> if ( err ) { <nl> clear_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status );
static bool tg_may_dispatch ( struct throtl_grp * tg , struct bio * bio , <nl> /* <nl> * If previous slice expired , start a new one otherwise renew / extend <nl> * existing slice to make sure it is at least throtl_slice interval <nl> - * long since now . <nl> + * long since now . New slice is started only for empty throttle group . <nl> + * If there is queued bio , that means there should be an active <nl> + * slice and it should be extended instead . <nl> */ <nl> - if ( throtl_slice_used ( tg , rw )) <nl> + if ( throtl_slice_used ( tg , rw ) && !( tg -> service_queue . nr_queued [ rw ])) <nl> throtl_start_new_slice ( tg , rw ); <nl> else { <nl> if ( time_before ( tg -> slice_end [ rw ], jiffies + throtl_slice ))
void __init setup_arch ( char ** cmdline_p ) <nl> init_mm . brk = ( unsigned long ) & _end ; <nl>  <nl> * cmdline_p = m68k_command_line ; <nl> - memcpy ( saved_command_line , * cmdline_p , CL_SIZE ); <nl> + memcpy ( boot_command_line , * cmdline_p , CL_SIZE ); <nl>  <nl> /* Parse the command line for arch - specific options . <nl> * For the m68k , this is currently only " debug = xxx " to enable printing
static void card_cd_debounce ( struct rts51x_chip * chip , u8 * need_reset , <nl> release_map |= MS_CARD ; <nl> } <nl> } else { <nl> - if ( chip -> card_status & XD_CD ) { <nl> + if ( chip -> card_status & XD_CD ) <nl> reset_map |= XD_CARD ; <nl> - } else if ( chip -> card_status & SD_CD ) { <nl> + else if ( chip -> card_status & SD_CD ) <nl> reset_map |= SD_CARD ; <nl> - } else if ( chip -> card_status & MS_CD ) { <nl> + else if ( chip -> card_status & MS_CD ) <nl> reset_map |= MS_CARD ; <nl> - } <nl> } <nl>  <nl> if ( CHECK_PKG ( chip , QFN24 ) && reset_map ) {
static int vidioc_streamoff ( struct file * file , void * priv , <nl> if ( rc < 0 ) <nl> return rc ; <nl>  <nl> - if (( fh -> type != V4L2_BUF_TYPE_VIDEO_CAPTURE ) || <nl> + if (( fh -> type != V4L2_BUF_TYPE_VIDEO_CAPTURE ) && <nl> ( fh -> type != V4L2_BUF_TYPE_VBI_CAPTURE )) <nl> return - EINVAL ; <nl> if ( type != fh -> type )
static void slic_card_cleanup ( struct sliccard * card ) <nl> { <nl> if ( card -> loadtimerset ) { <nl> card -> loadtimerset = 0 ; <nl> - del_timer (& card -> loadtimer ); <nl> + del_timer_sync (& card -> loadtimer ); <nl> } <nl>  <nl> slic_debug_card_destroy ( card );
vc4_get_bcl ( struct drm_device * dev , struct vc4_exec_info * exec ) <nl> args -> shader_rec_count ); <nl> struct vc4_bo * bo ; <nl>  <nl> - if ( uniforms_offset < shader_rec_offset || <nl> + if ( shader_rec_offset < args -> bin_cl_size || <nl> + uniforms_offset < shader_rec_offset || <nl> exec_size < uniforms_offset || <nl> args -> shader_rec_count >= ( UINT_MAX / <nl> sizeof ( struct vc4_shader_state )) ||
static const struct sectioncheck * section_mismatch ( <nl> int elems = sizeof ( sectioncheck ) / sizeof ( struct sectioncheck ); <nl> const struct sectioncheck * check = & sectioncheck [ 0 ]; <nl>  <nl> + /* <nl> + * The target section could be the SHT_NUL section when we ' re <nl> + * handling relocations to un - resolved symbols , trying to match it <nl> + * doesn ' t make much sense and causes build failures on parisc and <nl> + * mn10300 architectures . <nl> + */ <nl> + if (* tosec == '\ 0 ') <nl> + return NULL ; <nl> + <nl> for ( i = 0 ; i < elems ; i ++) { <nl> if ( match ( fromsec , check -> fromsec )) { <nl> if ( check -> bad_tosec [ 0 ] && match ( tosec , check -> bad_tosec ))
i915_gem_detect_bit_6_swizzle ( struct drm_device * dev ) <nl> } <nl> } <nl>  <nl> + /* FIXME : check with memory config on IGDNG */ <nl> + if ( IS_IGDNG ( dev )) { <nl> + DRM_ERROR (" disable tiling on IGDNG ...\ n "); <nl> + swizzle_x = I915_BIT_6_SWIZZLE_UNKNOWN ; <nl> + swizzle_y = I915_BIT_6_SWIZZLE_UNKNOWN ; <nl> + } <nl> + <nl> dev_priv -> mm . bit_6_swizzle_x = swizzle_x ; <nl> dev_priv -> mm . bit_6_swizzle_y = swizzle_y ; <nl> }
static void set_type ( struct i2c_client * c , unsigned int type , <nl> buffer [ 3 ] = 0xa4 ; <nl> i2c_master_send ( c , buffer , 4 ); <nl> default_tuner_init ( c ); <nl> + break ; <nl> default : <nl> default_tuner_init ( c ); <nl> break ;
static void atiixp_set_dmamode ( struct ata_port * ap , struct ata_device * adev ) <nl> * We must now look at the PIO mode situation . We may need to <nl> * adjust the PIO mode to keep the timings acceptable <nl> */ <nl> - if ( adev -> dma_mode >= XFER_MW_DMA_2 ) <nl> - wanted_pio = 4 ; <nl> + if ( adev -> dma_mode >= XFER_MW_DMA_2 ) <nl> + wanted_pio = 4 ; <nl> else if ( adev -> dma_mode == XFER_MW_DMA_1 ) <nl> wanted_pio = 3 ; <nl> else if ( adev -> dma_mode == XFER_MW_DMA_0 )
static int misc_open ( struct inode * inode , struct file * file ) <nl> old_fops = file -> f_op ; <nl> file -> f_op = new_fops ; <nl> if ( file -> f_op -> open ) { <nl> + file -> private_data = c ; <nl> err = file -> f_op -> open ( inode , file ); <nl> if ( err ) { <nl> fops_put ( file -> f_op );
int sr_do_ioctl ( Scsi_CD * cd , struct packet_command * cgc ) <nl> struct scsi_device * SDev ; <nl> struct scsi_sense_hdr sshdr ; <nl> int result , err = 0 , retries = 0 ; <nl> + unsigned char sense_buffer [ SCSI_SENSE_BUFFERSIZE ], * senseptr = NULL ; <nl>  <nl> SDev = cd -> device ; <nl>  <nl> + if ( cgc -> sense ) <nl> + senseptr = sense_buffer ; <nl> + <nl> retry : <nl> if (! scsi_block_when_processing_errors ( SDev )) { <nl> err = - ENODEV ; <nl> int sr_do_ioctl ( Scsi_CD * cd , struct packet_command * cgc ) <nl> } <nl>  <nl> result = scsi_execute ( SDev , cgc -> cmd , cgc -> data_direction , <nl> - cgc -> buffer , cgc -> buflen , <nl> - ( unsigned char *) cgc -> sense , & sshdr , <nl> + cgc -> buffer , cgc -> buflen , senseptr , & sshdr , <nl> cgc -> timeout , IOCTL_RETRIES , 0 , 0 , NULL ); <nl>  <nl> + if ( cgc -> sense ) <nl> + memcpy ( cgc -> sense , sense_buffer , sizeof (* cgc -> sense )); <nl> + <nl> /* Minimal error checking . Ignore cases we know about , and report the rest . */ <nl> if ( driver_byte ( result ) != 0 ) { <nl> switch ( sshdr . sense_key ) {
static int tower_probe ( struct usb_interface * interface , const struct usb_device <nl> USB_MAJOR , dev -> minor ); <nl>  <nl> exit : <nl> + kfree ( get_version_reply ); <nl> return retval ; <nl>  <nl> error :
int drm_atomic_check_only ( struct drm_atomic_state * state ) <nl> } <nl> } <nl>  <nl> - if ( config -> funcs -> atomic_check ) <nl> + if ( config -> funcs -> atomic_check ) { <nl> ret = config -> funcs -> atomic_check ( state -> dev , state ); <nl>  <nl> - if ( ret ) <nl> - return ret ; <nl> + if ( ret ) { <nl> + DRM_DEBUG_ATOMIC (" atomic driver check for % p failed : % d \ n ", <nl> + state , ret ); <nl> + return ret ; <nl> + } <nl> + } <nl>  <nl> if (! state -> allow_modeset ) { <nl> for_each_new_crtc_in_state ( state , crtc , crtc_state , i ) {
static int ca8210_probe ( struct spi_device * spi_device ) <nl> goto error ; <nl> } <nl>  <nl> + priv -> spi -> dev . platform_data = pdata ; <nl> ret = ca8210_get_platform_data ( priv -> spi , pdata ); <nl> if ( ret ) { <nl> dev_crit (& spi_device -> dev , " ca8210_get_platform_data failed \ n "); <nl> goto error ; <nl> } <nl> - priv -> spi -> dev . platform_data = pdata ; <nl>  <nl> ret = ca8210_dev_com_init ( priv ); <nl> if ( ret ) {
static void iwl_mvm_power_build_cmd ( struct iwl_mvm * mvm , <nl> cmd -> flags |= cpu_to_le16 ( POWER_FLAGS_POWER_SAVE_ENA_MSK ); <nl>  <nl> if (! vif -> bss_conf . ps || iwl_mvm_vif_low_latency ( mvmvif ) || <nl> - ! mvmvif -> pm_enabled || iwl_mvm_tdls_sta_count ( mvm , vif )) <nl> + ! mvmvif -> pm_enabled ) <nl> return ; <nl>  <nl> cmd -> flags |= cpu_to_le16 ( POWER_FLAGS_POWER_MANAGEMENT_ENA_MSK ); <nl> static void iwl_mvm_power_set_pm ( struct iwl_mvm * mvm , <nl> if ( vifs -> ap_vif ) <nl> ap_mvmvif = iwl_mvm_vif_from_mac80211 ( vifs -> ap_vif ); <nl>  <nl> + /* don ' t allow PM if any TDLS stations exist */ <nl> + if ( iwl_mvm_tdls_sta_count ( mvm , NULL )) <nl> + return ; <nl> + <nl> /* enable PM on bss if bss stand alone */ <nl> if ( vifs -> bss_active && ! vifs -> p2p_active && ! vifs -> ap_active ) { <nl> bss_mvmvif -> pm_enabled = true ;
device_create_groups_vargs ( struct class * class , struct device * parent , <nl> goto error ; <nl> } <nl>  <nl> + device_initialize ( dev ); <nl> dev -> devt = devt ; <nl> dev -> class = class ; <nl> dev -> parent = parent ; <nl> device_create_groups_vargs ( struct class * class , struct device * parent , <nl> if ( retval ) <nl> goto error ; <nl>  <nl> - retval = device_register ( dev ); <nl> + retval = device_add ( dev ); <nl> if ( retval ) <nl> goto error ; <nl> 
# define __NR_mknodat ( __NR_Linux + 290 ) <nl> # define __NR_fchownat ( __NR_Linux + 291 ) <nl> # define __NR_futimesat ( __NR_Linux + 292 ) <nl> -# define __NR_fstatat ( __NR_Linux + 293 ) <nl> +# define __NR_fstatat64 ( __NR_Linux + 293 ) <nl> # define __NR_unlinkat ( __NR_Linux + 294 ) <nl> # define __NR_renameat ( __NR_Linux + 295 ) <nl> # define __NR_linkat ( __NR_Linux + 296 ) <nl> # define __NR_mknodat ( __NR_Linux + 249 ) <nl> # define __NR_fchownat ( __NR_Linux + 250 ) <nl> # define __NR_futimesat ( __NR_Linux + 251 ) <nl> -# define __NR_fstatat ( __NR_Linux + 252 ) <nl> +# define __NR_newfstatat ( __NR_Linux + 252 ) <nl> # define __NR_unlinkat ( __NR_Linux + 253 ) <nl> # define __NR_renameat ( __NR_Linux + 254 ) <nl> # define __NR_linkat ( __NR_Linux + 255 ) <nl> # define __NR_mknodat ( __NR_Linux + 253 ) <nl> # define __NR_fchownat ( __NR_Linux + 254 ) <nl> # define __NR_futimesat ( __NR_Linux + 255 ) <nl> -# define __NR_fstatat ( __NR_Linux + 256 ) <nl> +# define __NR_newfstatat ( __NR_Linux + 256 ) <nl> # define __NR_unlinkat ( __NR_Linux + 257 ) <nl> # define __NR_renameat ( __NR_Linux + 258 ) <nl> # define __NR_linkat ( __NR_Linux + 259 )
static void net_free ( struct net * net ) <nl> return ; <nl> } <nl> # endif <nl> - <nl> + kfree ( net -> gen ); <nl> kmem_cache_free ( net_cachep , net ); <nl> } <nl> 
static int cx24116_send_diseqc_msg ( struct dvb_frontend * fe , <nl> struct cx24116_state * state = fe -> demodulator_priv ; <nl> int i , ret ; <nl>  <nl> + /* Validate length */ <nl> + if ( d -> msg_len > sizeof ( d -> msg )) <nl> + return - EINVAL ; <nl> + <nl> /* Dump DiSEqC message */ <nl> if ( debug ) { <nl> printk ( KERN_INFO " cx24116 : % s (", __func__ ); <nl> static int cx24116_send_diseqc_msg ( struct dvb_frontend * fe , <nl> printk (") toneburst =% d \ n ", toneburst ); <nl> } <nl>  <nl> - /* Validate length */ <nl> - if ( d -> msg_len > ( CX24116_ARGLEN - CX24116_DISEQC_MSGOFS )) <nl> - return - EINVAL ; <nl> - <nl> /* DiSEqC message */ <nl> for ( i = 0 ; i < d -> msg_len ; i ++) <nl> state -> dsec_cmd . args [ CX24116_DISEQC_MSGOFS + i ] = d -> msg [ i ];
static const struct snd_pci_quirk alc662_fixup_tbl [] = { <nl> SND_PCI_QUIRK ( 0x1025 , 0x038b , " Acer Aspire 8943G ", ALC662_FIXUP_ASPIRE ), <nl> SND_PCI_QUIRK ( 0x1028 , 0x05d8 , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> SND_PCI_QUIRK ( 0x1028 , 0x05db , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> + SND_PCI_QUIRK ( 0x1028 , 0x0626 , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> SND_PCI_QUIRK ( 0x103c , 0x1632 , " HP RP5800 ", ALC662_FIXUP_HP_RP5800 ), <nl> SND_PCI_QUIRK ( 0x1043 , 0x1477 , " ASUS N56VZ ", ALC662_FIXUP_BASS_CHMAP ), <nl> SND_PCI_QUIRK ( 0x1043 , 0x1bf3 , " ASUS N76VZ ", ALC662_FIXUP_BASS_CHMAP ),
void lguest_arch_run_guest ( struct lg_cpu * cpu ) <nl> * we set it now , so we can trap and pass that trap to the Guest if it <nl> * uses the FPU . <nl> */ <nl> - if ( cpu -> ts ) <nl> - unlazy_fpu ( current ); <nl> + if ( cpu -> ts && user_has_fpu ()) <nl> + stts (); <nl>  <nl> /* <nl> * SYSENTER is an optimized way of doing system calls . We can ' t allow <nl> void lguest_arch_run_guest ( struct lg_cpu * cpu ) <nl> if ( boot_cpu_has ( X86_FEATURE_SEP )) <nl> wrmsr ( MSR_IA32_SYSENTER_CS , __KERNEL_CS , 0 ); <nl>  <nl> + /* Clear the host TS bit if it was set above . */ <nl> + if ( cpu -> ts && user_has_fpu ()) <nl> + clts (); <nl> + <nl> /* <nl> * If the Guest page faulted , then the cr2 register will tell us the <nl> * bad virtual address . We have to grab this now , because once we <nl> void lguest_arch_run_guest ( struct lg_cpu * cpu ) <nl> * a different CPU . So all the critical stuff should be done <nl> * before this . <nl> */ <nl> - else if ( cpu -> regs -> trapnum == 7 ) <nl> + else if ( cpu -> regs -> trapnum == 7 && ! user_has_fpu ()) <nl> math_state_restore (); <nl> } <nl> 
static bool new_idmap_permitted ( const struct file * file , <nl> u32 id = new_map -> extent [ 0 ]. lower_first ; <nl> if ( cap_setid == CAP_SETUID ) { <nl> kuid_t uid = make_kuid ( ns -> parent , id ); <nl> - if ( uid_eq ( uid , current_fsuid ())) <nl> + if ( uid_eq ( uid , file -> f_cred -> fsuid )) <nl> return true ; <nl> } <nl> else if ( cap_setid == CAP_SETGID ) { <nl> kgid_t gid = make_kgid ( ns -> parent , id ); <nl> - if ( gid_eq ( gid , current_fsgid ())) <nl> + if ( gid_eq ( gid , file -> f_cred -> fsgid )) <nl> return true ; <nl> } <nl> }
struct btrfs_root * open_ctree ( struct super_block * sb , <nl> kfree ( tree_root ); <nl> bdi_destroy (& fs_info -> bdi ); <nl> kfree ( fs_info ); <nl> + kfree ( chunk_root ); <nl> + kfree ( dev_root ); <nl> return ERR_PTR ( err ); <nl> } <nl> 
static int sctp_wait_for_sndbuf ( struct sctp_association * asoc , long * timeo_p , <nl> */ <nl> release_sock ( sk ); <nl> current_timeo = schedule_timeout ( current_timeo ); <nl> - BUG_ON ( sk != asoc -> base . sk ); <nl> + if ( sk != asoc -> base . sk ) <nl> + goto do_error ; <nl> lock_sock ( sk ); <nl>  <nl> * timeo_p = current_timeo ;
struct se_portal_group * tcm_loop_make_naa_tpg ( <nl> tpgt_str += 5 ; /* Skip ahead of " tpgt_ " */ <nl> tpgt = ( unsigned short int ) simple_strtoul ( tpgt_str , & end_ptr , 0 ); <nl>  <nl> - if ( tpgt > TL_TPGS_PER_HBA ) { <nl> + if ( tpgt >= TL_TPGS_PER_HBA ) { <nl> printk ( KERN_ERR " Passed tpgt : % hu exceeds TL_TPGS_PER_HBA :" <nl> " % u \ n ", tpgt , TL_TPGS_PER_HBA ); <nl> return ERR_PTR (- EINVAL );
int v4l2_async_notifier_register ( struct v4l2_device * v4l2_dev , <nl> struct v4l2_async_subdev * asd ; <nl> int i ; <nl>  <nl> - if (! notifier -> num_subdevs || notifier -> num_subdevs > V4L2_MAX_SUBDEVS ) <nl> + if (! v4l2_dev || ! notifier -> num_subdevs || <nl> + notifier -> num_subdevs > V4L2_MAX_SUBDEVS ) <nl> return - EINVAL ; <nl>  <nl> notifier -> v4l2_dev = v4l2_dev ;
static netdev_tx_t geneve_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl>  <nl> static int geneve_change_mtu ( struct net_device * dev , int new_mtu ) <nl> { <nl> - /* Only possible if called internally , ndo_change_mtu path ' s new_mtu <nl> - * is guaranteed to be between dev -> min_mtu and dev -> max_mtu . <nl> - */ <nl> if ( new_mtu > dev -> max_mtu ) <nl> new_mtu = dev -> max_mtu ; <nl> + else if ( new_mtu < dev -> min_mtu ) <nl> + new_mtu = dev -> min_mtu ; <nl>  <nl> dev -> mtu = new_mtu ; <nl> return 0 ;
static inline int access_ok ( int type , const void __user * addr , <nl> if (( get_fs (). seg < (( unsigned long ) addr )) || <nl> ( get_fs (). seg < (( unsigned long ) addr + size - 1 ))) { <nl> pr_debug (" ACCESS fail : % s at 0x % 08x ( size 0x % x ), seg 0x % 08x \ n ", <nl> - type ? " WRITE " : " READ ", ( u32 ) addr , ( u32 ) size , <nl> + type ? " WRITE " : " READ ", ( __force u32 ) addr , ( u32 ) size , <nl> ( u32 ) get_fs (). seg ); <nl> return 0 ; <nl> } <nl> ok : <nl> pr_debug (" ACCESS OK : % s at 0x % 08x ( size 0x % x ), seg 0x % 08x \ n ", <nl> - type ? " WRITE " : " READ ", ( u32 ) addr , ( u32 ) size , <nl> + type ? " WRITE " : " READ ", ( __force u32 ) addr , ( u32 ) size , <nl> ( u32 ) get_fs (). seg ); <nl> return 1 ; <nl> }
static void kick_requests ( struct ceph_osd_client * osdc , int force_resend ) <nl> __register_request ( osdc , req ); <nl> __unregister_linger_request ( osdc , req ); <nl> } <nl> + reset_changed_osds ( osdc ); <nl> mutex_unlock (& osdc -> request_mutex ); <nl>  <nl> if ( needmap ) { <nl> dout ("% d requests for down osds , need new map \ n ", needmap ); <nl> ceph_monc_request_next_osdmap (& osdc -> client -> monc ); <nl> } <nl> - reset_changed_osds ( osdc ); <nl> } <nl>  <nl> 
static int mlx5_load_one ( struct mlx5_core_dev * dev , struct mlx5_priv * priv , <nl> dev_info (& pdev -> dev , " firmware version : % d .% d .% d \ n ", fw_rev_maj ( dev ), <nl> fw_rev_min ( dev ), fw_rev_sub ( dev )); <nl>  <nl> + /* Only PFs hold the relevant PCIe information for this query */ <nl> + if ( mlx5_core_is_pf ( dev )) <nl> + pcie_print_link_status ( dev -> pdev ); <nl> + <nl> /* on load removing any previous indication of internal error , device is <nl> * up <nl> */
static int tipc_l2_device_event ( struct notifier_block * nb , unsigned long evt , <nl> break ; <nl> case NETDEV_UNREGISTER : <nl> case NETDEV_CHANGENAME : <nl> - bearer_disable ( dev_net ( dev ), b ); <nl> + bearer_disable ( net , b ); <nl> break ; <nl> } <nl> return NOTIFY_OK ;
static int parse_redboot_partitions ( struct mtd_info * master , <nl>  <nl> if ( directory < 0 ) { <nl> offset = master -> size + directory * master -> erasesize ; <nl> - while ( mtd_can_have_bb ( master ) && <nl> - mtd_block_isbad ( master , offset )) { <nl> + while ( mtd_block_isbad ( master , offset )) { <nl> if (! offset ) { <nl> nogood : <nl> printk ( KERN_NOTICE " Failed to find a non - bad block to check for RedBoot partition table \ n "); <nl> static int parse_redboot_partitions ( struct mtd_info * master , <nl> } <nl> } else { <nl> offset = directory * master -> erasesize ; <nl> - while ( mtd_can_have_bb ( master ) && <nl> - mtd_block_isbad ( master , offset )) { <nl> + while ( mtd_block_isbad ( master , offset )) { <nl> offset += master -> erasesize ; <nl> if ( offset == master -> size ) <nl> goto nogood ;
nf_nat_redirect_ipv4 ( struct sk_buff * skb , <nl>  <nl> rcu_read_lock (); <nl> indev = __in_dev_get_rcu ( skb -> dev ); <nl> - if ( indev != NULL ) { <nl> + if ( indev && indev -> ifa_list ) { <nl> ifa = indev -> ifa_list ; <nl> newdst = ifa -> ifa_local ; <nl> }
static void hvc_close ( struct tty_struct * tty , struct file * filp ) <nl> hp = tty -> driver_data ; <nl>  <nl> spin_lock_irqsave (& hp -> lock , flags ); <nl> - tty_kref_get ( tty ); <nl>  <nl> if (-- hp -> count == 0 ) { <nl> /* We are done with the tty pointer now . */ <nl> hp -> tty = NULL ; <nl> spin_unlock_irqrestore (& hp -> lock , flags ); <nl>  <nl> - /* Put the ref obtained in hvc_open () */ <nl> - tty_kref_put ( tty ); <nl> - <nl> if ( hp -> ops -> notifier_del ) <nl> hp -> ops -> notifier_del ( hp , hp -> data ); <nl> 
static char irq_user_affinity [ NR_IRQS ]; <nl>  <nl> int irq_select_affinity ( unsigned int irq ) <nl> { <nl> - struct irq_desc * desc = irq_to_desc [ irq ]; <nl> + struct irq_desc * desc = irq_to_desc ( irq ); <nl> static int last_cpu ; <nl> int cpu = last_cpu + 1 ; <nl> 
int usb_add_gadget_udc_release ( struct device * parent , struct usb_gadget * gadget , <nl> if ( ret != - EPROBE_DEFER ) <nl> list_del (& driver -> pending ); <nl> if ( ret ) <nl> - goto err4 ; <nl> + goto err5 ; <nl> break ; <nl> } <nl> } <nl> int usb_add_gadget_udc_release ( struct device * parent , struct usb_gadget * gadget , <nl>  <nl> return 0 ; <nl>  <nl> + err5 : <nl> + device_del (& udc -> dev ); <nl> + <nl> err4 : <nl> list_del (& udc -> list ); <nl> mutex_unlock (& udc_lock );
flush_signal_handlers ( struct task_struct * t , int force_default ) <nl> if ( force_default || ka -> sa . sa_handler != SIG_IGN ) <nl> ka -> sa . sa_handler = SIG_DFL ; <nl> ka -> sa . sa_flags = 0 ; <nl> +# ifdef SA_RESTORER <nl> + ka -> sa . sa_restorer = NULL ; <nl> +# endif <nl> sigemptyset (& ka -> sa . sa_mask ); <nl> ka ++; <nl> }
static int pn_send ( struct sk_buff * skb , struct net_device * dev , <nl> struct phonethdr * ph ; <nl> int err ; <nl>  <nl> - if ( skb -> len + 2 > 0xffff ) { <nl> - /* Phonet length field would overflow */ <nl> + if ( skb -> len + 2 > 0xffff /* Phonet length field limit */ || <nl> + skb -> len + sizeof ( struct phonethdr ) > dev -> mtu ) { <nl> err = - EMSGSIZE ; <nl> goto drop ; <nl> }
static int device_rx_srv ( struct vnt_private * pDevice , unsigned int uIdx ) <nl> pRD = pRD -> next ) { <nl> if ( works ++ > 15 ) <nl> break ; <nl> + <nl> + if (! pRD -> pRDInfo -> skb ) <nl> + break ; <nl> + <nl> if ( vnt_receive_frame ( pDevice , pRD )) { <nl> if (! device_alloc_rx_buf ( pDevice , pRD )) { <nl> dev_err (& pDevice -> pcid -> dev ,
int sr_drive_status ( struct cdrom_device_info * cdi , int slot ) <nl> if ( 0 == sr_test_unit_ready ( cd -> device , & sshdr )) <nl> return CDS_DISC_OK ; <nl>  <nl> + /* SK / ASC / ASCQ of 2 / 4 / 1 means " unit is becoming ready " */ <nl> + if ( scsi_sense_valid (& sshdr ) && sshdr . sense_key == NOT_READY <nl> + && sshdr . asc == 0x04 && sshdr . ascq == 0x01 ) <nl> + return CDS_DRIVE_NOT_READY ; <nl> + <nl> if (! cdrom_get_media_event ( cdi , & med )) { <nl> if ( med . media_present ) <nl> return CDS_DISC_OK ;
static const struct labpc_board_struct labpc_cs_boards [] = { <nl> }, <nl> }; <nl>  <nl> -/* <nl> - * Useful for shorthand access to the particular board structure <nl> - */ <nl> -# define thisboard (( const struct labpc_board_struct *) dev -> board_ptr ) <nl> - <nl> static int labpc_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> { <nl> + const struct labpc_board_struct * thisboard = comedi_board ( dev ); <nl> struct labpc_private * devpriv ; <nl> unsigned long iobase = 0 ; <nl> unsigned int irq = 0 ;
static int _mei_irq_thread_read ( struct mei_device * dev , s32 * slots , <nl> struct mei_cl * cl , <nl> struct mei_io_list * cmpl_list ) <nl> { <nl> - if ((* slots * sizeof ( u32 )) >= ( sizeof ( struct mei_msg_hdr ) + <nl> + if ((* slots * sizeof ( u32 )) < ( sizeof ( struct mei_msg_hdr ) + <nl> sizeof ( struct hbm_flow_control ))) { <nl> /* return the cancel routine */ <nl> list_del (& cb_pos -> cb_list );
static struct tegra_dma_desc * tegra_dma_desc_get ( <nl> if ( async_tx_test_ack (& dma_desc -> txd )) { <nl> list_del (& dma_desc -> node ); <nl> spin_unlock_irqrestore (& tdc -> lock , flags ); <nl> + dma_desc -> txd . flags = 0 ; <nl> return dma_desc ; <nl> } <nl> } <nl> struct dma_async_tx_descriptor * tegra_dma_prep_dma_cyclic ( <nl> TEGRA_APBDMA_AHBSEQ_WRAP_SHIFT ; <nl> ahb_seq |= TEGRA_APBDMA_AHBSEQ_BUS_WIDTH_32 ; <nl>  <nl> - csr |= TEGRA_APBDMA_CSR_FLOW | TEGRA_APBDMA_CSR_IE_EOC ; <nl> + csr |= TEGRA_APBDMA_CSR_FLOW ; <nl> + if ( flags & DMA_PREP_INTERRUPT ) <nl> + csr |= TEGRA_APBDMA_CSR_IE_EOC ; <nl> csr |= tdc -> dma_sconfig . slave_id << TEGRA_APBDMA_CSR_REQ_SEL_SHIFT ; <nl>  <nl> apb_seq |= TEGRA_APBDMA_APBSEQ_WRAP_WORD_1 ; <nl> struct dma_async_tx_descriptor * tegra_dma_prep_dma_cyclic ( <nl> mem += len ; <nl> } <nl> sg_req -> last_sg = true ; <nl> - dma_desc -> txd . flags = 0 ; <nl> + if ( flags & DMA_CTRL_ACK ) <nl> + dma_desc -> txd . flags = DMA_CTRL_ACK ; <nl>  <nl> /* <nl> * Make sure that mode should not be conflicting with currently
int kvm_set_msr_common ( struct kvm_vcpu * vcpu , struct msr_data * msr_info ) <nl> /* ... but clean it before doing the actual write */ <nl> vcpu -> arch . time_offset = data & ~( PAGE_MASK | 1 ); <nl>  <nl> + /* Check that the address is 32 - byte aligned . */ <nl> + if ( vcpu -> arch . time_offset & <nl> + ( sizeof ( struct pvclock_vcpu_time_info ) - 1 )) <nl> + break ; <nl> + <nl> vcpu -> arch . time_page = <nl> gfn_to_page ( vcpu -> kvm , data >> PAGE_SHIFT ); <nl> 
static int ax25_release ( struct socket * sock ) <nl> ax25_destroy_socket ( ax25 ); <nl> } <nl> if ( ax25_dev ) { <nl> + del_timer_sync (& ax25 -> timer ); <nl> + del_timer_sync (& ax25 -> t1timer ); <nl> + del_timer_sync (& ax25 -> t2timer ); <nl> + del_timer_sync (& ax25 -> t3timer ); <nl> + del_timer_sync (& ax25 -> idletimer ); <nl> dev_put_track ( ax25_dev -> dev , & ax25_dev -> dev_tracker ); <nl> ax25_dev_put ( ax25_dev ); <nl> }
static int get_port_device_capability ( struct pci_dev * dev ) <nl> if ( reg32 & SLOT_HP_CAPABLE_MASK ) <nl> services |= PCIE_PORT_SERVICE_HP ; <nl> } <nl> - /* PME Capable */ <nl> - pos = pci_find_capability ( dev , PCI_CAP_ID_PME ); <nl> - if ( pos ) <nl> + /* PME Capable - root port capability */ <nl> + if ((( reg16 >> 4 ) & PORT_TYPE_MASK ) == PCIE_RC_PORT ) <nl> services |= PCIE_PORT_SERVICE_PME ; <nl>  <nl> pos = PCI_CFG_SPACE_SIZE ;
struct runqueue { <nl> unsigned long ttwu_cnt ; <nl> unsigned long ttwu_local ; <nl> # endif <nl> + struct lock_class_key rq_lock_key ; <nl> }; <nl>  <nl> static DEFINE_PER_CPU ( struct runqueue , runqueues ); <nl> void __init sched_init ( void ) <nl>  <nl> rq = cpu_rq ( i ); <nl> spin_lock_init (& rq -> lock ); <nl> + lockdep_set_class (& rq -> lock , & rq -> rq_lock_key ); <nl> rq -> nr_running = 0 ; <nl> rq -> active = rq -> arrays ; <nl> rq -> expired = rq -> arrays + 1 ;
static ssize_t environ_read ( struct file * file , char __user * buf , <nl> struct mm_struct * mm = file -> private_data ; <nl> unsigned long env_start , env_end ; <nl>  <nl> - if (! mm ) <nl> + /* Ensure the process spawned far enough to have an environment . */ <nl> + if (! mm || ! mm -> env_end ) <nl> return 0 ; <nl>  <nl> page = ( char *) __get_free_page ( GFP_TEMPORARY );
static int tegra_output_hdmi_check_mode ( struct tegra_output * output , <nl> parent = clk_get_parent ( hdmi -> clk_parent ); <nl>  <nl> err = clk_round_rate ( parent , pclk * 4 ); <nl> - if ( err < 0 ) <nl> + if ( err <= 0 ) <nl> * status = MODE_NOCLOCK ; <nl> else <nl> * status = MODE_OK ;
unsigned long parse_tag_value ( const char * str , struct parse_tag * tags ) <nl> if ( s != endptr ) <nl> break ; <nl>  <nl> + if ( value > ULONG_MAX / i -> mult ) <nl> + break ; <nl> value *= i -> mult ; <nl> return value ; <nl> }
int dccp_disconnect ( struct sock * sk , int flags ) <nl> sk -> sk_err = ECONNRESET ; <nl>  <nl> dccp_clear_xmit_timers ( sk ); <nl> + <nl> __skb_queue_purge (& sk -> sk_receive_queue ); <nl> + __skb_queue_purge (& sk -> sk_write_queue ); <nl> if ( sk -> sk_send_head != NULL ) { <nl> __kfree_skb ( sk -> sk_send_head ); <nl> sk -> sk_send_head = NULL ;
static void * skd_alloc_dma ( struct skd_device * skdev , struct kmem_cache * s , <nl> return NULL ; <nl> * dma_handle = dma_map_single ( dev , buf , s -> size , dir ); <nl> if ( dma_mapping_error ( dev , * dma_handle )) { <nl> - kfree ( buf ); <nl> + kmem_cache_free ( s , buf ); <nl> buf = NULL ; <nl> } <nl> return buf ;
static const struct i2c_device_id retu_id [] = { <nl> }; <nl> MODULE_DEVICE_TABLE ( i2c , retu_id ); <nl>  <nl> + static const struct of_device_id retu_of_match [] = { <nl> + { . compatible = " nokia , retu " }, <nl> + { . compatible = " nokia , tahvo " }, <nl> + { } <nl> +}; <nl> + MODULE_DEVICE_TABLE ( of , retu_of_match ); <nl> + <nl> static struct i2c_driver retu_driver = { <nl> . driver = { <nl> . name = " retu - mfd ", <nl> + . of_match_table = retu_of_match , <nl> }, <nl> . probe = retu_probe , <nl> . remove = retu_remove ,
static int hpsa_ioctl32_passthru ( struct scsi_device * dev , int cmd , void * arg ) <nl> int err ; <nl> u32 cp ; <nl>  <nl> + memset (& arg64 , 0 , sizeof ( arg64 )); <nl> err = 0 ; <nl> err |= copy_from_user (& arg64 . LUN_info , & arg32 -> LUN_info , <nl> sizeof ( arg64 . LUN_info )); <nl> static int hpsa_ioctl32_big_passthru ( struct scsi_device * dev , <nl> int err ; <nl> u32 cp ; <nl>  <nl> + memset (& arg64 , 0 , sizeof ( arg64 )); <nl> err = 0 ; <nl> err |= copy_from_user (& arg64 . LUN_info , & arg32 -> LUN_info , <nl> sizeof ( arg64 . LUN_info ));
static int eql_g_master_cfg ( struct net_device * dev , master_config_t __user * mcp ) <nl> equalizer_t * eql ; <nl> master_config_t mc ; <nl>  <nl> + memset (& mc , 0 , sizeof ( master_config_t )); <nl> + <nl> if ( eql_is_master ( dev )) { <nl> eql = netdev_priv ( dev ); <nl> mc . max_slaves = eql -> max_slaves ;
i915_gem_create ( struct drm_file * file , <nl> u32 handle ; <nl>  <nl> size = roundup ( size , PAGE_SIZE ); <nl> + if ( size == 0 ) <nl> + return - EINVAL ; <nl>  <nl> /* Allocate the new object */ <nl> obj = i915_gem_alloc_object ( dev , size );
static void pwmled_brightness ( struct led_classdev * cdev , enum led_brightness b ) <nl> * NOTE : we reuse the platform_data structure of GPIO leds , <nl> * but repurpose its " gpio " number as a PWM channel number . <nl> */ <nl> - static int __init pwmled_probe ( struct platform_device * pdev ) <nl> + static int __devinit pwmled_probe ( struct platform_device * pdev ) <nl> { <nl> const struct gpio_led_platform_data * pdata ; <nl> struct pwmled * leds ;
static void __init tegra210_pll_init ( void __iomem * clk_base , <nl>  <nl> /* PLLU_VCO */ <nl> val = readl ( clk_base + pll_u_vco_params . base_reg ); <nl> - val &= ~ BIT ( 24 ); /* disable PLLU_OVERRIDE */ <nl> + val &= ~ PLLU_BASE_OVERRIDE ; /* disable PLLU_OVERRIDE */ <nl> writel ( val , clk_base + pll_u_vco_params . base_reg ); <nl>  <nl> clk = tegra_clk_register_pllre (" pll_u_vco ", " pll_ref ", clk_base , pmc ,
static int eni_start ( struct atm_dev * dev ) <nl> /* initialize memory management */ <nl> buffer_mem = eni_dev -> mem - ( buf - eni_dev -> ram ); <nl> eni_dev -> free_list_size = buffer_mem / MID_MIN_BUF_SIZE / 2 ; <nl> - eni_dev -> free_list = kmalloc ( <nl> - sizeof ( struct eni_free )*( eni_dev -> free_list_size + 1 ), GFP_KERNEL ); <nl> + eni_dev -> free_list = kmalloc_array ( eni_dev -> free_list_size + 1 , <nl> + sizeof (* eni_dev -> free_list ), <nl> + GFP_KERNEL ); <nl> if (! eni_dev -> free_list ) { <nl> printk ( KERN_ERR DEV_LABEL "( itf % d ): couldn ' t get free page \ n ", <nl> dev -> number );
static int mxs_spi_setup_transfer ( struct spi_device * dev , <nl>  <nl> static int mxs_spi_setup ( struct spi_device * dev ) <nl> { <nl> - int err = 0 ; <nl> - <nl> if (! dev -> bits_per_word ) <nl> dev -> bits_per_word = 8 ; <nl>  <nl> if ( dev -> mode & ~( SPI_CPOL | SPI_CPHA )) <nl> return - EINVAL ; <nl>  <nl> - err = mxs_spi_setup_transfer ( dev , NULL ); <nl> - if ( err ) { <nl> - dev_err (& dev -> dev , <nl> - " Failed to setup transfer , error = % d \ n ", err ); <nl> - } <nl> - <nl> - return err ; <nl> + return 0 ; <nl> } <nl>  <nl> static uint32_t mxs_spi_cs_to_reg ( unsigned cs )
static void walk_linearmapping ( struct pg_state * st ) <nl> unsigned long psize = 1 << mmu_psize_defs [ mmu_linear_psize ]. shift ; <nl>  <nl> for ( addr = PAGE_OFFSET ; addr < PAGE_OFFSET + <nl> - memblock_phys_mem_size (); addr += psize ) <nl> + memblock_end_of_DRAM (); addr += psize ) <nl> hpte_find ( st , addr , mmu_linear_psize ); <nl> } <nl> 
static struct cl_lock * cl_lock_find ( const struct lu_env * env , <nl> spin_lock (& head -> coh_lock_guard ); <nl> ghost = cl_lock_lookup ( env , obj , io , need ); <nl> if ( ghost == NULL ) { <nl> + cl_lock_get_trust ( lock ); <nl> list_add_tail (& lock -> cll_linkage , <nl> & head -> coh_locks ); <nl> spin_unlock (& head -> coh_lock_guard ); <nl> static void cl_lock_delete0 ( const struct lu_env * env , struct cl_lock * lock ) <nl> LINVRNT ( cl_lock_invariant ( env , lock )); <nl>  <nl> if ( lock -> cll_state < CLS_FREEING ) { <nl> + bool in_cache ; <nl> + <nl> LASSERT ( lock -> cll_state != CLS_INTRANSIT ); <nl> cl_lock_state_set ( env , lock , CLS_FREEING ); <nl>  <nl> head = cl_object_header ( lock -> cll_descr . cld_obj ); <nl>  <nl> spin_lock (& head -> coh_lock_guard ); <nl> - list_del_init (& lock -> cll_linkage ); <nl> + in_cache = ! list_empty (& lock -> cll_linkage ); <nl> + if ( in_cache ) <nl> + list_del_init (& lock -> cll_linkage ); <nl> spin_unlock (& head -> coh_lock_guard ); <nl>  <nl> + if ( in_cache ) /* coh_locks cache holds a refcount . */ <nl> + cl_lock_put ( env , lock ); <nl> + <nl> /* <nl> * From now on , no new references to this lock can be acquired <nl> * by cl_lock_lookup ().
bool wil_fw_verify_file_exists ( struct wil6210_priv * wil , const char * name ) <nl> rc = request_firmware (& fw , name , wil_to_dev ( wil )); <nl> if (! rc ) <nl> release_firmware ( fw ); <nl> - return rc != - ENOENT ; <nl> + else <nl> + wil_dbg_fw ( wil , "<% s > not available : % d \ n ", name , rc ); <nl> + return ! rc ; <nl> }
EXPORT_SYMBOL_GPL ( crypto_givcipher_type ); <nl>  <nl> const char * crypto_default_geniv ( const struct crypto_alg * alg ) <nl> { <nl> + if ((( alg -> cra_flags & CRYPTO_ALG_TYPE_MASK ) == <nl> + CRYPTO_ALG_TYPE_BLKCIPHER ? alg -> cra_blkcipher . ivsize : <nl> + alg -> cra_ablkcipher . ivsize ) != <nl> + alg -> cra_blocksize ) <nl> + return " chainiv "; <nl> + <nl> return alg -> cra_flags & CRYPTO_ALG_ASYNC ? <nl> " eseqiv " : skcipher_default_geniv ; <nl> }
static int cs_etm__sample ( struct cs_etm_queue * etmq ) <nl> static int cs_etm__flush ( struct cs_etm_queue * etmq ) <nl> { <nl> int err = 0 ; <nl> + struct cs_etm_auxtrace * etm = etmq -> etm ; <nl> struct cs_etm_packet * tmp ; <nl>  <nl> if (! etmq -> prev_packet ) <nl> static int cs_etm__flush ( struct cs_etm_queue * etmq ) <nl>  <nl> } <nl>  <nl> + if ( etm -> sample_branches && <nl> + etmq -> prev_packet -> sample_type == CS_ETM_RANGE ) { <nl> + err = cs_etm__synth_branch_sample ( etmq ); <nl> + if ( err ) <nl> + return err ; <nl> + } <nl> + <nl> swap_packet : <nl> if ( etmq -> etm -> synth_opts . last_branch ) { <nl> /*
async_syndrome_val ( struct page ** blocks , unsigned int offset , int disks , <nl>  <nl> dma_set_unmap ( tx , unmap ); <nl> async_tx_submit ( chan , tx , submit ); <nl> - <nl> - return tx ; <nl> } else { <nl> struct page * p_src = P ( blocks , disks ); <nl> struct page * q_src = Q ( blocks , disks ); <nl> async_syndrome_val ( struct page ** blocks , unsigned int offset , int disks , <nl> submit -> cb_param = cb_param_orig ; <nl> submit -> flags = flags_orig ; <nl> async_tx_sync_epilog ( submit ); <nl> - <nl> - return NULL ; <nl> + tx = NULL ; <nl> } <nl> + dmaengine_unmap_put ( unmap ); <nl> + <nl> + return tx ; <nl> } <nl> EXPORT_SYMBOL_GPL ( async_syndrome_val ); <nl> 
int i915_gem_freeze_late ( struct drm_i915_private * dev_priv ) <nl> */ <nl>  <nl> i915_gem_shrink ( dev_priv , - 1UL , I915_SHRINK_UNBOUND ); <nl> + i915_gem_drain_freed_objects ( dev_priv ); <nl>  <nl> mutex_lock (& dev_priv -> drm . struct_mutex ); <nl> for ( p = phases ; * p ; p ++) {
static int show_stat ( struct seq_file * p , void * v ) <nl> struct timespec boottime ; <nl> unsigned int * per_irq_sum ; <nl>  <nl> - per_irq_sum = kzalloc ( sizeof ( unsigned int )* NR_IRQS , GFP_KERNEL ); <nl> + per_irq_sum = kzalloc ( sizeof ( unsigned int )* nr_irqs , GFP_KERNEL ); <nl> if (! per_irq_sum ) <nl> return - ENOMEM ; <nl>  <nl> static int show_stat ( struct seq_file * p , void * v ) <nl> softirq = cputime64_add ( softirq , kstat_cpu ( i ). cpustat . softirq ); <nl> steal = cputime64_add ( steal , kstat_cpu ( i ). cpustat . steal ); <nl> guest = cputime64_add ( guest , kstat_cpu ( i ). cpustat . guest ); <nl> - for ( j = 0 ; j < NR_IRQS ; j ++) { <nl> + for ( j = 0 ; j < nr_irqs ; j ++) { <nl> unsigned int temp = kstat_cpu ( i ). irqs [ j ]; <nl> sum += temp ; <nl> per_irq_sum [ j ] += temp ; <nl> static int show_stat ( struct seq_file * p , void * v ) <nl> } <nl> seq_printf ( p , " intr % llu ", ( unsigned long long ) sum ); <nl>  <nl> - for ( i = 0 ; i < NR_IRQS ; i ++) <nl> + for ( i = 0 ; i < nr_irqs ; i ++) <nl> seq_printf ( p , " % u ", per_irq_sum [ i ]); <nl>  <nl> seq_printf ( p , <nl> static const struct file_operations proc_stat_operations = { <nl> */ <nl> static void * int_seq_start ( struct seq_file * f , loff_t * pos ) <nl> { <nl> - return (* pos <= NR_IRQS ) ? pos : NULL ; <nl> + return (* pos <= nr_irqs ) ? pos : NULL ; <nl> } <nl>  <nl> static void * int_seq_next ( struct seq_file * f , void * v , loff_t * pos ) <nl> { <nl> (* pos )++; <nl> - if (* pos > NR_IRQS ) <nl> + if (* pos > nr_irqs ) <nl> return NULL ; <nl> return pos ; <nl> }
static int max30102_probe ( struct i2c_client * client , <nl> dev_err (& client -> dev , " regmap initialization failed \ n "); <nl> return PTR_ERR ( data -> regmap ); <nl> } <nl> - max30102_set_powermode ( data , false ); <nl> + <nl> + ret = max30102_set_powermode ( data , false ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> ret = max30102_chip_init ( data ); <nl> if ( ret )
int bond_enslave ( struct net_device * bond_dev , struct net_device * slave_dev ) <nl> return - EBUSY ; <nl> } <nl>  <nl> + if ( bond_dev == slave_dev ) { <nl> + pr_err ("% s : cannot enslave bond to itself .\ n ", bond_dev -> name ); <nl> + return - EPERM ; <nl> + } <nl> + <nl> /* vlan challenged mutual exclusion */ <nl> /* no need to lock since we ' re protected by rtnl_lock */ <nl> if ( slave_dev -> features & NETIF_F_VLAN_CHALLENGED ) {
static bool fib6_rule_suppress ( struct fib_rule * rule , struct fib_lookup_arg * arg <nl> return false ; <nl>  <nl> suppress_route : <nl> - ip6_rt_put ( rt ); <nl> + if (!( arg -> flags & FIB_LOOKUP_NOREF )) <nl> + ip6_rt_put ( rt ); <nl> return true ; <nl> } <nl> 
struct ath_node ; <nl>  <nl> /* Macro to expand scalars to 64 - bit objects */ <nl>  <nl> -# define ito64 ( x ) ( sizeof ( x ) == 8 ) ? \ <nl> +# define ito64 ( x ) ( sizeof ( x ) == 1 ) ? \ <nl> ((( unsigned long long int )( x )) & ( 0xff )) : \ <nl> - ( sizeof ( x ) == 16 ) ? \ <nl> + ( sizeof ( x ) == 2 ) ? \ <nl> ((( unsigned long long int )( x )) & 0xffff ) : \ <nl> - (( sizeof ( x ) == 32 ) ? \ <nl> + (( sizeof ( x ) == 4 ) ? \ <nl> ((( unsigned long long int )( x )) & 0xffffffff ) : \ <nl> ( unsigned long long int )( x )) <nl> 
static void pch_gpio_setup ( struct pch_gpio * chip ) <nl> static int pch_irq_type ( struct irq_data * d , unsigned int type ) <nl> { <nl> u32 im ; <nl> - u32 * im_reg ; <nl> + u32 __iomem * im_reg ; <nl> u32 ien ; <nl> u32 im_pos ; <nl> int ch ;
static int __mdio_read ( struct net_device * dev , int phy_id , int reg ) <nl> return mdio_read ( tp -> mmio_addr , phy_id , reg ); <nl> } <nl>  <nl> + static u16 mdio_read_latched ( void __iomem * ioaddr , int phy_id , int reg ) <nl> +{ <nl> + mdio_read ( ioaddr , phy_id , reg ); <nl> + return mdio_read ( ioaddr , phy_id , reg ); <nl> +} <nl> + <nl> static u16 __devinit sis190_read_eeprom ( void __iomem * ioaddr , u32 reg ) <nl> { <nl> u16 data = 0xffff ; <nl> static void sis190_phy_task ( void * data ) <nl> if ( val & BMCR_RESET ) { <nl> // FIXME : needlessly high ? -- FR 02 / 07 / 2005 <nl> mod_timer (& tp -> timer , jiffies + HZ / 10 ); <nl> - } else if (!( mdio_read ( ioaddr , phy_id , MII_BMSR ) & BMSR_ANEGCOMPLETE )) { <nl> + } else if (!( mdio_read_latched ( ioaddr , phy_id , MII_BMSR ) & <nl> + BMSR_ANEGCOMPLETE )) { <nl> net_link ( tp , KERN_WARNING "% s : PHY reset until link up .\ n ", <nl> dev -> name ); <nl> mdio_write ( ioaddr , phy_id , MII_BMCR , val | BMCR_RESET );
static int ion_carveout_heap_allocate ( struct ion_heap * heap , <nl> unsigned long size , unsigned long align , <nl> unsigned long flags ) <nl> { <nl> + if ( align > PAGE_SIZE ) <nl> + return - EINVAL ; <nl> + <nl> buffer -> priv_phys = ion_carveout_allocate ( heap , size , align ); <nl> return buffer -> priv_phys == ION_CARVEOUT_ALLOCATE_FAIL ? - ENOMEM : 0 ; <nl> }
static int cuse_channel_release ( struct inode * inode , struct file * file ) <nl> unregister_chrdev_region ( cc -> cdev -> dev , 1 ); <nl> cdev_del ( cc -> cdev ); <nl> } <nl> + /* Base reference is now owned by " fud " */ <nl> + fuse_conn_put (& cc -> fc ); <nl>  <nl> rc = fuse_dev_release ( inode , file ); /* puts the base reference */ <nl> 
static void uli526x_rx_packet ( struct net_device * dev , struct uli526x_board_info <nl>  <nl> if ( !( rdes0 & 0x8000 ) || <nl> (( db -> cr6_data & CR6_PM ) && ( rxlen > 6 )) ) { <nl> + struct sk_buff * new_skb = NULL ; <nl> + <nl> skb = rxptr -> rx_skb_ptr ; <nl>  <nl> /* Good packet , send to upper layer */ <nl> /* Shorst packet used new SKB */ <nl> - if ( ( rxlen < RX_COPY_SIZE ) && <nl> - ( ( skb = dev_alloc_skb ( rxlen + 2 ) ) <nl> - != NULL ) ) { <nl> + if (( rxlen < RX_COPY_SIZE ) && <nl> + (( new_skb = dev_alloc_skb ( rxlen + 2 ) != NULL ))) { <nl> + skb = new_skb ; <nl> /* size less than COPY_SIZE , allocate a rxlen SKB */ <nl> skb_reserve ( skb , 2 ); /* 16byte align */ <nl> memcpy ( skb_put ( skb , rxlen ),
static int btrfs_add_system_chunk ( struct btrfs_root * root , <nl> u8 * ptr ; <nl>  <nl> array_size = btrfs_super_sys_array_size ( super_copy ); <nl> - if ( array_size + item_size > BTRFS_SYSTEM_CHUNK_ARRAY_SIZE ) <nl> + if ( array_size + item_size + sizeof ( disk_key ) <nl> + > BTRFS_SYSTEM_CHUNK_ARRAY_SIZE ) <nl> return - EFBIG ; <nl>  <nl> ptr = super_copy -> sys_chunk_array + array_size ;
static int hists_evsel__init ( struct perf_evsel * evsel ) <nl> return 0 ; <nl> } <nl>  <nl> + static void hists_evsel__exit ( struct perf_evsel * evsel ) <nl> +{ <nl> + struct hists * hists = evsel__hists ( evsel ); <nl> + <nl> + hists__delete_entries ( hists ); <nl> +} <nl> + <nl> /* <nl> * XXX We probably need a hists_evsel__exit () to free the hist_entries <nl> * stored in the rbtree ... <nl> static int hists_evsel__init ( struct perf_evsel * evsel ) <nl> int hists__init ( void ) <nl> { <nl> int err = perf_evsel__object_config ( sizeof ( struct hists_evsel ), <nl> - hists_evsel__init , NULL ); <nl> + hists_evsel__init , <nl> + hists_evsel__exit ); <nl> if ( err ) <nl> fputs (" FATAL ERROR : Couldn ' t setup hists class \ n ", stderr ); <nl> 
static struct drm_driver tegra_drm_driver = { <nl> . debugfs_cleanup = tegra_debugfs_cleanup , <nl> # endif <nl>  <nl> - . gem_free_object = tegra_bo_free_object , <nl> + . gem_free_object_unlocked = tegra_bo_free_object , <nl> . gem_vm_ops = & tegra_bo_vm_ops , <nl>  <nl> . prime_handle_to_fd = drm_gem_prime_handle_to_fd ,
static int show_cpuinfo ( struct seq_file * m , void * v ) <nl> unsigned short min ; <nl>  <nl> if ( cpu_id == NR_CPUS ) { <nl> + struct device_node * root ; <nl> + const char * model = NULL ; <nl> # if defined ( CONFIG_SMP ) && defined ( CONFIG_PPC32 ) <nl> unsigned long bogosum = 0 ; <nl> int i ; <nl> static int show_cpuinfo ( struct seq_file * m , void * v ) <nl> seq_printf ( m , " timebase \ t : % lu \ n ", ppc_tb_freq ); <nl> if ( ppc_md . name ) <nl> seq_printf ( m , " platform \ t : % s \ n ", ppc_md . name ); <nl> + root = of_find_node_by_path ("/"); <nl> + if ( root ) <nl> + model = of_get_property ( root , " model ", NULL ); <nl> + if ( model ) <nl> + seq_printf ( m , " model \ t \ t : % s \ n ", model ); <nl> + of_node_put ( root ); <nl> + <nl> if ( ppc_md . show_cpuinfo != NULL ) <nl> ppc_md . show_cpuinfo ( m ); <nl> 
mlx5_fw_fatal_reporter_dump ( struct devlink_health_reporter * reporter , <nl> return - ENOMEM ; <nl> err = mlx5_crdump_collect ( dev , cr_data ); <nl> if ( err ) <nl> - return err ; <nl> + goto free_data ; <nl>  <nl> if ( priv_ctx ) { <nl> struct mlx5_fw_reporter_ctx * fw_reporter_ctx = priv_ctx ;
aoenet_xmit ( struct sk_buff_head * queue ) <nl> { <nl> struct sk_buff * skb , * tmp ; <nl>  <nl> - skb_queue_walk_safe ( queue , skb , tmp ) <nl> + skb_queue_walk_safe ( queue , skb , tmp ) { <nl> + __skb_unlink ( skb , queue ); <nl> dev_queue_xmit ( skb ); <nl> + } <nl> } <nl>  <nl> /*
static int s3c2410_udc_ep_enable ( struct usb_ep * _ep , <nl>  <nl> ep = to_s3c2410_ep ( _ep ); <nl>  <nl> - if (! _ep || ! desc || ep -> ep . desc <nl> + if (! _ep || ! desc <nl> || _ep -> name == ep0name <nl> || desc -> bDescriptorType != USB_DT_ENDPOINT ) <nl> return - EINVAL ;
static void unbind_from_irq ( unsigned int irq ) <nl>  <nl> spin_lock (& irq_mapping_update_lock ); <nl>  <nl> - if ( VALID_EVTCHN ( evtchn ) && (-- irq_bindcount [ irq ] == 0 )) { <nl> + if ((-- irq_bindcount [ irq ] == 0 ) && VALID_EVTCHN ( evtchn )) { <nl> close . port = evtchn ; <nl> if ( HYPERVISOR_event_channel_op ( EVTCHNOP_close , & close ) != 0 ) <nl> BUG (); <nl> static void unbind_from_irq ( unsigned int irq ) <nl> evtchn_to_irq [ evtchn ] = - 1 ; <nl> irq_info [ irq ] = IRQ_UNBOUND ; <nl>  <nl> - dynamic_irq_init ( irq ); <nl> + dynamic_irq_cleanup ( irq ); <nl> } <nl>  <nl> spin_unlock (& irq_mapping_update_lock );
static struct platform_driver mxc_w1_driver = { <nl> . name = " mxc_w1 ", <nl> }, <nl> . probe = mxc_w1_probe , <nl> - . remove = __devexit_p ( mxc_w1_remove ), <nl> + . remove = mxc_w1_remove , <nl> }; <nl> module_platform_driver ( mxc_w1_driver ); <nl> 
static inline __be16 x25_type_trans ( struct sk_buff * skb , struct net_device * dev ) <nl> { <nl> skb -> mac . raw = skb -> data ; <nl> + skb -> dev = dev ; <nl> skb -> pkt_type = PACKET_HOST ; <nl>  <nl> return htons ( ETH_P_X25 );
static inline void * kvmalloc_array ( size_t n , size_t size , gfp_t flags ) <nl> return kvmalloc ( bytes , flags ); <nl> } <nl>  <nl> + static inline void * kvcalloc ( size_t n , size_t size , gfp_t flags ) <nl> +{ <nl> + return kvmalloc_array ( n , size , flags | __GFP_ZERO ); <nl> +} <nl> + <nl> extern void kvfree ( const void * addr ); <nl>  <nl> static inline atomic_t * compound_mapcount_ptr ( struct page * page )
cifs_lookup ( struct inode * parent_dir_inode , struct dentry * direntry , <nl> /* if it was once a directory ( but how can we tell ?) we could do <nl> shrink_dcache_parent ( direntry ); */ <nl> } else if ( rc != - EACCES ) { <nl> - cifs_dbg ( VFS , " Unexpected lookup error % d \ n ", rc ); <nl> + cifs_dbg ( FYI , " Unexpected lookup error % d \ n ", rc ); <nl> /* We special case check for Access Denied - since that <nl> is a common return code */ <nl> }
extern struct group_info init_groups ; <nl> # define INIT_STRUCT_PID { \ <nl> . count = ATOMIC_INIT ( 1 ), \ <nl> . tasks = { \ <nl> - { . first = & init_task . pids [ PIDTYPE_PID ]. node }, \ <nl> - { . first = & init_task . pids [ PIDTYPE_PGID ]. node }, \ <nl> - { . first = & init_task . pids [ PIDTYPE_SID ]. node }, \ <nl> + { . first = NULL }, \ <nl> + { . first = NULL }, \ <nl> + { . first = NULL }, \ <nl> }, \ <nl> . level = 0 , \ <nl> . numbers = { { \ <nl> extern struct group_info init_groups ; <nl> { \ <nl> . node = { \ <nl> . next = NULL , \ <nl> - . pprev = & init_struct_pid . tasks [ type ]. first , \ <nl> + . pprev = NULL , \ <nl> }, \ <nl> . pid = & init_struct_pid , \ <nl> }
static void collapse_huge_page ( struct mm_struct * mm , <nl> # ifndef CONFIG_NUMA <nl> VM_BUG_ON (!* hpage ); <nl> new_page = * hpage ; <nl> + if ( unlikely ( mem_cgroup_newpage_charge ( new_page , mm , GFP_KERNEL ))) { <nl> + up_read (& mm -> mmap_sem ); <nl> + return ; <nl> + } <nl> # else <nl> VM_BUG_ON (* hpage ); <nl> /* <nl> static void collapse_huge_page ( struct mm_struct * mm , <nl> * hpage = ERR_PTR (- ENOMEM ); <nl> return ; <nl> } <nl> -# endif <nl> if ( unlikely ( mem_cgroup_newpage_charge ( new_page , mm , GFP_KERNEL ))) { <nl> up_read (& mm -> mmap_sem ); <nl> put_page ( new_page ); <nl> return ; <nl> } <nl> +# endif <nl>  <nl> /* after allocating the hugepage upgrade to mmap_sem write mode */ <nl> up_read (& mm -> mmap_sem );
static int kvm_vm_ioctl_create_vcpu ( struct kvm * kvm , u32 id ) <nl> int r ; <nl> struct kvm_vcpu * vcpu , * v ; <nl>  <nl> + if ( id >= KVM_MAX_VCPUS ) <nl> + return - EINVAL ; <nl> + <nl> vcpu = kvm_arch_vcpu_create ( kvm , id ); <nl> if ( IS_ERR ( vcpu )) <nl> return PTR_ERR ( vcpu );
static int sctp_getsockopt_disable_fragments ( struct sock * sk , int len , <nl> static int sctp_getsockopt_events ( struct sock * sk , int len , char __user * optval , <nl> int __user * optlen ) <nl> { <nl> - if ( len < sizeof ( struct sctp_event_subscribe )) <nl> + if ( len <= 0 ) <nl> return - EINVAL ; <nl> - len = sizeof ( struct sctp_event_subscribe ); <nl> + if ( len > sizeof ( struct sctp_event_subscribe )) <nl> + len = sizeof ( struct sctp_event_subscribe ); <nl> if ( put_user ( len , optlen )) <nl> return - EFAULT ; <nl> if ( copy_to_user ( optval , & sctp_sk ( sk )-> subscribe , len ))
static long vbg_misc_device_ioctl ( struct file * filp , unsigned int req , <nl> if (! buf ) <nl> return - ENOMEM ; <nl>  <nl> - if ( copy_from_user ( buf , ( void *) arg , hdr . size_in )) { <nl> + *(( struct vbg_ioctl_hdr *) buf ) = hdr ; <nl> + if ( copy_from_user ( buf + sizeof ( hdr ), ( void *) arg + sizeof ( hdr ), <nl> + hdr . size_in - sizeof ( hdr ))) { <nl> ret = - EFAULT ; <nl> goto out ; <nl> }
lqasc_startup ( struct uart_port * port ) <nl> struct ltq_uart_port * ltq_port = to_ltq_uart_port ( port ); <nl> int retval ; <nl>  <nl> - if ( ltq_port -> clk ) <nl> + if (! IS_ERR ( ltq_port -> clk )) <nl> clk_enable ( ltq_port -> clk ); <nl> port -> uartclk = clk_get_rate ( ltq_port -> fpiclk ); <nl>  <nl> lqasc_shutdown ( struct uart_port * port ) <nl> port -> membase + LTQ_ASC_RXFCON ); <nl> ltq_w32_mask ( ASCTXFCON_TXFEN , ASCTXFCON_TXFFLU , <nl> port -> membase + LTQ_ASC_TXFCON ); <nl> - if ( ltq_port -> clk ) <nl> + if (! IS_ERR ( ltq_port -> clk )) <nl> clk_disable ( ltq_port -> clk ); <nl> } <nl> 
static int nfs_statfs ( struct dentry * dentry , struct kstatfs * buf ) <nl> goto out_err ; <nl>  <nl> error = server -> nfs_client -> rpc_ops -> statfs ( server , fh , & res ); <nl> + if ( unlikely ( error == - ESTALE )) { <nl> + struct dentry * pd_dentry ; <nl>  <nl> + pd_dentry = dget_parent ( dentry ); <nl> + if ( pd_dentry != NULL ) { <nl> + nfs_zap_caches ( pd_dentry -> d_inode ); <nl> + dput ( pd_dentry ); <nl> + } <nl> + } <nl> nfs_free_fattr ( res . fattr ); <nl> if ( error < 0 ) <nl> goto out_err ;
static struct sk_buff * macsec_encrypt ( struct sk_buff * skb , <nl> sg_init_table ( sg , ret ); <nl> ret = skb_to_sgvec ( skb , sg , 0 , skb -> len ); <nl> if ( unlikely ( ret < 0 )) { <nl> + aead_request_free ( req ); <nl> macsec_txsa_put ( tx_sa ); <nl> kfree_skb ( skb ); <nl> return ERR_PTR ( ret ); <nl> static struct sk_buff * macsec_decrypt ( struct sk_buff * skb , <nl> sg_init_table ( sg , ret ); <nl> ret = skb_to_sgvec ( skb , sg , 0 , skb -> len ); <nl> if ( unlikely ( ret < 0 )) { <nl> + aead_request_free ( req ); <nl> kfree_skb ( skb ); <nl> return ERR_PTR ( ret ); <nl> }
void btrfs_update_iflags ( struct inode * inode ) <nl> */ <nl> void btrfs_inherit_iflags ( struct inode * inode , struct inode * dir ) <nl> { <nl> - unsigned int flags = BTRFS_I ( dir )-> flags ; <nl> + unsigned int flags ; <nl> + <nl> + if (! dir ) <nl> + return ; <nl> + <nl> + flags = BTRFS_I ( dir )-> flags ; <nl>  <nl> if ( S_ISREG ( inode -> i_mode )) <nl> flags &= ~ BTRFS_INODE_DIRSYNC ;
static void efx_filter_rfs_work ( struct work_struct * data ) <nl> struct efx_channel * channel = efx_get_channel ( efx , req -> rxq_index ); <nl> int rc ; <nl>  <nl> - rc = efx -> type -> filter_insert ( efx , & req -> spec , false ); <nl> + rc = efx -> type -> filter_insert ( efx , & req -> spec , true ); <nl> if ( rc >= 0 ) { <nl> /* Remember this so we can check whether to expire the filter <nl> * later .
static int s3c64xx_i2s_set_sysclk ( struct snd_soc_dai * cpu_dai , <nl> struct clk * s3c64xx_i2s_get_clock ( struct snd_soc_dai * dai ) <nl> { <nl> struct s3c_i2sv2_info * i2s = to_info ( dai ); <nl> + u32 iismod = readl ( i2s -> regs + S3C2412_IISMOD ); <nl>  <nl> - return i2s -> iis_cclk ; <nl> + if ( iismod & S3C64XX_IISMOD_IMS_SYSMUX ) <nl> + return i2s -> iis_cclk ; <nl> + else <nl> + return i2s -> iis_pclk ; <nl> } <nl> EXPORT_SYMBOL_GPL ( s3c64xx_i2s_get_clock ); <nl> 
int afs_prepare_write ( struct file * file , struct page * page , <nl> _leave (" = % d [ prep ]", ret ); <nl> return ret ; <nl> } <nl> - SetPageUptodate ( page ); <nl> } <nl>  <nl> try_again : <nl> int afs_commit_write ( struct file * file , struct page * page , <nl> spin_unlock (& vnode -> writeback_lock ); <nl> } <nl>  <nl> + SetPageUptodate ( page ); <nl> set_page_dirty ( page ); <nl> - <nl> if ( PageDirty ( page )) <nl> _debug (" dirtied "); <nl> 
void dlm_lowcomms_stop ( void ) <nl> con = __nodeid2con ( i , 0 ); <nl> if ( con ) { <nl> close_connection ( con , true ); <nl> + if ( con -> othercon ) <nl> + kmem_cache_free ( con_cache , con -> othercon ); <nl> kmem_cache_free ( con_cache , con ); <nl> } <nl> }
static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> datao += off - key . offset ; <nl> datal -= off - key . offset ; <nl> } <nl> - if ( key . offset + datao + datal + key . offset > <nl> - off + len ) <nl> + if ( key . offset + datao + datal > off + len ) <nl> datal = off + len - key . offset - datao ; <nl> /* disko == 0 means it ' s a hole */ <nl> if (! disko )
static void ieee80211_xmit ( struct ieee80211_sub_if_data * sdata , <nl> list ) { <nl> if (! ieee80211_sdata_running ( tmp_sdata )) <nl> continue ; <nl> - if ( tmp_sdata -> vif . type != NL80211_IFTYPE_AP ) <nl> + if ( tmp_sdata -> vif . type == <nl> + NL80211_IFTYPE_MONITOR || <nl> + tmp_sdata -> vif . type == <nl> + NL80211_IFTYPE_AP_VLAN || <nl> + tmp_sdata -> vif . type == <nl> + NL80211_IFTYPE_WDS ) <nl> continue ; <nl> if ( compare_ether_addr ( tmp_sdata -> vif . addr , <nl> hdr -> addr2 ) == 0 ) {
void kvm_mmu_zap_all ( struct kvm * kvm ) <nl> kvm_flush_remote_tlbs ( kvm ); <nl> } <nl>  <nl> - static void kvm_mmu_remove_one_alloc_mmu_page ( struct kvm * kvm ) <nl> + static int kvm_mmu_remove_some_alloc_mmu_pages ( struct kvm * kvm ) <nl> { <nl> struct kvm_mmu_page * page ; <nl>  <nl> page = container_of ( kvm -> arch . active_mmu_pages . prev , <nl> struct kvm_mmu_page , link ); <nl> - kvm_mmu_zap_page ( kvm , page ); <nl> + return kvm_mmu_zap_page ( kvm , page ) + 1 ; <nl> } <nl>  <nl> static int mmu_shrink ( int nr_to_scan , gfp_t gfp_mask ) <nl> static int mmu_shrink ( int nr_to_scan , gfp_t gfp_mask ) <nl> spin_lock (& kvm_lock ); <nl>  <nl> list_for_each_entry ( kvm , & vm_list , vm_list ) { <nl> - int npages , idx ; <nl> + int npages , idx , freed_pages ; <nl>  <nl> idx = srcu_read_lock (& kvm -> srcu ); <nl> spin_lock (& kvm -> mmu_lock ); <nl> static int mmu_shrink ( int nr_to_scan , gfp_t gfp_mask ) <nl> kvm -> arch . n_free_mmu_pages ; <nl> cache_count += npages ; <nl> if (! kvm_freed && nr_to_scan > 0 && npages > 0 ) { <nl> - kvm_mmu_remove_one_alloc_mmu_page ( kvm ); <nl> - cache_count --; <nl> + freed_pages = kvm_mmu_remove_some_alloc_mmu_pages ( kvm ); <nl> + cache_count -= freed_pages ; <nl> kvm_freed = kvm ; <nl> } <nl> nr_to_scan --;
static int asus_laptop_get_info ( struct asus_laptop * asus ) <nl> } <nl> } <nl> asus -> name = kstrdup ( string , GFP_KERNEL ); <nl> - if (! asus -> name ) <nl> + if (! asus -> name ) { <nl> + kfree ( buffer . pointer ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> if (* string ) <nl> pr_notice (" % s model detected \ n ", string );
int acpi_bus_generate_proc_event4 ( const char * device_class , const char * bus_id , <nl> if (! event_is_open ) <nl> return 0 ; <nl>  <nl> - event = kmalloc ( sizeof ( struct acpi_bus_event ), GFP_ATOMIC ); <nl> + event = kzalloc ( sizeof ( struct acpi_bus_event ), GFP_ATOMIC ); <nl> if (! event ) <nl> return - ENOMEM ; <nl> 
static int wm_adsp_fw_put ( struct snd_kcontrol * kcontrol , <nl> if ( adsp [ e -> shift_l ]. running ) <nl> return - EBUSY ; <nl>  <nl> - adsp -> fw = ucontrol -> value . integer . value [ 0 ]; <nl> + adsp [ e -> shift_l ]. fw = ucontrol -> value . integer . value [ 0 ]; <nl>  <nl> return 0 ; <nl> }
static int snd_echo_resume ( struct pci_dev * pci ) <nl> DE_INIT ((" resume start \ n ")); <nl> pci_restore_state ( pci ); <nl> commpage_bak = kmalloc ( sizeof ( struct echoaudio ), GFP_KERNEL ); <nl> + if ( commpage_bak == NULL ) <nl> + return - ENOMEM ; <nl> commpage = chip -> comm_page ; <nl> memcpy ( commpage_bak , commpage , sizeof ( struct comm_page )); <nl> 
static int imx_ldb_connector_get_modes ( struct drm_connector * connector ) <nl> struct drm_display_mode * mode ; <nl>  <nl> mode = drm_mode_create ( connector -> dev ); <nl> + if (! mode ) <nl> + return - EINVAL ; <nl> drm_mode_copy ( mode , & imx_ldb_ch -> mode ); <nl> mode -> type |= DRM_MODE_TYPE_DRIVER | DRM_MODE_TYPE_PREFERRED ; <nl> drm_mode_probed_add ( connector , mode );
static void si_apply_state_adjust_rules ( struct radeon_device * rdev , <nl> if ( rdev -> pdev -> device == 0x6811 && <nl> rdev -> pdev -> revision == 0x81 ) <nl> max_mclk = 120000 ; <nl> + /* limit sclk / mclk on Jet parts for stability */ <nl> + if ( rdev -> pdev -> device == 0x6665 && <nl> + rdev -> pdev -> revision == 0xc3 ) { <nl> + max_sclk = 75000 ; <nl> + max_mclk = 80000 ; <nl> + } <nl>  <nl> if ( rps -> vce_active ) { <nl> rps -> evclk = rdev -> pm . dpm . vce_states [ rdev -> pm . dpm . vce_level ]. evclk ;
int fixup_user_fault ( struct task_struct * tsk , struct mm_struct * mm , <nl> unsigned long address , unsigned int fault_flags ) <nl> { <nl> struct vm_area_struct * vma ; <nl> + vm_flags_t vm_flags ; <nl> int ret ; <nl>  <nl> vma = find_extend_vma ( mm , address ); <nl> if (! vma || address < vma -> vm_start ) <nl> return - EFAULT ; <nl>  <nl> + vm_flags = ( fault_flags & FAULT_FLAG_WRITE ) ? VM_WRITE : VM_READ ; <nl> + if (!( vm_flags & vma -> vm_flags )) <nl> + return - EFAULT ; <nl> + <nl> ret = handle_mm_fault ( mm , vma , address , fault_flags ); <nl> if ( ret & VM_FAULT_ERROR ) { <nl> if ( ret & VM_FAULT_OOM )
static struct vfsmount * nfs_do_root_mount ( struct file_system_type * fs_type , <nl> char * root_devname ; <nl> size_t len ; <nl>  <nl> - len = strlen ( hostname ) + 3 ; <nl> + len = strlen ( hostname ) + 5 ; <nl> root_devname = kmalloc ( len , GFP_KERNEL ); <nl> if ( root_devname == NULL ) <nl> return ERR_PTR (- ENOMEM ); <nl> - snprintf ( root_devname , len , "% s :/", hostname ); <nl> + /* Does hostname needs to be enclosed in brackets ? */ <nl> + if ( strchr ( hostname , ':')) <nl> + snprintf ( root_devname , len , "[% s ]:/", hostname ); <nl> + else <nl> + snprintf ( root_devname , len , "% s :/", hostname ); <nl> root_mnt = vfs_kern_mount ( fs_type , flags , root_devname , data ); <nl> kfree ( root_devname ); <nl> return root_mnt ;
static int lguestblk_probe ( struct lguest_device * lgdev ) <nl> } <nl>  <nl> /* This allocates a " struct gendisk " where we pack all the information <nl> - * about the disk which the rest of Linux sees . We ask for one minor <nl> - * number ; I do wonder if we should be asking for more . */ <nl> - bd -> disk = alloc_disk ( 1 ); <nl> + * about the disk which the rest of Linux sees . The argument is the <nl> + * number of minor devices desired : we need one minor for the main <nl> + * disk , and one for each partition . Of course , we can ' t possibly know <nl> + * how many partitions are on the disk ( add_disk does that ). <nl> + */ <nl> + bd -> disk = alloc_disk ( 16 ); <nl> if (! bd -> disk ) { <nl> err = - ENOMEM ; <nl> goto out_unregister_blkdev ;
static void __exit cmdq_drv_exit ( void ) <nl>  <nl> subsys_initcall ( cmdq_drv_init ); <nl> module_exit ( cmdq_drv_exit ); <nl> + <nl> + MODULE_LICENSE (" GPL v2 ");
smb_send_kvec ( struct TCP_Server_Info * server , struct kvec * iov , size_t n_vec , <nl>  <nl> * sent = 0 ; <nl>  <nl> - if ( ssocket == NULL ) <nl> - return - ENOTSOCK ; /* BB eventually add reconnect code here */ <nl> - <nl> smb_msg . msg_name = ( struct sockaddr *) & server -> dstaddr ; <nl> smb_msg . msg_namelen = sizeof ( struct sockaddr ); <nl> smb_msg . msg_control = NULL ; <nl> smb_send_rqst ( struct TCP_Server_Info * server , struct smb_rqst * rqst ) <nl> struct socket * ssocket = server -> ssocket ; <nl> int val = 1 ; <nl>  <nl> + if ( ssocket == NULL ) <nl> + return - ENOTSOCK ; <nl> + <nl> cFYI ( 1 , " Sending smb : smb_len =% u ", smb_buf_length ); <nl> dump_smb ( iov [ 0 ]. iov_base , iov [ 0 ]. iov_len ); <nl> 
static struct page * saveable_highmem_page ( struct zone * zone , unsigned long pfn ) <nl> PageReserved ( page )) <nl> return NULL ; <nl>  <nl> + if ( page_is_guard ( page )) <nl> + return NULL ; <nl> + <nl> return page ; <nl> } <nl>  <nl> static struct page * saveable_page ( struct zone * zone , unsigned long pfn ) <nl> && (! kernel_page_present ( page ) || pfn_is_nosave ( pfn ))) <nl> return NULL ; <nl>  <nl> + if ( page_is_guard ( page )) <nl> + return NULL ; <nl> + <nl> return page ; <nl> } <nl> 
static void i40e_link_event ( struct i40e_pf * pf ) <nl> { <nl> bool new_link , old_link ; <nl> struct i40e_vsi * vsi = pf -> vsi [ pf -> lan_vsi ]; <nl> + u8 new_link_speed , old_link_speed ; <nl>  <nl> /* set this to force the get_link_status call to refresh state */ <nl> pf -> hw . phy . get_link_info = true ; <nl>  <nl> old_link = ( pf -> hw . phy . link_info_old . link_info & I40E_AQ_LINK_UP ); <nl> new_link = i40e_get_link_status (& pf -> hw ); <nl> + old_link_speed = pf -> hw . phy . link_info_old . link_speed ; <nl> + new_link_speed = pf -> hw . phy . link_info . link_speed ; <nl>  <nl> if ( new_link == old_link && <nl> + new_link_speed == old_link_speed && <nl> ( test_bit ( __I40E_DOWN , & vsi -> state ) || <nl> new_link == netif_carrier_ok ( vsi -> netdev ))) <nl> return ;
static int device_list_add ( const char * path , <nl>  <nl> fs_devices = find_fsid ( disk_super -> fsid ); <nl> if (! fs_devices ) { <nl> - fs_devices = kmalloc ( sizeof (* fs_devices ), GFP_NOFS ); <nl> + fs_devices = kzalloc ( sizeof (* fs_devices ), GFP_NOFS ); <nl> if (! fs_devices ) <nl> return - ENOMEM ; <nl> INIT_LIST_HEAD (& fs_devices -> devices ); <nl> static int device_list_add ( const char * path , <nl> memcpy ( fs_devices -> fsid , disk_super -> fsid , BTRFS_FSID_SIZE ); <nl> fs_devices -> latest_devid = devid ; <nl> fs_devices -> latest_trans = found_transid ; <nl> - fs_devices -> num_devices = 0 ; <nl> device = NULL ; <nl> } else { <nl> device = __find_device (& fs_devices -> devices , devid ,
static int xgbe_map_tx_skb ( struct xgbe_channel * channel , struct sk_buff * skb ) <nl> } <nl> } <nl>  <nl> - /* Save the skb address in the last entry */ <nl> + /* Save the skb address in the last entry . We always have some data <nl> + * that has been mapped so rdata is always advanced past the last <nl> + * piece of mapped data - use the entry pointed to by cur_index - 1 . <nl> + */ <nl> + rdata = XGBE_GET_DESC_DATA ( ring , cur_index - 1 ); <nl> rdata -> skb = skb ; <nl>  <nl> /* Save the number of descriptor entries used */
void __init check_wait ( void ) <nl> case CPU_BCM6358 : <nl> case CPU_CAVIUM_OCTEON : <nl> case CPU_CAVIUM_OCTEON_PLUS : <nl> + case CPU_CAVIUM_OCTEON2 : <nl> case CPU_JZRISC : <nl> cpu_wait = r4k_wait ; <nl> break ; <nl> static inline void cpu_probe_cavium ( struct cpuinfo_mips * c , unsigned int cpu ) <nl> if ( cpu == 0 ) <nl> __elf_platform = " octeon "; <nl> break ; <nl> + case PRID_IMP_CAVIUM_CN63XX : <nl> + c -> cputype = CPU_CAVIUM_OCTEON2 ; <nl> + __cpu_name [ cpu ] = " Cavium Octeon II "; <nl> + if ( cpu == 0 ) <nl> + __elf_platform = " octeon2 "; <nl> + break ; <nl> default : <nl> printk ( KERN_INFO " Unknown Octeon chip !\ n "); <nl> c -> cputype = CPU_UNKNOWN ;
int fib_nl_newrule ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl> if ( err < 0 ) <nl> goto errout_free ; <nl>  <nl> + err = call_fib_rule_notifiers ( net , FIB_EVENT_RULE_ADD , rule , ops , <nl> + extack ); <nl> + if ( err < 0 ) <nl> + goto errout_free ; <nl> + <nl> list_for_each_entry ( r , & ops -> rules_list , list ) { <nl> if ( r -> pref > rule -> pref ) <nl> break ; <nl> int fib_nl_newrule ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl> if ( rule -> tun_id ) <nl> ip_tunnel_need_metadata (); <nl>  <nl> - call_fib_rule_notifiers ( net , FIB_EVENT_RULE_ADD , rule , ops , extack ); <nl> notify_rule_change ( RTM_NEWRULE , rule , ops , nlh , NETLINK_CB ( skb ). portid ); <nl> flush_route_cache ( ops ); <nl> rules_ops_put ( ops );
static int perl_start_script ( const char * script , int argc , const char ** argv ) <nl> goto error ; <nl> } <nl>  <nl> - perl_run ( my_perl ); <nl> + if ( perl_run ( my_perl )) { <nl> + err = - 1 ; <nl> + goto error ; <nl> + } <nl> + <nl> if ( SvTRUE ( ERRSV )) { <nl> err = - 1 ; <nl> goto error ;
static int qlcnic_82xx_setup_intr ( struct qlcnic_adapter * adapter ) <nl> qlcnic_disable_multi_tx ( adapter ); <nl>  <nl> err = qlcnic_enable_msi_legacy ( adapter ); <nl> - if (! err ) <nl> + if ( err ) <nl> return err ; <nl> } <nl> }
void nfs_inode_reclaim_delegation ( struct inode * inode , struct rpc_cred * cred , <nl> clear_bit ( NFS_DELEGATION_NEED_RECLAIM , <nl> & delegation -> flags ); <nl> spin_unlock (& delegation -> lock ); <nl> - put_rpccred ( oldcred ); <nl> rcu_read_unlock (); <nl> + put_rpccred ( oldcred ); <nl> trace_nfs4_reclaim_delegation ( inode , res -> delegation_type ); <nl> } else { <nl> /* We appear to have raced with a delegation return . */
xfs_alloc_ag_vextent_small ( <nl>  <nl> bp = xfs_btree_get_bufs ( args -> mp , args -> tp , <nl> args -> agno , fbno , 0 ); <nl> + if (! bp ) { <nl> + error = - EFSCORRUPTED ; <nl> + goto error0 ; <nl> + } <nl> xfs_trans_binval ( args -> tp , bp ); <nl> } <nl> args -> len = 1 ; <nl> xfs_alloc_fix_freelist ( <nl> if ( error ) <nl> goto out_agbp_relse ; <nl> bp = xfs_btree_get_bufs ( mp , tp , args -> agno , bno , 0 ); <nl> + if (! bp ) { <nl> + error = - EFSCORRUPTED ; <nl> + goto out_agbp_relse ; <nl> + } <nl> xfs_trans_binval ( tp , bp ); <nl> } <nl> 
vxge_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> if ( unlikely ( skb -> len <= 0 )) { <nl> vxge_debug_tx ( VXGE_ERR , <nl> "% s : Buffer has no data ..", dev -> name ); <nl> - dev_kfree_skb ( skb ); <nl> + dev_kfree_skb_any ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl>  <nl> vxge_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> if ( unlikely (! is_vxge_card_up ( vdev ))) { <nl> vxge_debug_tx ( VXGE_ERR , <nl> "% s : vdev not initialized ", dev -> name ); <nl> - dev_kfree_skb ( skb ); <nl> + dev_kfree_skb_any ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl>  <nl> vxge_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> vxge_debug_tx ( VXGE_ERR , <nl> "% s : Failed to store the mac address ", <nl> dev -> name ); <nl> - dev_kfree_skb ( skb ); <nl> + dev_kfree_skb_any ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl> } <nl> vxge_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> vxge_hw_fifo_txdl_free ( fifo_hw , dtr ); <nl> _exit0 : <nl> netif_tx_stop_queue ( fifo -> txq ); <nl> - dev_kfree_skb ( skb ); <nl> + dev_kfree_skb_any ( skb ); <nl>  <nl> return NETDEV_TX_OK ; <nl> }
temac_start_xmit ( struct sk_buff * skb , struct net_device * ndev ) <nl> smp_mb (); <nl>  <nl> /* Space might have just been freed - check again */ <nl> - if ( temac_check_tx_bd_space ( lp , num_frag )) <nl> + if ( temac_check_tx_bd_space ( lp , num_frag + 1 )) <nl> return NETDEV_TX_BUSY ; <nl>  <nl> netif_wake_queue ( ndev );
static int ethtool_get_eeprom ( struct net_device * dev , void __user * useraddr ) <nl> bytes_remaining -= eeprom . len ; <nl> } <nl>  <nl> + eeprom . len = userbuf - ( useraddr + sizeof ( eeprom )); <nl> + eeprom . offset -= eeprom . len ; <nl> + if ( copy_to_user ( useraddr , & eeprom , sizeof ( eeprom ))) <nl> + ret = - EFAULT ; <nl> + <nl> kfree ( data ); <nl> return ret ; <nl> }
static int __init qeth_core_init ( void ) <nl> mutex_init (& qeth_mod_mutex ); <nl>  <nl> qeth_wq = create_singlethread_workqueue (" qeth_wq "); <nl> + if (! qeth_wq ) { <nl> + rc = - ENOMEM ; <nl> + goto out_err ; <nl> + } <nl>  <nl> rc = qeth_register_dbf_views (); <nl> if ( rc ) <nl> - goto out_err ; <nl> + goto dbf_err ; <nl> qeth_core_root_dev = root_device_register (" qeth "); <nl> rc = PTR_ERR_OR_ZERO ( qeth_core_root_dev ); <nl> if ( rc ) <nl> static int __init qeth_core_init ( void ) <nl> root_device_unregister ( qeth_core_root_dev ); <nl> register_err : <nl> qeth_unregister_dbf_views (); <nl> + dbf_err : <nl> + destroy_workqueue ( qeth_wq ); <nl> out_err : <nl> pr_err (" Initializing the qeth device driver failed \ n "); <nl> return rc ;
static int hidp_send_report ( struct hidp_session * session , struct hid_report * rep <nl> return hidp_queue_report ( session , buf , rsize ); <nl> } <nl>  <nl> + static int hidp_output_raw_report ( struct hid_device * hid , unsigned char * data , size_t count ) <nl> +{ <nl> + if ( hidp_queue_report ( hid -> driver_data , data , count )) <nl> + return - ENOMEM ; <nl> + return count ; <nl> +} <nl> + <nl> static void hidp_idle_timeout ( unsigned long arg ) <nl> { <nl> struct hidp_session * session = ( struct hidp_session *) arg ; <nl> static int hidp_setup_hid ( struct hidp_session * session , <nl> hid -> dev . parent = hidp_get_device ( session ); <nl> hid -> ll_driver = & hidp_hid_driver ; <nl>  <nl> + hid -> hid_output_raw_report = hidp_output_raw_report ; <nl> + <nl> err = hid_add_device ( hid ); <nl> if ( err < 0 ) <nl> goto failed ;
static int __devexit wm8753_spi_remove ( struct spi_device * spi ) <nl>  <nl> snd_soc_unregister_codec (& spi -> dev ); <nl> regmap_exit ( wm8753 -> regmap ); <nl> - kfree ( wm8753 ); <nl> return 0 ; <nl> } <nl> 
int hns_roce_uar_alloc ( struct hns_roce_dev * hr_dev , struct hns_roce_uar * uar ) <nl> ( hr_dev -> caps . phy_num_uars - 1 ) + 1 ; <nl>  <nl> res = platform_get_resource ( hr_dev -> pdev , IORESOURCE_MEM , 0 ); <nl> + if (! res ) { <nl> + dev_err (& hr_dev -> pdev -> dev , " memory resource not found !\ n "); <nl> + return - EINVAL ; <nl> + } <nl> uar -> pfn = (( res -> start ) >> PAGE_SHIFT ) + uar -> index ; <nl>  <nl> return 0 ;
int vmw_du_crtc_cursor_set ( struct drm_crtc * crtc , struct drm_file * file_priv , <nl> if (! ret ) { <nl> if (! surface -> snooper . image ) { <nl> DRM_ERROR (" surface not suitable for cursor \ n "); <nl> + vmw_surface_unreference (& surface ); <nl> return - EINVAL ; <nl> } <nl> } else {
void ceph_check_caps ( struct ceph_inode_info * ci , int flags , <nl>  <nl> if ( cap == ci -> i_auth_cap && ci -> i_dirty_caps ) <nl> flushing = __mark_caps_flushing ( inode , session ); <nl> + else <nl> + flushing = 0 ; <nl>  <nl> mds = cap -> mds ; /* remember mds , so we don ' t repeat */ <nl> sent ++;
static int i2c_mux_reg_probe_dt ( struct regmux * mux , <nl> mux -> data . idle_in_use = true ; <nl>  <nl> /* map address from " reg " if exists */ <nl> - if ( of_address_to_resource ( np , 0 , & res )) { <nl> + if ( of_address_to_resource ( np , 0 , & res ) == 0 ) { <nl> mux -> data . reg_size = resource_size (& res ); <nl> mux -> data . reg = devm_ioremap_resource (& pdev -> dev , & res ); <nl> if ( IS_ERR ( mux -> data . reg ))
int rhashtable_walk_start_check ( struct rhashtable_iter * iter ) <nl> skip ++; <nl> if ( list == iter -> list ) { <nl> iter -> p = p ; <nl> - skip = skip ; <nl> + iter -> skip = skip ; <nl> goto found ; <nl> } <nl> }
static int s5p64x0_alloc_gc ( void ) <nl> } <nl>  <nl> ct = gc -> chip_types ; <nl> - ct -> chip . irq_ack = irq_gc_ack ; <nl> + ct -> chip . irq_ack = irq_gc_ack_set_bit ; <nl> ct -> chip . irq_mask = irq_gc_mask_set_bit ; <nl> ct -> chip . irq_unmask = irq_gc_mask_clr_bit ; <nl> ct -> chip . irq_set_type = s5p64x0_irq_eint_set_type ;
static int kvm_events_hash_fn ( u64 key ) <nl> static bool kvm_event_expand ( struct kvm_event * event , int vcpu_id ) <nl> { <nl> int old_max_vcpu = event -> max_vcpu ; <nl> + void * prev ; <nl>  <nl> if ( vcpu_id < event -> max_vcpu ) <nl> return true ; <nl> static bool kvm_event_expand ( struct kvm_event * event , int vcpu_id ) <nl> while ( event -> max_vcpu <= vcpu_id ) <nl> event -> max_vcpu += DEFAULT_VCPU_NUM ; <nl>  <nl> + prev = event -> vcpu ; <nl> event -> vcpu = realloc ( event -> vcpu , <nl> event -> max_vcpu * sizeof (* event -> vcpu )); <nl> if (! event -> vcpu ) { <nl> + free ( prev ); <nl> pr_err (" Not enough memory \ n "); <nl> return false ; <nl> }
static int rmnet_newlink ( struct net * src_net , struct net_device * dev , <nl> err1 : <nl> rmnet_unregister_real_device ( real_dev , port ); <nl> err0 : <nl> + kfree ( ep ); <nl> return err ; <nl> } <nl> 
static int read_mii_word ( rtl8150_t * dev , u8 phy , __u8 indx , u16 * reg ) <nl> get_registers ( dev , PHYCNT , 1 , data ); <nl> } while (( data [ 0 ] & PHY_GO ) && ( i ++ < MII_TIMEOUT )); <nl>  <nl> - if ( i < MII_TIMEOUT ) { <nl> + if ( i <= MII_TIMEOUT ) { <nl> get_registers ( dev , PHYDAT , 2 , data ); <nl> * reg = data [ 0 ] | ( data [ 1 ] << 8 ); <nl> return 0 ; <nl> static int write_mii_word ( rtl8150_t * dev , u8 phy , __u8 indx , u16 reg ) <nl> get_registers ( dev , PHYCNT , 1 , data ); <nl> } while (( data [ 0 ] & PHY_GO ) && ( i ++ < MII_TIMEOUT )); <nl>  <nl> - if ( i < MII_TIMEOUT ) <nl> + if ( i <= MII_TIMEOUT ) <nl> return 0 ; <nl> else <nl> return 1 ;
static int crypto_authenc_verify ( struct aead_request * req , <nl> unsigned int authsize ; <nl>  <nl> areq_ctx -> complete = authenc_verify_ahash_done ; <nl> - areq_ctx -> complete = authenc_verify_ahash_update_done ; <nl> + areq_ctx -> update_complete = authenc_verify_ahash_update_done ; <nl>  <nl> ohash = authenc_ahash_fn ( req , CRYPTO_TFM_REQ_MAY_SLEEP ); <nl> if ( IS_ERR ( ohash ))
int radeon_cs_parser_init ( struct radeon_cs_parser * p , void * data ) <nl> cdata = ( uint32_t *)( unsigned long ) user_chunk . chunk_data ; <nl>  <nl> size = p -> chunks [ i ]. length_dw * sizeof ( uint32_t ); <nl> - p -> chunks [ i ]. kdata = kzalloc ( size , GFP_KERNEL ); <nl> + p -> chunks [ i ]. kdata = kmalloc ( size , GFP_KERNEL ); <nl> if ( p -> chunks [ i ]. kdata == NULL ) { <nl> return - ENOMEM ; <nl> }
static inline void debugfs_remove_domain_dir ( struct irq_domain * d ) { } <nl> # endif <nl>  <nl> const struct fwnode_operations irqchip_fwnode_ops ; <nl> + EXPORT_SYMBOL_GPL ( irqchip_fwnode_ops ); <nl>  <nl> /** <nl> * irq_domain_alloc_fwnode - Allocate a fwnode_handle suitable for
static void imx_pinconf_group_dbg_show ( struct pinctrl_dev * pctldev , <nl> const char * name ; <nl> int i , ret ; <nl>  <nl> - if ( group > pctldev -> num_groups ) <nl> + if ( group >= pctldev -> num_groups ) <nl> return ; <nl>  <nl> seq_puts ( s , "\ n ");
static int cachefiles_mark_object_active ( struct cachefiles_cache * cache , <nl> /* an old object from a previous incarnation is hogging the slot - we <nl> * need to wait for it to be destroyed */ <nl> wait_for_old_object : <nl> - if ( fscache_object_is_live (& object -> fscache )) { <nl> + if ( fscache_object_is_live (& xobject -> fscache )) { <nl> pr_err ("\ n "); <nl> pr_err (" Error : Unexpected object collision \ n "); <nl> cachefiles_printk_object ( object , xobject );
brcmf_c_pktfilter_offload_enable ( struct brcmf_pub * drvr , char * arg , int enable , <nl> const char * str ; <nl> int buf_len ; <nl> int str_len ; <nl> - char * arg_save = 0 , * arg_org = 0 ; <nl> + char * arg_save = NULL , * arg_org = NULL ; <nl> int rc ; <nl> char buf [ 128 ]; <nl> struct brcmf_pkt_filter_enable enable_parm ; <nl> void brcmf_c_pktfilter_offload_set ( struct brcmf_pub * drvr , char * arg ) <nl> int rc ; <nl> u32 mask_size ; <nl> u32 pattern_size ; <nl> - char * argv [ 8 ], * buf = 0 ; <nl> + char * argv [ 8 ], * buf = NULL ; <nl> int i = 0 ; <nl> - char * arg_save = 0 , * arg_org = 0 ; <nl> + char * arg_save = NULL , * arg_org = NULL ; <nl>  <nl> arg_save = kstrdup ( arg , GFP_ATOMIC ); <nl> if (! arg_save ) { <nl> int brcmf_c_preinit_ioctls ( struct brcmf_pub * drvr ) <nl> /* query for ' ver ' to get version info from firmware */ <nl> memset ( buf , 0 , sizeof ( buf )); <nl> ptr = buf ; <nl> - brcmu_mkiovar (" ver ", 0 , 0 , buf , sizeof ( buf )); <nl> + brcmu_mkiovar (" ver ", NULL , 0 , buf , sizeof ( buf )); <nl> brcmf_proto_cdc_query_ioctl ( drvr , 0 , BRCMF_C_GET_VAR , buf , sizeof ( buf )); <nl> strsep (& ptr , "\ n "); <nl> /* Print fw version info */
void tipc_mon_delete ( struct net * net , int bearer_id ) <nl> { <nl> struct tipc_net * tn = tipc_net ( net ); <nl> struct tipc_monitor * mon = tipc_monitor ( net , bearer_id ); <nl> - struct tipc_peer * self = get_self ( net , bearer_id ); <nl> + struct tipc_peer * self ; <nl> struct tipc_peer * peer , * tmp ; <nl>  <nl> + if (! mon ) <nl> + return ; <nl> + <nl> + self = get_self ( net , bearer_id ); <nl> write_lock_bh (& mon -> lock ); <nl> tn -> monitors [ bearer_id ] = NULL ; <nl> list_for_each_entry_safe ( peer , tmp , & self -> list , list ) {
static ssize_t gt_max_freq_mhz_store ( struct device * kdev , <nl>  <nl> flush_delayed_work (& dev_priv -> rps . delayed_resume_work ); <nl>  <nl> + intel_runtime_pm_get ( dev_priv ); <nl> + <nl> mutex_lock (& dev_priv -> rps . hw_lock ); <nl>  <nl> val = intel_freq_opcode ( dev_priv , val ); <nl> static ssize_t gt_max_freq_mhz_store ( struct device * kdev , <nl> val > dev_priv -> rps . max_freq || <nl> val < dev_priv -> rps . min_freq_softlimit ) { <nl> mutex_unlock (& dev_priv -> rps . hw_lock ); <nl> + intel_runtime_pm_put ( dev_priv ); <nl> return - EINVAL ; <nl> } <nl>  <nl> static ssize_t gt_max_freq_mhz_store ( struct device * kdev , <nl>  <nl> mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> + intel_runtime_pm_put ( dev_priv ); <nl> + <nl> return count ; <nl> } <nl>  <nl> static ssize_t gt_min_freq_mhz_store ( struct device * kdev , <nl>  <nl> flush_delayed_work (& dev_priv -> rps . delayed_resume_work ); <nl>  <nl> + intel_runtime_pm_get ( dev_priv ); <nl> + <nl> mutex_lock (& dev_priv -> rps . hw_lock ); <nl>  <nl> val = intel_freq_opcode ( dev_priv , val ); <nl> static ssize_t gt_min_freq_mhz_store ( struct device * kdev , <nl> val > dev_priv -> rps . max_freq || <nl> val > dev_priv -> rps . max_freq_softlimit ) { <nl> mutex_unlock (& dev_priv -> rps . hw_lock ); <nl> + intel_runtime_pm_put ( dev_priv ); <nl> return - EINVAL ; <nl> } <nl>  <nl> static ssize_t gt_min_freq_mhz_store ( struct device * kdev , <nl>  <nl> mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> + intel_runtime_pm_put ( dev_priv ); <nl> + <nl> return count ; <nl>  <nl> }
static void udf_sb_free_partitions ( struct super_block * sb ) <nl> { <nl> struct udf_sb_info * sbi = UDF_SB ( sb ); <nl> int i ; <nl> - <nl> + if ( sbi -> s_partmaps == NULL ) <nl> + return ; <nl> for ( i = 0 ; i < sbi -> s_partitions ; i ++) <nl> udf_free_partition (& sbi -> s_partmaps [ i ]); <nl> kfree ( sbi -> s_partmaps );
int dw_hdmi_probe ( struct platform_device * pdev , <nl> const struct dw_hdmi_plat_data * plat_data ) <nl> { <nl> struct dw_hdmi * hdmi ; <nl> - int ret ; <nl>  <nl> hdmi = __dw_hdmi_probe ( pdev , plat_data ); <nl> if ( IS_ERR ( hdmi )) <nl> return PTR_ERR ( hdmi ); <nl>  <nl> - ret = drm_bridge_add (& hdmi -> bridge ); <nl> - if ( ret < 0 ) { <nl> - __dw_hdmi_remove ( hdmi ); <nl> - return ret ; <nl> - } <nl> + drm_bridge_add (& hdmi -> bridge ); <nl>  <nl> return 0 ; <nl> }
static int max8973_probe ( struct i2c_client * client , <nl> } <nl>  <nl> if ( pdata ) { <nl> - max -> dvs_gpio = pdata -> dvs_gpio ; <nl> + max -> dvs_gpio = ( pdata -> dvs_gpio ) ? pdata -> dvs_gpio : - EINVAL ; <nl> max -> enable_external_control = pdata -> enable_ext_control ; <nl> max -> curr_gpio_val = pdata -> dvs_def_state ; <nl> max -> curr_vout_reg = MAX8973_VOUT + pdata -> dvs_def_state ;
extern int vdso_enabled ; <nl>  <nl> # endif /* ! CONFIG_X86_32 */ <nl>  <nl> +# define CORE_DUMP_USE_REGSET <nl> # define USE_ELF_CORE_DUMP <nl> # define ELF_EXEC_PAGESIZE 4096 <nl> 
static int aac_send_raw_srb ( struct aac_dev * dev , void __user * arg ) <nl> goto cleanup ; <nl> } <nl>  <nl> - user_srbcmd = kmalloc ( GFP_KERNEL , fibsize ); <nl> + user_srbcmd = kmalloc ( fibsize , GFP_KERNEL ); <nl> if (! user_srbcmd ) { <nl> dprintk (( KERN_DEBUG " aacraid : Could not make a copy of the srb \ n ")); <nl> rcode = - ENOMEM ;
static int rtl2832u_tuner_attach ( struct dvb_usb_adapter * adap ) <nl> struct i2c_board_info info ; <nl> struct i2c_client * client ; <nl> struct v4l2_subdev * subdev = NULL ; <nl> + struct platform_device * pdev ; <nl> + struct rtl2832_sdr_platform_data pdata ; <nl>  <nl> dev_dbg (& d -> intf -> dev , "\ n "); <nl>  <nl> memset (& info , 0 , sizeof ( struct i2c_board_info )); <nl> + memset (& pdata , 0 , sizeof ( pdata )); <nl>  <nl> switch ( dev -> tuner ) { <nl> case TUNER_RTL2832_FC0012 : <nl> static int rtl2832u_tuner_attach ( struct dvb_usb_adapter * adap ) <nl>  <nl> /* register SDR */ <nl> switch ( dev -> tuner ) { <nl> - struct platform_device * pdev ; <nl> - struct rtl2832_sdr_platform_data pdata = {}; <nl> - <nl> case TUNER_RTL2832_FC0012 : <nl> case TUNER_RTL2832_FC0013 : <nl> case TUNER_RTL2832_E4000 :
int cfg80211_validate_key_settings ( struct cfg80211_registered_device * rdev , <nl> struct key_params * params , int key_idx , <nl> bool pairwise , const u8 * mac_addr ) <nl> { <nl> - if ( key_idx > 5 ) <nl> + if ( key_idx < 0 || key_idx > 5 ) <nl> return - EINVAL ; <nl>  <nl> if (! pairwise && mac_addr && !( rdev -> wiphy . flags & WIPHY_FLAG_IBSS_RSN )) <nl> int cfg80211_validate_key_settings ( struct cfg80211_registered_device * rdev , <nl> /* Disallow BIP ( group - only ) cipher as pairwise cipher */ <nl> if ( pairwise ) <nl> return - EINVAL ; <nl> + if ( key_idx < 4 ) <nl> + return - EINVAL ; <nl> break ; <nl> + case WLAN_CIPHER_SUITE_WEP40 : <nl> + case WLAN_CIPHER_SUITE_WEP104 : <nl> + if ( key_idx > 3 ) <nl> + return - EINVAL ; <nl> default : <nl> break ; <nl> }
static noinline int btrfs_ioctl_resize ( struct btrfs_root * root , <nl> } <nl> ret = btrfs_grow_device ( trans , device , new_size ); <nl> btrfs_commit_transaction ( trans , root ); <nl> - } else { <nl> + } else if ( new_size < old_size ) { <nl> ret = btrfs_shrink_device ( device , new_size ); <nl> } <nl> 
module_init ( rif_init ); <nl>  <nl> EXPORT_SYMBOL ( tr_type_trans ); <nl> EXPORT_SYMBOL ( alloc_trdev ); <nl> + <nl> + MODULE_LICENSE (" GPL ");
long drm_ioctl ( struct file * filp , <nl> retcode = - EFAULT ; <nl> goto err_i1 ; <nl> } <nl> - } <nl> + } else <nl> + memset ( kdata , 0 , _IOC_SIZE ( cmd )); <nl> + <nl> if ( ioctl -> flags & DRM_UNLOCKED ) <nl> retcode = func ( dev , kdata , file_priv ); <nl> else {
int perf_cpu_time_max_percent_handler ( struct ctl_table * table , int write , <nl> void __user * buffer , size_t * lenp , <nl> loff_t * ppos ) <nl> { <nl> - int ret = proc_dointvec ( table , write , buffer , lenp , ppos ); <nl> + int ret = proc_dointvec_minmax ( table , write , buffer , lenp , ppos ); <nl>  <nl> if ( ret || ! write ) <nl> return ret ;
static int parse_tc_fdb_actions ( struct mlx5e_priv * priv , struct tcf_exts * exts , <nl> if ( tcf_vlan_action ( a ) == TCA_VLAN_ACT_POP ) { <nl> attr -> action |= MLX5_FLOW_CONTEXT_ACTION_VLAN_POP ; <nl> } else if ( tcf_vlan_action ( a ) == TCA_VLAN_ACT_PUSH ) { <nl> - if ( tcf_vlan_push_proto ( a ) != htons ( ETH_P_8021Q )) <nl> + if ( tcf_vlan_push_proto ( a ) != htons ( ETH_P_8021Q ) || <nl> + tcf_vlan_push_prio ( a )) <nl> return - EOPNOTSUPP ; <nl>  <nl> attr -> action |= MLX5_FLOW_CONTEXT_ACTION_VLAN_PUSH ;
static int kvm_set_spte_handler ( struct kvm * kvm , gpa_t gpa , u64 size , void * data <nl> void kvm_set_spte_hva ( struct kvm * kvm , unsigned long hva , pte_t pte ) <nl> { <nl> unsigned long end = hva + PAGE_SIZE ; <nl> + kvm_pfn_t pfn = pte_pfn ( pte ); <nl> pte_t stage2_pte ; <nl>  <nl> if (! kvm -> arch . pgd ) <nl> return ; <nl>  <nl> trace_kvm_set_spte_hva ( hva ); <nl> - stage2_pte = pfn_pte ( pte_pfn ( pte ), PAGE_S2 ); <nl> + <nl> + /* <nl> + * We ' ve moved a page around , probably through CoW , so let ' s treat it <nl> + * just like a translation fault and clean the cache to the PoC . <nl> + */ <nl> + clean_dcache_guest_page ( pfn , PAGE_SIZE ); <nl> + stage2_pte = pfn_pte ( pfn , PAGE_S2 ); <nl> handle_hva_to_gpa ( kvm , hva , end , & kvm_set_spte_handler , & stage2_pte ); <nl> } <nl> 
static int mmci_probe ( struct amba_device * dev , <nl> dev_dbg ( mmc_dev ( mmc ), " clocking block at % u Hz \ n ", mmc -> f_max ); <nl>  <nl> /* Get regulators and the supported OCR mask */ <nl> - mmc_regulator_get_supply ( mmc ); <nl> + ret = mmc_regulator_get_supply ( mmc ); <nl> + if ( ret == - EPROBE_DEFER ) <nl> + goto clk_disable ; <nl> + <nl> if (! mmc -> ocr_avail ) <nl> mmc -> ocr_avail = plat -> ocr_mask ; <nl> else if ( plat -> ocr_mask )
static void hpfs_write_failed ( struct address_space * mapping , loff_t to ) <nl> { <nl> struct inode * inode = mapping -> host ; <nl>  <nl> + hpfs_lock ( inode -> i_sb ); <nl> + <nl> if ( to > inode -> i_size ) { <nl> truncate_pagecache ( inode , to , inode -> i_size ); <nl> hpfs_truncate ( inode ); <nl> } <nl> + <nl> + hpfs_unlock ( inode -> i_sb ); <nl> } <nl>  <nl> static int hpfs_write_begin ( struct file * file , struct address_space * mapping ,
int sk_convert_filter ( struct sock_filter * prog , int len , <nl> BUILD_BUG_ON ( BPF_MEMWORDS * sizeof ( u32 ) > MAX_BPF_STACK ); <nl> BUILD_BUG_ON ( BPF_REG_FP + 1 != MAX_BPF_REG ); <nl>  <nl> - if ( len <= 0 || len >= BPF_MAXINSNS ) <nl> + if ( len <= 0 || len > BPF_MAXINSNS ) <nl> return - EINVAL ; <nl>  <nl> if ( new_prog ) {
static int __tipc_sendmsg ( struct socket * sock , struct msghdr * m , size_t dlen ) <nl> msg_set_syn ( hdr , 1 ); <nl> } <nl>  <nl> + memset (& skaddr , 0 , sizeof ( skaddr )); <nl> + <nl> /* Determine destination */ <nl> if ( atype == TIPC_SERVICE_RANGE ) { <nl> return tipc_sendmcast ( sock , ua , m , dlen , timeout );
struct device * driver_find_device ( struct device_driver * drv , <nl> struct klist_iter i ; <nl> struct device * dev ; <nl>  <nl> - if (! drv ) <nl> + if (! drv || ! drv -> p ) <nl> return NULL ; <nl>  <nl> klist_iter_init_node (& drv -> p -> klist_devices , & i ,
typedef __s64 int64_t ; <nl> # endif <nl>  <nl> /* this is a special 64bit data type that is 8 - byte aligned */ <nl> -# define aligned_u64 unsigned long long __attribute__ (( aligned ( 8 ))) <nl> +# define aligned_u64 __u64 __attribute__ (( aligned ( 8 ))) <nl> # define aligned_be64 __be64 __attribute__ (( aligned ( 8 ))) <nl> # define aligned_le64 __le64 __attribute__ (( aligned ( 8 ))) <nl> 
static int replace_map_fd_with_map_ptr ( struct verifier_env * env ) <nl> if ( IS_ERR ( map )) { <nl> verbose (" fd % d is not pointing to valid bpf_map \ n ", <nl> insn -> imm ); <nl> - fdput ( f ); <nl> return PTR_ERR ( map ); <nl> } <nl> 
int fib_nh_match ( struct fib_config * cfg , struct fib_info * fi , <nl> fi -> fib_nh , cfg , extack )) <nl> return 1 ; <nl> } <nl> +# ifdef CONFIG_IP_ROUTE_CLASSID <nl> + if ( cfg -> fc_flow && <nl> + cfg -> fc_flow != fi -> fib_nh -> nh_tclassid ) <nl> + return 1 ; <nl> +# endif <nl> if ((! cfg -> fc_oif || cfg -> fc_oif == fi -> fib_nh -> nh_oif ) && <nl> (! cfg -> fc_gw || cfg -> fc_gw == fi -> fib_nh -> nh_gw )) <nl> return 0 ;
int ide_register_hw ( hw_regs_t * hw , void (* quirkproc )( ide_drive_t *), <nl>  <nl> do { <nl> hwif = ide_deprecated_find_port ( hw -> io_ports [ IDE_DATA_OFFSET ]); <nl> - index = hwif -> index ; <nl> if ( hwif ) <nl> goto found ; <nl> for ( index = 0 ; index < MAX_HWIFS ; index ++) <nl> int ide_register_hw ( hw_regs_t * hw , void (* quirkproc )( ide_drive_t *), <nl> } while ( retry --); <nl> return - 1 ; <nl> found : <nl> + index = hwif -> index ; <nl> if ( hwif -> present ) <nl> ide_unregister ( index , 0 , 1 ); <nl> else if (! hwif -> hold )
static inline void hci_pin_code_request_evt ( struct hci_dev * hdev , struct sk_buff <nl> hci_dev_lock ( hdev ); <nl>  <nl> conn = hci_conn_hash_lookup_ba ( hdev , ACL_LINK , & ev -> bdaddr ); <nl> - if ( conn && conn -> state == BT_CONNECTED ) { <nl> + if (! conn ) <nl> + goto unlock ; <nl> + <nl> + if ( conn -> state == BT_CONNECTED ) { <nl> hci_conn_hold ( conn ); <nl> conn -> disc_timeout = HCI_PAIRING_TIMEOUT ; <nl> hci_conn_put ( conn ); <nl> static inline void hci_pin_code_request_evt ( struct hci_dev * hdev , struct sk_buff <nl> mgmt_pin_code_request ( hdev -> id , & ev -> bdaddr , secure ); <nl> } <nl>  <nl> + unlock : <nl> hci_dev_unlock ( hdev ); <nl> } <nl> 
static int handle_conflicting_encoders ( struct drm_atomic_state * state , <nl>  <nl> if ( funcs -> atomic_best_encoder ) <nl> new_encoder = funcs -> atomic_best_encoder ( connector , conn_state ); <nl> - else <nl> + else if ( funcs -> best_encoder ) <nl> new_encoder = funcs -> best_encoder ( connector ); <nl> + else <nl> + new_encoder = drm_atomic_helper_best_encoder ( connector ); <nl>  <nl> if ( new_encoder ) { <nl> if ( encoder_mask & ( 1 << drm_encoder_index ( new_encoder ))) {
SMB2_negotiate ( const unsigned int xid , struct cifs_ses * ses ) <nl> } else if ( rsp -> DialectRevision == cpu_to_le16 ( SMB21_PROT_ID )) { <nl> /* ops set to 3 . 0 by default for default so update */ <nl> ses -> server -> ops = & smb21_operations ; <nl> - } else if ( rsp -> DialectRevision == cpu_to_le16 ( SMB311_PROT_ID )) <nl> + ses -> server -> vals = & smb21_values ; <nl> + } else if ( rsp -> DialectRevision == cpu_to_le16 ( SMB311_PROT_ID )) { <nl> ses -> server -> ops = & smb311_operations ; <nl> + ses -> server -> vals = & smb311_values ; <nl> + } <nl> } else if ( le16_to_cpu ( rsp -> DialectRevision ) != <nl> ses -> server -> vals -> protocol_id ) { <nl> /* if requested single dialect ensure returned dialect matched */
int vt_do_kdskled ( int console , int cmd , unsigned long arg , int perm ) <nl> kbd -> default_ledflagstate = (( arg >> 4 ) & 7 ); <nl> set_leds (); <nl> spin_unlock_irqrestore (& kbd_event_lock , flags ); <nl> - break ; <nl> + return 0 ; <nl>  <nl> /* the ioctls below only set the lights , not the functions */ <nl> /* for those , see KDGKBLED and KDSKBLED above */
static const struct ieee80211_rate hwsim_rates [] = { <nl>  <nl> static spinlock_t hwsim_radio_lock ; <nl> static struct list_head hwsim_radios ; <nl> + static int hwsim_radio_idx ; <nl>  <nl> struct mac80211_hwsim_data { <nl> struct list_head list ; <nl> static const struct ieee80211_iface_combination hwsim_if_comb [] = { <nl> } <nl> }; <nl>  <nl> - static int __init mac80211_hwsim_create_radio ( int idx ) <nl> + static int __init mac80211_hwsim_create_radio ( void ) <nl> { <nl> int err ; <nl> u8 addr [ ETH_ALEN ]; <nl> static int __init mac80211_hwsim_create_radio ( int idx ) <nl> struct ieee80211_hw * hw ; <nl> enum ieee80211_band band ; <nl> const struct ieee80211_ops * ops = & mac80211_hwsim_ops ; <nl> + int idx ; <nl> + <nl> + spin_lock_bh (& hwsim_radio_lock ); <nl> + idx = hwsim_radio_idx ++; <nl> + spin_unlock_bh (& hwsim_radio_lock ); <nl>  <nl> if ( channels > 1 ) <nl> ops = & mac80211_hwsim_mchan_ops ; <nl> static int __init init_mac80211_hwsim ( void ) <nl> } <nl>  <nl> for ( i = 0 ; i < radios ; i ++) { <nl> - err = mac80211_hwsim_create_radio ( i ); <nl> + err = mac80211_hwsim_create_radio (); <nl> if ( err ) <nl> goto out_free_radios ; <nl> }
static int alc_mux_select ( struct hda_codec * codec , unsigned int adc_idx , <nl> int i , type , num_conns ; <nl> hda_nid_t nid ; <nl>  <nl> + if (! spec -> input_mux ) <nl> + return 0 ; <nl> + <nl> mux_idx = adc_idx >= spec -> num_mux_defs ? 0 : adc_idx ; <nl> imux = & spec -> input_mux [ mux_idx ]; <nl> if (! imux -> num_items && mux_idx > 0 )
static inline struct neigh_parms * lookup_neigh_parms ( struct neigh_table * tbl , <nl>  <nl> for ( p = & tbl -> parms ; p ; p = p -> next ) { <nl> if (( p -> dev && p -> dev -> ifindex == ifindex && net_eq ( neigh_parms_net ( p ), net )) || <nl> - (! p -> dev && ! ifindex )) <nl> + (! p -> dev && ! ifindex && net_eq ( net , & init_net ))) <nl> return p ; <nl> } <nl> 
static int do_proc_dointvec_jiffies_conv ( int * negp , unsigned long * lvalp , <nl> int write , void * data ) <nl> { <nl> if ( write ) { <nl> + if (* lvalp > LONG_MAX / HZ ) <nl> + return 1 ; <nl> * valp = * negp ? -(* lvalp * HZ ) : (* lvalp * HZ ); <nl> } else { <nl> int val = * valp ; <nl> static int do_proc_dointvec_userhz_jiffies_conv ( int * negp , unsigned long * lvalp , <nl> int write , void * data ) <nl> { <nl> if ( write ) { <nl> + if ( USER_HZ < HZ && * lvalp > ( LONG_MAX / HZ ) * USER_HZ ) <nl> + return 1 ; <nl> * valp = clock_t_to_jiffies (* negp ? -* lvalp : * lvalp ); <nl> } else { <nl> int val = * valp ;
enum ep_state { <nl> struct ep_data { <nl> struct mutex lock ; <nl> enum ep_state state ; <nl> - atomic_t count ; <nl> + refcount_t count ; <nl> struct dev_data * dev ; <nl> /* must hold dev -> lock before accessing ep or req */ <nl> struct usb_ep * ep ; <nl> struct ep_data { <nl>  <nl> static inline void get_ep ( struct ep_data * data ) <nl> { <nl> - atomic_inc (& data -> count ); <nl> + refcount_inc (& data -> count ); <nl> } <nl>  <nl> static void put_ep ( struct ep_data * data ) <nl> { <nl> - if ( likely (! atomic_dec_and_test (& data -> count ))) <nl> + if ( likely (! refcount_dec_and_test (& data -> count ))) <nl> return ; <nl> put_dev ( data -> dev ); <nl> /* needs no more cleanup */ <nl> static int activate_ep_files ( struct dev_data * dev ) <nl> init_waitqueue_head (& data -> wait ); <nl>  <nl> strncpy ( data -> name , ep -> name , sizeof ( data -> name ) - 1 ); <nl> - atomic_set (& data -> count , 1 ); <nl> + refcount_set (& data -> count , 1 ); <nl> data -> dev = dev ; <nl> get_dev ( dev ); <nl> 
static noinline struct module * load_module ( void __user * umod , <nl> free_unload : <nl> module_unload_free ( mod ); <nl> # if defined ( CONFIG_MODULE_UNLOAD ) && defined ( CONFIG_SMP ) <nl> - free_init : <nl> percpu_modfree ( mod -> refptr ); <nl> + free_init : <nl> # endif <nl> module_free ( mod , mod -> module_init ); <nl> free_core :
static int clk_fetch_parent_index ( struct clk_core * core , <nl> { <nl> int i ; <nl>  <nl> + if (! parent ) <nl> + return - EINVAL ; <nl> + <nl> /* <nl> * find index of new parent clock using cached parent ptrs , <nl> * or if not yet cached , use string name comparison and cache
static void * etm_setup_aux ( int event_cpu , void ** pages , <nl> if (! sink_ops ( sink )-> alloc_buffer ) <nl> goto err ; <nl>  <nl> + cpu = cpumask_first ( mask ); <nl> /* Get the AUX specific data from the sink buffer */ <nl> event_data -> snk_config = <nl> sink_ops ( sink )-> alloc_buffer ( sink , cpu , pages ,
void drbd_start_resync ( struct drbd_device * device , enum drbd_conns side ) <nl> return ; <nl> } <nl>  <nl> + if (! connection ) { <nl> + drbd_err ( device , " No connection to peer , aborting !\ n "); <nl> + return ; <nl> + } <nl> + <nl> if (! test_bit ( B_RS_H_DONE , & device -> flags )) { <nl> if ( side == C_SYNC_TARGET ) { <nl> /* Since application IO was locked out during C_WF_BITMAP_T and
static int mv_udc_get_frame ( struct usb_gadget * gadget ) <nl>  <nl> udc = container_of ( gadget , struct mv_udc , gadget ); <nl>  <nl> - retval = readl ( udc -> op_regs -> frindex ) & USB_FRINDEX_MASKS ; <nl> + retval = readl (& udc -> op_regs -> frindex ) & USB_FRINDEX_MASKS ; <nl>  <nl> return retval ; <nl> }
typhoon_request_firmware ( struct typhoon * tp ) <nl> err = - ENOMEM ; <nl> goto out_err ; <nl> } <nl> + memcpy ( typhoon_fw_image , typhoon_fw -> data , typhoon_fw -> size ); <nl>  <nl> return 0 ; <nl> 
static void ops_complete_reconstruct ( void * stripe_head_ref ) <nl> struct r5dev * dev = & sh -> dev [ i ]; <nl>  <nl> if ( dev -> written || i == pd_idx || i == qd_idx ) { <nl> - if (! discard && ! test_bit ( R5_SkipCopy , & dev -> flags )) <nl> + if (! discard && ! test_bit ( R5_SkipCopy , & dev -> flags )) { <nl> set_bit ( R5_UPTODATE , & dev -> flags ); <nl> + if ( test_bit ( STRIPE_EXPAND_READY , & sh -> state )) <nl> + set_bit ( R5_Expanded , & dev -> flags ); <nl> + } <nl> if ( fua ) <nl> set_bit ( R5_WantFUA , & dev -> flags ); <nl> if ( sync )
int mthca_alloc_srq ( struct mthca_dev * dev , struct mthca_pd * pd , <nl> srq -> first_free = 0 ; <nl> srq -> last_free = srq -> max - 1 ; <nl>  <nl> - attr -> max_wr = srq -> max ; <nl> + attr -> max_wr = ( mthca_is_memfree ( dev )) ? srq -> max - 1 : srq -> max ; <nl> attr -> max_sge = srq -> max_gs ; <nl>  <nl> return 0 ; <nl> int mthca_query_srq ( struct ib_srq * ibsrq , struct ib_srq_attr * srq_attr ) <nl> } else <nl> srq_attr -> srq_limit = 0 ; <nl>  <nl> - srq_attr -> max_wr = srq -> max ; <nl> + srq_attr -> max_wr = ( mthca_is_memfree ( dev )) ? srq -> max - 1 : srq -> max ; <nl> srq_attr -> max_sge = srq -> max_gs ; <nl>  <nl> out :
int bnxt_re_create_srq ( struct ib_srq * ib_srq , <nl> dev_err ( rdev_to_dev ( rdev ), " SRQ copy to udata failed !"); <nl> bnxt_qplib_destroy_srq (& rdev -> qplib_res , <nl> & srq -> qplib_srq ); <nl> - goto exit ; <nl> + goto fail ; <nl> } <nl> } <nl> if ( nq )
static void ixgbe_tx_map ( struct ixgbe_ring * tx_ring , <nl> tx_buffer_info -> dma = dma ; <nl>  <nl> tx_desc -> read . buffer_addr = cpu_to_le64 ( dma + offset ); <nl> + if ( unlikely ( skb -> no_fcs )) <nl> + cmd_type &= ~( cpu_to_le32 ( IXGBE_ADVTXD_DCMD_IFCS )); <nl> tx_desc -> read . cmd_type_len = cmd_type | cpu_to_le32 ( size ); <nl> tx_desc -> read . olinfo_status = olinfo_status ; <nl>  <nl> static int __devinit ixgbe_probe ( struct pci_dev * pdev , <nl> netdev -> vlan_features |= NETIF_F_SG ; <nl>  <nl> netdev -> priv_flags |= IFF_UNICAST_FLT ; <nl> + netdev -> priv_flags |= IFF_SUPP_NOFCS ; <nl>  <nl> if ( adapter -> flags & IXGBE_FLAG_SRIOV_ENABLED ) <nl> adapter -> flags &= ~( IXGBE_FLAG_RSS_ENABLED |
int usb_sg_init ( struct usb_sg_request * io , struct usb_device * dev , <nl> } <nl>  <nl> /* initialize all the urbs we ' ll use */ <nl> - io -> urbs = kmalloc ( io -> entries * sizeof * io -> urbs , mem_flags ); <nl> + io -> urbs = kmalloc ( io -> entries * sizeof (* io -> urbs ), mem_flags ); <nl> if (! io -> urbs ) <nl> goto nomem ; <nl> 
void i915_ppgtt_bind_object ( struct i915_hw_ppgtt * ppgtt , <nl>  <nl> switch ( cache_level ) { <nl> case I915_CACHE_LLC_MLC : <nl> - pte_flags |= GEN6_PTE_CACHE_LLC_MLC ; <nl> + /* Haswell doesn ' t set L3 this way */ <nl> + if ( IS_HASWELL ( obj -> base . dev )) <nl> + pte_flags |= GEN6_PTE_CACHE_LLC ; <nl> + else <nl> + pte_flags |= GEN6_PTE_CACHE_LLC_MLC ; <nl> break ; <nl> case I915_CACHE_LLC : <nl> pte_flags |= GEN6_PTE_CACHE_LLC ; <nl> static unsigned int cache_level_to_agp_type ( struct drm_device * dev , <nl> { <nl> switch ( cache_level ) { <nl> case I915_CACHE_LLC_MLC : <nl> - if ( INTEL_INFO ( dev )-> gen >= 6 ) <nl> - return AGP_USER_CACHED_MEMORY_LLC_MLC ; <nl> /* Older chipsets do not have this extra level of CPU <nl> * cacheing , so fallthrough and request the PTE simply <nl> * as cached . <nl> */ <nl> + if ( INTEL_INFO ( dev )-> gen >= 6 && ! IS_HASWELL ( dev )) <nl> + return AGP_USER_CACHED_MEMORY_LLC_MLC ; <nl> case I915_CACHE_LLC : <nl> return AGP_USER_CACHED_MEMORY ; <nl> default :
static ssize_t channel_dimm_label_store ( struct device * dev , <nl> if ( data [ count - 1 ] == '\ 0 ' || data [ count - 1 ] == '\ n ') <nl> copy_count -= 1 ; <nl>  <nl> - if ( copy_count >= sizeof ( rank -> dimm -> label )) <nl> + if ( copy_count == 0 || copy_count >= sizeof ( rank -> dimm -> label )) <nl> return - EINVAL ; <nl>  <nl> strncpy ( rank -> dimm -> label , data , copy_count ); <nl> static ssize_t dimmdev_label_store ( struct device * dev , <nl> if ( data [ count - 1 ] == '\ 0 ' || data [ count - 1 ] == '\ n ') <nl> copy_count -= 1 ; <nl>  <nl> - if ( copy_count >= sizeof ( dimm -> label )) <nl> + if ( copy_count == 0 || copy_count >= sizeof ( dimm -> label )) <nl> return - EINVAL ; <nl>  <nl> strncpy ( dimm -> label , data , copy_count );
static char dtlk_read_tts ( void ) <nl> portval = inb_p ( dtlk_port_tts ); <nl> } while (( portval & TTS_READABLE ) == 0 && <nl> retries ++ < DTLK_MAX_RETRIES ); <nl> - if ( retries == DTLK_MAX_RETRIES ) <nl> + if ( retries > DTLK_MAX_RETRIES ) <nl> printk ( KERN_ERR " dtlk_read_tts () timeout \ n "); <nl>  <nl> ch = inb_p ( dtlk_port_tts ); /* input from TTS port */ <nl> static char dtlk_read_tts ( void ) <nl> portval = inb_p ( dtlk_port_tts ); <nl> } while (( portval & TTS_READABLE ) != 0 && <nl> retries ++ < DTLK_MAX_RETRIES ); <nl> - if ( retries == DTLK_MAX_RETRIES ) <nl> + if ( retries > DTLK_MAX_RETRIES ) <nl> printk ( KERN_ERR " dtlk_read_tts () timeout \ n "); <nl>  <nl> TRACE_RET ; <nl> static char dtlk_write_tts ( char ch ) <nl> while (( inb_p ( dtlk_port_tts ) & TTS_WRITABLE ) == 0 && <nl> retries ++ < DTLK_MAX_RETRIES ) /* DT ready ? */ <nl> ; <nl> - if ( retries == DTLK_MAX_RETRIES ) <nl> + if ( retries > DTLK_MAX_RETRIES ) <nl> printk ( KERN_ERR " dtlk_write_tts () timeout \ n "); <nl>  <nl> outb_p ( ch , dtlk_port_tts ); /* output to TTS port */
static int s_bHandleRxEncryption ( struct vnt_private * pDevice , u8 * pbyFrame , <nl> if ( byDecMode == KEY_CTL_WEP ) { <nl> // handle WEP <nl> if (( pDevice -> byLocalID <= REV_ID_VT3253_A1 ) || <nl> - ((( PSKeyTable )(& pKey -> pvKeyTable ))-> bSoftWEP == TRUE )) { <nl> + ((( PSKeyTable )( pKey -> pvKeyTable ))-> bSoftWEP == TRUE )) { <nl> // Software WEP <nl> // 1 . 3253A <nl> // 2 . WEP 256 <nl> static int s_bHostWepRxEncryption ( struct vnt_private * pDevice , u8 * pbyFrame , <nl> // handle WEP <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO " byDecMode == KEY_CTL_WEP \ n "); <nl> if (( pDevice -> byLocalID <= REV_ID_VT3253_A1 ) || <nl> - ((( PSKeyTable )(& pKey -> pvKeyTable ))-> bSoftWEP == TRUE ) || <nl> + ((( PSKeyTable )( pKey -> pvKeyTable ))-> bSoftWEP == TRUE ) || <nl> ( bOnFly == FALSE )) { <nl> // Software WEP <nl> // 1 . 3253A
static int adjust_scalar_min_max_vals ( struct bpf_verifier_env * env , <nl> u64 umin_val , umax_val ; <nl> u64 insn_bitness = ( BPF_CLASS ( insn -> code ) == BPF_ALU64 ) ? 64 : 32 ; <nl>  <nl> + if ( insn_bitness == 32 ) { <nl> + /* Relevant for 32 - bit RSH : Information can propagate towards <nl> + * LSB , so it isn ' t sufficient to only truncate the output to <nl> + * 32 bits . <nl> + */ <nl> + coerce_reg_to_size ( dst_reg , 4 ); <nl> + coerce_reg_to_size (& src_reg , 4 ); <nl> + } <nl> + <nl> smin_val = src_reg . smin_value ; <nl> smax_val = src_reg . smax_value ; <nl> umin_val = src_reg . umin_value ; <nl> static int adjust_scalar_min_max_vals ( struct bpf_verifier_env * env , <nl> if ( BPF_CLASS ( insn -> code ) != BPF_ALU64 ) { <nl> /* 32 - bit ALU ops are ( 32 , 32 )-> 32 */ <nl> coerce_reg_to_size ( dst_reg , 4 ); <nl> - coerce_reg_to_size (& src_reg , 4 ); <nl> } <nl>  <nl> __reg_deduce_bounds ( dst_reg );
static void regulator_ena_gpio_free ( struct regulator_dev * rdev ) <nl> gpiod_put ( pin -> gpiod ); <nl> list_del (& pin -> list ); <nl> kfree ( pin ); <nl> + rdev -> ena_pin = NULL ; <nl> + return ; <nl> } else { <nl> pin -> request_count --; <nl> }
static int snd_hrtimer_start ( struct snd_timer * t ) <nl> struct snd_hrtimer * stime = t -> private_data ; <nl>  <nl> atomic_set (& stime -> running , 0 ); <nl> - hrtimer_cancel (& stime -> hrt ); <nl> + hrtimer_try_to_cancel (& stime -> hrt ); <nl> hrtimer_start (& stime -> hrt , ns_to_ktime ( t -> sticks * resolution ), <nl> HRTIMER_MODE_REL ); <nl> atomic_set (& stime -> running , 1 ); <nl> static int snd_hrtimer_stop ( struct snd_timer * t ) <nl> { <nl> struct snd_hrtimer * stime = t -> private_data ; <nl> atomic_set (& stime -> running , 0 ); <nl> + hrtimer_try_to_cancel (& stime -> hrt ); <nl> return 0 ; <nl> } <nl> 
static int rockchip_usb_phy_init ( struct rockchip_usb_phy_base * base , <nl> goto err_clk_prov ; <nl> } <nl>  <nl> - err = devm_add_action ( base -> dev , rockchip_usb_phy_action , rk_phy ); <nl> + err = devm_add_action_or_reset ( base -> dev , rockchip_usb_phy_action , <nl> + rk_phy ); <nl> if ( err ) <nl> - goto err_devm_action ; <nl> + return err ; <nl>  <nl> rk_phy -> phy = devm_phy_create ( base -> dev , child , & ops ); <nl> if ( IS_ERR ( rk_phy -> phy )) { <nl> static int rockchip_usb_phy_init ( struct rockchip_usb_phy_base * base , <nl> else <nl> return rockchip_usb_phy_power ( rk_phy , 1 ); <nl>  <nl> - err_devm_action : <nl> - if (! rk_phy -> uart_enabled ) <nl> - of_clk_del_provider ( child ); <nl> err_clk_prov : <nl> if (! rk_phy -> uart_enabled ) <nl> clk_unregister ( rk_phy -> clk480m );
static inline u32 skb_mstamp_us_delta ( const struct skb_mstamp * t1 , <nl> return delta_us ; <nl> } <nl>  <nl> + static inline bool skb_mstamp_after ( const struct skb_mstamp * t1 , <nl> + const struct skb_mstamp * t0 ) <nl> +{ <nl> + s32 diff = t1 -> stamp_jiffies - t0 -> stamp_jiffies ; <nl> + <nl> + if (! diff ) <nl> + diff = t1 -> stamp_us - t0 -> stamp_us ; <nl> + return diff > 0 ; <nl> +} <nl>  <nl> /** <nl> * struct sk_buff - socket buffer
int azx_codec_configure ( struct azx * chip ) <nl> list_for_each_codec_safe ( codec , next , & chip -> bus ) { <nl> snd_hda_codec_configure ( codec ); <nl> } <nl> + <nl> + if (! azx_bus ( chip )-> num_codecs ) <nl> + return - ENODEV ; <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( azx_codec_configure );
static struct sk_buff * isdn_ppp_decompress ( struct sk_buff * skb , struct ippp_struc <nl> rsparm . maxdlen = IPPP_RESET_MAXDATABYTES ; <nl>  <nl> skb_out = dev_alloc_skb ( is -> mru + PPP_HDRLEN ); <nl> + if (! skb_out ) { <nl> + kfree_skb ( skb ); <nl> + printk ( KERN_ERR " ippp : decomp memory allocation failure \ n "); <nl> + return NULL ; <nl> + } <nl> len = ipc -> decompress ( stat , skb , skb_out , & rsparm ); <nl> kfree_skb ( skb ); <nl> if ( len <= 0 ) {
static int transport_generic_cmd_sequencer ( <nl> cmd -> se_cmd_flags |= SCF_SCSI_CONTROL_SG_IO_CDB ; <nl>  <nl> if ( target_check_write_same_discard (& cdb [ 10 ], dev ) < 0 ) <nl> - goto out_invalid_cdb_field ; <nl> + goto out_unsupported_cdb ; <nl> if (! passthrough ) <nl> cmd -> execute_task = target_emulate_write_same ; <nl> break ; <nl> static int transport_generic_cmd_sequencer ( <nl> cmd -> se_cmd_flags |= SCF_SCSI_CONTROL_SG_IO_CDB ; <nl>  <nl> if ( target_check_write_same_discard (& cdb [ 1 ], dev ) < 0 ) <nl> - goto out_invalid_cdb_field ; <nl> + goto out_unsupported_cdb ; <nl> if (! passthrough ) <nl> cmd -> execute_task = target_emulate_write_same ; <nl> break ; <nl> static int transport_generic_cmd_sequencer ( <nl> * of byte 1 bit 3 UNMAP instead of original reserved field <nl> */ <nl> if ( target_check_write_same_discard (& cdb [ 1 ], dev ) < 0 ) <nl> - goto out_invalid_cdb_field ; <nl> + goto out_unsupported_cdb ; <nl> if (! passthrough ) <nl> cmd -> execute_task = target_emulate_write_same ; <nl> break ;
static inline int ccid_hc_rx_getsockopt ( struct ccid * ccid , struct sock * sk , <nl> u32 __user * optval , int __user * optlen ) <nl> { <nl> int rc = - ENOPROTOOPT ; <nl> - if ( ccid -> ccid_ops -> ccid_hc_rx_getsockopt != NULL ) <nl> + if ( ccid != NULL && ccid -> ccid_ops -> ccid_hc_rx_getsockopt != NULL ) <nl> rc = ccid -> ccid_ops -> ccid_hc_rx_getsockopt ( sk , optname , len , <nl> optval , optlen ); <nl> return rc ; <nl> static inline int ccid_hc_tx_getsockopt ( struct ccid * ccid , struct sock * sk , <nl> u32 __user * optval , int __user * optlen ) <nl> { <nl> int rc = - ENOPROTOOPT ; <nl> - if ( ccid -> ccid_ops -> ccid_hc_tx_getsockopt != NULL ) <nl> + if ( ccid != NULL && ccid -> ccid_ops -> ccid_hc_tx_getsockopt != NULL ) <nl> rc = ccid -> ccid_ops -> ccid_hc_tx_getsockopt ( sk , optname , len , <nl> optval , optlen ); <nl> return rc ;
static struct zonelist * policy_zonelist ( gfp_t gfp , struct mempolicy * policy ) <nl> /* <nl> * Normally , MPOL_BIND allocations are node - local within the <nl> * allowed nodemask . However , if __GFP_THISNODE is set and the <nl> - * current node is part of the mask , we use the zonelist for <nl> + * current node isn ' t part of the mask , we use the zonelist for <nl> * the first node in the mask instead . <nl> */ <nl> if ( unlikely ( gfp & __GFP_THISNODE ) && <nl> unlikely (! node_isset ( nd , policy -> v . nodes ))) <nl> nd = first_node ( policy -> v . nodes ); <nl> break ; <nl> - case MPOL_INTERLEAVE : /* should not happen */ <nl> - break ; <nl> default : <nl> BUG (); <nl> }
int cx23888_ir_probe ( struct cx23885_dev * dev ) <nl> return - ENOMEM ; <nl>  <nl> spin_lock_init (& state -> rx_kfifo_lock ); <nl> - if ( kfifo_alloc (& state -> rx_kfifo , CX23888_IR_RX_KFIFO_SIZE , GFP_KERNEL )) <nl> + if ( kfifo_alloc (& state -> rx_kfifo , CX23888_IR_RX_KFIFO_SIZE , <nl> + GFP_KERNEL )) { <nl> + kfree ( state ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> state -> dev = dev ; <nl> sd = & state -> sd ;
enum siginfo_layout siginfo_layout ( int sig , int si_code ) <nl> [ SIGSEGV ] = { NSIGSEGV , SIL_FAULT }, <nl> [ SIGBUS ] = { NSIGBUS , SIL_FAULT }, <nl> [ SIGTRAP ] = { NSIGTRAP , SIL_FAULT }, <nl> -# if defined ( SIGMET ) && defined ( NSIGEMT ) <nl> +# if defined ( SIGEMT ) && defined ( NSIGEMT ) <nl> [ SIGEMT ] = { NSIGEMT , SIL_FAULT }, <nl> # endif <nl> [ SIGCHLD ] = { NSIGCHLD , SIL_CHLD },
static inline void __generic_make_request ( struct bio * bio ) <nl> if ( old_sector != - 1 ) <nl> trace_block_remap ( q , bio , old_dev , old_sector ); <nl>  <nl> - trace_block_bio_queue ( q , bio ); <nl> - <nl> old_sector = bio -> bi_sector ; <nl> old_dev = bio -> bi_bdev -> bd_dev ; <nl>  <nl> static inline void __generic_make_request ( struct bio * bio ) <nl> goto end_io ; <nl> } <nl>  <nl> + trace_block_bio_queue ( q , bio ); <nl> + <nl> ret = q -> make_request_fn ( q , bio ); <nl> } while ( ret ); <nl> 
static int serial_omap_probe_rs485 ( struct uart_omap_port * up , <nl> /* check for tx enable gpio */ <nl> up -> rts_gpio = of_get_named_gpio_flags ( np , " rts - gpio ", 0 , & flags ); <nl> if ( gpio_is_valid ( up -> rts_gpio )) { <nl> - ret = gpio_request ( up -> rts_gpio , " omap - serial "); <nl> + ret = devm_gpio_request ( up -> dev , up -> rts_gpio , " omap - serial "); <nl> if ( ret < 0 ) <nl> return ret ; <nl> ret = gpio_direction_output ( up -> rts_gpio , <nl> static int serial_omap_probe ( struct platform_device * pdev ) <nl>  <nl> if ( gpio_is_valid ( omap_up_info -> DTR_gpio ) && <nl> omap_up_info -> DTR_present ) { <nl> - ret = gpio_request ( omap_up_info -> DTR_gpio , " omap - serial "); <nl> + ret = devm_gpio_request (& pdev -> dev , omap_up_info -> DTR_gpio , <nl> + " omap - serial "); <nl> if ( ret < 0 ) <nl> return ret ; <nl> ret = gpio_direction_output ( omap_up_info -> DTR_gpio ,
static int shash_update_unaligned ( struct shash_desc * desc , const u8 * data , <nl> u8 buf [ shash_align_buffer_size ( unaligned_len , alignmask )] <nl> __attribute__ (( aligned )); <nl>  <nl> + if ( unaligned_len > len ) <nl> + unaligned_len = len ; <nl> + <nl> memcpy ( buf , data , unaligned_len ); <nl>  <nl> return shash -> update ( desc , buf , unaligned_len ) ?:
int key_reject_and_link ( struct key * key , <nl>  <nl> mutex_unlock (& key_construction_mutex ); <nl>  <nl> - if ( keyring ) <nl> + if ( keyring && link_ret == 0 ) <nl> __key_link_end ( keyring , & key -> index_key , edit ); <nl>  <nl> /* wake up anyone waiting for a key to be constructed */
mwifiex_cmd_802_11_key_material ( struct mwifiex_private * priv , <nl> struct host_cmd_tlv_mac_addr * tlv_mac ; <nl> u16 key_param_len = 0 , cmd_size ; <nl> int ret = 0 ; <nl> - const u8 bc_mac [] = { 0xff , 0xff , 0xff , 0xff , 0xff , 0xff }; <nl>  <nl> cmd -> command = cpu_to_le16 ( HostCmd_CMD_802_11_KEY_MATERIAL ); <nl> key_material -> action = cpu_to_le16 ( cmd_action ); <nl> mwifiex_cmd_802_11_key_material ( struct mwifiex_private * priv , <nl> /* set 0 when re - key */ <nl> key_material -> key_param_set . key [ 1 ] = 0 ; <nl>  <nl> - if ( 0 != memcmp ( enc_key -> mac_addr , bc_mac , sizeof ( bc_mac ))) { <nl> + if (! is_broadcast_ether_addr ( enc_key -> mac_addr )) { <nl> /* WAPI pairwise key : unicast */ <nl> key_material -> key_param_set . key_info |= <nl> cpu_to_le16 ( KEY_UNICAST );
static int mem_cgroup_resize_max ( struct mem_cgroup * memcg , <nl> unsigned long max , bool memsw ) <nl> { <nl> bool enlarge = false ; <nl> + bool drained = false ; <nl> int ret ; <nl> bool limits_invariant ; <nl> struct page_counter * counter = memsw ? & memcg -> memsw : & memcg -> memory ; <nl> static int mem_cgroup_resize_max ( struct mem_cgroup * memcg , <nl> if (! ret ) <nl> break ; <nl>  <nl> + if (! drained ) { <nl> + drain_all_stock ( memcg ); <nl> + drained = true ; <nl> + continue ; <nl> + } <nl> + <nl> if (! try_to_free_mem_cgroup_pages ( memcg , 1 , <nl> GFP_KERNEL , ! memsw )) { <nl> ret = - EBUSY ;
void dccp_close ( struct sock * sk , long timeout ) <nl> __kfree_skb ( skb ); <nl> } <nl>  <nl> + /* If socket has been already reset kill it . */ <nl> + if ( sk -> sk_state == DCCP_CLOSED ) <nl> + goto adjudge_to_death ; <nl> + <nl> if ( data_was_unread ) { <nl> /* Unread data was tossed , send an appropriate Reset Code */ <nl> DCCP_WARN (" ABORT with % u bytes unread \ n ", data_was_unread );
void __init mount_block_root ( char * name , int flags ) <nl> case 0 : <nl> goto out ; <nl> case - EACCES : <nl> - flags |= MS_RDONLY ; <nl> - goto retry ; <nl> case - EINVAL : <nl> continue ; <nl> } <nl> void __init mount_block_root ( char * name , int flags ) <nl> # endif <nl> panic (" VFS : Unable to mount root fs on % s ", b ); <nl> } <nl> + if (!( flags & MS_RDONLY )) { <nl> + flags |= MS_RDONLY ; <nl> + goto retry ; <nl> + } <nl>  <nl> printk (" List of all partitions :\ n "); <nl> printk_all_partitions ();
static int rfcomm_tty_open ( struct tty_struct * tty , struct file * filp ) <nl> break ; <nl> } <nl>  <nl> + tty_unlock (); <nl> schedule (); <nl> + tty_lock (); <nl> } <nl> set_current_state ( TASK_RUNNING ); <nl> remove_wait_queue (& dev -> wait , & wait );
# include " tg3 . h " <nl>  <nl> # define DRV_MODULE_NAME " tg3 " <nl> -# define DRV_MODULE_VERSION " 3 . 108 " <nl> -# define DRV_MODULE_RELDATE " February 17 , 2010 " <nl> +# define DRV_MODULE_VERSION " 3 . 109 " <nl> +# define DRV_MODULE_RELDATE " April 2 , 2010 " <nl>  <nl> # define TG3_DEF_MAC_MODE 0 <nl> # define TG3_DEF_RX_MODE 0
int vfs_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> bool new_is_dir = false ; <nl> unsigned max_links = new_dir -> i_sb -> s_max_links ; <nl>  <nl> - if ( source == target ) <nl> + /* <nl> + * Check source == target . <nl> + * On overlayfs need to look at underlying inodes . <nl> + */ <nl> + if ( vfs_select_inode ( old_dentry , 0 ) == vfs_select_inode ( new_dentry , 0 )) <nl> return 0 ; <nl>  <nl> error = may_delete ( old_dir , old_dentry , is_dir );
/* <nl> - * Copyright ( c ) 2006 , 2018 Oracle and / or its affiliates . All rights reserved . <nl> + * Copyright ( c ) 2006 , 2019 Oracle and / or its affiliates . All rights reserved . <nl> * <nl> * This software is available to you under a choice of one of two <nl> * licenses . You may choose to be licensed under the terms of the GNU <nl> void rds6_inc_info_copy ( struct rds_incoming * inc , <nl>  <nl> minfo6 . seq = be64_to_cpu ( inc -> i_hdr . h_sequence ); <nl> minfo6 . len = be32_to_cpu ( inc -> i_hdr . h_len ); <nl> + minfo6 . tos = inc -> i_conn -> c_tos ; <nl>  <nl> if ( flip ) { <nl> minfo6 . laddr = * daddr ; <nl> void rds6_inc_info_copy ( struct rds_incoming * inc , <nl> minfo6 . fport = inc -> i_hdr . h_dport ; <nl> } <nl>  <nl> + minfo6 . flags = 0 ; <nl> + <nl> rds_info_copy ( iter , & minfo6 , sizeof ( minfo6 )); <nl> } <nl> # endif
static int iommu_map_page ( struct protection_domain * dom , <nl> count = PAGE_SIZE_PTE_COUNT ( page_size ); <nl> pte = alloc_pte ( dom , bus_addr , page_size , NULL , GFP_KERNEL ); <nl>  <nl> + if (! pte ) <nl> + return - ENOMEM ; <nl> + <nl> for ( i = 0 ; i < count ; ++ i ) <nl> if ( IOMMU_PTE_PRESENT ( pte [ i ])) <nl> return - EBUSY ;
void * nf_ct_alloc_hashtable ( unsigned int * sizep , int nulls ) <nl> unsigned int nr_slots , i ; <nl> size_t sz ; <nl>  <nl> + if (* sizep > ( UINT_MAX / sizeof ( struct hlist_nulls_head ))) <nl> + return NULL ; <nl> + <nl> BUILD_BUG_ON ( sizeof ( struct hlist_nulls_head ) != sizeof ( struct hlist_head )); <nl> nr_slots = * sizep = roundup (* sizep , PAGE_SIZE / sizeof ( struct hlist_nulls_head )); <nl> + <nl> + if ( nr_slots > ( UINT_MAX / sizeof ( struct hlist_nulls_head ))) <nl> + return NULL ; <nl> + <nl> sz = nr_slots * sizeof ( struct hlist_nulls_head ); <nl> hash = ( void *) __get_free_pages ( GFP_KERNEL | __GFP_NOWARN | __GFP_ZERO , <nl> get_order ( sz ));
void global_dirty_limits ( unsigned long * pbackground , unsigned long * pdirty ) <nl> { <nl> unsigned long background ; <nl> unsigned long dirty ; <nl> - unsigned long available_memory = determine_dirtyable_memory (); <nl> + unsigned long uninitialized_var ( available_memory ); <nl> struct task_struct * tsk ; <nl>  <nl> + if (! vm_dirty_bytes || ! dirty_background_bytes ) <nl> + available_memory = determine_dirtyable_memory (); <nl> + <nl> if ( vm_dirty_bytes ) <nl> dirty = DIV_ROUND_UP ( vm_dirty_bytes , PAGE_SIZE ); <nl> else
EXPORT_SYMBOL ( xt_check_entry_offsets ); <nl> */ <nl> unsigned int * xt_alloc_entry_offsets ( unsigned int size ) <nl> { <nl> + if ( size > XT_MAX_TABLE_SIZE / sizeof ( unsigned int )) <nl> + return NULL ; <nl> + <nl> return kvmalloc_array ( size , sizeof ( unsigned int ), GFP_KERNEL | __GFP_ZERO ); <nl>  <nl> }
static void dwc2_hcd_cleanup_channels ( struct dwc2_hsotg * hsotg ) <nl> */ <nl> channel -> qh = NULL ; <nl> } <nl> + /* All channels have been freed , mark them available */ <nl> + if ( hsotg -> core_params -> uframe_sched > 0 ) { <nl> + hsotg -> available_host_channels = <nl> + hsotg -> core_params -> host_channels ; <nl> + } else { <nl> + hsotg -> non_periodic_channels = 0 ; <nl> + hsotg -> periodic_channels = 0 ; <nl> + } <nl> } <nl>  <nl> /**
static int emc1403_detect ( struct i2c_client * client , <nl> } <nl>  <nl> id = i2c_smbus_read_byte_data ( client , THERMAL_REVISION_REG ); <nl> - if ( id != 0x01 ) <nl> + if ( id < 0x01 || id > 0x04 ) <nl> return - ENODEV ; <nl>  <nl> return 0 ;
static void __init cps_prepare_cpus ( unsigned int max_cpus ) <nl> } <nl> } <nl>  <nl> - static void boot_core ( unsigned core ) <nl> + static void boot_core ( unsigned int core , unsigned int vpe_id ) <nl> { <nl> u32 access , stat , seq_state ; <nl> unsigned timeout ; <nl> static void boot_core ( unsigned core ) <nl> mips_cpc_lock_other ( core ); <nl>  <nl> if ( mips_cm_revision () >= CM_REV_CM3 ) { <nl> - /* Run VP0 following the reset */ <nl> - write_cpc_co_vp_run ( 0x1 ); <nl> + /* Run only the requested VP following the reset */ <nl> + write_cpc_co_vp_stop ( 0xf ); <nl> + write_cpc_co_vp_run ( 1 << vpe_id ); <nl>  <nl> /* <nl> * Ensure that the VP_RUN register is written before the <nl> static void cps_boot_secondary ( int cpu , struct task_struct * idle ) <nl>  <nl> if (! test_bit ( core , core_power )) { <nl> /* Boot a VPE on a powered down core */ <nl> - boot_core ( core ); <nl> + boot_core ( core , vpe_id ); <nl> goto out ; <nl> } <nl> 
tc_dump_action ( struct sk_buff * skb , struct netlink_callback * cb ) <nl> nla_nest_end ( skb , nest ); <nl> ret = skb -> len ; <nl> } else <nl> - nla_nest_cancel ( skb , nest ); <nl> + nlmsg_trim ( skb , b ); <nl>  <nl> nlh -> nlmsg_len = skb_tail_pointer ( skb ) - b ; <nl> if ( NETLINK_CB ( cb -> skb ). portid && ret )
int sst_hsw_dsp_runtime_resume ( struct sst_hsw * hsw ) <nl> ret = wait_event_timeout ( hsw -> boot_wait , hsw -> boot_complete , <nl> msecs_to_jiffies ( IPC_BOOT_MSECS )); <nl> if ( ret == 0 ) { <nl> - dev_err ( hsw -> dev , " error : audio DSP boot timeout \ n "); <nl> + dev_err ( hsw -> dev , " error : audio DSP boot timeout IPCD 0x % x IPCX 0x % x \ n ", <nl> + sst_dsp_shim_read_unlocked ( hsw -> dsp , SST_IPCD ), <nl> + sst_dsp_shim_read_unlocked ( hsw -> dsp , SST_IPCX )); <nl> return - EIO ; <nl> } <nl>  <nl> int sst_hsw_dsp_init ( struct device * dev , struct sst_pdata * pdata ) <nl> msecs_to_jiffies ( IPC_BOOT_MSECS )); <nl> if ( ret == 0 ) { <nl> ret = - EIO ; <nl> - dev_err ( hsw -> dev , " error : ADSP boot timeout \ n "); <nl> + dev_err ( hsw -> dev , " error : audio DSP boot timeout IPCD 0x % x IPCX 0x % x \ n ", <nl> + sst_dsp_shim_read_unlocked ( hsw -> dsp , SST_IPCD ), <nl> + sst_dsp_shim_read_unlocked ( hsw -> dsp , SST_IPCX )); <nl> goto boot_err ; <nl> } <nl> 
do_send_specific ( pid_t tgid , pid_t pid , int sig , struct siginfo * info ) <nl>  <nl> static int do_tkill ( pid_t tgid , pid_t pid , int sig ) <nl> { <nl> - struct siginfo info ; <nl> + struct siginfo info = {}; <nl>  <nl> info . si_signo = sig ; <nl> info . si_errno = 0 ;
int nvdimm_has_flush ( struct nd_region * nd_region ) <nl> { <nl> int i ; <nl>  <nl> - /* no nvdimm == flushing capability unknown */ <nl> - if ( nd_region -> ndr_mappings == 0 ) <nl> + /* no nvdimm or pmem api == flushing capability unknown */ <nl> + if ( nd_region -> ndr_mappings == 0 <nl> + || ! IS_ENABLED ( CONFIG_ARCH_HAS_PMEM_API )) <nl> return - ENXIO ; <nl>  <nl> for ( i = 0 ; i < nd_region -> ndr_mappings ; i ++) {
cnp_setup_backlight ( struct intel_connector * connector , enum pipe unused ) <nl> u32 pwm_ctl , val ; <nl>  <nl> /* <nl> - * CNP has the BXT implementation of backlight , but with only <nl> - * one controller . Future platforms could have multiple controllers <nl> - * so let ' s make this extensible and prepared for the future . <nl> + * CNP has the BXT implementation of backlight , but with only one <nl> + * controller . TODO : ICP has multiple controllers but we only use <nl> + * controller 0 for now . <nl> */ <nl> panel -> backlight . controller = 0 ; <nl>  <nl> intel_panel_init_backlight_funcs ( struct intel_panel * panel ) <nl> panel -> backlight . set = bxt_set_backlight ; <nl> panel -> backlight . get = bxt_get_backlight ; <nl> panel -> backlight . hz_to_pwm = bxt_hz_to_pwm ; <nl> - } else if ( HAS_PCH_CNP ( dev_priv )) { <nl> + } else if ( HAS_PCH_CNP ( dev_priv ) || HAS_PCH_ICP ( dev_priv )) { <nl> panel -> backlight . setup = cnp_setup_backlight ; <nl> panel -> backlight . enable = cnp_enable_backlight ; <nl> panel -> backlight . disable = cnp_disable_backlight ;
void rtw_alloc_hwxmits ( struct adapter * padapter ) <nl>  <nl> pxmitpriv -> hwxmit_entry = HWXMIT_ENTRY ; <nl>  <nl> - pxmitpriv -> hwxmits = kzalloc ( sizeof ( struct hw_xmit ) * pxmitpriv -> hwxmit_entry , GFP_KERNEL ); <nl> + pxmitpriv -> hwxmits = kcalloc ( pxmitpriv -> hwxmit_entry , <nl> + sizeof ( struct hw_xmit ), GFP_KERNEL ); <nl>  <nl> hwxmits = pxmitpriv -> hwxmits ; <nl> 
int aer_osc_setup ( struct pcie_device * pciedev ) <nl> } <nl>  <nl> if ( handle ) { <nl> - pci_osc_support_set ( OSC_EXT_PCI_CONFIG_SUPPORT ); <nl> + pcie_osc_support_set ( OSC_EXT_PCI_CONFIG_SUPPORT ); <nl> status = pci_osc_control_set ( handle , <nl> OSC_PCI_EXPRESS_AER_CONTROL | <nl> OSC_PCI_EXPRESS_CAP_STRUCTURE_CONTROL );
static int tg3_bmcr_reset ( struct tg3 * tp ) <nl> } <nl> udelay ( 10 ); <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ; <nl> static int tg3_wait_macro_done ( struct tg3 * tp ) <nl> break ; <nl> } <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ;
# include < linux / syscore_ops . h > <nl> # include < linux / mutex . h > <nl> # include < linux / module . h > <nl> +# include < linux / interrupt . h > <nl>  <nl> static LIST_HEAD ( syscore_ops_list ); <nl> static DEFINE_MUTEX ( syscore_ops_lock ); <nl> int syscore_suspend ( void ) <nl> struct syscore_ops * ops ; <nl> int ret = 0 ; <nl>  <nl> + pr_debug (" Checking wakeup interrupts \ n "); <nl> + <nl> + /* Return error code if there are any wakeup interrupts pending . */ <nl> + ret = check_wakeup_irqs (); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> WARN_ONCE (! irqs_disabled (), <nl> " Interrupts enabled before system core suspend .\ n "); <nl> 
static struct dmi_system_id video_dmi_table [] = { <nl> DMI_MATCH ( DMI_PRODUCT_NAME , " PORTEGE R830 "), <nl> }, <nl> }, <nl> + { <nl> + /* https :// bugzilla . kernel . org / show_bug . cgi ? id = 21012 */ <nl> + . callback = video_disable_backlight_sysfs_if , <nl> + . ident = " Toshiba Satellite R830 ", <nl> + . matches = { <nl> + DMI_MATCH ( DMI_SYS_VENDOR , " TOSHIBA "), <nl> + DMI_MATCH ( DMI_PRODUCT_NAME , " SATELLITE R830 "), <nl> + }, <nl> + }, <nl> /* <nl> * Some machine ' s _DOD IDs don ' t have bit 31 ( Device ID Scheme ) set <nl> * but the IDs actually follow the Device ID Scheme .
void regulatory_hint_11d ( struct wiphy * wiphy , <nl> * it as it would indicate a mistake in the current design <nl> */ <nl> if ( WARN_ON ( reg_same_country_ie_hint ( wiphy , checksum ))) <nl> - goto out ; <nl> + goto free_rd_out ; <nl>  <nl> /* We keep this around for when CRDA comes back with a response so <nl> * we can intersect with that */ <nl> void regulatory_hint_11d ( struct wiphy * wiphy , <nl> __regulatory_hint ( wiphy , REGDOM_SET_BY_COUNTRY_IE , <nl> country_ie_regdomain -> alpha2 , checksum , env ); <nl>  <nl> + goto out ; <nl> + <nl> + free_rd_out : <nl> + kfree ( rd ); <nl> out : <nl> mutex_unlock (& cfg80211_mutex ); <nl> }
static void tcp_illinois_info ( struct sock * sk , u32 ext , <nl> . tcpv_rttcnt = ca -> cnt_rtt , <nl> . tcpv_minrtt = ca -> base_rtt , <nl> }; <nl> - u64 t = ca -> sum_rtt ; <nl>  <nl> - do_div ( t , ca -> cnt_rtt ); <nl> - info . tcpv_rtt = t ; <nl> + if ( info . tcpv_rttcnt > 0 ) { <nl> + u64 t = ca -> sum_rtt ; <nl>  <nl> + do_div ( t , info . tcpv_rttcnt ); <nl> + info . tcpv_rtt = t ; <nl> + } <nl> nla_put ( skb , INET_DIAG_VEGASINFO , sizeof ( info ), & info ); <nl> } <nl> }
void ovs_flow_stats_update ( struct sw_flow * flow , __be16 tcp_flags , <nl> * allocated stats as we have already locked them . <nl> */ <nl> if ( likely ( flow -> stats_last_writer != NUMA_NO_NODE ) <nl> - && likely (! rcu_dereference ( flow -> stats [ node ]))) { <nl> + && likely (! rcu_access_pointer ( flow -> stats [ node ]))) { <nl> /* Try to allocate node - specific stats . */ <nl> struct flow_stats * new_stats ; <nl> 
void drm_modeset_acquire_init ( struct drm_modeset_acquire_ctx * ctx , <nl> uint32_t flags ) <nl> { <nl> + memset ( ctx , 0 , sizeof (* ctx )); <nl> ww_acquire_init (& ctx -> ww_ctx , & crtc_ww_class ); <nl> INIT_LIST_HEAD (& ctx -> locked ); <nl> }
int device_rename ( struct device * dev , char * new_name ) <nl>  <nl> if ( dev -> class ) { <nl> old_symlink_name = kmalloc ( BUS_ID_SIZE , GFP_KERNEL ); <nl> - if (! old_symlink_name ) <nl> - return - ENOMEM ; <nl> + if (! old_symlink_name ) { <nl> + error = - ENOMEM ; <nl> + goto out_free_old_class ; <nl> + } <nl> strlcpy ( old_symlink_name , dev -> bus_id , BUS_ID_SIZE ); <nl> } <nl>  <nl> int device_rename ( struct device * dev , char * new_name ) <nl> } <nl> put_device ( dev ); <nl>  <nl> - kfree ( old_class_name ); <nl> kfree ( new_class_name ); <nl> kfree ( old_symlink_name ); <nl> + out_free_old_class : <nl> + kfree ( old_class_name ); <nl>  <nl> return error ; <nl> }
static int64_t _sort__sym_cmp ( struct symbol * sym_l , struct symbol * sym_r ) <nl> if ( sym_l == sym_r ) <nl> return 0 ; <nl>  <nl> + if ( sym_l -> inlined || sym_r -> inlined ) <nl> + return strcmp ( sym_l -> name , sym_r -> name ); <nl> + <nl> if ( sym_l -> start != sym_r -> start ) <nl> return ( int64_t )( sym_r -> start - sym_l -> start ); <nl> 
int g_audio_setup ( struct g_audio * g_audio , const char * pcm_name , <nl> if ( err < 0 ) <nl> goto snd_fail ; <nl>  <nl> - strcpy ( pcm -> name , pcm_name ); <nl> + strlcpy ( pcm -> name , pcm_name , sizeof ( pcm -> name )); <nl> pcm -> private_data = uac ; <nl> uac -> pcm = pcm ; <nl>  <nl> snd_pcm_set_ops ( pcm , SNDRV_PCM_STREAM_PLAYBACK , & uac_pcm_ops ); <nl> snd_pcm_set_ops ( pcm , SNDRV_PCM_STREAM_CAPTURE , & uac_pcm_ops ); <nl>  <nl> - strcpy ( card -> driver , card_name ); <nl> - strcpy ( card -> shortname , card_name ); <nl> + strlcpy ( card -> driver , card_name , sizeof ( card -> driver )); <nl> + strlcpy ( card -> shortname , card_name , sizeof ( card -> shortname )); <nl> sprintf ( card -> longname , "% s % i ", card_name , card -> dev -> id ); <nl>  <nl> snd_pcm_lib_preallocate_pages_for_all ( pcm , SNDRV_DMA_TYPE_CONTINUOUS ,
static void handle_swbp ( struct pt_regs * regs ) <nl> if ( unlikely (! test_bit ( UPROBE_COPY_INSN , & uprobe -> flags ))) <nl> goto out ; <nl>  <nl> + /* Tracing handlers use -> utask to communicate with fetch methods */ <nl> + if (! get_utask ()) <nl> + goto out ; <nl> + <nl> handler_chain ( uprobe , regs ); <nl> if ( can_skip_sstep ( uprobe , regs )) <nl> goto out ;
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> err = kstrtoul ( buf , 10 , & val ); <nl> if ( err ) <nl> return err ; <nl> + <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
bool kvm_vgic_map_is_active ( struct kvm_vcpu * vcpu , struct irq_phys_map * map ) <nl> return true ; <nl> } <nl>  <nl> - return dist_active_irq ( vcpu ); <nl> + return vgic_irq_is_active ( vcpu , map -> virt_irq ); <nl> } <nl>  <nl> /*
static struct console usbcons = { <nl>  <nl> void usb_serial_console_disconnect ( struct usb_serial * serial ) <nl> { <nl> - if ( serial -> port [ 0 ] == usbcons_info . port ) { <nl> + if ( serial -> port [ 0 ] && serial -> port [ 0 ] == usbcons_info . port ) { <nl> usb_serial_console_exit (); <nl> usb_serial_put ( serial ); <nl> }
skl_tplg_init_pipe_modules ( struct skl * skl , struct skl_pipe * pipe ) <nl> if ( mconfig -> id . module_id < 0 ) { <nl> struct skl_dfw_module * dfw_config ; <nl>  <nl> - dfw_config = kzalloc ( sizeof ( dfw_config ), GFP_KERNEL ); <nl> + dfw_config = kzalloc ( sizeof (* dfw_config ), GFP_KERNEL ); <nl> if (! dfw_config ) <nl> return - ENOMEM ; <nl> 
static struct sock * pep_sock_accept ( struct sock * sk , int flags , int * errp , <nl>  <nl> err = pep_accept_conn ( newsk , skb ); <nl> if ( err ) { <nl> + __sock_put ( sk ); <nl> sock_put ( newsk ); <nl> newsk = NULL ; <nl> goto drop ;
static int create_qp_common ( struct mlx5_ib_dev * dev , struct ib_pd * pd , <nl> struct mlx5_ib_resources * devr = & dev -> devr ; <nl> int inlen = MLX5_ST_SZ_BYTES ( create_qp_in ); <nl> struct mlx5_core_dev * mdev = dev -> mdev ; <nl> - struct mlx5_ib_create_qp_resp resp ; <nl> + struct mlx5_ib_create_qp_resp resp = {}; <nl> struct mlx5_ib_cq * send_cq ; <nl> struct mlx5_ib_cq * recv_cq ; <nl> unsigned long flags ;
static int doc_ecc_decode ( struct rs_control * rs , uint8_t * data , uint8_t * ecc ) <nl> uint8_t parity ; <nl> uint16_t ds [ 4 ], s [ 5 ], tmp , errval [ 8 ], syn [ 4 ]; <nl>  <nl> + memset ( syn , 0 , sizeof ( syn )); <nl> /* Convert the ecc bytes into words */ <nl> ds [ 0 ] = (( ecc [ 4 ] & 0xff ) >> 0 ) | (( ecc [ 5 ] & 0x03 ) << 8 ); <nl> ds [ 1 ] = (( ecc [ 5 ] & 0xfc ) >> 2 ) | (( ecc [ 2 ] & 0x0f ) << 6 ); <nl> static int doc_ecc_decode ( struct rs_control * rs , uint8_t * data , uint8_t * ecc ) <nl> s [ i ] ^= rs -> alpha_to [ rs_modnn ( rs , tmp + ( FCR + i ) * j )]; <nl> } <nl>  <nl> - /* Calc s [ i ] = s [ i ] / alpha ^( v + i ) */ <nl> + /* Calc syn [ i ] = s [ i ] / alpha ^( v + i ) */ <nl> for ( i = 0 ; i < NROOTS ; i ++) { <nl> - if ( syn [ i ]) <nl> + if ( s [ i ]) <nl> syn [ i ] = rs_modnn ( rs , rs -> index_of [ s [ i ]] + ( NN - FCR - i )); <nl> } <nl> /* Call the decoder library */
static int cnl_calc_wrpll_link ( struct drm_i915_private * dev_priv , <nl> uint32_t cfgcr0 , cfgcr1 ; <nl> uint32_t p0 , p1 , p2 , dco_freq , ref_clock ; <nl>  <nl> - cfgcr0 = I915_READ ( CNL_DPLL_CFGCR0 ( pll_id )); <nl> - cfgcr1 = I915_READ ( CNL_DPLL_CFGCR1 ( pll_id )); <nl> + if ( INTEL_GEN ( dev_priv ) >= 11 ) { <nl> + cfgcr0 = I915_READ ( ICL_DPLL_CFGCR0 ( pll_id )); <nl> + cfgcr1 = I915_READ ( ICL_DPLL_CFGCR1 ( pll_id )); <nl> + } else { <nl> + cfgcr0 = I915_READ ( CNL_DPLL_CFGCR0 ( pll_id )); <nl> + cfgcr1 = I915_READ ( CNL_DPLL_CFGCR1 ( pll_id )); <nl> + } <nl>  <nl> p0 = cfgcr1 & DPLL_CFGCR1_PDIV_MASK ; <nl> p2 = cfgcr1 & DPLL_CFGCR1_KDIV_MASK ;
int st21nfca_connectivity_event_received ( struct nfc_hci_dev * hdev , u8 host , <nl> return - ENOMEM ; <nl>  <nl> transaction -> aid_len = skb -> data [ 1 ]; <nl> + <nl> + /* Checking if the length of the AID is valid */ <nl> + if ( transaction -> aid_len > sizeof ( transaction -> aid )) <nl> + return - EINVAL ; <nl> + <nl> memcpy ( transaction -> aid , & skb -> data [ 2 ], <nl> transaction -> aid_len ); <nl>  <nl> int st21nfca_connectivity_event_received ( struct nfc_hci_dev * hdev , u8 host , <nl> return - EPROTO ; <nl>  <nl> transaction -> params_len = skb -> data [ transaction -> aid_len + 3 ]; <nl> + <nl> + /* Total size is allocated ( skb -> len - 2 ) minus fixed array members */ <nl> + if ( transaction -> params_len > (( skb -> len - 2 ) - sizeof ( struct nfc_evt_transaction ))) <nl> + return - EINVAL ; <nl> + <nl> memcpy ( transaction -> params , skb -> data + <nl> transaction -> aid_len + 4 , transaction -> params_len ); <nl> 
enum { <nl> ALC889_FIXUP_IMAC91_VREF , <nl> ALC882_FIXUP_INV_DMIC , <nl> ALC882_FIXUP_NO_PRIMARY_HP , <nl> + ALC887_FIXUP_ASUS_BASS , <nl> }; <nl>  <nl> static void alc889_fixup_coef ( struct hda_codec * codec , <nl> static const struct hda_fixup alc882_fixups [] = { <nl> . type = HDA_FIXUP_FUNC , <nl> . v . func = alc882_fixup_no_primary_hp , <nl> }, <nl> + [ ALC887_FIXUP_ASUS_BASS ] = { <nl> + . type = HDA_FIXUP_PINS , <nl> + . v . pins = ( const struct hda_pintbl []) { <nl> + { 0x16 , 0x99130130 }, /* bass speaker */ <nl> + {} <nl> + }, <nl> + }, <nl> }; <nl>  <nl> static const struct snd_pci_quirk alc882_fixup_tbl [] = { <nl> static const struct snd_pci_quirk alc882_fixup_tbl [] = { <nl> SND_PCI_QUIRK ( 0x1043 , 0x1873 , " ASUS W90V ", ALC882_FIXUP_ASUS_W90V ), <nl> SND_PCI_QUIRK ( 0x1043 , 0x1971 , " Asus W2JC ", ALC882_FIXUP_ASUS_W2JC ), <nl> SND_PCI_QUIRK ( 0x1043 , 0x835f , " Asus Eee 1601 ", ALC888_FIXUP_EEE1601 ), <nl> + SND_PCI_QUIRK ( 0x1043 , 0x84bc , " ASUS ET2700 ", ALC887_FIXUP_ASUS_BASS ), <nl> SND_PCI_QUIRK ( 0x104d , 0x9047 , " Sony Vaio TT ", ALC889_FIXUP_VAIO_TT ), <nl> SND_PCI_QUIRK ( 0x104d , 0x905a , " Sony Vaio Z ", ALC882_FIXUP_NO_PRIMARY_HP ), <nl> SND_PCI_QUIRK ( 0x104d , 0x9043 , " Sony Vaio VGC - LN51JGB ", ALC882_FIXUP_NO_PRIMARY_HP ),
static void __br_multicast_send_query ( struct net_bridge * br , <nl> return ; <nl>  <nl> if ( port ) { <nl> - __skb_push ( skb , sizeof ( struct ethhdr )); <nl> skb -> dev = port -> dev ; <nl> NF_HOOK ( NFPROTO_BRIDGE , NF_BR_LOCAL_OUT , skb , NULL , skb -> dev , <nl> - dev_queue_xmit ); <nl> + br_dev_queue_push_xmit ); <nl> } else { <nl> br_multicast_select_own_querier ( br , ip , skb ); <nl> netif_rx ( skb );
static int fcoe_interface_setup ( struct fcoe_interface * fcoe , <nl> if (( netdev -> priv_flags & IFF_MASTER_ALB ) || <nl> ( netdev -> priv_flags & IFF_SLAVE_INACTIVE ) || <nl> ( netdev -> priv_flags & IFF_MASTER_8023AD )) { <nl> + FCOE_NETDEV_DBG ( netdev , " Bonded interfaces not supported \ n "); <nl> return - EOPNOTSUPP ; <nl> } <nl>  <nl> static int fcoe_interface_setup ( struct fcoe_interface * fcoe , <nl> static struct fcoe_interface * fcoe_interface_create ( struct net_device * netdev ) <nl> { <nl> struct fcoe_interface * fcoe ; <nl> + int err ; <nl>  <nl> fcoe = kzalloc ( sizeof (* fcoe ), GFP_KERNEL ); <nl> if (! fcoe ) { <nl> static struct fcoe_interface * fcoe_interface_create ( struct net_device * netdev ) <nl> fcoe -> ctlr . update_mac = fcoe_update_src_mac ; <nl> fcoe -> ctlr . get_src_addr = fcoe_get_src_mac ; <nl>  <nl> - fcoe_interface_setup ( fcoe , netdev ); <nl> + err = fcoe_interface_setup ( fcoe , netdev ); <nl> + if ( err ) { <nl> + fcoe_ctlr_destroy (& fcoe -> ctlr ); <nl> + kfree ( fcoe ); <nl> + dev_put ( netdev ); <nl> + return NULL ; <nl> + } <nl>  <nl> return fcoe ; <nl> }
int ieee80211_register_hw ( struct ieee80211_hw * hw ) <nl> fail_workqueue : <nl> wiphy_unregister ( local -> hw . wiphy ); <nl> fail_wiphy_register : <nl> - kfree ( local -> int_scan_req -> channels ); <nl> + kfree ( local -> int_scan_req ); <nl> return result ; <nl> } <nl> EXPORT_SYMBOL ( ieee80211_register_hw );
compat_ptr ( compat_uptr_t uptr ) <nl> return ( void __user *) ( unsigned long ) uptr ; <nl> } <nl>  <nl> + static inline compat_uptr_t <nl> + ptr_to_compat ( void __user * uptr ) <nl> +{ <nl> + return ( u32 )( unsigned long ) uptr ; <nl> +} <nl> + <nl> static __inline__ void __user * <nl> compat_alloc_user_space ( long len ) <nl> {
static bool is_fullscreen ( struct drm_crtc_state * cstate , <nl> (( pstate -> crtc_y + pstate -> crtc_h ) >= cstate -> mode . vdisplay ); <nl> } <nl>  <nl> - enum mdp_mixer_stage_id get_start_stage ( struct drm_crtc * crtc , <nl> + static enum mdp_mixer_stage_id get_start_stage ( struct drm_crtc * crtc , <nl> struct drm_crtc_state * new_crtc_state , <nl> struct drm_plane_state * bpstate ) <nl> {
static void finish_csr_load ( const struct firmware * fw , void * context ) <nl> } <nl> csr -> mmio_count = dmc_header -> mmio_count ; <nl> for ( i = 0 ; i < dmc_header -> mmio_count ; i ++) { <nl> - if ( dmc_header -> mmioaddr [ i ] < CSR_MMIO_START_RANGE && <nl> + if ( dmc_header -> mmioaddr [ i ] < CSR_MMIO_START_RANGE || <nl> dmc_header -> mmioaddr [ i ] > CSR_MMIO_END_RANGE ) { <nl> DRM_ERROR (" Firmware has wrong mmio address 0x % x \ n ", <nl> dmc_header -> mmioaddr [ i ]);
static int e100_rx_indicate ( struct nic * nic , struct rx * rx , <nl>  <nl> if ( ioread8 (& nic -> csr -> scb . status ) & rus_no_res ) <nl> nic -> ru_running = RU_SUSPENDED ; <nl> + pci_dma_sync_single_for_device ( nic -> pdev , rx -> dma_addr , <nl> + sizeof ( struct rfd ), <nl> + PCI_DMA_BIDIRECTIONAL ); <nl> return - ENODATA ; <nl> } <nl> 
static int usb_console_setup ( struct console * co , char * options ) <nl> tty_kref_put ( tty ); <nl> reset_open_count : <nl> port -> port . count = 0 ; <nl> + info -> port = NULL ; <nl> usb_autopm_put_interface ( serial -> interface ); <nl> error_get_interface : <nl> usb_serial_put ( serial );
static struct ucma_multicast * ucma_alloc_multicast ( struct ucma_context * ctx ) <nl> return NULL ; <nl>  <nl> mutex_lock (& mut ); <nl> - mc -> id = idr_alloc (& multicast_idr , mc , 0 , 0 , GFP_KERNEL ); <nl> + mc -> id = idr_alloc (& multicast_idr , NULL , 0 , 0 , GFP_KERNEL ); <nl> mutex_unlock (& mut ); <nl> if ( mc -> id < 0 ) <nl> goto error ; <nl> static ssize_t ucma_process_join ( struct ucma_file * file , <nl> goto err3 ; <nl> } <nl>  <nl> + mutex_lock (& mut ); <nl> + idr_replace (& multicast_idr , mc , mc -> id ); <nl> + mutex_unlock (& mut ); <nl> + <nl> mutex_unlock (& file -> mut ); <nl> ucma_put_ctx ( ctx ); <nl> return 0 ;
static int ucode_init ( loader_block * lb , amb_dev * dev ) <nl> const struct firmware * fw ; <nl> unsigned long start_address ; <nl> const struct ihex_binrec * rec ; <nl> - const char * errmsg = 0 ; <nl> + const char * errmsg = NULL ; <nl> int res ; <nl>  <nl> res = request_ihex_firmware (& fw , " atmsar11 . fw ", & dev -> pci_dev -> dev );
static int sun4i_tv_comp_get_modes ( struct drm_connector * connector ) <nl> int i ; <nl>  <nl> for ( i = 0 ; i < ARRAY_SIZE ( tv_modes ); i ++) { <nl> - struct drm_display_mode * mode = drm_mode_create ( connector -> dev ); <nl> + struct drm_display_mode * mode ; <nl> const struct tv_mode * tv_mode = & tv_modes [ i ]; <nl>  <nl> + mode = drm_mode_create ( connector -> dev ); <nl> + if (! mode ) { <nl> + DRM_ERROR (" Failed to create a new display mode \ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> strcpy ( mode -> name , tv_mode -> name ); <nl>  <nl> sun4i_tv_mode_to_drm_mode ( tv_mode , mode );
static int tegra_ehci_probe ( struct platform_device * pdev ) <nl> struct tegra_ehci_hcd * tegra ; <nl> int err = 0 ; <nl> int irq ; <nl> - struct device_node * np_phy ; <nl> struct usb_phy * u_phy ; <nl>  <nl> /* Right now device - tree probed devices don ' t get dma_mask set . <nl> static int tegra_ehci_probe ( struct platform_device * pdev ) <nl> udelay ( 1 ); <nl> tegra_periph_reset_deassert ( tegra -> clk ); <nl>  <nl> - np_phy = of_parse_phandle ( pdev -> dev . of_node , " nvidia , phy ", 0 ); <nl> - if (! np_phy ) { <nl> - err = - ENODEV ; <nl> - goto cleanup_clk_en ; <nl> - } <nl> - <nl> - u_phy = tegra_usb_get_phy ( np_phy ); <nl> + u_phy = devm_usb_get_phy_by_phandle (& pdev -> dev , " nvidia , phy ", 0 ); <nl> if ( IS_ERR ( u_phy )) { <nl> err = PTR_ERR ( u_phy ); <nl> goto cleanup_clk_en ;
static u64 __skb_get_nlattr ( u64 ctx , u64 A , u64 X , u64 r4 , u64 r5 ) <nl> if ( skb_is_nonlinear ( skb )) <nl> return 0 ; <nl>  <nl> + if ( skb -> len < sizeof ( struct nlattr )) <nl> + return 0 ; <nl> + <nl> if ( A > skb -> len - sizeof ( struct nlattr )) <nl> return 0 ; <nl>  <nl> static u64 __skb_get_nlattr_nest ( u64 ctx , u64 A , u64 X , u64 r4 , u64 r5 ) <nl> if ( skb_is_nonlinear ( skb )) <nl> return 0 ; <nl>  <nl> + if ( skb -> len < sizeof ( struct nlattr )) <nl> + return 0 ; <nl> + <nl> if ( A > skb -> len - sizeof ( struct nlattr )) <nl> return 0 ; <nl>  <nl> nla = ( struct nlattr *) & skb -> data [ A ]; <nl> - if ( nla -> nla_len > A - skb -> len ) <nl> + if ( nla -> nla_len > skb -> len - A ) <nl> return 0 ; <nl>  <nl> nla = nla_find_nested ( nla , X );
ip_set_net_exit ( struct net * net ) <nl>  <nl> inst -> is_deleted = true ; /* flag for ip_set_nfnl_put */ <nl>  <nl> + nfnl_lock ( NFNL_SUBSYS_IPSET ); <nl> for ( i = 0 ; i < inst -> ip_set_max ; i ++) { <nl> set = ip_set ( inst , i ); <nl> if ( set ) { <nl> ip_set_net_exit ( struct net * net ) <nl> ip_set_destroy_set ( set ); <nl> } <nl> } <nl> + nfnl_unlock ( NFNL_SUBSYS_IPSET ); <nl> kfree ( rcu_dereference_protected ( inst -> ip_set_list , 1 )); <nl> } <nl> 
int thermal_add_hwmon_sysfs ( struct thermal_zone_device * tz ) <nl>  <nl> INIT_LIST_HEAD (& hwmon -> tz_list ); <nl> strlcpy ( hwmon -> type , tz -> type , THERMAL_NAME_LENGTH ); <nl> - hwmon -> device = hwmon_device_register ( NULL ); <nl> + hwmon -> device = hwmon_device_register (& tz -> device ); <nl> if ( IS_ERR ( hwmon -> device )) { <nl> result = PTR_ERR ( hwmon -> device ); <nl> goto free_mem ;
lpfc_issue_els_prli ( struct lpfc_vport * vport , struct lpfc_nodelist * ndlp , <nl> uint16_t cmdsize ; <nl> u32 local_nlp_type , elscmd ; <nl>  <nl> + /* <nl> + * If we are in RSCN mode , the FC4 types supported from a <nl> + * previous GFT_ID command may not be accurate . So , if we <nl> + * are a NVME Initiator , always look for the possibility of <nl> + * the remote NPort beng a NVME Target . <nl> + */ <nl> + if ( phba -> sli_rev == LPFC_SLI_REV4 && <nl> + vport -> fc_flag & FC_RSCN_MODE && <nl> + vport -> nvmei_support ) <nl> + ndlp -> nlp_fc4_type |= NLP_FC4_NVME ; <nl> local_nlp_type = ndlp -> nlp_fc4_type ; <nl>  <nl> send_next_prli :
int reiserfs_acl_chmod ( struct inode * inode ) <nl> return 0 ; <nl> } <nl>  <nl> + reiserfs_write_unlock ( inode -> i_sb ); <nl> acl = reiserfs_get_acl ( inode , ACL_TYPE_ACCESS ); <nl> + reiserfs_write_lock ( inode -> i_sb ); <nl> if (! acl ) <nl> return 0 ; <nl> if ( IS_ERR ( acl ))
static int pid_revalidate ( struct dentry * dentry , struct nameidata * nd ) <nl> } else { <nl> inode -> i_uid = 0 ; <nl> inode -> i_gid = 0 ; <nl> + inode -> i_mode = 0 ; <nl> } <nl> security_task_to_inode ( task , inode ); <nl> put_task_struct ( task );
static void __ibmvnic_reset ( struct work_struct * work ) <nl>  <nl> if ( rc ) { <nl> free_all_rwi ( adapter ); <nl> + mutex_unlock (& adapter -> reset_lock ); <nl> return ; <nl> } <nl> 
void stmmac_remove_config_dt ( struct platform_device * pdev , <nl> if ( of_phy_is_fixed_link ( np )) <nl> of_phy_deregister_fixed_link ( np ); <nl> of_node_put ( plat -> phy_node ); <nl> + of_node_put ( plat -> mdio_node ); <nl> } <nl> # else <nl> struct plat_stmmacenet_data *
static inline int _loop ( unsigned dry_run , u8 buf [], <nl> unsigned lcnt0 , lcnt1 , ljmp0 , ljmp1 ; <nl> struct _arg_LPEND lpend ; <nl>  <nl> + if (* bursts == 1 ) <nl> + return _bursts ( dry_run , buf , pxs , 1 ); <nl> + <nl> /* Max iterations possible in DMALP is 256 */ <nl> if (* bursts >= 256 * 256 ) { <nl> lcnt1 = 256 ;
static int sctp_getsockopt_assoc_stats ( struct sock * sk , int len , <nl> if ( len < sizeof ( sctp_assoc_t )) <nl> return - EINVAL ; <nl>  <nl> + /* Allow the struct to grow and fill in as much as possible */ <nl> + len = min_t ( size_t , len , sizeof ( sas )); <nl> + <nl> if ( copy_from_user (& sas , optval , len )) <nl> return - EFAULT ; <nl>  <nl> static int sctp_getsockopt_assoc_stats ( struct sock * sk , int len , <nl> /* Mark beginning of a new observation period */ <nl> asoc -> stats . max_obs_rto = asoc -> rto_min ; <nl>  <nl> - /* Allow the struct to grow and fill in as much as possible */ <nl> - len = min_t ( size_t , len , sizeof ( sas )); <nl> - <nl> if ( put_user ( len , optlen )) <nl> return - EFAULT ; <nl> 
static int mon_bin_mmap ( struct file * filp , struct vm_area_struct * vma ) <nl> { <nl> /* don ' t do anything here : " fault " will set up page table entries */ <nl> vma -> vm_ops = & mon_bin_vm_ops ; <nl> + <nl> + if ( vma -> vm_flags & VM_WRITE ) <nl> + return - EPERM ; <nl> + <nl> + vma -> vm_flags &= ~ VM_MAYWRITE ; <nl> vma -> vm_flags |= VM_DONTEXPAND | VM_DONTDUMP ; <nl> vma -> vm_private_data = filp -> private_data ; <nl> mon_bin_vma_open ( vma );
int nfc_genl_fw_download_done ( struct nfc_dev * dev , const char * firmware_name , <nl> struct sk_buff * msg ; <nl> void * hdr ; <nl>  <nl> - msg = nlmsg_new ( NLMSG_DEFAULT_SIZE , GFP_KERNEL ); <nl> + msg = nlmsg_new ( NLMSG_DEFAULT_SIZE , GFP_ATOMIC ); <nl> if (! msg ) <nl> return - ENOMEM ; <nl>  <nl> int nfc_genl_fw_download_done ( struct nfc_dev * dev , const char * firmware_name , <nl>  <nl> genlmsg_end ( msg , hdr ); <nl>  <nl> - genlmsg_multicast (& nfc_genl_family , msg , 0 , 0 , GFP_KERNEL ); <nl> + genlmsg_multicast (& nfc_genl_family , msg , 0 , 0 , GFP_ATOMIC ); <nl>  <nl> return 0 ; <nl> 
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> if ( sta -> sta . txq [ 0 ]) <nl> kfree ( to_txq_info ( sta -> sta . txq [ 0 ])); <nl> free : <nl> + free_percpu ( sta -> pcpu_rx_stats ); <nl> # ifdef CONFIG_MAC80211_MESH <nl> kfree ( sta -> mesh ); <nl> # endif
int sctp_inet_listen ( struct socket * sock , int backlog ) <nl> goto out ; <nl>  <nl> /* Allocate HMAC for generating cookie . */ <nl> - if ( sctp_hmac_alg ) { <nl> + if (! sctp_sk ( sk )-> hmac && sctp_hmac_alg ) { <nl> tfm = crypto_alloc_hash ( sctp_hmac_alg , 0 , CRYPTO_ALG_ASYNC ); <nl> if ( IS_ERR ( tfm )) { <nl> if ( net_ratelimit ()) { <nl> int sctp_inet_listen ( struct socket * sock , int backlog ) <nl> goto cleanup ; <nl>  <nl> /* Store away the transform reference . */ <nl> - sctp_sk ( sk )-> hmac = tfm ; <nl> + if (! sctp_sk ( sk )-> hmac ) <nl> + sctp_sk ( sk )-> hmac = tfm ; <nl> out : <nl> sctp_release_sock ( sk ); <nl> return err ;
long dgnc_mgmt_ioctl ( struct file * file , unsigned int cmd , unsigned long arg ) <nl>  <nl> spin_lock_irqsave (& dgnc_global_lock , flags ); <nl>  <nl> + memset (& ddi , 0 , sizeof ( ddi )); <nl> ddi . dinfo_nboards = dgnc_NumBoards ; <nl> sprintf ( ddi . dinfo_version , "% s ", DG_PART ); <nl> 
static void tcp_mark_head_lost ( struct sock * sk , int packets ) <nl> cnt += tcp_skb_pcount ( skb ); <nl>  <nl> if ( cnt > packets ) { <nl> - if ( tcp_is_sack ( tp ) || ( oldcnt >= packets )) <nl> + if (( tcp_is_sack ( tp ) && ! tcp_is_fack ( tp )) || <nl> + ( oldcnt >= packets )) <nl> break ; <nl>  <nl> mss = skb_shinfo ( skb )-> gso_size ;
void ath_tx_aggr_sleep ( struct ieee80211_sta * sta , struct ath_softc * sc , <nl> for ( tidno = 0 , tid = & an -> tid [ tidno ]; <nl> tidno < IEEE80211_NUM_TIDS ; tidno ++, tid ++) { <nl>  <nl> - if (! tid -> sched ) <nl> - continue ; <nl> - <nl> ac = tid -> ac ; <nl> txq = ac -> txq ; <nl>  <nl> ath_txq_lock ( sc , txq ); <nl>  <nl> + if (! tid -> sched ) { <nl> + ath_txq_unlock ( sc , txq ); <nl> + continue ; <nl> + } <nl> + <nl> buffered = ath_tid_has_buffered ( tid ); <nl>  <nl> tid -> sched = false ;
int st_sensors_check_device_support ( struct iio_dev * indio_dev , <nl> break ; <nl> } <nl> if ( n == ARRAY_SIZE ( sensor_settings [ i ]. sensors_supported )) { <nl> - dev_err (& indio_dev -> dev , " device name and WhoAmI mismatch .\ n "); <nl> + dev_err (& indio_dev -> dev , " device name \"% s \" and WhoAmI ( 0x % 02x ) mismatch ", <nl> + indio_dev -> name , wai ); <nl> goto sensor_name_mismatch ; <nl> } <nl> 
static int e1000_set_ringparam ( struct net_device * netdev , <nl> err_alloc_rx : <nl> kfree ( txdr ); <nl> err_alloc_tx : <nl> - e1000_up ( adapter ); <nl> + if ( netif_running ( adapter -> netdev )) <nl> + e1000_up ( adapter ); <nl> err_setup : <nl> clear_bit ( __E1000_RESETTING , & adapter -> flags ); <nl> return err ;
int tcp_sendmsg ( struct sock * sk , struct msghdr * msg , size_t size ) <nl>  <nl> if (! skb_can_coalesce ( skb , i , pfrag -> page , <nl> pfrag -> offset )) { <nl> - if ( i == sysctl_max_skb_frags || ! sg ) { <nl> + if ( i >= sysctl_max_skb_frags || ! sg ) { <nl> tcp_mark_push ( tp , skb ); <nl> goto new_segment ; <nl> }
static __always_inline int kmalloc_index ( size_t size ) <nl> if ( size <= 4 * 1024 ) return 12 ; <nl> /* <nl> * The following is only needed to support architectures with a larger page <nl> - * size than 4k . <nl> + * size than 4k . We need to support 2 * PAGE_SIZE here . So for a 64k page <nl> + * size we would have to go up to 128k . <nl> */ <nl> if ( size <= 8 * 1024 ) return 13 ; <nl> if ( size <= 16 * 1024 ) return 14 ; <nl> static __always_inline int kmalloc_index ( size_t size ) <nl> if ( size <= 512 * 1024 ) return 19 ; <nl> if ( size <= 1024 * 1024 ) return 20 ; <nl> if ( size <= 2 * 1024 * 1024 ) return 21 ; <nl> - return - 1 ; <nl> + BUG (); <nl> + return - 1 ; /* Will never be reached */ <nl>  <nl> /* <nl> * What we really wanted to do and cannot do because of compiler issues is :
long ext4_fallocate ( struct file * file , int mode , loff_t offset , loff_t len ) <nl> blkbits ) >> blkbits )) <nl> new_size = offset + len ; <nl> else <nl> - new_size = ( map . m_lblk + ret ) << blkbits ; <nl> + new_size = (( loff_t ) map . m_lblk + ret ) << blkbits ; <nl>  <nl> ext4_falloc_update_inode ( inode , mode , new_size , <nl> ( map . m_flags & EXT4_MAP_NEW ));
char * cifs_compose_mount_options ( const char * sb_mountdata , <nl> * string to the length of the original string to allow for worst case . <nl> */ <nl> md_len = strlen ( sb_mountdata ) + INET6_ADDRSTRLEN ; <nl> - mountdata = kzalloc ( md_len + 1 , GFP_KERNEL ); <nl> + mountdata = kzalloc ( md_len + sizeof (" ip =") + 1 , GFP_KERNEL ); <nl> if ( mountdata == NULL ) { <nl> rc = - ENOMEM ; <nl> goto compose_mount_options_err ;
static int udf_symlink ( struct inode * dir , struct dentry * dentry , <nl>  <nl> fi = udf_add_entry ( dir , dentry , & fibh , & cfi , & err ); <nl> if (! fi ) <nl> - goto out_no_entry ; <nl> + goto out_fail ; <nl> cfi . icb . extLength = cpu_to_le32 ( sb -> s_blocksize ); <nl> cfi . icb . extLocation = cpu_to_lelb ( iinfo -> i_location ); <nl> if ( UDF_SB ( inode -> i_sb )-> s_lvid_bh ) { <nl> static int udf_symlink ( struct inode * dir , struct dentry * dentry , <nl>  <nl> out_no_entry : <nl> up_write (& iinfo -> i_data_sem ); <nl> + out_fail : <nl> inode_dec_link_count ( inode ); <nl> iput ( inode ); <nl> goto out ;
static int snd_nativeinstruments_control_get ( struct snd_kcontrol * kcontrol , <nl> if ( mixer -> chip -> shutdown ) <nl> ret = - ENODEV ; <nl> else <nl> - ret = usb_control_msg ( dev , usb_rcvctrlpipe ( dev , 0 ), bRequest , <nl> + ret = snd_usb_ctl_msg ( dev , usb_rcvctrlpipe ( dev , 0 ), bRequest , <nl> USB_TYPE_VENDOR | USB_RECIP_DEVICE | USB_DIR_IN , <nl> 0 , wIndex , <nl> - & tmp , sizeof ( tmp ), 1000 ); <nl> + & tmp , sizeof ( tmp )); <nl> up_read (& mixer -> chip -> shutdown_rwsem ); <nl>  <nl> if ( ret < 0 ) {
static void perf_top__start_counters ( struct perf_top * top ) <nl> * based cpu - clock - tick sw counter , which <nl> * is always available even if no PMU support : <nl> */ <nl> - if ( attr -> type == PERF_TYPE_HARDWARE && <nl> - attr -> config == PERF_COUNT_HW_CPU_CYCLES ) { <nl> + if (( err == ENOENT || err == ENXIO ) && <nl> + ( attr -> type == PERF_TYPE_HARDWARE ) && <nl> + ( attr -> config == PERF_COUNT_HW_CPU_CYCLES )) { <nl> + <nl> if ( verbose ) <nl> ui__warning (" Cycles event not supported ,\ n " <nl> " trying to fall back to cpu - clock - ticks \ n ");
static void ath5k_configure_filter ( struct ieee80211_hw * hw , <nl> sc -> opmode != NL80211_IFTYPE_MESH_POINT && <nl> test_bit ( ATH_STAT_PROMISC , sc -> status )) <nl> rfilt |= AR5K_RX_FILTER_PROM ; <nl> - if ( sc -> opmode == NL80211_IFTYPE_ADHOC ) <nl> + if ( sc -> opmode == NL80211_IFTYPE_STATION || <nl> + sc -> opmode == NL80211_IFTYPE_ADHOC ) { <nl> rfilt |= AR5K_RX_FILTER_BEACON ; <nl> + } <nl>  <nl> /* Set filters */ <nl> ath5k_hw_set_rx_filter ( ah , rfilt );
static int nand_default_block_markbad ( struct mtd_info * mtd , loff_t ofs ) <nl> int block , ret ; <nl>  <nl> /* Get block number */ <nl> - block = (( int ) ofs ) >> chip -> bbt_erase_shift ; <nl> + block = ( int )( ofs >> chip -> bbt_erase_shift ); <nl> if ( chip -> bbt ) <nl> chip -> bbt [ block >> 2 ] |= 0x01 << (( block & 0x03 ) << 1 ); <nl> 
int __kprobes arch_prepare_kprobe ( struct kprobe * p ) <nl> if (! ret ) { <nl> memcpy ( p -> ainsn . insn , p -> addr , MAX_INSN_SIZE * sizeof ( kprobe_opcode_t )); <nl> p -> opcode = * p -> addr ; <nl> + flush_icache_range (( unsigned long ) p -> ainsn . insn , <nl> + ( unsigned long ) p -> ainsn . insn + sizeof ( kprobe_opcode_t )); <nl> } <nl>  <nl> return ret ;
nfs4_open_delegation ( struct svc_fh * fh , struct nfsd4_open * open , struct nfs4_sta <nl> locks_init_lock (& fl ); <nl> fl . fl_lmops = & nfsd_lease_mng_ops ; <nl> fl . fl_flags = FL_LEASE ; <nl> + fl . fl_type = flag == NFS4_OPEN_DELEGATE_READ ? F_RDLCK : F_WRLCK ; <nl> fl . fl_end = OFFSET_MAX ; <nl> fl . fl_owner = ( fl_owner_t ) dp ; <nl> fl . fl_file = stp -> st_vfs_file ; <nl> nfs4_open_delegation ( struct svc_fh * fh , struct nfsd4_open * open , struct nfs4_sta <nl> /* vfs_setlease checks to see if delegation should be handed out . <nl> * the lock_manager callbacks fl_mylease and fl_change are used <nl> */ <nl> - if (( status = vfs_setlease ( stp -> st_vfs_file , <nl> - flag == NFS4_OPEN_DELEGATE_READ ? F_RDLCK : F_WRLCK , & flp ))) { <nl> + if (( status = vfs_setlease ( stp -> st_vfs_file , fl . fl_type , & flp ))) { <nl> dprintk (" NFSD : setlease failed [% d ], no delegation \ n ", status ); <nl> unhash_delegation ( dp ); <nl> flag = NFS4_OPEN_DELEGATE_NONE ;
static int ems_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> goto err ; <nl> } <nl>  <nl> - emsff_init ( hdev ); <nl> + ret = emsff_init ( hdev ); <nl> + if ( ret ) { <nl> + dev_err (& hdev -> dev , " force feedback init failed \ n "); <nl> + hid_hw_stop ( hdev ); <nl> + goto err ; <nl> + } <nl>  <nl> return 0 ; <nl> err :
static int mwifiex_pcie_init_evt_ring ( struct mwifiex_adapter * adapter ) <nl> skb_put ( skb , MAX_EVENT_SIZE ); <nl>  <nl> if ( mwifiex_map_pci_memory ( adapter , skb , MAX_EVENT_SIZE , <nl> - PCI_DMA_FROMDEVICE )) <nl> + PCI_DMA_FROMDEVICE )) { <nl> + kfree_skb ( skb ); <nl> + kfree ( card -> evtbd_ring_vbase ); <nl> return - 1 ; <nl> + } <nl>  <nl> buf_pa = MWIFIEX_SKB_DMA_ADDR ( skb ); <nl> 
static __latent_entropy struct task_struct * copy_process ( <nl> /* ok , now we should be set up .. */ <nl> p -> pid = pid_nr ( pid ); <nl> if ( clone_flags & CLONE_THREAD ) { <nl> - p -> exit_signal = - 1 ; <nl> p -> group_leader = current -> group_leader ; <nl> p -> tgid = current -> tgid ; <nl> } else { <nl> - if ( clone_flags & CLONE_PARENT ) <nl> - p -> exit_signal = current -> group_leader -> exit_signal ; <nl> - else <nl> - p -> exit_signal = args -> exit_signal ; <nl> p -> group_leader = p ; <nl> p -> tgid = p -> pid ; <nl> } <nl> static __latent_entropy struct task_struct * copy_process ( <nl> if ( clone_flags & ( CLONE_PARENT | CLONE_THREAD )) { <nl> p -> real_parent = current -> real_parent ; <nl> p -> parent_exec_id = current -> parent_exec_id ; <nl> + if ( clone_flags & CLONE_THREAD ) <nl> + p -> exit_signal = - 1 ; <nl> + else <nl> + p -> exit_signal = current -> group_leader -> exit_signal ; <nl> } else { <nl> p -> real_parent = current ; <nl> p -> parent_exec_id = current -> self_exec_id ; <nl> + p -> exit_signal = args -> exit_signal ; <nl> } <nl>  <nl> klp_copy_process ( p );
int sock_setsockopt ( struct socket * sock , int level , int optname , <nl> sock_reset_flag ( sk , SOCK_LINGER ); <nl> else { <nl> # if ( BITS_PER_LONG == 32 ) <nl> - if ( ling . l_linger >= MAX_SCHEDULE_TIMEOUT / HZ ) <nl> + if (( unsigned int ) ling . l_linger >= MAX_SCHEDULE_TIMEOUT / HZ ) <nl> sk -> sk_lingertime = MAX_SCHEDULE_TIMEOUT ; <nl> else <nl> # endif <nl> - sk -> sk_lingertime = ling . l_linger * HZ ; <nl> + sk -> sk_lingertime = ( unsigned int ) ling . l_linger * HZ ; <nl> sock_set_flag ( sk , SOCK_LINGER ); <nl> } <nl> break ;
int msp34xxg_thread ( void * data ) <nl> /* setup the chip */ <nl> msp34xxg_reset ( client ); <nl> state -> std = state -> radio ? 0x40 : msp_standard ; <nl> - if ( state -> std != 1 ) <nl> - goto unmute ; <nl> /* start autodetect */ <nl> msp_write_dem ( client , 0x20 , state -> std ); <nl> + if ( state -> std != 1 ) <nl> + goto unmute ; <nl>  <nl> /* watch autodetect */ <nl> v4l_dbg ( 1 , msp_debug , client , " started autodetect , waiting for result \ n ");
static long __validate_layout ( struct ceph_mds_client * mdsc , <nl> /* validate striping parameters */ <nl> if (( l -> object_size & ~ PAGE_MASK ) || <nl> ( l -> stripe_unit & ~ PAGE_MASK ) || <nl> - (( unsigned ) l -> object_size % ( unsigned ) l -> stripe_unit )) <nl> + ( l -> stripe_unit != 0 && <nl> + (( unsigned ) l -> object_size % ( unsigned ) l -> stripe_unit ))) <nl> return - EINVAL ; <nl>  <nl> /* make sure it ' s a valid data pool */
int rt2x00lib_probe_dev ( struct rt2x00_dev * rt2x00dev ) <nl> BIT ( NL80211_IFTYPE_MESH_POINT ) | <nl> BIT ( NL80211_IFTYPE_WDS ); <nl>  <nl> + /* <nl> + * Initialize configuration work . <nl> + */ <nl> + INIT_WORK (& rt2x00dev -> intf_work , rt2x00lib_intf_scheduled ); <nl> + <nl> /* <nl> * Let the driver probe the device to detect the capabilities . <nl> */ <nl> int rt2x00lib_probe_dev ( struct rt2x00_dev * rt2x00dev ) <nl> goto exit ; <nl> } <nl>  <nl> - /* <nl> - * Initialize configuration work . <nl> - */ <nl> - INIT_WORK (& rt2x00dev -> intf_work , rt2x00lib_intf_scheduled ); <nl> - <nl> /* <nl> * Allocate queue array . <nl> */
static int find_first_block_group ( struct btrfs_root * root , <nl> } else { <nl> ret = 0 ; <nl> } <nl> + free_extent_map ( em ); <nl> goto out ; <nl> } <nl> path -> slots [ 0 ]++;
struct ring_buffer * __ring_buffer_alloc ( unsigned long size , unsigned flags , <nl> buffer -> reader_lock_key = key ; <nl>  <nl> /* need at least two pages */ <nl> - if ( buffer -> pages == 1 ) <nl> - buffer -> pages ++; <nl> + if ( buffer -> pages < 2 ) <nl> + buffer -> pages = 2 ; <nl>  <nl> /* <nl> * In case of non - hotplug cpu , if the ring - buffer is allocated
static void tcp_cwnd_reduction ( struct sock * sk , const int prior_unsacked , <nl> int newly_acked_sacked = prior_unsacked - <nl> ( tp -> packets_out - tp -> sacked_out ); <nl>  <nl> + if ( newly_acked_sacked <= 0 || WARN_ON_ONCE (! tp -> prior_cwnd )) <nl> + return ; <nl> + <nl> tp -> prr_delivered += newly_acked_sacked ; <nl> if ( delta < 0 ) { <nl> u64 dividend = ( u64 ) tp -> snd_ssthresh * tp -> prr_delivered +
int wusb_dev_sec_add ( struct wusbhc * wusbhc , <nl> const void * itr , * top ; <nl> char buf [ 64 ]; <nl>  <nl> - secd = kmalloc ( sizeof ( struct usb_security_descriptor ), GFP_KERNEL ); <nl> + secd = kmalloc ( sizeof (* secd ), GFP_KERNEL ); <nl> if ( secd == NULL ) { <nl> result = - ENOMEM ; <nl> goto out ; <nl> } <nl>  <nl> result = usb_get_descriptor ( usb_dev , USB_DT_SECURITY , <nl> - 0 , secd , sizeof ( struct usb_security_descriptor )); <nl> - if ( result < sizeof ( secd )) { <nl> + 0 , secd , sizeof (* secd )); <nl> + if ( result < sizeof (* secd )) { <nl> dev_err ( dev , " Can ' t read security descriptor or " <nl> " not enough data : % d \ n ", result ); <nl> goto out ;
static long __media_device_enum_links ( struct media_device * mdev , <nl>  <nl> for ( p = 0 ; p < entity -> num_pads ; p ++) { <nl> struct media_pad_desc pad ; <nl> + <nl> + memset (& pad , 0 , sizeof ( pad )); <nl> media_device_kpad_to_upad (& entity -> pads [ p ], & pad ); <nl> if ( copy_to_user (& links -> pads [ p ], & pad , sizeof ( pad ))) <nl> return - EFAULT ; <nl> static long __media_device_enum_links ( struct media_device * mdev , <nl> if ( entity -> links [ l ]. source -> entity != entity ) <nl> continue ; <nl>  <nl> + memset (& link , 0 , sizeof ( link )); <nl> media_device_kpad_to_upad ( entity -> links [ l ]. source , <nl> & link . source ); <nl> media_device_kpad_to_upad ( entity -> links [ l ]. sink ,
static ssize_t state_store ( struct subsystem * subsys , const char * buf , size_t n <nl> if (* s && ! strncmp ( buf , * s , len )) <nl> break ; <nl> } <nl> - if (* s ) <nl> + if ( state < PM_SUSPEND_MAX && * s ) <nl> error = enter_state ( state ); <nl> else <nl> error = - EINVAL ;
int svc_register ( const struct svc_serv * serv , struct net * net , <nl> if ( vers -> vs_hidden ) <nl> continue ; <nl>  <nl> + /* <nl> + * Don ' t register a UDP port if we need congestion <nl> + * control . <nl> + */ <nl> + if ( vers -> vs_need_cong_ctrl && proto == IPPROTO_UDP ) <nl> + continue ; <nl> + <nl> error = __svc_register ( net , progp -> pg_name , progp -> pg_prog , <nl> i , family , proto , port ); <nl> 
static int ccs811_start_sensor_application ( struct i2c_client * client ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if (( ret & CCS811_STATUS_FW_MODE_APPLICATION )) <nl> + return 0 ; <nl> + <nl> if (( ret & CCS811_STATUS_APP_VALID_MASK ) != <nl> CCS811_STATUS_APP_VALID_LOADED ) <nl> return - EIO ;
int tracing_update_buffers ( void ) <nl> { <nl> int ret = 0 ; <nl>  <nl> + mutex_lock (& trace_types_lock ); <nl> if (! ring_buffer_expanded ) <nl> ret = tracing_resize_ring_buffer ( trace_buf_size ); <nl> + mutex_unlock (& trace_types_lock ); <nl>  <nl> return ret ; <nl> } <nl> static int tracing_set_tracer ( const char * buf ) <nl> struct tracer * t ; <nl> int ret = 0 ; <nl>  <nl> + mutex_lock (& trace_types_lock ); <nl> + <nl> if (! ring_buffer_expanded ) { <nl> ret = tracing_resize_ring_buffer ( trace_buf_size ); <nl> if ( ret < 0 ) <nl> static int tracing_set_tracer ( const char * buf ) <nl> ret = 0 ; <nl> } <nl>  <nl> - mutex_lock (& trace_types_lock ); <nl> for ( t = trace_types ; t ; t = t -> next ) { <nl> if ( strcmp ( t -> name , buf ) == 0 ) <nl> break ;
void rtl92se_disable_interrupt ( struct ieee80211_hw * hw ) <nl> rtl_write_dword ( rtlpriv , INTA_MASK + 4 , 0 ); <nl>  <nl> rtlpci -> irq_enabled = false ; <nl> + synchronize_irq ( rtlpci -> pdev -> irq ); <nl> } <nl>  <nl> 
static inline void wilc_wfi_cfg_parse_ch_attr ( u8 * buf , u32 len , u8 sta_ch ) <nl> if ( index + sizeof (* e ) + attr_size > len ) <nl> return ; <nl>  <nl> - if ( e -> attr_type == IEEE80211_P2P_ATTR_CHANNEL_LIST ) <nl> + if ( e -> attr_type == IEEE80211_P2P_ATTR_CHANNEL_LIST && <nl> + attr_size >= ( sizeof ( struct wilc_attr_ch_list ) - sizeof (* e ))) <nl> ch_list_idx = index ; <nl> else if ( e -> attr_type == IEEE80211_P2P_ATTR_OPER_CHANNEL && <nl> attr_size == ( sizeof ( struct wilc_attr_oper_ch ) - sizeof (* e )))
int kobject_uevent_env ( struct kobject * kobj , enum kobject_action action , <nl> /* keys passed in from the caller */ <nl> if ( envp_ext ) { <nl> for ( i = 0 ; envp_ext [ i ]; i ++) { <nl> - retval = add_uevent_var ( env , envp_ext [ i ]); <nl> + retval = add_uevent_var ( env , "% s ", envp_ext [ i ]); <nl> if ( retval ) <nl> goto exit ; <nl> }
static int ll_ioctl_fiemap ( struct inode * inode , unsigned long arg ) <nl> if ( get_user ( extent_count , <nl> &(( struct ll_user_fiemap __user *) arg )-> fm_extent_count )) <nl> return - EFAULT ; <nl> + <nl> + if ( extent_count >= <nl> + ( SIZE_MAX - sizeof (* fiemap_s )) / sizeof ( struct ll_fiemap_extent )) <nl> + return - EINVAL ; <nl> num_bytes = sizeof (* fiemap_s ) + ( extent_count * <nl> sizeof ( struct ll_fiemap_extent )); <nl> 
static int __devinit tps65910_rtc_probe ( struct platform_device * pdev ) <nl> return ret ; <nl>  <nl> dev_dbg (& pdev -> dev , " Enabling rtc - tps65910 .\ n "); <nl> + <nl> + /* Enable RTC digital power domain */ <nl> + ret = regmap_update_bits ( tps65910 -> regmap , TPS65910_DEVCTRL , <nl> + DEVCTRL_RTC_PWDN_MASK , 0 << DEVCTRL_RTC_PWDN_SHIFT ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + <nl> rtc_reg = TPS65910_RTC_CTRL_STOP_RTC ; <nl> ret = regmap_write ( tps65910 -> regmap , TPS65910_RTC_CTRL , rtc_reg ); <nl> if ( ret < 0 )
static void ndisc_router_discovery ( struct sk_buff * skb ) <nl> if ( rt ) <nl> rt6_set_expires ( rt , jiffies + ( HZ * lifetime )); <nl> if ( ra_msg -> icmph . icmp6_hop_limit ) { <nl> - in6_dev -> cnf . hop_limit = ra_msg -> icmph . icmp6_hop_limit ; <nl> + /* Only set hop_limit on the interface if it is higher than <nl> + * the current hop_limit . <nl> + */ <nl> + if ( in6_dev -> cnf . hop_limit < ra_msg -> icmph . icmp6_hop_limit ) { <nl> + in6_dev -> cnf . hop_limit = ra_msg -> icmph . icmp6_hop_limit ; <nl> + } else { <nl> + ND_PRINTK ( 2 , warn , " RA : Got route advertisement with lower hop_limit than current \ n "); <nl> + } <nl> if ( rt ) <nl> dst_metric_set (& rt -> dst , RTAX_HOPLIMIT , <nl> ra_msg -> icmph . icmp6_hop_limit );
* <nl> * For licensing information , see the file ' LICENCE ' in this directory . <nl> * <nl> - * $ Id : nodemgmt . c , v 1 . 115 2004 / 11 / 22 11 : 07 : 21 dwmw2 Exp $ <nl> + * $ Id : nodemgmt . c , v 1 . 116 2005 / 01 / 24 21 : 30 : 22 hammache Exp $ <nl> * <nl> */ <nl>  <nl> int jffs2_add_physical_node_ref ( struct jffs2_sb_info * c , struct jffs2_raw_node_r <nl>  <nl> D1 ( printk ( KERN_DEBUG " jffs2_add_physical_node_ref (): Node at 0x % x (% d ), size 0x % x \ n ", ref_offset ( new ), ref_flags ( new ), len )); <nl> # if 1 <nl> - if ( jeb != c -> nextblock || ( ref_offset ( new )) != jeb -> offset + ( c -> sector_size - jeb -> free_size )) { <nl> + /* we could get some obsolete nodes after nextblock was refiled <nl> + in wbuf . c */ <nl> + if ( ( c -> nextblock || ! ref_obsolete ( new )) <nl> + &&( jeb != c -> nextblock || ( ref_offset ( new )) != jeb -> offset + ( c -> sector_size - jeb -> free_size ))) { <nl> printk ( KERN_WARNING " argh . node added in wrong place \ n "); <nl> jffs2_free_raw_node_ref ( new ); <nl> return - EINVAL ; <nl> int jffs2_add_physical_node_ref ( struct jffs2_sb_info * c , struct jffs2_raw_node_r <nl> c -> used_size += len ; <nl> } <nl>  <nl> - if (! jeb -> free_size && ! jeb -> dirty_size ) { <nl> + if (! jeb -> free_size && ! jeb -> dirty_size && ! jeb -> wasted_size ) { <nl> /* If it lives on the dirty_list , jffs2_reserve_space will put it there */ <nl> D1 ( printk ( KERN_DEBUG " Adding full erase block at 0x % 08x to clean_list ( free 0x % 08x , dirty 0x % 08x , used 0x % 08x \ n ", <nl> jeb -> offset , jeb -> free_size , jeb -> dirty_size , jeb -> used_size ));
static int _search_rsb ( struct dlm_ls * ls , char * name , int len , int b , <nl> kref_get (& r -> res_ref ); <nl> goto out ; <nl> } <nl> + if ( error == - ENOTBLK ) <nl> + goto out ; <nl> + <nl> error = dlm_search_rsb_tree (& ls -> ls_rsbtbl [ b ]. toss , name , len , flags , & r ); <nl> if ( error ) <nl> goto out ;
static void outbound_phy_packet_callback ( struct fw_packet * packet , <nl> { <nl> struct outbound_phy_packet_event * e = <nl> container_of ( packet , struct outbound_phy_packet_event , p ); <nl> + struct client * e_client ; <nl>  <nl> switch ( status ) { <nl> /* expected : */ <nl> static void outbound_phy_packet_callback ( struct fw_packet * packet , <nl> } <nl> e -> phy_packet . data [ 0 ] = packet -> timestamp ; <nl>  <nl> + e_client = e -> client ; <nl> queue_event ( e -> client , & e -> event , & e -> phy_packet , <nl> sizeof ( e -> phy_packet ) + e -> phy_packet . length , NULL , 0 ); <nl> - client_put ( e -> client ); <nl> + client_put ( e_client ); <nl> } <nl>  <nl> static int ioctl_send_phy_packet ( struct client * client , union ioctl_arg * arg )
static int edac_create_csrow_object ( struct mem_ctl_info * mci , <nl> csrow -> dev . bus = & mci -> bus ; <nl> device_initialize (& csrow -> dev ); <nl> csrow -> dev . parent = & mci -> dev ; <nl> + csrow -> mci = mci ; <nl> dev_set_name (& csrow -> dev , " csrow % d ", index ); <nl> dev_set_drvdata (& csrow -> dev , csrow ); <nl> 
void iwl_mvm_rs_tx_status ( struct iwl_mvm * mvm , struct ieee80211_sta * sta , <nl> * first index into rate scale table . <nl> */ <nl> if ( info -> flags & IEEE80211_TX_STAT_AMPDU ) { <nl> + /* ampdu_ack_len = 0 marks no BA was received . In this case <nl> + * treat it as a single frame loss as we don ' t want the success <nl> + * ratio to dip too quickly because a BA wasn ' t received <nl> + */ <nl> + if ( info -> status . ampdu_ack_len == 0 ) <nl> + info -> status . ampdu_len = 1 ; <nl> + <nl> ucode_rate = le32_to_cpu ( table -> rs_table [ 0 ]); <nl> rs_rate_from_ucode_rate ( ucode_rate , info -> band , & rate ); <nl> rs_collect_tx_data ( lq_sta , curr_tbl , rate . index ,
void <nl> free_session ( struct kref * kref ) <nl> { <nl> struct nfsd4_session * ses ; <nl> + int mem ; <nl>  <nl> ses = container_of ( kref , struct nfsd4_session , se_ref ); <nl> spin_lock (& nfsd_drc_lock ); <nl> - nfsd_drc_mem_used -= ses -> se_fchannel . maxreqs * NFSD_SLOT_CACHE_SIZE ; <nl> + mem = ses -> se_fchannel . maxreqs <nl> + * ( ses -> se_fchannel . maxresp_cached - NFSD_MIN_HDR_SEQ_SZ ); <nl> + nfsd_drc_mem_used -= mem ; <nl> spin_unlock (& nfsd_drc_lock ); <nl> free_session_slots ( ses ); <nl> kfree ( ses );
void __iomem * __ioremap ( unsigned long addr , unsigned long size , <nl> pa = addr & PAGE_MASK ; <nl> size = PAGE_ALIGN ( addr + size ) - pa ; <nl>  <nl> - if ( size == 0 ) <nl> + if (( size == 0 ) || ( pa == 0 )) <nl> return NULL ; <nl>  <nl> if ( mem_init_done ) {
minstrel_get_rate ( void * priv , struct ieee80211_sta * sta , <nl> return ; <nl> # endif <nl>  <nl> + /* Don ' t use EAPOL frames for sampling on non - mrr hw */ <nl> + if ( mp -> hw -> max_rates == 1 && <nl> + ( info -> control . flags & IEEE80211_TX_CTRL_PORT_CTRL_PROTO )) <nl> + return ; <nl> + <nl> delta = ( mi -> total_packets * sampling_ratio / 100 ) - <nl> ( mi -> sample_packets + mi -> sample_deferred / 2 ); <nl> 
static int get_neighbors ( struct tree_balance * tb , int h ) <nl> tb -> FL [ h ]) ? tb -> lkey [ h ] : B_NR_ITEMS ( tb -> <nl> FL [ h ]); <nl> son_number = B_N_CHILD_NUM ( tb -> FL [ h ], child_position ); <nl> + reiserfs_write_unlock ( sb ); <nl> bh = sb_bread ( sb , son_number ); <nl> + reiserfs_write_lock ( sb ); <nl> if (! bh ) <nl> return IO_ERROR ; <nl> if ( FILESYSTEM_CHANGED_TB ( tb )) { <nl> static int get_neighbors ( struct tree_balance * tb , int h ) <nl> child_position = <nl> ( bh == tb -> FR [ h ]) ? tb -> rkey [ h ] + 1 : 0 ; <nl> son_number = B_N_CHILD_NUM ( tb -> FR [ h ], child_position ); <nl> + reiserfs_write_unlock ( sb ); <nl> bh = sb_bread ( sb , son_number ); <nl> + reiserfs_write_lock ( sb ); <nl> if (! bh ) <nl> return IO_ERROR ; <nl> if ( FILESYSTEM_CHANGED_TB ( tb )) {
void cs46xx_dsp_destroy_pcm_channel ( struct snd_cs46xx * chip , <nl> if (! pcm_channel -> src_scb -> ref_count ) { <nl> cs46xx_dsp_remove_scb ( chip , pcm_channel -> src_scb ); <nl>  <nl> - snd_assert ( pcm_channel -> src_slot >= 0 && pcm_channel -> src_slot <= DSP_MAX_SRC_NR , <nl> + snd_assert ( pcm_channel -> src_slot >= 0 && pcm_channel -> src_slot < DSP_MAX_SRC_NR , <nl> return ); <nl>  <nl> ins -> src_scb_slots [ pcm_channel -> src_slot ] = 0 ;
ext4_mb_load_buddy ( struct super_block * sb , ext4_group_t group , <nl> grp = ext4_get_group_info ( sb , group ); <nl>  <nl> e4b -> bd_blkbits = sb -> s_blocksize_bits ; <nl> - e4b -> bd_info = ext4_get_group_info ( sb , group ); <nl> + e4b -> bd_info = grp ; <nl> e4b -> bd_sb = sb ; <nl> e4b -> bd_group = group ; <nl> e4b -> bd_buddy_page = NULL ;
intel_edp_init_dpcd ( struct intel_dp * intel_dp ) <nl> /* Read the eDP Display control capabilities registers */ <nl> if (( intel_dp -> dpcd [ DP_EDP_CONFIGURATION_CAP ] & DP_DPCD_DISPLAY_CONTROL_CAPABLE ) && <nl> drm_dp_dpcd_read (& intel_dp -> aux , DP_EDP_DPCD_REV , <nl> - intel_dp -> edp_dpcd , sizeof ( intel_dp -> edp_dpcd ) == <nl> - sizeof ( intel_dp -> edp_dpcd ))) <nl> + intel_dp -> edp_dpcd , sizeof ( intel_dp -> edp_dpcd )) == <nl> + sizeof ( intel_dp -> edp_dpcd )) <nl> DRM_DEBUG_KMS (" EDP DPCD : %* ph \ n ", ( int ) sizeof ( intel_dp -> edp_dpcd ), <nl> intel_dp -> edp_dpcd ); <nl> 
static void cpufreq_stats_free_table ( unsigned int cpu ) <nl> static void cpufreq_stats_free_sysfs ( unsigned int cpu ) <nl> { <nl> struct cpufreq_policy * policy = cpufreq_cpu_get ( cpu ); <nl> + <nl> + if (! cpufreq_frequency_get_table ( cpu )) <nl> + return ; <nl> + <nl> if ( policy && ! policy_is_shared ( policy )) { <nl> pr_debug ("% s : Free sysfs stat \ n ", __func__ ); <nl> sysfs_remove_group (& policy -> kobj , & stats_attr_group );
static void send_handler ( struct ib_mad_agent * agent , <nl> memcpy ( timeout -> mad . data , packet -> mad . data , <nl> sizeof ( struct ib_mad_hdr )); <nl>  <nl> - if (! queue_packet ( file , agent , timeout )) <nl> - return ; <nl> + if ( queue_packet ( file , agent , timeout )) <nl> + kfree ( timeout ); <nl> } <nl> out : <nl> kfree ( packet );
struct lcd_device * lcd_device_register ( const char * name , struct device * parent , <nl>  <nl> rc = device_register (& new_ld -> dev ); <nl> if ( rc ) { <nl> - kfree ( new_ld ); <nl> + put_device (& new_ld -> dev ); <nl> return ERR_PTR ( rc ); <nl> } <nl> 
static void set_hp_led_gpio ( struct hda_codec * codec ) <nl> struct sigmatel_spec * spec = codec -> spec ; <nl> unsigned int gpio ; <nl>  <nl> + if ( spec -> gpio_led ) <nl> + return ; <nl> + <nl> gpio = snd_hda_param_read ( codec , codec -> afg , AC_PAR_GPIO_CAP ); <nl> gpio &= AC_GPIO_IO_COUNT ; <nl> if ( gpio > 3 ) <nl> static int patch_stac92hd71bxx ( struct hda_codec * codec ) <nl> * detection . <nl> */ <nl> spec -> hp_detect = 1 ; <nl> + spec -> gpio_led = 0x01 ; <nl> break ; <nl> case STAC_HP_HDX : <nl> spec -> num_dmics = 1 ; <nl> spec -> num_dmuxes = 1 ; <nl> spec -> num_smuxes = 1 ; <nl> + spec -> gpio_led = 0x08 ; <nl> break ; <nl> } <nl> 
struct g2d_runqueue_node { <nl> struct list_head list ; <nl> struct list_head run_cmdlist ; <nl> struct list_head event_list ; <nl> + pid_t pid ; <nl> struct completion complete ; <nl> int async ; <nl> }; <nl> int exynos_g2d_exec_ioctl ( struct drm_device * drm_dev , void * data , <nl> } <nl>  <nl> mutex_lock (& g2d -> runqueue_mutex ); <nl> + runqueue_node -> pid = current -> pid ; <nl> list_add_tail (& runqueue_node -> list , & g2d -> runqueue ); <nl> if (! g2d -> runqueue_node ) <nl> g2d_exec_runqueue ( g2d );
void intel_irq_init ( struct drm_device * dev ) <nl> dev -> driver -> get_vblank_counter = gm45_get_vblank_counter ; <nl> } <nl>  <nl> - <nl> - dev -> driver -> get_vblank_timestamp = i915_get_vblank_timestamp ; <nl> + if ( drm_core_check_feature ( dev , DRIVER_MODESET )) <nl> + dev -> driver -> get_vblank_timestamp = i915_get_vblank_timestamp ; <nl> + else <nl> + dev -> driver -> get_vblank_timestamp = NULL ; <nl> dev -> driver -> get_scanout_position = i915_get_crtc_scanoutpos ; <nl>  <nl> if ( IS_IVYBRIDGE ( dev )) {
static int configure_tda827x_fe ( struct saa7134_dev * dev , <nl> /* Get the first frontend */ <nl> fe0 = videobuf_dvb_get_frontend (& dev -> frontends , 1 ); <nl>  <nl> + if (! fe0 ) <nl> + return - EINVAL ; <nl> + <nl> fe0 -> dvb . frontend = dvb_attach ( tda10046_attach , cdec_conf , & dev -> i2c_adap ); <nl> if ( fe0 -> dvb . frontend ) { <nl> if ( cdec_conf -> i2c_gate )
static char ql_stats_str_arr [][ ETH_GSTRING_LEN ] = { <nl> static void ql_get_strings ( struct net_device * dev , u32 stringset , u8 * buf ) <nl> { <nl> switch ( stringset ) { <nl> + case ETH_SS_TEST : <nl> + memcpy ( buf , * ql_gstrings_test , QLGE_TEST_LEN * ETH_GSTRING_LEN ); <nl> + break ; <nl> case ETH_SS_STATS : <nl> memcpy ( buf , ql_stats_str_arr , sizeof ( ql_stats_str_arr )); <nl> break ; <nl> static void ql_self_test ( struct net_device * ndev , <nl> { <nl> struct ql_adapter * qdev = netdev_priv ( ndev ); <nl>  <nl> + memset ( data , 0 , sizeof ( u64 ) * QLGE_TEST_LEN ); <nl> + <nl> if ( netif_running ( ndev )) { <nl> set_bit ( QL_SELFTEST , & qdev -> flags ); <nl> if ( eth_test -> flags == ETH_TEST_FL_OFFLINE ) {
bool acpi_dev_resource_memory ( struct acpi_resource * ares , struct resource * res ) <nl> switch ( ares -> type ) { <nl> case ACPI_RESOURCE_TYPE_MEMORY24 : <nl> memory24 = & ares -> data . memory24 ; <nl> + if (! memory24 -> address_length ) <nl> + return false ; <nl> acpi_dev_get_memresource ( res , memory24 -> minimum , <nl> memory24 -> address_length , <nl> memory24 -> write_protect ); <nl> break ; <nl> case ACPI_RESOURCE_TYPE_MEMORY32 : <nl> memory32 = & ares -> data . memory32 ; <nl> + if (! memory32 -> address_length ) <nl> + return false ; <nl> acpi_dev_get_memresource ( res , memory32 -> minimum , <nl> memory32 -> address_length , <nl> memory32 -> write_protect ); <nl> break ; <nl> case ACPI_RESOURCE_TYPE_FIXED_MEMORY32 : <nl> fixed_memory32 = & ares -> data . fixed_memory32 ; <nl> + if (! fixed_memory32 -> address_length ) <nl> + return false ; <nl> acpi_dev_get_memresource ( res , fixed_memory32 -> address , <nl> fixed_memory32 -> address_length , <nl> fixed_memory32 -> write_protect ); <nl> bool acpi_dev_resource_io ( struct acpi_resource * ares , struct resource * res ) <nl> switch ( ares -> type ) { <nl> case ACPI_RESOURCE_TYPE_IO : <nl> io = & ares -> data . io ; <nl> + if (! io -> address_length ) <nl> + return false ; <nl> acpi_dev_get_ioresource ( res , io -> minimum , <nl> io -> address_length , <nl> io -> io_decode ); <nl> break ; <nl> case ACPI_RESOURCE_TYPE_FIXED_IO : <nl> fixed_io = & ares -> data . fixed_io ; <nl> + if (! fixed_io -> address_length ) <nl> + return false ; <nl> acpi_dev_get_ioresource ( res , fixed_io -> address , <nl> fixed_io -> address_length , <nl> ACPI_DECODE_10 );
static __always_inline cycles_t get_cycles_sync ( void ) <nl> unsigned long long ret ; <nl> unsigned eax ; <nl>  <nl> + /* <nl> + * Use RDTSCP if possible ; it is guaranteed to be synchronous <nl> + * and doesn ' t cause a VMEXIT on Hypervisors <nl> + */ <nl> + alternative_io ( ASM_NOP3 , ". byte 0x0f , 0x01 , 0xf9 ", X86_FEATURE_RDTSCP , <nl> + "= A " ( ret ), " 0 " ( 0ULL ) : " ecx ", " memory "); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> /* <nl> * Don ' t do an additional sync on CPUs where we know <nl> * RDTSC is already synchronous :
static const struct drm_connector_funcs dm_dp_mst_connector_funcs = { <nl> . atomic_get_property = amdgpu_dm_connector_atomic_get_property <nl> }; <nl>  <nl> - static int dm_connector_update_modes ( struct drm_connector * connector , <nl> - struct edid * edid ) <nl> -{ <nl> - return drm_add_edid_modes ( connector , edid ); <nl> -} <nl> - <nl> void dm_dp_mst_dc_sink_create ( struct drm_connector * connector ) <nl> { <nl> struct amdgpu_dm_connector * aconnector = to_amdgpu_dm_connector ( connector ); <nl> static int dm_dp_mst_get_modes ( struct drm_connector * connector ) <nl> int ret = 0 ; <nl>  <nl> if (! aconnector ) <nl> - return dm_connector_update_modes ( connector , NULL ); <nl> + return drm_add_edid_modes ( connector , NULL ); <nl>  <nl> if (! aconnector -> edid ) { <nl> struct edid * edid ; <nl> static int dm_dp_mst_get_modes ( struct drm_connector * connector ) <nl> & aconnector -> base , edid ); <nl> } <nl>  <nl> - ret = dm_connector_update_modes ( connector , aconnector -> edid ); <nl> + ret = drm_add_edid_modes ( connector , aconnector -> edid ); <nl>  <nl> return ret ; <nl> }
static int try_smi_init ( struct smi_info * new_smi ) <nl> */ <nl> new_smi -> pdev = platform_device_alloc (" ipmi_si ", <nl> new_smi -> intf_num ); <nl> - if ( rv ) { <nl> + if (! new_smi -> pdev ) { <nl> printk ( KERN_ERR <nl> " ipmi_si_intf :" <nl> " Unable to allocate platform device \ n ");
static int lz4_uncompress ( const char * source , char * dest , int osize ) <nl> len = * ip ++; <nl> for (; len == 255 ; length += 255 ) <nl> len = * ip ++; <nl> + if ( unlikely ( length > ( size_t )( length + len ))) <nl> + goto _output_error ; <nl> length += len ; <nl> } <nl> 
static int sunxi_mmc_clk_set_rate ( struct sunxi_mmc_host * host , <nl> u32 rval , clock = ios -> clock ; <nl> int ret ; <nl>  <nl> + ret = sunxi_mmc_oclk_onoff ( host , 0 ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> /* 8 bit DDR requires a higher module clock */ <nl> if ( ios -> timing == MMC_TIMING_MMC_DDR52 && <nl> ios -> bus_width == MMC_BUS_WIDTH_8 ) <nl> static int sunxi_mmc_clk_set_rate ( struct sunxi_mmc_host * host , <nl> return ret ; <nl> } <nl>  <nl> - ret = sunxi_mmc_oclk_onoff ( host , 0 ); <nl> - if ( ret ) <nl> - return ret ; <nl> - <nl> /* clear internal divider */ <nl> rval = mmc_readl ( host , REG_CLKCR ); <nl> rval &= ~ 0xff ;
# define IS_DIGITAL ( c ) ( c -> output_flag & ( SDVO_TMDS_MASK | SDVO_LVDS_MASK )) <nl>  <nl>  <nl> - static const char * tv_format_names [] = { <nl> + static const char * const tv_format_names [] = { <nl> " NTSC_M " , " NTSC_J " , " NTSC_443 ", <nl> " PAL_B " , " PAL_D " , " PAL_G " , <nl> " PAL_H " , " PAL_I " , " PAL_M " , <nl> static void intel_sdvo_debug_write ( struct intel_sdvo * intel_sdvo , u8 cmd , <nl> DRM_DEBUG_KMS ("% s : W : % 02X % s \ n ", SDVO_NAME ( intel_sdvo ), cmd , buffer ); <nl> } <nl>  <nl> - static const char * cmd_status_names [] = { <nl> + static const char * const cmd_status_names [] = { <nl> " Power on ", <nl> " Success ", <nl> " Not supported ",
static void drm_cleanup ( struct drm_device * dev ) <nl> DRM_ERROR (" Cannot unload module \ n "); <nl> } <nl>  <nl> - int drm_minors_cleanup ( int id , void * ptr , void * data ) <nl> + static int drm_minors_cleanup ( int id , void * ptr , void * data ) <nl> { <nl> struct drm_minor * minor = ptr ; <nl> struct drm_device * dev ;
static void mct_u232_msr_to_state ( struct usb_serial_port * port , <nl>  <nl> static int mct_u232_port_probe ( struct usb_serial_port * port ) <nl> { <nl> + struct usb_serial * serial = port -> serial ; <nl> struct mct_u232_private * priv ; <nl>  <nl> + /* check first to simplify error handling */ <nl> + if (! serial -> port [ 1 ] || ! serial -> port [ 1 ]-> interrupt_in_urb ) { <nl> + dev_err (& port -> dev , " expected endpoint missing \ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> priv = kzalloc ( sizeof (* priv ), GFP_KERNEL ); <nl> if (! priv ) <nl> return - ENOMEM ; <nl>  <nl> /* Use second interrupt - in endpoint for reading . */ <nl> - priv -> read_urb = port -> serial -> port [ 1 ]-> interrupt_in_urb ; <nl> + priv -> read_urb = serial -> port [ 1 ]-> interrupt_in_urb ; <nl> priv -> read_urb -> context = port ; <nl>  <nl> spin_lock_init (& priv -> lock );
static void alc_eapd_shutup ( struct hda_codec * codec ) <nl> { <nl> alc_auto_setup_eapd ( codec , false ); <nl> msleep ( 200 ); <nl> + snd_hda_shutup_pins ( codec ); <nl> } <nl>  <nl> /* generic EAPD initialization */ <nl> static inline void alc_shutup ( struct hda_codec * codec ) <nl>  <nl> if ( spec && spec -> shutup ) <nl> spec -> shutup ( codec ); <nl> - snd_hda_shutup_pins ( codec ); <nl> + else <nl> + snd_hda_shutup_pins ( codec ); <nl> } <nl>  <nl> # define alc_free snd_hda_gen_free <nl> static void alc269_shutup ( struct hda_codec * codec ) <nl> { <nl> struct alc_spec * spec = codec -> spec ; <nl>  <nl> - if ( spec -> codec_variant != ALC269_TYPE_ALC269VB ) <nl> - return ; <nl> - <nl> if ( spec -> codec_variant == ALC269_TYPE_ALC269VB ) <nl> alc269vb_toggle_power_output ( codec , 0 ); <nl> if ( spec -> codec_variant == ALC269_TYPE_ALC269VB && <nl> ( alc_get_coef0 ( codec ) & 0x00ff ) == 0x018 ) { <nl> msleep ( 150 ); <nl> } <nl> + snd_hda_shutup_pins ( codec ); <nl> } <nl>  <nl> static void alc5505_coef_set ( struct hda_codec * codec , unsigned int index_reg ,
int neigh_table_clear ( struct neigh_table * tbl ) <nl> kfree ( tbl -> phash_buckets ); <nl> tbl -> phash_buckets = NULL ; <nl>  <nl> + free_percpu ( tbl -> stats ); <nl> + tbl -> stats = NULL ; <nl> + <nl> return 0 ; <nl> } <nl> 
static netdev_tx_t ems_usb_start_xmit ( struct sk_buff * skb , struct net_device * ne <nl>  <nl> usb_unanchor_urb ( urb ); <nl> usb_free_coherent ( dev -> udev , size , buf , urb -> transfer_dma ); <nl> - dev_kfree_skb ( skb ); <nl>  <nl> atomic_dec (& dev -> active_tx_urbs ); <nl> 
static irqreturn_t me4000_ai_isr ( int irq , void * dev_id ) <nl> { <nl> unsigned int tmp ; <nl> struct comedi_device * dev = dev_id ; <nl> - struct comedi_subdevice * s = dev -> subdevices ; <nl> + struct comedi_subdevice * s = & dev -> subdevices [ 0 ]; <nl> struct me4000_ai_context * ai_context = & info -> ai_context ; <nl> int i ; <nl> int c = 0 ; <nl> static int me4000_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> Analog input subdevice <nl> ========================================================================*/ <nl>  <nl> - s = dev -> subdevices + 0 ; <nl> + s = & dev -> subdevices [ 0 ]; <nl>  <nl> if ( thisboard -> ai . count ) { <nl> s -> type = COMEDI_SUBD_AI ; <nl> static int me4000_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> Analog output subdevice <nl> ========================================================================*/ <nl>  <nl> - s = dev -> subdevices + 1 ; <nl> + s = & dev -> subdevices [ 1 ]; <nl>  <nl> if ( thisboard -> ao . count ) { <nl> s -> type = COMEDI_SUBD_AO ; <nl> static int me4000_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> Digital I / O subdevice <nl> ========================================================================*/ <nl>  <nl> - s = dev -> subdevices + 2 ; <nl> + s = & dev -> subdevices [ 2 ]; <nl>  <nl> if ( thisboard -> dio . count ) { <nl> s -> type = COMEDI_SUBD_DIO ; <nl> static int me4000_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> Counter subdevice <nl> ========================================================================*/ <nl>  <nl> - s = dev -> subdevices + 3 ; <nl> + s = & dev -> subdevices [ 3 ]; <nl>  <nl> if ( thisboard -> cnt . count ) { <nl> s -> type = COMEDI_SUBD_COUNTER ;
static struct rc_map_list empty_map = { <nl> static int ir_create_table ( struct rc_map * rc_map , <nl> const char * name , u64 rc_type , size_t size ) <nl> { <nl> - rc_map -> name = name ; <nl> + rc_map -> name = kstrdup ( name , GFP_KERNEL ); <nl> + if (! rc_map -> name ) <nl> + return - ENOMEM ; <nl> rc_map -> rc_type = rc_type ; <nl> rc_map -> alloc = roundup_pow_of_two ( size * sizeof ( struct rc_map_table )); <nl> rc_map -> size = rc_map -> alloc / sizeof ( struct rc_map_table ); <nl> rc_map -> scan = kmalloc ( rc_map -> alloc , GFP_KERNEL ); <nl> - if (! rc_map -> scan ) <nl> + if (! rc_map -> scan ) { <nl> + kfree ( rc_map -> name ); <nl> + rc_map -> name = NULL ; <nl> return - ENOMEM ; <nl> + } <nl>  <nl> IR_dprintk ( 1 , " Allocated space for % u keycode entries (% u bytes )\ n ", <nl> rc_map -> size , rc_map -> alloc ); <nl> static int ir_create_table ( struct rc_map * rc_map , <nl> static void ir_free_table ( struct rc_map * rc_map ) <nl> { <nl> rc_map -> size = 0 ; <nl> + kfree ( rc_map -> name ); <nl> kfree ( rc_map -> scan ); <nl> rc_map -> scan = NULL ; <nl> }
static void local_exit ( void ) <nl> DMINFO (" cleaned up "); <nl> } <nl>  <nl> - int (* _inits [])( void ) __initdata = { <nl> + static int (* _inits [])( void ) __initdata = { <nl> local_init , <nl> dm_target_init , <nl> dm_linear_init , <nl> int (* _inits [])( void ) __initdata = { <nl> dm_interface_init , <nl> }; <nl>  <nl> - void (* _exits [])( void ) = { <nl> + static void (* _exits [])( void ) = { <nl> local_exit , <nl> dm_target_exit , <nl> dm_linear_exit ,
void gspca_frame_add ( struct gspca_dev * gspca_dev , <nl> /* if there are no queued buffer , discard the whole frame */ <nl> if ( i == atomic_read (& gspca_dev -> fr_q )) { <nl> gspca_dev -> last_packet_type = DISCARD_PACKET ; <nl> + gspca_dev -> sequence ++; <nl> return ; <nl> } <nl> j = gspca_dev -> fr_queue [ i ]; <nl> frame = & gspca_dev -> frame [ j ]; <nl> frame -> v4l2_buf . timestamp = ktime_to_timeval ( ktime_get ()); <nl> - frame -> v4l2_buf . sequence = ++ gspca_dev -> sequence ; <nl> + frame -> v4l2_buf . sequence = gspca_dev -> sequence ++; <nl> gspca_dev -> image = frame -> data ; <nl> gspca_dev -> image_len = 0 ; <nl> } else {
void free_fib_info ( struct fib_info * fi ) <nl> # endif <nl> call_rcu (& fi -> rcu , free_fib_info_rcu ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( free_fib_info ); <nl>  <nl> void fib_release_info ( struct fib_info * fi ) <nl> {
static inline struct sk_buff * udp_tunnel_handle_offloads ( struct sk_buff * skb , <nl> { <nl> int type = udp_csum ? SKB_GSO_UDP_TUNNEL_CSUM : SKB_GSO_UDP_TUNNEL ; <nl>  <nl> - return iptunnel_handle_offloads ( skb , udp_csum , type ); <nl> + /* As we ' re a UDP tunnel , we support LCO , so don ' t need csum_help */ <nl> + return iptunnel_handle_offloads ( skb , false , type ); <nl> } <nl>  <nl> static inline void udp_tunnel_gro_complete ( struct sk_buff * skb , int nhoff )
static void iwl4965_rx_reply_tx ( struct iwl_priv * priv , <nl> struct ieee80211_tx_info * info ; <nl> struct iwl4965_tx_resp * tx_resp = ( void *)& pkt -> u . raw [ 0 ]; <nl> u32 status = le32_to_cpu ( tx_resp -> u . status ); <nl> - int tid = MAX_TID_COUNT ; <nl> + int tid = MAX_TID_COUNT - 1 ; <nl> int sta_id ; <nl> int freed ; <nl> u8 * qc = NULL ;
static void __devinit bnx2x_link_settings_supported ( struct bnx2x * bp , <nl> break ; <nl>  <nl> case PORT_HW_CFG_XGXS_EXT_PHY_TYPE_BCM8481 : <nl> - BNX2X_DEV_INFO (" ext_phy_type 0x % x ( BCM8481 )\ n ", <nl> + case PORT_HW_CFG_XGXS_EXT_PHY_TYPE_BCM84823 : <nl> + BNX2X_DEV_INFO (" ext_phy_type 0x % x ( BCM848xx )\ n ", <nl> ext_phy_type ); <nl>  <nl> bp -> port . supported |= ( SUPPORTED_10baseT_Half |
static int dwmac4_wrback_get_rx_timestamp_status ( void * desc , u32 ats ) <nl> goto exit ; <nl> i ++; <nl>  <nl> - } while (( ret == 1 ) || ( i < 10 )); <nl> + } while (( ret == 1 ) && ( i < 10 )); <nl>  <nl> if ( i == 10 ) <nl> ret = - EBUSY ;
static void stats_read ( struct flow_stats * stats , <nl> unsigned long * used , __be16 * tcp_flags ) <nl> { <nl> spin_lock (& stats -> lock ); <nl> - if ( time_after ( stats -> used , * used )) <nl> + if (!* used || time_after ( stats -> used , * used )) <nl> * used = stats -> used ; <nl> * tcp_flags |= stats -> tcp_flags ; <nl> ovs_stats -> n_packets += stats -> packet_count ;
static int intel_pstate_set_policy ( struct cpufreq_policy * policy ) <nl> limits -> max_sysfs_pct ); <nl> limits -> max_perf_pct = max ( limits -> min_policy_pct , <nl> limits -> max_perf_pct ); <nl> + limits -> max_perf = round_up ( limits -> max_perf , 8 ); <nl>  <nl> /* Make sure min_perf_pct <= max_perf_pct */ <nl> limits -> min_perf_pct = min ( limits -> max_perf_pct , limits -> min_perf_pct );
ecryptfs_setxattr ( struct dentry * dentry , const char * name , const void * value , <nl> } <nl>  <nl> rc = vfs_setxattr ( lower_dentry , name , value , size , flags ); <nl> + if (! rc ) <nl> + fsstack_copy_attr_all ( dentry -> d_inode , lower_dentry -> d_inode ); <nl> out : <nl> return rc ; <nl> }
int ocfs2_setattr ( struct dentry * dentry , struct iattr * attr ) <nl> } <nl> size_change = S_ISREG ( inode -> i_mode ) && attr -> ia_valid & ATTR_SIZE ; <nl> if ( size_change ) { <nl> + /* <nl> + * Here we should wait dio to finish before inode lock <nl> + * to avoid a deadlock between ocfs2_setattr () and <nl> + * ocfs2_dio_end_io_write () <nl> + */ <nl> + inode_dio_wait ( inode ); <nl> + <nl> status = ocfs2_rw_lock ( inode , 1 ); <nl> if ( status < 0 ) { <nl> mlog_errno ( status ); <nl> int ocfs2_setattr ( struct dentry * dentry , struct iattr * attr ) <nl> if ( status ) <nl> goto bail_unlock ; <nl>  <nl> - inode_dio_wait ( inode ); <nl> - <nl> if ( i_size_read ( inode ) >= attr -> ia_size ) { <nl> if ( ocfs2_should_order_data ( inode )) { <nl> status = ocfs2_begin_ordered_truncate ( inode ,
static int __init proc_kcore_init ( void ) <nl> { <nl> proc_root_kcore = proc_create (" kcore ", S_IRUSR , NULL , <nl> & proc_kcore_operations ); <nl> + if (! proc_root_kcore ) { <nl> + printk ( KERN_ERR " couldn ' t create / proc / kcore \ n "); <nl> + return 0 ; /* Always returns 0 . */ <nl> + } <nl> /* Store text area if it ' s special */ <nl> proc_kcore_text_init (); <nl> /* Store vmalloc area */ <nl> static int __init proc_kcore_init ( void ) <nl> /* Store direct - map area from physical memory map */ <nl> kcore_update_ram (); <nl> hotplug_memory_notifier ( kcore_callback , 0 ); <nl> - /* Other special area , area - for - module etc is arch specific . */ <nl>  <nl> return 0 ; <nl> }
static int gfx_v9_0_ngg_init ( struct amdgpu_device * adev ) <nl> adev -> gfx . ngg . gds_reserve_size = ALIGN ( 5 * 4 , 0x40 ); <nl> adev -> gds . mem . total_size -= adev -> gfx . ngg . gds_reserve_size ; <nl> adev -> gds . mem . gfx_partition_size -= adev -> gfx . ngg . gds_reserve_size ; <nl> - adev -> gfx . ngg . gds_reserve_addr = SOC15_REG_OFFSET ( GC , 0 , mmGDS_VMID0_BASE ); <nl> - adev -> gfx . ngg . gds_reserve_addr += adev -> gds . mem . gfx_partition_size ; <nl> + adev -> gfx . ngg . gds_reserve_addr = RREG32_SOC15 ( GC , 0 , mmGDS_VMID0_BASE ); <nl> + adev -> gfx . ngg . gds_reserve_addr += RREG32_SOC15 ( GC , 0 , mmGDS_VMID0_SIZE ); <nl>  <nl> /* Primitive Buffer */ <nl> r = gfx_v9_0_ngg_create_buf ( adev , & adev -> gfx . ngg . buf [ NGG_PRIM ], <nl> static int gfx_v9_0_ngg_en ( struct amdgpu_device * adev ) <nl>  <nl> amdgpu_ring_write ( ring , PACKET3 ( PACKET3_DMA_DATA , 5 )); <nl> amdgpu_ring_write ( ring , ( PACKET3_DMA_DATA_CP_SYNC | <nl> + PACKET3_DMA_DATA_DST_SEL ( 1 ) | <nl> PACKET3_DMA_DATA_SRC_SEL ( 2 ))); <nl> amdgpu_ring_write ( ring , 0 ); <nl> amdgpu_ring_write ( ring , 0 ); <nl> amdgpu_ring_write ( ring , adev -> gfx . ngg . gds_reserve_addr ); <nl> amdgpu_ring_write ( ring , 0 ); <nl> - amdgpu_ring_write ( ring , adev -> gfx . ngg . gds_reserve_size ); <nl> - <nl> + amdgpu_ring_write ( ring , PACKET3_DMA_DATA_CMD_RAW_WAIT | <nl> + adev -> gfx . ngg . gds_reserve_size ); <nl>  <nl> gfx_v9_0_write_data_to_reg ( ring , 0 , false , <nl> SOC15_REG_OFFSET ( GC , 0 , mmGDS_VMID0_SIZE ), 0 );
void dma_run_dependencies ( struct dma_async_tx_descriptor * tx ) <nl> if (! dep ) <nl> return ; <nl>  <nl> + /* we ' ll submit tx -> next now , so clear the link */ <nl> + tx -> next = NULL ; <nl> chan = dep -> chan ; <nl>  <nl> /* keep submitting up until a channel switch is detected
static int process_ctrl_td ( struct xhci_hcd * xhci , struct xhci_td * td , <nl> case TRB_NORMAL : <nl> td -> urb -> actual_length = requested - remaining ; <nl> goto finish_td ; <nl> + case TRB_STATUS : <nl> + td -> urb -> actual_length = requested ; <nl> + goto finish_td ; <nl> default : <nl> xhci_warn ( xhci , " WARN : unexpected TRB Type % d \ n ", <nl> trb_type );
int rds_tcp_conn_connect ( struct rds_connection * conn ) <nl> rds_tcp_set_callbacks ( sock , conn ); <nl> ret = sock -> ops -> connect ( sock , ( struct sockaddr *)& dest , sizeof ( dest ), <nl> O_NONBLOCK ); <nl> - sock = NULL ; <nl>  <nl> rdsdebug (" connect to address % pI4 returned % d \ n ", & conn -> c_faddr , ret ); <nl> if ( ret == - EINPROGRESS ) <nl> ret = 0 ; <nl> + if ( ret == 0 ) <nl> + sock = NULL ; <nl> + else <nl> + rds_tcp_restore_callbacks ( sock , conn -> c_transport_data ); <nl>  <nl> out : <nl> if ( sock )
nf_ct_frag6_reasm ( struct nf_ct_frag6_queue * fq , struct net_device * dev ) <nl>  <nl> /* all original skbs are linked into the NFCT_FRAG6_CB ( head ). orig */ <nl> fp = skb_shinfo ( head )-> frag_list ; <nl> - if ( NFCT_FRAG6_CB ( fp )-> orig == NULL ) <nl> + if ( fp && NFCT_FRAG6_CB ( fp )-> orig == NULL ) <nl> /* at above code , head skb is divided into two skbs . */ <nl> fp = fp -> next ; <nl>  <nl> struct sk_buff * nf_ct_frag6_gather ( struct sk_buff * skb , u32 user ) <nl> hdr = ipv6_hdr ( clone ); <nl> fhdr = ( struct frag_hdr *) skb_transport_header ( clone ); <nl>  <nl> - if (!( fhdr -> frag_off & htons ( 0xFFF9 ))) { <nl> - pr_debug (" Invalid fragment offset \ n "); <nl> - /* It is not a fragmented frame */ <nl> - goto ret_orig ; <nl> - } <nl> - <nl> if ( atomic_read (& nf_init_frags . mem ) > nf_init_frags . high_thresh ) <nl> nf_ct_frag6_evictor (); <nl> 
static int cpu_has_dma ( void ) <nl> */ <nl> static void atmel_nand_enable ( struct atmel_nand_host * host ) <nl> { <nl> - if ( host -> board -> enable_pin ) <nl> + if ( gpio_is_valid ( host -> board -> enable_pin )) <nl> gpio_set_value ( host -> board -> enable_pin , 0 ); <nl> } <nl>  <nl> static void atmel_nand_enable ( struct atmel_nand_host * host ) <nl> */ <nl> static void atmel_nand_disable ( struct atmel_nand_host * host ) <nl> { <nl> - if ( host -> board -> enable_pin ) <nl> + if ( gpio_is_valid ( host -> board -> enable_pin )) <nl> gpio_set_value ( host -> board -> enable_pin , 1 ); <nl> } <nl>  <nl> static int __init atmel_nand_probe ( struct platform_device * pdev ) <nl> nand_chip -> IO_ADDR_W = host -> io_base ; <nl> nand_chip -> cmd_ctrl = atmel_nand_cmd_ctrl ; <nl>  <nl> - if ( host -> board -> rdy_pin ) <nl> + if ( gpio_is_valid ( host -> board -> rdy_pin )) <nl> nand_chip -> dev_ready = atmel_nand_device_ready ; <nl>  <nl> regs = platform_get_resource ( pdev , IORESOURCE_MEM , 1 ); <nl> static int __init atmel_nand_probe ( struct platform_device * pdev ) <nl> platform_set_drvdata ( pdev , host ); <nl> atmel_nand_enable ( host ); <nl>  <nl> - if ( host -> board -> det_pin ) { <nl> + if ( gpio_is_valid ( host -> board -> det_pin )) { <nl> if ( gpio_get_value ( host -> board -> det_pin )) { <nl> printk ( KERN_INFO " No SmartMedia card inserted .\ n "); <nl> res = - ENXIO ;
static __devinit int da9055_gpio_init ( struct da9055_regulator * regulator , <nl> goto err ; <nl> } <nl>  <nl> - if ( pdata -> gpio_rsel && pdata -> gpio_ren [ id ]) { <nl> + if ( pdata -> gpio_rsel && pdata -> gpio_rsel [ id ]) { <nl> char name [ 18 ]; <nl> int gpio_mux = pdata -> gpio_rsel [ id ]; <nl> 
static struct sms_board sms_boards [] = { <nl> . board_cfg . leds_power = 26 , <nl> . board_cfg . led0 = 27 , <nl> . board_cfg . led1 = 28 , <nl> + . board_cfg . ir = 9 , <nl> . led_power = 26 , <nl> . led_lo = 27 , <nl> . led_hi = 28 ,
static int __init macide_init ( void ) <nl> int irq ; <nl> hw_regs_t hw ; <nl>  <nl> + if (! MACH_IS_MAC ) <nl> + return - ENODEV ; <nl> + <nl> switch ( macintosh_config -> ide_type ) { <nl> case MAC_IDE_QUADRA : <nl> base = IDE_BASE ;
store_priv_session_ ## field ( struct device * dev , \ <nl> # define iscsi_priv_session_rw_attr ( field , format ) \ <nl> iscsi_priv_session_attr_show ( field , format ) \ <nl> iscsi_priv_session_attr_store ( field ) \ <nl> - static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUGO , \ <nl> + static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUSR , \ <nl> show_priv_session_ ## field , \ <nl> store_priv_session_ ## field ) <nl> iscsi_priv_session_rw_attr ( recovery_tmo , "% d ");
xfs_itruncate_start ( <nl> mp = ip -> i_mount ; <nl>  <nl> /* wait for the completion of any pending DIOs */ <nl> - if ( new_size < ip -> i_size ) <nl> + if ( new_size == 0 || new_size < ip -> i_size ) <nl> vn_iowait ( ip ); <nl>  <nl> /*
static int mos7840_tiocmget ( struct tty_struct * tty ) <nl> return - ENODEV ; <nl>  <nl> status = mos7840_get_uart_reg ( port , MODEM_STATUS_REGISTER , & msr ); <nl> + if ( status != 1 ) <nl> + return - EIO ; <nl> status = mos7840_get_uart_reg ( port , MODEM_CONTROL_REGISTER , & mcr ); <nl> + if ( status != 1 ) <nl> + return - EIO ; <nl> result = (( mcr & MCR_DTR ) ? TIOCM_DTR : 0 ) <nl> | (( mcr & MCR_RTS ) ? TIOCM_RTS : 0 ) <nl> | (( mcr & MCR_LOOPBACK ) ? TIOCM_LOOP : 0 )
static int ad5820_probe ( struct i2c_client * client , <nl> return ret ; <nl> } <nl>  <nl> - static int __exit ad5820_remove ( struct i2c_client * client ) <nl> + static int ad5820_remove ( struct i2c_client * client ) <nl> { <nl> struct v4l2_subdev * subdev = i2c_get_clientdata ( client ); <nl> struct ad5820_device * coil = to_ad5820_device ( subdev ); <nl> static struct i2c_driver ad5820_i2c_driver = { <nl> . pm = & ad5820_pm , <nl> }, <nl> . probe = ad5820_probe , <nl> - . remove = __exit_p ( ad5820_remove ), <nl> + . remove = ad5820_remove , <nl> . id_table = ad5820_id_table , <nl> }; <nl> 
static void mtk_plane_atomic_update ( struct drm_plane * plane , <nl> pitch = fb -> pitches [ 0 ]; <nl> format = fb -> pixel_format ; <nl>  <nl> - addr += ( plane -> state -> src . x1 >> 16 ) * 4 ; <nl> + addr += ( plane -> state -> src . x1 >> 16 ) * drm_format_plane_cpp ( format , 0 ); <nl> addr += ( plane -> state -> src . y1 >> 16 ) * pitch ; <nl>  <nl> state -> pending . enable = true ;
nvkm_pmu_reset ( struct nvkm_pmu * pmu ) <nl> ); <nl>  <nl> /* Reset . */ <nl> - pmu -> func -> reset ( pmu ); <nl> + if ( pmu -> func -> reset ) <nl> + pmu -> func -> reset ( pmu ); <nl>  <nl> /* Wait for IMEM / DMEM scrubbing to be complete . */ <nl> nvkm_msec ( device , 2000 ,
static int __devinit igb_probe ( struct pci_dev * pdev , <nl> state &= ~ PCIE_LINK_STATE_L0S ; <nl> pci_write_config_word ( us_dev , pos + PCI_EXP_LNKCTL , <nl> state ); <nl> - printk ( KERN_INFO " Disabling ASPM L0s upstream switch " <nl> - " port % x :% x .% x \ n ", us_dev -> bus -> number , <nl> - PCI_SLOT ( us_dev -> devfn ), <nl> - PCI_FUNC ( us_dev -> devfn )); <nl> + dev_info (& pdev -> dev , <nl> + " Disabling ASPM L0s upstream switch port % s \ n ", <nl> + pci_name ( us_dev )); <nl> } <nl> default : <nl> break ;
int __xipram cfi_qry_mode_on ( uint32_t base , struct map_info * map , <nl> cfi_send_gen_cmd ( 0xAA , 0x5555 , base , map , cfi , cfi -> device_type , NULL ); <nl> cfi_send_gen_cmd ( 0x55 , 0x2AAA , base , map , cfi , cfi -> device_type , NULL ); <nl> cfi_send_gen_cmd ( 0x98 , 0x5555 , base , map , cfi , cfi -> device_type , NULL ); <nl> + if ( cfi_qry_present ( map , base , cfi )) <nl> + return 1 ; <nl> + /* SST 39VF640xB */ <nl> + cfi_send_gen_cmd ( 0xF0 , 0 , base , map , cfi , cfi -> device_type , NULL ); <nl> + cfi_send_gen_cmd ( 0xAA , 0x555 , base , map , cfi , cfi -> device_type , NULL ); <nl> + cfi_send_gen_cmd ( 0x55 , 0x2AA , base , map , cfi , cfi -> device_type , NULL ); <nl> + cfi_send_gen_cmd ( 0x98 , 0x555 , base , map , cfi , cfi -> device_type , NULL ); <nl> if ( cfi_qry_present ( map , base , cfi )) <nl> return 1 ; <nl> /* QRY not found */
static int nf_flow_dnat_ip ( const struct flow_offload * flow , struct sk_buff * skb , <nl> default : <nl> return - 1 ; <nl> } <nl> + csum_replace4 (& iph -> check , addr , new_addr ); <nl>  <nl> return nf_flow_nat_ip_l4proto ( skb , iph , thoff , addr , new_addr ); <nl> }
static int bcm_sysport_set_wol ( struct net_device * dev , <nl> /* Flag the device and relevant IRQ as wakeup capable */ <nl> if ( wol -> wolopts ) { <nl> device_set_wakeup_enable ( kdev , 1 ); <nl> - enable_irq_wake ( priv -> wol_irq ); <nl> + if ( priv -> wol_irq_disabled ) <nl> + enable_irq_wake ( priv -> wol_irq ); <nl> priv -> wol_irq_disabled = 0 ; <nl> } else { <nl> device_set_wakeup_enable ( kdev , 0 );
mmc_omap_xfer_done ( struct mmc_omap_host * host , struct mmc_data * data ) <nl> if (! data ) { <nl> struct mmc_request * mrq = host -> mrq ; <nl>  <nl> + /* TC before CC from CMD6 - don ' t know why , but it happens */ <nl> + if ( host -> cmd && host -> cmd -> opcode == 6 && <nl> + host -> response_busy ) { <nl> + host -> response_busy = 0 ; <nl> + return ; <nl> + } <nl> + <nl> host -> mrq = NULL ; <nl> mmc_request_done ( host -> mmc , mrq ); <nl> return ;
static int nsp_gpio_get_strength ( struct nsp_gpio * chip , unsigned gpio , <nl> return 0 ; <nl> } <nl>  <nl> - int nsp_pin_config_group_get ( struct pinctrl_dev * pctldev , unsigned selector , <nl> + static int nsp_pin_config_group_get ( struct pinctrl_dev * pctldev , <nl> + unsigned selector , <nl> unsigned long * config ) <nl> { <nl> return 0 ; <nl> } <nl>  <nl> - int nsp_pin_config_group_set ( struct pinctrl_dev * pctldev , unsigned selector , <nl> + static int nsp_pin_config_group_set ( struct pinctrl_dev * pctldev , <nl> + unsigned selector , <nl> unsigned long * configs , unsigned num_configs ) <nl> { <nl> return 0 ;
static struct snd_seq_queue * queue_new ( int owner , int locked ) <nl> static void queue_delete ( struct snd_seq_queue * q ) <nl> { <nl> /* stop and release the timer */ <nl> + mutex_lock (& q -> timer_mutex ); <nl> snd_seq_timer_stop ( q -> timer ); <nl> snd_seq_timer_close ( q ); <nl> + mutex_unlock (& q -> timer_mutex ); <nl> /* wait until access free */ <nl> snd_use_lock_sync (& q -> use_lock ); <nl> /* release resources ... */
event_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_preds ( call ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl>  <nl> subsystem_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_subsystem_preds ( system ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl> 
static bool srcu_readers_active_idx_check ( struct srcu_struct * sp , int idx ) <nl> */ <nl> static int srcu_readers_active ( struct srcu_struct * sp ) <nl> { <nl> - return srcu_readers_active_idx ( sp , 0 ) + srcu_readers_active_idx ( sp , 1 ); <nl> + int cpu ; <nl> + unsigned long sum = 0 ; <nl> + <nl> + for_each_possible_cpu ( cpu ) { <nl> + sum += ACCESS_ONCE ( per_cpu_ptr ( sp -> per_cpu_ref , cpu )-> c [ 0 ]); <nl> + sum += ACCESS_ONCE ( per_cpu_ptr ( sp -> per_cpu_ref , cpu )-> c [ 1 ]); <nl> + } <nl> + return sum ; <nl> } <nl>  <nl> /**
static void ilk_pipe_wm_get_hw_state ( struct drm_crtc * crtc ) <nl> if ( IS_HASWELL ( dev ) || IS_BROADWELL ( dev )) <nl> hw -> wm_linetime [ pipe ] = I915_READ ( PIPE_WM_LINETIME ( pipe )); <nl>  <nl> + memset ( active , 0 , sizeof (* active )); <nl> + <nl> active -> pipe_enabled = intel_crtc -> active ; <nl>  <nl> if ( active -> pipe_enabled ) {
static int clcdfb_register ( struct clcd_fb * fb ) <nl>  <nl> fb_set_var (& fb -> fb , & fb -> fb . var ); <nl>  <nl> - printk ( KERN_INFO " CLCD : % s hardware , % s display \ n ", <nl> - fb -> board -> name , fb -> panel -> mode . name ); <nl> + dev_info (& fb -> dev -> dev , "% s hardware , % s display \ n ", <nl> + fb -> board -> name , fb -> panel -> mode . name ); <nl>  <nl> ret = register_framebuffer (& fb -> fb ); <nl> if ( ret == 0 ) <nl> static int clcdfb_probe ( struct amba_device * dev , struct amba_id * id ) <nl> fb -> dev = dev ; <nl> fb -> board = board ; <nl>  <nl> + dev_info (& fb -> dev -> dev , " PL % 03x rev % u at 0x % 08llx \ n ", <nl> + amba_part ( dev ), amba_rev ( dev ), <nl> + ( unsigned long long ) dev -> res . start ); <nl> + <nl> ret = fb -> board -> setup ( fb ); <nl> if ( ret ) <nl> goto free_fb ;
static int rsi_usb_reg_read ( struct usb_device * usbdev , <nl> if (! buf ) <nl> return status ; <nl>  <nl> + if ( len > RSI_USB_CTRL_BUF_SIZE ) <nl> + return - EINVAL ; <nl> + <nl> status = usb_control_msg ( usbdev , <nl> usb_rcvctrlpipe ( usbdev , 0 ), <nl> USB_VENDOR_REGISTER_READ , <nl> static int rsi_usb_reg_write ( struct usb_device * usbdev , <nl> if (! usb_reg_buf ) <nl> return status ; <nl>  <nl> + if ( len > RSI_USB_CTRL_BUF_SIZE ) <nl> + return - EINVAL ; <nl> + <nl> usb_reg_buf [ 0 ] = ( value & 0x00ff ); <nl> usb_reg_buf [ 1 ] = ( value & 0xff00 ) >> 8 ; <nl> usb_reg_buf [ 2 ] = 0x0 ;
void radeon_compute_pll_avivo ( struct radeon_pll * pll , <nl>  <nl> /* avoid high jitter with small fractional dividers */ <nl> if ( pll -> flags & RADEON_PLL_USE_FRAC_FB_DIV && ( fb_div % 10 )) { <nl> - fb_div_min = max ( fb_div_min , ( 9 - ( fb_div % 10 )) * 20 + 60 ); <nl> + fb_div_min = max ( fb_div_min , ( 9 - ( fb_div % 10 )) * 20 + 50 ); <nl> if ( fb_div < fb_div_min ) { <nl> unsigned tmp = DIV_ROUND_UP ( fb_div_min , fb_div ); <nl> fb_div *= tmp ;
static int mxcmci_probe ( struct platform_device * pdev ) <nl> goto out_release_mem ; <nl> } <nl>  <nl> - mmc_of_parse ( mmc ); <nl> + ret = mmc_of_parse ( mmc ); <nl> + if ( ret ) <nl> + goto out_free ; <nl> mmc -> ops = & mxcmci_ops ; <nl>  <nl> /* For devicetree parsing , the bus width is read from devicetree */
ieee80211_deliver_skb ( struct ieee80211_rx_data * rx ) <nl> } <nl>  <nl> if ( xmit_skb ) { <nl> - /* send to wireless media */ <nl> + /* <nl> + * Send to wireless media and increase priority by 256 to <nl> + * keep the received priority instead of reclassifying <nl> + * the frame ( see cfg80211_classify8021d ). <nl> + */ <nl> + xmit_skb -> priority += 256 ; <nl> xmit_skb -> protocol = htons ( ETH_P_802_3 ); <nl> skb_reset_network_header ( xmit_skb ); <nl> skb_reset_mac_header ( xmit_skb );
static int f2fs_write_begin ( struct file * file , struct address_space * mapping , <nl>  <nl> /* check inline_data */ <nl> ipage = get_node_page ( sbi , inode -> i_ino ); <nl> - if ( IS_ERR ( ipage )) <nl> + if ( IS_ERR ( ipage )) { <nl> + err = PTR_ERR ( ipage ); <nl> goto unlock_fail ; <nl> + } <nl>  <nl> set_new_dnode (& dn , inode , ipage , ipage , 0 ); <nl> 
static int patch_alc269 ( struct hda_codec * codec ) <nl>  <nl> spec = codec -> spec ; <nl> spec -> gen . shared_mic_vref_pin = 0x18 ; <nl> + codec -> power_save_node = 1 ; <nl>  <nl> snd_hda_pick_fixup ( codec , alc269_fixup_models , <nl> alc269_fixup_tbl , alc269_fixups );
struct rock_state { <nl> int cont_size ; <nl> int cont_extent ; <nl> int cont_offset ; <nl> + int cont_loops ; <nl> struct inode * inode ; <nl> }; <nl>  <nl> static void init_rock_state ( struct rock_state * rs , struct inode * inode ) <nl> rs -> inode = inode ; <nl> } <nl>  <nl> +/* Maximum number of Rock Ridge continuation entries */ <nl> +# define RR_MAX_CE_ENTRIES 32 <nl> + <nl> /* <nl> * Returns 0 if the caller should continue scanning , 1 if the scan must end <nl> * and - ve on error . <nl> static int rock_continue ( struct rock_state * rs ) <nl> goto out ; <nl> } <nl> ret = - EIO ; <nl> + if (++ rs -> cont_loops >= RR_MAX_CE_ENTRIES ) <nl> + goto out ; <nl> bh = sb_bread ( rs -> inode -> i_sb , rs -> cont_extent ); <nl> if ( bh ) { <nl> memcpy ( rs -> buffer , bh -> b_data + rs -> cont_offset ,
gf100_pm_gpc [] = { <nl> { 0x0c , " gpc00_tex_02 ", gf100_tex_sources }, <nl> { 0x0d , " gpc00_tex_03 ", gf100_tex_sources }, <nl> { 0x0e , " gpc00_tex_04 ", gf100_tex_sources }, <nl> - { 0x0e , " gpc00_tex_05 ", gf100_tex_sources }, <nl> - { 0x0f , " gpc00_tex_06 ", gf100_tex_sources }, <nl> - { 0x10 , " gpc00_tex_07 ", gf100_tex_sources }, <nl> - { 0x11 , " gpc00_tex_08 ", gf100_tex_sources }, <nl> - { 0x12 , " gpc00_tex_09 ", gf100_tex_sources }, <nl> + { 0x0f , " gpc00_tex_05 ", gf100_tex_sources }, <nl> + { 0x10 , " gpc00_tex_06 ", gf100_tex_sources }, <nl> + { 0x11 , " gpc00_tex_07 ", gf100_tex_sources }, <nl> + { 0x12 , " gpc00_tex_08 ", gf100_tex_sources }, <nl> { 0x26 , " gpc00_unk400_00 ", gf100_unk400_sources }, <nl> {} <nl> }, & gf100_perfctr_func },
static int falcon_mtd_probe ( struct efx_nic * efx ) <nl>  <nl> /* Allocate space for maximum number of partitions */ <nl> parts = kcalloc ( 2 , sizeof (* parts ), GFP_KERNEL ); <nl> + if (! parts ) <nl> + return - ENOMEM ; <nl> n_parts = 0 ; <nl>  <nl> spi = & nic_data -> spi_flash ;
mwifiex_cmd_append_vsie_tlv ( struct mwifiex_private * priv , <nl> vs_param_set -> header . len = <nl> cpu_to_le16 (((( u16 ) priv -> vs_ie [ id ]. ie [ 1 ]) <nl> & 0x00FF ) + 2 ); <nl> + if ( le16_to_cpu ( vs_param_set -> header . len ) > <nl> + MWIFIEX_MAX_VSIE_LEN ) { <nl> + mwifiex_dbg ( priv -> adapter , ERROR , <nl> + " Invalid param length !\ n "); <nl> + break ; <nl> + } <nl> + <nl> memcpy ( vs_param_set -> ie , priv -> vs_ie [ id ]. ie , <nl> le16_to_cpu ( vs_param_set -> header . len )); <nl> * buffer += le16_to_cpu ( vs_param_set -> header . len ) +
static int __br_mdb_add ( struct net * net , struct net_bridge * br , <nl> if (! p || p -> br != br || p -> state == BR_STATE_DISABLED ) <nl> return - EINVAL ; <nl>  <nl> + memset (& ip , 0 , sizeof ( ip )); <nl> ip . proto = entry -> addr . proto ; <nl> if ( ip . proto == htons ( ETH_P_IP )) <nl> ip . u . ip4 = entry -> addr . u . ip4 ; <nl> static int __br_mdb_del ( struct net_bridge * br , struct br_mdb_entry * entry ) <nl> if (! netif_running ( br -> dev ) || br -> multicast_disabled ) <nl> return - EINVAL ; <nl>  <nl> + memset (& ip , 0 , sizeof ( ip )); <nl> ip . proto = entry -> addr . proto ; <nl> if ( ip . proto == htons ( ETH_P_IP )) { <nl> if ( timer_pending (& br -> ip4_other_query . timer ))
void snd_seq_device_load_drivers ( void ) <nl> flush_work (& autoload_work ); <nl> } <nl> EXPORT_SYMBOL ( snd_seq_device_load_drivers ); <nl> +# define cancel_autoload_drivers () cancel_work_sync (& autoload_work ) <nl> # else <nl> # define queue_autoload_drivers () /* NOP */ <nl> +# define cancel_autoload_drivers () /* NOP */ <nl> # endif <nl>  <nl> /* <nl> static int snd_seq_device_dev_free ( struct snd_device * device ) <nl> { <nl> struct snd_seq_device * dev = device -> device_data ; <nl>  <nl> + cancel_autoload_drivers (); <nl> put_device (& dev -> dev ); <nl> return 0 ; <nl> }
xfs_zero_remaining_bytes ( <nl> bp = xfs_buf_get_noaddr ( mp -> m_sb . sb_blocksize , <nl> XFS_IS_REALTIME_INODE ( ip ) ? <nl> mp -> m_rtdev_targp : mp -> m_ddev_targp ); <nl> + if (! bp ) <nl> + return XFS_ERROR ( ENOMEM ); <nl>  <nl> for ( offset = startoff ; offset <= endoff ; offset = lastoffset + 1 ) { <nl> offset_fsb = XFS_B_TO_FSBT ( mp , offset );
int asn1_ber_decoder ( const struct asn1_decoder * decoder , <nl> else <nl> act = machine [ pc + 1 ]; <nl> ret = actions [ act ]( context , hdr , 0 , data + tdp , len ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> } <nl> pc += asn1_op_lengths [ op ]; <nl> goto next_op ;
static inline struct dentry * ovl_lookup_real ( struct dentry * dir , <nl> { <nl> struct dentry * dentry ; <nl>  <nl> - inode_lock ( dir -> d_inode ); <nl> - dentry = lookup_one_len ( name -> name , dir , name -> len ); <nl> - inode_unlock ( dir -> d_inode ); <nl> + dentry = lookup_hash ( name , dir ); <nl>  <nl> if ( IS_ERR ( dentry )) { <nl> if ( PTR_ERR ( dentry ) == - ENOENT )
static void oz_usb_handle_ep_data ( struct oz_usb_ctx * usb_ctx , <nl> struct oz_multiple_fixed * body = <nl> ( struct oz_multiple_fixed *) data_hdr ; <nl> u8 * data = body -> data ; <nl> - int n ; <nl> - if (! body -> unit_size ) <nl> + unsigned int n ; <nl> + if (! body -> unit_size || <nl> + len < sizeof ( struct oz_multiple_fixed ) - 1 ) <nl> break ; <nl> - n = ( len - sizeof ( struct oz_multiple_fixed )+ 1 ) <nl> + n = ( len - ( sizeof ( struct oz_multiple_fixed ) - 1 )) <nl> / body -> unit_size ; <nl> while ( n --) { <nl> oz_hcd_data_ind ( usb_ctx -> hport , body -> endpoint ,
int rad_packet_recv ( int fd , struct rad_packet_t ** p , struct sockaddr_in * addr ) <nl> len -= vendor -> tag + vendor -> len ; <nl>  <nl> n -= 4 + vendor -> tag + vendor -> len ; <nl> + if ( len < 0 ) { <nl> + log_ppp_warn (" radius : packet invalid vendor attribute len received \ n "); <nl> + goto out_err ; <nl> + } <nl> + if ( 2 + len > n ) { <nl> + log_ppp_warn (" radius : packet : too long vendor attribute received (% i , % i )\ n ", id , len ); <nl> + goto out_err ; <nl> + } <nl> } else <nl> log_ppp_warn (" radius : packet : vendor % i not found \ n ", id ); <nl> } else
MultiPartInputFile :: initialize () <nl> // Perform usual check on headers . <nl> // <nl>  <nl> + if ( _data -> _headers . size () == 0 ) <nl> + { <nl> + throw IEX_NAMESPACE :: ArgExc (" Files must contain at least one header "); <nl> + } <nl> + <nl> for ( size_t i = 0 ; i < _data -> _headers . size (); i ++) <nl> { <nl> //
find_sig8_target_as_global_offset ( Dwarf_Attribute attr , <nl> Dwarf_Bool targ_is_info = 0 ; <nl> Dwarf_Off localoff = 0 ; <nl> int res = 0 ; <nl> - <nl> targ_is_info = attr -> ar_cu_context -> cc_is_info ; <nl> memcpy ( sig8 , attr -> ar_debug_ptr , sizeof (* sig8 )); <nl> res = dwarf_find_die_given_sig8 ( attr -> ar_dbg , <nl> dwarf_global_formref_b ( Dwarf_Attribute attr , <nl> Dwarf_Bool t_is_info = TRUE ; <nl> Dwarf_Unsigned t_offset = 0 ; <nl>  <nl> + if (( attr -> ar_debug_ptr + sizeof ( Dwarf_Sig8 )) > section_end ) { <nl> + _dwarf_error_string ( dbg , error , <nl> + DW_DLE_REF_SIG8_NOT_HANDLED , <nl> + " DW_DLE_REF_SIG8_NOT_HANDLED : " <nl> + " Dwarf_Sig8 content runs off the end of its section "); <nl> + return DW_DLV_ERROR ; <nl> + } <nl> memcpy (& sig8 , attr -> ar_debug_ptr , sizeof ( Dwarf_Sig8 )); <nl> res = find_sig8_target_as_global_offset ( attr , <nl> & sig8 ,& t_is_info ,& t_offset , error );
main ( int argc , char ** argv ) <nl>  <nl> # ifdef HAVE_LOGIN_CAP_H <nl> if ( setusercontext ( NULL , targpw , target , LOGIN_SETGROUP | <nl> + LOGIN_SETPATH | <nl> LOGIN_SETPRIORITY | LOGIN_SETRESOURCES | LOGIN_SETUMASK | <nl> LOGIN_SETUSER ) != 0 ) <nl> errx ( 1 , " failed to set user context for target "); <nl> main ( int argc , char ** argv ) <nl> err ( 1 , " initgroups "); <nl> if ( setresuid ( target , target , target ) != 0 ) <nl> err ( 1 , " setresuid "); <nl> + if ( setenv (" PATH ", safepath , 1 ) == - 1 ) <nl> + err ( 1 , " failed to set PATH '% s '", safepath ); <nl> # endif <nl>  <nl> if ( getcwd ( cwdpath , sizeof ( cwdpath )) == NULL )
fribidi_cap_rtl_to_unicode ( <nl> } <nl> } <nl> else <nl> - us [ j ++] = caprtl_to_unicode [( int ) s [ i ]]; <nl> + { <nl> + if (( int ) s [ i ] < 0 ) <nl> + us [ j ++] = '?'; <nl> + else <nl> + us [ j ++] = caprtl_to_unicode [( int ) s [ i ]]; <nl> + } <nl> } <nl>  <nl> return j ;
void IdentifierHashTable :: remove ( const StringPrimitive * str ) { <nl> } <nl>  <nl> void IdentifierHashTable :: growAndRehash ( uint32_t newCapacity ) { <nl> + // Guard against potential overflow in the calculation of new capacity . <nl> + if ( LLVM_UNLIKELY ( newCapacity <= capacity ())) { <nl> + hermes_fatal (" too many identifiers created "); <nl> + } <nl> assert ( llvh :: isPowerOf2_32 ( newCapacity ) && " capacity must be power of 2 "); <nl> CompactTable tmpTable ( newCapacity , table_ . getCurrentScale ()); <nl> tmpTable . swap ( table_ );
_gnutls_x509_dn_to_string ( const char * oid , void * value , <nl> if ( ret < 0 ) { <nl> gnutls_assert (); <nl> gnutls_free ( str -> data ); <nl> + str -> data = NULL ; <nl> return ret ; <nl> } <nl> str -> size = size ;
int gnutls_x509_ext_import_crl_dist_points ( const gnutls_datum_t * ext , <nl> int len , ret ; <nl> uint8_t reasons [ 2 ]; <nl> unsigned i , type , rflags , j ; <nl> - gnutls_datum_t san ; <nl> + gnutls_datum_t san = { NULL , 0 }; <nl>  <nl> result = asn1_create_element <nl> ( _gnutls_get_pkix (), " PKIX1 . CRLDistributionPoints ", & c2 ); <nl> int gnutls_x509_ext_import_crl_dist_points ( const gnutls_datum_t * ext , <nl>  <nl> i = 0 ; <nl> do { <nl> - san . data = NULL ; <nl> - san . size = 0 ; <nl> - <nl> snprintf ( name , sizeof ( name ), "?% u . reasons ", ( unsigned ) i + 1 ); <nl>  <nl> len = sizeof ( reasons ); <nl> int gnutls_x509_ext_import_crl_dist_points ( const gnutls_datum_t * ext , <nl>  <nl> j = 0 ; <nl> do { <nl> + san . data = NULL ; <nl> + san . size = 0 ; <nl> + <nl> ret = <nl> _gnutls_parse_general_name2 ( c2 , name , j , & san , <nl> & type , 0 ); <nl> int gnutls_x509_ext_import_crl_dist_points ( const gnutls_datum_t * ext , <nl> ret = crl_dist_points_set ( cdp , type , & san , rflags ); <nl> if ( ret < 0 ) <nl> break ; <nl> + san . data = NULL ; /* it is now in cdp */ <nl>  <nl> j ++; <nl> } while ( ret >= 0 ); <nl> int gnutls_x509_ext_import_crl_dist_points ( const gnutls_datum_t * ext , <nl>  <nl> if ( ret < 0 && ret != GNUTLS_E_REQUESTED_DATA_NOT_AVAILABLE ) { <nl> gnutls_assert (); <nl> + gnutls_free ( san . data ); <nl> goto cleanup ; <nl> } <nl> 
static void http_manage_server_side_cookies ( struct stream * s , struct channel * re <nl> while ( 1 ) { <nl> int is_first = 1 ; <nl>  <nl> - if (! http_find_header ( htx , ist (" Set - Cookie "), & ctx , 1 )) { <nl> + if ( is_cookie2 || ! http_find_header ( htx , ist (" Set - Cookie "), & ctx , 1 )) { <nl> if (! http_find_header ( htx , ist (" Set - Cookie2 "), & ctx , 1 )) <nl> break ; <nl> is_cookie2 = 1 ;
lzw_result lzw_decode ( struct lzw_ctx * ctx , <nl> /* Code is invalid */ <nl> return LZW_BAD_CODE ; <nl>  <nl> + } else if ( code_new >= 1 << LZW_CODE_MAX ) { <nl> + /* Don ' t access out of bound */ <nl> + return LZW_BAD_CODE ; <nl> + <nl> } else if ( code_new < current_entry ) { <nl> /* Code is in table */ <nl> code_out = code_new ;
int enc_untrusted_rename ( const char * oldpath , const char * newpath ) { <nl> } <nl>  <nl> ssize_t enc_untrusted_read ( int fd , void * buf , size_t count ) { <nl> - return static_cast < ssize_t >( EnsureInitializedAndDispatchSyscall ( <nl> + ssize_t ret = static_cast < ssize_t >( EnsureInitializedAndDispatchSyscall ( <nl> asylo :: system_call :: kSYS_read , fd , buf , count )); <nl> + if ( ret != - 1 && ret > count ) { <nl> + :: asylo :: primitives :: TrustedPrimitives :: BestEffortAbort ( <nl> + " enc_untrusted_read : read result exceeds requested "); <nl> + } <nl> + return ret ; <nl> } <nl>  <nl> ssize_t enc_untrusted_write ( int fd , const void * buf , size_t count ) {
create_new_keys ( const char * jwkdir ) <nl> { <nl> const char * alg [] = {" ES512 ", " ECMR ", NULL }; <nl> char path [ PATH_MAX ]; <nl> + <nl> + /* Set default umask for file creation . */ <nl> + umask ( 0337 ); <nl> for ( int i = 0 ; alg [ i ] != NULL ; i ++) { <nl> json_auto_t * jwk = jwk_generate ( alg [ i ]); <nl> if (! jwk ) {
static int decode_font ( ASS_Track * track ) <nl> ass_msg ( track -> library , MSGL_ERR , " Bad encoded data size "); <nl> goto error_decode_font ; <nl> } <nl> - buf = malloc ( size / 4 * 3 + FFMAX ( size % 4 - 1 , 0 )); <nl> + buf = malloc ( size / 4 * 3 + FFMAX ( size % 4 , 1 ) - 1 ); <nl> if (! buf ) <nl> goto error_decode_font ; <nl> q = buf ; <nl> static int decode_font ( ASS_Track * track ) <nl> q = decode_chars ( p , q , 3 ); <nl> } <nl> dsize = q - buf ; <nl> - assert ( dsize == size / 4 * 3 + FFMAX ( size % 4 - 1 , 0 )); <nl> + assert ( dsize == size / 4 * 3 + FFMAX ( size % 4 , 1 ) - 1 ); <nl>  <nl> if ( track -> library -> extract_fonts ) { <nl> ass_add_font ( track -> library , track -> parser_priv -> fontname ,
 <nl> # include " idn2 . h " <nl>  <nl> +# include < sys / types . h > <nl> # include < stdbool . h > <nl>  <nl> # include " bidi . h " <nl> static bool <nl> _isBidi ( const uint32_t * label , size_t llen ) <nl> { <nl> - while ( llen -- > 0 ) { <nl> + for (; ( ssize_t ) llen > 0 ; llen --) { <nl> int bc = uc_bidi_category (* label ++); <nl>  <nl> if ( bc == UC_BIDI_R || bc == UC_BIDI_AL || bc == UC_BIDI_AN )
main ( int argc , char * argv []) <nl> shortv != PHOTOMETRIC_PALETTE ) { <nl> fprintf ( stderr , "% s : Expecting a palette image .\ n ", <nl> argv [ optind ]); <nl> + ( void ) TIFFClose ( in ); <nl> return (- 1 ); <nl> } <nl> if (! TIFFGetField ( in , TIFFTAG_COLORMAP , & rmap , & gmap , & bmap )) { <nl> fprintf ( stderr , <nl> "% s : No colormap ( not a valid palette image ).\ n ", <nl> argv [ optind ]); <nl> + ( void ) TIFFClose ( in ); <nl> return (- 1 ); <nl> } <nl> bitspersample = 0 ; <nl> main ( int argc , char * argv []) <nl> if ( bitspersample != 8 ) { <nl> fprintf ( stderr , "% s : Sorry , can only handle 8 - bit images .\ n ", <nl> argv [ optind ]); <nl> + ( void ) TIFFClose ( in ); <nl> return (- 1 ); <nl> } <nl> out = TIFFOpen ( argv [ optind + 1 ], " w "); <nl> - if ( out == NULL ) <nl> + if ( out == NULL ) { <nl> + ( void ) TIFFClose ( in ); <nl> return (- 2 ); <nl> + } <nl> cpTags ( in , out ); <nl> TIFFGetField ( in , TIFFTAG_IMAGEWIDTH , & imagewidth ); <nl> TIFFGetField ( in , TIFFTAG_IMAGELENGTH , & imagelength );
writefile ( const char * name , struct string * s ) <nl> return - 1 ; <nl> } <nl> ret = 0 ; <nl> - if ( fwrite ( s -> s , 1 , s -> n , f ) != s -> n || fflush ( f ) != 0 ) { <nl> + if ( s && ( fwrite ( s -> s , 1 , s -> n , f ) != s -> n || fflush ( f ) != 0 )) { <nl> warn (" write % s :", name ); <nl> ret = - 1 ; <nl> }
const char * /* O - File extension */ <nl> file_extension ( const char * s ) /* I - Filename or URL */ <nl> { <nl> const char * extension ; /* Pointer to directory separator */ <nl> + char * bufptr ; /* Pointer into buffer */ <nl> static char buf [ 1024 ]; /* Buffer for files with targets */ <nl>  <nl>  <nl> file_extension ( const char * s ) /* I - Filename or URL */ <nl>  <nl> strlcpy ( buf , extension , sizeof ( buf )); <nl>  <nl> - *( char *) strchr ( buf , '#') = '\ 0 '; <nl> + if (( bufptr = strchr ( buf , '#')) != NULL ) <nl> + * bufptr = '\ 0 '; <nl>  <nl> return ( buf ); <nl> }
pdf_write_names ( FILE * out ) /* I - Output file */ <nl> pdf_start_object ( out ); <nl> float x , y ; <nl>  <nl> + check_pages ( link -> page ); <nl> + <nl> x = 0 . 0f ; <nl> y = link -> top + pages [ link -> page ]. bottom ; <nl> pspdf_transform_coords ( pages + link -> page , x , y );
static void DetectRunCleanup ( DetectEngineThreadCtx * det_ctx , <nl>  <nl> if ( pflow != NULL ) { <nl> /* update inspected tracker for raw reassembly */ <nl> - if ( p -> proto == IPPROTO_TCP && pflow -> protoctx != NULL ) { <nl> + if ( p -> proto == IPPROTO_TCP && pflow -> protoctx != NULL && <nl> + ( p -> flags & PKT_STREAM_EST )) <nl> + { <nl> StreamReassembleRawUpdateProgress ( pflow -> protoctx , p , <nl> det_ctx -> raw_stream_progress ); <nl> 
void lppTransposer ( HANDLE_SBR_LPP_TRANS hLppTrans , /*!< Handle of lpp transp <nl> } <nl>  <nl> /* init bwIndex for each patch */ <nl> - FDKmemclear ( bwIndex , pSettings -> noOfPatches * sizeof ( INT )); <nl> + FDKmemclear ( bwIndex , MAX_NUM_PATCHES * sizeof ( INT )); <nl>  <nl> /* <nl> Calc common low band scale factor <nl> void lppTransposer ( HANDLE_SBR_LPP_TRANS hLppTrans , /*!< Handle of lpp transp <nl> FDK_ASSERT ( hiBand < ( 64 ) ); <nl>  <nl> /* bwIndex [ patch ] is already initialized with value from previous band inside this patch */ <nl> - while ( hiBand >= pSettings -> bwBorders [ bwIndex [ patch ]]) <nl> + while ( hiBand >= pSettings -> bwBorders [ bwIndex [ patch ]] && bwIndex [ patch ] < MAX_NUM_PATCHES - 1 ) { <nl> bwIndex [ patch ]++; <nl> - <nl> + } <nl>  <nl> /* <nl> Filter Step 2 : add the left slope with the current filter to the buffer <nl> resetLppTransposer ( HANDLE_SBR_LPP_TRANS hLppTrans , /*!< Handle of lpp transpos <nl> for ( i = 0 ; i < noNoiseBands ; i ++){ <nl> pSettings -> bwBorders [ i ] = noiseBandTable [ i + 1 ]; <nl> } <nl> + for (; i < MAX_NUM_NOISE_VALUES ; i ++) { <nl> + pSettings -> bwBorders [ i ] = 255 ; <nl> + } <nl> + <nl>  <nl> /* <nl> * Choose whitening factors
static void ProcessExifDir ( unsigned char * DirStart , unsigned char * OffsetBase , <nl> unsigned OffsetVal ; <nl> OffsetVal = Get32u ( DirEntry + 8 ); <nl> // If its bigger than 4 bytes , the dir entry contains an offset . <nl> - if ( OffsetVal + ByteCount > ExifLength ){ <nl> + if ( OffsetVal > UINT32_MAX - ByteCount || OffsetVal + ByteCount > ExifLength ){ <nl> // Bogus pointer offset and / or bytecount value <nl> ErrNonfatal (" Illegal value pointer for tag % 04x ", Tag , 0 ); <nl> continue ;
WORD32 ih264d_video_decode ( iv_obj_t * dec_hdl , void * pv_api_ip , void * pv_api_op ) <nl> if ( buflen == - 1 ) <nl> buflen = 0 ; <nl> /* Ignore bytes beyond the allocated size of intermediate buffer */ <nl> - buflen = MIN ( buflen , buf_size ); <nl> + /* Since 8 bytes are read ahead , ensure 8 bytes are free at the <nl> + end of the buffer , which will be memset to 0 after emulation prevention */ <nl> + buflen = MIN ( buflen , buf_size - 8 ); <nl>  <nl> bytes_consumed = buflen + u4_length_of_start_code ; <nl> ps_dec_op -> u4_num_bytes_consumed += bytes_consumed ;
WORD32 ih264d_start_of_pic ( dec_struct_t * ps_dec , <nl> ps_cur_pic -> pu1_col_zero_flag = ( UWORD8 *) ps_col_mv -> pv_col_zero_flag ; <nl> ps_cur_pic -> ps_mv = ( mv_pred_t *) ps_col_mv -> pv_mv ; <nl> ps_dec -> au1_pic_buf_ref_flag [ cur_pic_buf_id ] = 0 ; <nl> - if ( ps_dec -> u1_first_slice_in_stream ) <nl> + <nl> { <nl> /* make first entry of list0 point to cur pic , so that if first Islice is in error , ref pic struct will have valid entries */ <nl> ps_dec -> ps_ref_pic_buf_lx [ 0 ] = ps_dec -> ps_dpb_mgr -> ps_init_dpb [ 0 ];
# include " libxml . h " <nl>  <nl> # include < string . h > <nl> +# include < limits . h > <nl>  <nl> # include < libxml / xmlmemory . h > <nl> # include < libxml / uri . h > <nl> xmlParse3986Port ( xmlURIPtr uri , const char ** str ) <nl> cur ++; <nl> } <nl> if ( uri != NULL ) <nl> - uri -> port = port & INT_MAX ; /* port value modulo INT_MAX + 1 */ <nl> + uri -> port = port & USHRT_MAX ; /* port value modulo INT_MAX + 1 */ <nl> * str = cur ; <nl> return ( 0 ); <nl> }
EAS_RESULT DLSParser ( EAS_HW_DATA_HANDLE hwInstData , EAS_FILE_HANDLE fileHandle , <nl> } <nl>  <nl> /* create the default articulation */ <nl> - Convert_art (& dls , & defaultArt , 0 ); <nl> - dls . artCount = 1 ; <nl> + if ( dls . pDLS ) { <nl> + Convert_art (& dls , & defaultArt , 0 ); <nl> + dls . artCount = 1 ; <nl> + } <nl>  <nl> /* parse the lins chunk and load instruments */ <nl> dls . regionCount = dls . instCount = 0 ;
typedef struct { <nl> int (* init )( bt_callbacks_t * callbacks ); <nl>  <nl> /** Enable Bluetooth . */ <nl> - int (* enable )( void ); <nl> + int (* enable )( bool guest_mode ); <nl>  <nl> /** Disable Bluetooth . */ <nl> int (* disable )( void );
status_t BnGraphicBufferConsumer :: onTransact ( <nl> CHECK_INTERFACE ( IGraphicBufferConsumer , data , reply ); <nl> sp < GraphicBuffer > buffer = new GraphicBuffer (); <nl> data . read (* buffer . get ()); <nl> - int slot ; <nl> + int slot = - 1 ; <nl> int result = attachBuffer (& slot , buffer ); <nl> reply -> writeInt32 ( slot ); <nl> reply -> writeInt32 ( result );
void SimpleSoftOMXComponent :: onPortEnable ( OMX_U32 portIndex , bool enable ) { <nl> CHECK_EQ (( int ) port -> mTransition , ( int ) PortInfo :: NONE ); <nl> CHECK ( port -> mDef . bEnabled == ! enable ); <nl>  <nl> + if ( port -> mDef . eDir != OMX_DirOutput ) { <nl> + ALOGE (" Port enable / disable allowed only on output ports ."); <nl> + notify ( OMX_EventError , OMX_ErrorUndefined , 0 , NULL ); <nl> + android_errorWriteLog ( 0x534e4554 , " 29421804 "); <nl> + return ; <nl> + } <nl> + <nl> if (! enable ) { <nl> port -> mDef . bEnabled = OMX_FALSE ; <nl> port -> mTransition = PortInfo :: DISABLING ;
sp < IMemory > MetadataRetrieverClient :: getFrameAtTime ( int64_t timeUs , int option ) <nl> ALOGV (" rotation : % d ", frameCopy -> mRotationAngle ); <nl> frameCopy -> mData = ( uint8_t *) frameCopy + sizeof ( VideoFrame ); <nl> memcpy ( frameCopy -> mData , frame -> mData , frame -> mSize ); <nl> + frameCopy -> mData = 0 ; <nl> delete frame ; // Fix memory leakage <nl> return mThumbnail ; <nl> }
OMX_ERRORTYPE SimpleSoftOMXComponent :: useBuffer ( <nl> Mutex :: Autolock autoLock ( mLock ); <nl> CHECK_LT ( portIndex , mPorts . size ()); <nl>  <nl> + PortInfo * port = & mPorts . editItemAt ( portIndex ); <nl> + if ( size < port -> mDef . nBufferSize ) { <nl> + ALOGE (" b / 63522430 , Buffer size is too small ."); <nl> + android_errorWriteLog ( 0x534e4554 , " 63522430 "); <nl> + return OMX_ErrorBadParameter ; <nl> + } <nl> + <nl> * header = new OMX_BUFFERHEADERTYPE ; <nl> (* header )-> nSize = sizeof ( OMX_BUFFERHEADERTYPE ); <nl> (* header )-> nVersion . s . nVersionMajor = 1 ; <nl> OMX_ERRORTYPE SimpleSoftOMXComponent :: useBuffer ( <nl> (* header )-> nOutputPortIndex = portIndex ; <nl> (* header )-> nInputPortIndex = portIndex ; <nl>  <nl> - PortInfo * port = & mPorts . editItemAt ( portIndex ); <nl> - <nl> CHECK ( mState == OMX_StateLoaded || port -> mDef . bEnabled == OMX_FALSE ); <nl>  <nl> CHECK_LT ( port -> mBuffers . size (), port -> mDef . nBufferCountActual );
status_t MPEG4Extractor :: parseChunk ( off64_t * offset , int depth ) { <nl> size = 0 ; <nl> } <nl>  <nl> - if ( SIZE_MAX - chunk_size <= size ) { <nl> + if (( chunk_size > SIZE_MAX ) || ( SIZE_MAX - chunk_size <= size )) { <nl> return ERROR_MALFORMED ; <nl> } <nl> 
void QPaintEngineEx :: stroke ( const QVectorPath & path , const QPen & inPen ) <nl> patternLength *= pen . widthF (); <nl> if ( qFuzzyIsNull ( patternLength )) { <nl> pen . setStyle ( Qt :: NoPen ); <nl> - } else if ( extent / patternLength > 10000 ) { <nl> + } else if ( qFuzzyIsNull ( extent ) || extent / patternLength > 10000 ) { <nl> // approximate stream of tiny dashes with semi - transparent solid line <nl> pen . setStyle ( Qt :: SolidLine ); <nl> QColor color ( pen . color ());
static RzDyldRebaseInfos * get_rebase_infos ( RzDyldCache * cache ) { <nl> } <nl>  <nl> if (! cache -> hdr -> slideInfoOffset || ! cache -> hdr -> slideInfoSize ) { <nl> - ut32 total_slide_infos = 0 ; <nl> + size_t total_slide_infos = 0 ; <nl> ut32 n_slide_infos [ MAX_N_HDR ]; <nl>  <nl> ut32 i ; <nl> static RzDyldRebaseInfos * get_rebase_infos ( RzDyldCache * cache ) { <nl> if (! rz_buf_read_le32_at ( cache -> buf , 0x13c + hdr_offset , & n_slide_infos [ i ])) { <nl> goto beach ; <nl> } <nl> - total_slide_infos += n_slide_infos [ i ]; <nl> + ut32 total = total_slide_infos + n_slide_infos [ i ]; <nl> + if ( total < total_slide_infos ) { <nl> + // overflow <nl> + goto beach ; <nl> + } <nl> + total_slide_infos = total ; <nl> } <nl>  <nl> if (! total_slide_infos ) {
sixel_dither_new ( <nl> quality_mode = SIXEL_QUALITY_HIGHCOLOR ; <nl> } else { <nl> if ( ncolors > SIXEL_PALETTE_MAX ) { <nl> + status = SIXEL_BAD_INPUT ; <nl> ncolors = 256 ; <nl> - } else if ( ncolors < 2 ) { <nl> - ncolors = 2 ; <nl> + } else if ( ncolors < 1 ) { <nl> + status = SIXEL_BAD_INPUT ; <nl> + sixel_helper_set_additional_message ( <nl> + " sixel_dither_new : palette colors must be more than 0 "); <nl> + goto end ; <nl> } <nl> quality_mode = SIXEL_QUALITY_LOW ; <nl> }
bool Scanner :: fill ( size_t need ) <nl> if (! buf ) fatal (" out of memory "); <nl>  <nl> memmove ( buf , tok , copy ); <nl> - shift_ptrs_and_fpos ( buf - bot ); <nl> + shift_ptrs_and_fpos ( buf - tok ); <nl> delete [] bot ; <nl> bot = buf ; <nl>  <nl> free = BSIZE - copy ; <nl> } <nl>  <nl> + DASSERT ( lim + free <= bot + BSIZE ); <nl> if (! read ( free )) { <nl> eof = lim ; <nl> memset ( lim , 0 , YYMAXFILL );
int modbus_reply ( modbus_t * ctx , const uint8_t * req , <nl> nb_write , nb , MODBUS_MAX_WR_WRITE_REGISTERS , MODBUS_MAX_WR_READ_REGISTERS ); <nl> } else if ( mapping_address < 0 || <nl> ( mapping_address + nb ) > mb_mapping -> nb_registers || <nl> - mapping_address < 0 || <nl> + mapping_address_write < 0 || <nl> ( mapping_address_write + nb_write ) > mb_mapping -> nb_registers ) { <nl> rsp_length = response_exception ( <nl> ctx , & sft , MODBUS_EXCEPTION_ILLEGAL_DATA_ADDRESS , rsp , FALSE ,
Error HeifContext :: get_id_of_non_virtual_child_image ( heif_item_id id , heif_item_ <nl> image_type ==" iden " || <nl> image_type ==" iovl ") { <nl> auto iref_box = m_heif_file -> get_iref_box (); <nl> + if (! iref_box ) { <nl> + return Error ( heif_error_Invalid_input , <nl> + heif_suberror_No_item_data , <nl> + " Derived image does not reference any other image items "); <nl> + } <nl> + <nl> std :: vector < heif_item_id > image_references = iref_box -> get_references ( id , fourcc (" dimg ")); <nl>  <nl> // TODO : check whether this really can be recursive ( e . g . overlay of grid images )
daemon_AuthUserPwd ( char * username , char * password , char * errbuf ) <nl> # ifdef HAVE_GETSPNAM <nl> struct spwd * usersp ; <nl> # endif <nl> + char * crypt_password ; <nl>  <nl> // This call is needed to get the uid <nl> if (( user = getpwnam ( username )) == NULL ) <nl> daemon_AuthUserPwd ( char * username , char * password , char * errbuf ) <nl> user_password = user -> pw_passwd ; <nl> # endif <nl>  <nl> - if ( strcmp ( user_password , ( char *) crypt ( password , user_password )) != 0 ) <nl> + crypt_password = crypt ( password , user_password ); <nl> + if ( crypt_password == NULL ) <nl> + { <nl> + pcap_snprintf ( errbuf , PCAP_ERRBUF_SIZE , " Authentication failed "); <nl> + return - 1 ; <nl> + } <nl> + if ( strcmp ( user_password , crypt_password ) != 0 ) <nl> { <nl> pcap_snprintf ( errbuf , PCAP_ERRBUF_SIZE , " Authentication failed : user name or password incorrect "); <nl> return - 1 ;
int init_result ( RESULT & result , void *& data ) { <nl> log_messages . printf ( MSG_DEBUG , " Check result \ n "); <nl>  <nl> char buff [ 256 ]; <nl> - n = fscanf ( f , "% s ", buff ); <nl> + // n = fscanf ( f , "% s ", buff ); <nl> + fgets ( buff , 256 , f ); <nl> char * pch ; <nl> pch = strtok ( buff , " ,"); <nl> if ( pch != NULL ) {
class EncodePngOp : public OpKernel { <nl> OP_REQUIRES ( context , image . dims () == 3 , <nl> errors :: InvalidArgument (" image must be 3 - dimensional ", <nl> image . shape (). DebugString ())); <nl> + OP_REQUIRES ( context , image . NumElements () > 0 , <nl> + errors :: Internal (" Invalid image provided .")); <nl> OP_REQUIRES ( <nl> context , <nl> FastBoundsCheck ( image . NumElements (), std :: numeric_limits < int32 >:: max ()),
class CudnnRnnSequenceTensorDescriptor <nl> static port :: StatusOr < CudnnRnnSequenceTensorDescriptor > Create ( <nl> GpuExecutor * parent , int max_seq_length , int batch_size , int data_size , <nl> cudnnDataType_t data_type ) { <nl> - CHECK_GT ( max_seq_length , 0 ); <nl> + if ( max_seq_length <= 0 ) { <nl> + return port :: Status ( port :: error :: INVALID_ARGUMENT , " max_seq_length <= 0 "); <nl> + } <nl> int dims [] = { batch_size , data_size , 1 }; <nl> int strides [] = { dims [ 1 ] * dims [ 2 ], dims [ 2 ], 1 }; <nl> TensorDescriptor tensor_desc = CreateTensorDescriptor (); <nl> class CudnnRnnSequenceTensorDescriptor <nl> GpuExecutor * parent , int max_seq_length , int batch_size , int data_size , <nl> const absl :: Span < const int >& seq_lengths , bool time_major , <nl> cudnnDataType_t data_type ) { <nl> - CHECK_GT ( max_seq_length , 0 ); <nl> + if ( max_seq_length <= 0 ) { <nl> + return port :: Status ( port :: error :: INVALID_ARGUMENT , " max_seq_length <= 0 "); <nl> + } <nl> int dims [] = { batch_size , data_size , 1 }; <nl> int strides [] = { dims [ 1 ] * dims [ 2 ], dims [ 2 ], 1 }; <nl> TensorDescriptor tensor_desc = CreateTensorDescriptor ();
class LoadAndRemapMatrixOp : public OpKernel { <nl> // Processes the checkpoint source and the provided Tensor name . <nl> const Tensor * ckpt_path_t ; <nl> OP_REQUIRES_OK ( context , context -> input (" ckpt_path ", & ckpt_path_t )); <nl> + OP_REQUIRES ( <nl> + context , ckpt_path_t -> NumElements () == 1 , <nl> + errors :: InvalidArgument (" The ` ckpt_path ` tensor must have exactly one " <nl> + " element , got tensor of shape ", <nl> + ckpt_path_t -> shape (). DebugString ())); <nl> const string & ckpt_path = ckpt_path_t -> scalar < tstring >()(); <nl> const Tensor * old_tensor_name_t ; <nl> OP_REQUIRES_OK ( context ,
Status Examples :: Initialize ( OpKernelContext * const context , <nl> const Tensor * example_labels_t ; <nl> TF_RETURN_IF_ERROR ( context -> input (" example_labels ", & example_labels_t )); <nl> auto example_labels = example_labels_t -> flat < float >(); <nl> + if ( example_labels . size () != num_examples ) { <nl> + return errors :: InvalidArgument (" Expected ", num_examples , <nl> + " example labels but got ", <nl> + example_labels . size ()); <nl> + } <nl>  <nl> OpInputList dense_features_inputs ; <nl> TF_RETURN_IF_ERROR (
bool IsIdentityConsumingSwitch ( const MutableGraphView & graph , <nl> } <nl>  <nl> NodeDef * input_node = graph . GetNode ( tensor_id . node ()); <nl> + if ( input_node == nullptr ) { <nl> + return false ; <nl> + } <nl> return IsSwitch (* input_node ); <nl> } <nl> return false ;
class ReverseSequenceOp : public OpKernel { <nl> : OpKernel ( context ) { <nl> OP_REQUIRES_OK ( context , context -> GetAttr (" batch_dim ", & batch_dim_ )); <nl> OP_REQUIRES_OK ( context , context -> GetAttr (" seq_dim ", & seq_dim_ )); <nl> + OP_REQUIRES ( context , batch_dim_ >= 0 , <nl> + errors :: InvalidArgument (" Invalid batch_dim ", batch_dim_ )); <nl> + OP_REQUIRES ( context , seq_dim_ >= 0 , <nl> + errors :: InvalidArgument (" Invalid seq_dim ", seq_dim_ )); <nl> } <nl>  <nl> void Compute ( OpKernelContext * context ) override {
struct scalar_product_traits < QInt32 , double > { <nl> // the compiler from silently type cast the mantissa into a bigger or a smaller <nl> // representation . <nl> struct QInt8 { <nl> - QInt8 () {} <nl> + QInt8 () : value ( 0 ) {} <nl> QInt8 ( const int8_t v ) : value ( v ) {} <nl> QInt8 ( const QInt32 v ); <nl>  <nl> struct QInt8 { <nl> }; <nl>  <nl> struct QUInt8 { <nl> - QUInt8 () {} <nl> + QUInt8 () : value ( 0 ) {} <nl> QUInt8 ( const uint8_t v ) : value ( v ) {} <nl> QUInt8 ( const QInt32 v ); <nl>  <nl> struct QUInt8 { <nl> }; <nl>  <nl> struct QInt16 { <nl> - QInt16 () {} <nl> + QInt16 () : value ( 0 ) {} <nl> QInt16 ( const int16_t v ) : value ( v ) {} <nl> QInt16 ( const QInt32 v ); <nl> operator int () const { return static_cast < int >( value ); } <nl> struct QInt16 { <nl> }; <nl>  <nl> struct QUInt16 { <nl> - QUInt16 () {} <nl> + QUInt16 () : value ( 0 ) {} <nl> QUInt16 ( const uint16_t v ) : value ( v ) {} <nl> QUInt16 ( const QInt32 v ); <nl> operator int () const { return static_cast < int >( value ); } <nl> struct QUInt16 { <nl> }; <nl>  <nl> struct QInt32 { <nl> - QInt32 () {} <nl> + QInt32 () : value ( 0 ) {} <nl> QInt32 ( const int8_t v ) : value ( v ) {} <nl> QInt32 ( const int32_t v ) : value ( v ) {} <nl> QInt32 ( const uint32_t v ) : value ( static_cast < int32_t >( v )) {}
class SparseBincountOp : public OpKernel { <nl> for ( int64_t i = 0 ; i < indices_mat . dimension ( 0 ); ++ i ) { <nl> const int64_t batch = indices_mat ( i , 0 ); <nl> const Tidx bin = values ( i ); <nl> + OP_REQUIRES ( <nl> + ctx , batch < out . dimension ( 0 ), <nl> + errors :: InvalidArgument (" Index out of bound . ` batch ` (", batch , <nl> + ") must be less than the dimension size (", <nl> + out . dimension ( 0 ), ").")); <nl> + OP_REQUIRES ( <nl> + ctx , bin < out . dimension ( 1 ), <nl> + errors :: InvalidArgument (" Index out ouf bound . ` bin ` (", bin , <nl> + ") must be less then the dimension size (", <nl> + out . dimension ( 1 ), ").")); <nl> if ( bin < size ) { <nl> if ( binary_output_ ) { <nl> out ( batch , bin ) = T ( 1 );
class SummaryTensorOpV2 : public OpKernel { <nl> errors :: InvalidArgument (" tag must be scalar ")); <nl> const Tensor & tensor = c -> input ( 1 ); <nl> const Tensor & serialized_summary_metadata_tensor = c -> input ( 2 ); <nl> + OP_REQUIRES ( <nl> + c , <nl> + TensorShapeUtils :: IsScalar ( serialized_summary_metadata_tensor . shape ()), <nl> + errors :: InvalidArgument (" serialized_summary_metadata must be scalar ")); <nl>  <nl> Summary s ; <nl> Summary :: Value * v = s . add_value ();
void InferenceContext :: PreInputInit ( <nl> const std :: vector < ShapeHandle >& input_tensors_as_shapes ) { <nl> // TODO ( mdan ): This is also done at graph construction . Run only here instead ? <nl> const auto ret = full_type :: SpecializeType ( attrs_ , op_def ); <nl> - DCHECK ( ret . status (). ok ()) << " while instantiating types : " << ret . status (); <nl> + if (! ret . status (). ok ()) { <nl> + construction_status_ = ret . status (); <nl> + return ; <nl> + } <nl> ret_types_ = ret . ValueOrDie (); <nl>  <nl> input_tensors_ = input_tensors ;
class SummaryAudioOp : public OpKernel { <nl> float sample_rate = sample_rate_attr_ ; <nl> if (! has_sample_rate_attr_ ) { <nl> const Tensor & sample_rate_tensor = c -> input ( 2 ); <nl> + OP_REQUIRES ( c , <nl> + sample_rate_tensor . IsAligned () && <nl> + sample_rate_tensor . NumElements () == 1 , <nl> + errors :: InvalidArgument ( <nl> + " sample_rate must be rank - 0 or contain a single value ")); <nl> sample_rate = sample_rate_tensor . scalar < float >()(); <nl> } <nl> OP_REQUIRES ( c , sample_rate > 0 . 0f ,
limitations under the License . <nl> # include " tensorflow / core / framework / tensor_types . h " <nl> # include " tensorflow / core / lib / core / errors . h " <nl> # include " tensorflow / core / lib / core / status . h " <nl> +# include " tensorflow / core / platform / errors . h " <nl>  <nl> namespace tensorflow { <nl> namespace functor { <nl> Status SparseTensorToCSRSparseMatrixCPUFunctor :: operator ()( <nl>  <nl> for ( int64 i = 0 ; i < total_nnz ; ++ i ) { <nl> // For now , the rows pointers store the corresponding row counts . <nl> + int64 ix = indices ( i , 0 ) + 1 ; <nl> + if ( ix >= csr_row_ptr . size ()) { <nl> + return errors :: InvalidArgument (" Got an index ", ix , <nl> + " that is outside of csr_row_ptr "); <nl> + } <nl> csr_row_ptr ( indices ( i , 0 ) + 1 ) += 1 ; <nl> csr_col_ind ( i ) = indices ( i , 1 ); <nl> }
bool DependencyOptimizer :: SafeToRemoveIdentity ( const NodeDef & node ) const { <nl> } <nl>  <nl> const NodeDef * input = node_map_ -> GetNode ( NodeName ( node . input ( 0 ))); <nl> - CHECK ( input != nullptr ) << " node = " << node . name () <nl> - << " input = " << node . input ( 0 ); <nl> + if ( input == nullptr ) { <nl> + VLOG ( 1 ) << " node = " << node . name () << " input = " << node . input ( 0 ); <nl> + return false ; <nl> + } <nl> // Don ' t remove Identity nodes corresponding to Variable reads or following <nl> // Recv . <nl> if ( IsVariable (* input ) || IsRecv (* input )) {
class Conv3DBackpropInputOp < GPUDevice , T > : public OpKernel { <nl> Tensor * in_backprop ; <nl> OP_REQUIRES_OK ( context , <nl> context -> allocate_output ( 0 , input_shape , & in_backprop )); <nl> + for ( std :: size_t i = 0 ; i < input_shape . dims (); ++ i ) { <nl> + if ( input_shape . dim_size ( i ) == 0 ) { <nl> + return ; <nl> + } <nl> + } <nl>  <nl> auto * stream = context -> op_device_context ()-> stream (); <nl> OP_REQUIRES ( context , stream , errors :: Internal (" No GPU stream available ."));
Status ImmutableExecutorState :: Initialize ( const Graph & graph ) { <nl>  <nl> Status s = params_ . create_kernel ( n -> properties (), & item -> kernel ); <nl> if (! s . ok ()) { <nl> + params_ . delete_kernel ( item -> kernel ); <nl> item -> kernel = nullptr ; <nl> s = AttachDef ( s , * n ); <nl> return s ;
See the License for the specific language governing permissions and <nl> limitations under the License . <nl> ==============================================================================*/ <nl>  <nl> +# include < limits > <nl> + <nl> # include " absl / container / flat_hash_map . h " <nl> # include " tensorflow / core / framework / op_kernel . h " <nl> # include " tensorflow / core / framework / op_requires . h " <nl> limitations under the License . <nl>  <nl> namespace tensorflow { <nl>  <nl> +// Don ' t allocate too large ` BatchedMap < T >` objects <nl> + static int kMaxBatches = std :: numeric_limits < int >:: max (); <nl> + <nl> template < class T > <nl> using BatchedMap = std :: vector < absl :: flat_hash_map < int64_t , T >>; <nl>  <nl> class SparseCount : public OpKernel { <nl>  <nl> bool is_1d = shape . NumElements () == 1 ; <nl> int num_batches = is_1d ? 1 : shape_vector ( 0 ); <nl> + OP_REQUIRES ( <nl> + context , 0 < num_batches && num_batches < kMaxBatches , <nl> + errors :: InvalidArgument (" Cannot allocate ", num_batches , <nl> + " batches , is the dense shape too wide ?")); <nl>  <nl> const auto values_values = values . flat < T >(); <nl> const auto weight_values = weights . flat < W >();
template <> <nl> string SummarizeArray < bool >( int64_t limit , int64_t num_elts , <nl> const TensorShape & tensor_shape , const char * data , <nl> const bool print_v2 ) { <nl> + if ( data == nullptr ) { <nl> + return strings :: StrCat (""); // we already print type and shape <nl> + } <nl> // We first convert all chars to be 0 / 1 to not get InvalidEnumValue sanitizer <nl> // error <nl> auto mutable_data = std :: unique_ptr < char []>( new char [ num_elts ]);
Status ConvBackpropComputeDimensionsV2 ( <nl> // dimensions of the filter Tensor . <nl> VLOG ( 2 ) << " input vs filter_in depth " << dims -> in_depth << " " <nl> << filter_shape . dim_size ( num_dims - 2 ); <nl> + if ( filter_shape . dim_size ( num_dims - 2 ) <= 0 ) { <nl> + return errors :: InvalidArgument ( <nl> + label , ": filter depth must be strictly greated than zero "); <nl> + } <nl> if ( dims -> in_depth % filter_shape . dim_size ( num_dims - 2 )) { <nl> return errors :: InvalidArgument ( <nl> label , ": input depth must be evenly divisible by filter depth ");
TfLiteStatus Prepare ( TfLiteContext * context , TfLiteNode * node ) { <nl> output_temp_size_array )); <nl>  <nl> // Calculate effective scales . <nl> + TF_LITE_ENSURE ( context , input -> quantization . type != kTfLiteNoQuantization ); <nl> auto * input_params = <nl> reinterpret_cast < TfLiteAffineQuantization *>( input -> quantization . params ); <nl> + TF_LITE_ENSURE ( context , <nl> + weights_feature -> quantization . type != kTfLiteNoQuantization ); <nl> auto * weights_feature_params = reinterpret_cast < TfLiteAffineQuantization *>( <nl> weights_feature -> quantization . params ); <nl> + TF_LITE_ENSURE ( context , state -> quantization . type != kTfLiteNoQuantization ); <nl> auto * state_params = <nl> reinterpret_cast < TfLiteAffineQuantization *>( state -> quantization . params ); <nl> + TF_LITE_ENSURE ( context , <nl> + weights_time -> quantization . type != kTfLiteNoQuantization ); <nl> auto * weight_time_params = reinterpret_cast < TfLiteAffineQuantization *>( <nl> weights_time -> quantization . params ); <nl> + TF_LITE_ENSURE ( context , output -> quantization . type != kTfLiteNoQuantization ); <nl> auto * output_params = reinterpret_cast < TfLiteAffineQuantization *>( <nl> output -> quantization . params ); <nl> const double effective_scale_1 = input_params -> scale -> data [ 0 ] *
StatusOr < FullTypeDef > SpecializeType ( const AttrSlice & attrs , <nl> // verifications are needed , they should be done by separately , and in a <nl> // way that can be reused for type inference . <nl> for ( int j = 0 ; j < t -> args_size (); j ++) { <nl> - auto * arg = t -> mutable_args ( i ); <nl> + auto * arg = t -> mutable_args ( j ); <nl> if ( arg -> type_id () == TFT_VAR ) { <nl> const auto * attr = attrs . Find ( arg -> s ()); <nl> if ( attr == nullptr ) {
void SparseFillEmptyRowsOpImpl ( OpKernelContext * context , <nl> default_value_t . shape (). DebugString ()), <nl> done ); <nl> // TODO ( ebrevdo ): add shape checks between values , indices , <nl> - // dense_shape . Also add check that dense rank > 0 . <nl> + // Also add check that dense rank > 0 . <nl> + OP_REQUIRES_ASYNC ( context , dense_shape_t . NumElements () != 0 , <nl> + errors :: InvalidArgument (" Dense shape cannot be empty ."), <nl> + done ); <nl>  <nl> using FunctorType = functor :: SparseFillEmptyRows < Device , T , Tindex >; <nl> OP_REQUIRES_OK_ASYNC ( context ,
class RaggedTensorToVariantOp : public OpKernel { <nl> batched_ragged_input . mutable_nested_splits ()-> reserve ( <nl> ragged_nested_splits_len ); <nl> for ( int i = 0 ; i < ragged_nested_splits_len ; i ++) { <nl> + OP_REQUIRES ( context , ragged_nested_splits_in [ i ]. dims () == 1 , <nl> + errors :: InvalidArgument (" Requires nested_row_splits [", i , "]", <nl> + " to be rank 1 but is rank ", <nl> + ragged_nested_splits_in [ i ]. dims ())); <nl> batched_ragged_input . append_splits ( ragged_nested_splits_in [ i ]); <nl> } <nl> 
class CTCLossOp : public OpKernel { <nl>  <nl> const TensorShape & inputs_shape = inputs -> shape (); <nl> const int64 max_time = inputs_shape . dim_size ( 0 ); <nl> + OP_REQUIRES ( ctx , max_time != 0 , <nl> + errors :: InvalidArgument ( <nl> + " Max time or first dimension of input cannot be 0 .")); <nl> const int64 batch_size = inputs_shape . dim_size ( 1 ); <nl> const int64 num_classes_raw = inputs_shape . dim_size ( 2 ); <nl> OP_REQUIRES (
inline bool ResolveAxis ( const int num_dims , const int * axis , <nl> // eg : For num_dims = 3 , [ 0 , 1 , 2 ] is the same as [- 3 , - 2 , - 1 ] */ <nl> int current = axis [ idx ] < 0 ? ( axis [ idx ] + num_dims ) : axis [ idx ]; <nl> TFLITE_DCHECK ( current >= 0 && current < num_dims ); <nl> + if ( current < 0 || current >= num_dims ) { <nl> + return false ; <nl> + } <nl> bool is_dup = false ; <nl> for ( int j = 0 ; j < * out_num_axis ; ++ j ) { <nl> if ( out_axis [ j ] == current ) {
bool Tensor :: FromProto ( Allocator * a , const TensorProto & proto ) { <nl> dtype_error = true , dtype_error = true ); <nl> } <nl> if ( dtype_error || p == nullptr ) return false ; <nl> + } else { <nl> + // Handle the case of empty tensors ( N = 0 ) or tensors with incomplete shape <nl> + // ( N = - 1 ). All other values of ` shape . num_elements ()` should be invalid by <nl> + // construction . <nl> + // Here , we just need to validate that the ` proto . dtype ()` value is valid . <nl> + bool dtype_error = false ; <nl> + CASES_WITH_DEFAULT ( proto . dtype (), break , dtype_error = true , <nl> + dtype_error = true ); <nl> + if ( dtype_error ) return false ; <nl> } <nl> shape_ = shape ; <nl> set_dtype ( proto . dtype ());
class SparseCount : public OpKernel { <nl> " The shape argument requires at least one element .")); <nl>  <nl> bool is_1d = shape . NumElements () == 1 ; <nl> - int num_batches = is_1d ? 1 : shape . flat < int64 >()( 0 ); <nl> + auto shape_vector = shape . flat < int64 >(); <nl> + int num_batches = is_1d ? 1 : shape_vector ( 0 ); <nl> int num_values = values . NumElements (); <nl>  <nl> + for ( int b = 0 ; b < shape_vector . size (); b ++) { <nl> + OP_REQUIRES ( context , shape_vector ( b ) >= 0 , <nl> + errors :: InvalidArgument ( <nl> + " Elements in dense_shape must be >= 0 . Instead got :", <nl> + shape . DebugString ())); <nl> + } <nl> + <nl> OP_REQUIRES ( context , num_values == indices . shape (). dim_size ( 0 ), <nl> errors :: InvalidArgument ( <nl> " Number of values must match first dimension of indices .",
Status ImportGenericFunction ( <nl> // Import the function attributes with a ` tf .` prefix to match the current <nl> // infrastructure expectations . <nl> for ( const auto & namedAttr : func . attr ()) { <nl> + if ( namedAttr . first . empty ()) <nl> + return InvalidArgument (" Invalid function attribute name "); <nl> const std :: string & name = " tf ." + namedAttr . first ; <nl> const AttrValue & tf_attr = namedAttr . second ; <nl> TF_ASSIGN_OR_RETURN ( Attribute attr ,
tport_t * tport_tsend ( tport_t * self , <nl> tp_name_t tpn [ 1 ]; <nl> struct sigcomp_compartment * cc ; <nl>  <nl> - assert ( self ); <nl> - <nl> if (! self || ! msg || ! _tpn ) { <nl> msg_set_errno ( msg , EINVAL ); <nl> return NULL ;
new_address ( const char * hostname_or_ip ) { <nl> if ( hostname_or_ip [ 0 ] == '[' && <nl> ( port = strchr ( hostname_or_ip , ']')) != NULL ) { <nl> len = ( size_t )( port - hostname_or_ip - 1 ); <nl> + if ( len >= INET6_ADDRSTRLEN ) <nl> + return NULL ; <nl>  <nl> /* inet_pton () will not parse the IP correctly unless it is in a <nl> * separate string .
static int parse_packet ( const char * payload , struct ncrx_msg * msg ) <nl> goto einval ; <nl> if (! msg -> text_len || <nl> nf_len >= NCRX_LINE_MAX || <nl> + nf_off >= nf_len || <nl> nf_off + msg -> text_len > nf_len ) <nl> goto einval ; <nl> 
IN_PROC_BROWSER_TEST_F ( WebUIResourceBrowserTest , ListTest ) { <nl> LoadFile ( base :: FilePath ( FILE_PATH_LITERAL (" list_test . html "))); <nl> } <nl>  <nl> +# if defined ( OS_CHROMEOS ) <nl> IN_PROC_BROWSER_TEST_F ( WebUIResourceBrowserTest , GridTest ) { <nl> AddLibrary ( IDR_WEBUI_JS_CR ); <nl> AddLibrary ( IDR_WEBUI_JS_CR_EVENT_TARGET ); <nl> IN_PROC_BROWSER_TEST_F ( WebUIResourceBrowserTest , GridTest ) { <nl> AddLibrary ( IDR_WEBUI_JS_CR_UI_GRID ); <nl> LoadFile ( base :: FilePath ( FILE_PATH_LITERAL (" grid_test . html "))); <nl> } <nl> +# endif <nl>  <nl> IN_PROC_BROWSER_TEST_F ( WebUIResourceBrowserTest , ListSelectionModelTest ) { <nl> AddLibrary ( IDR_WEBUI_JS_CR );
static HB_Bool myanmar_shape_syllable ( HB_Bool openType , HB_ShaperItem * item , HB_ <nl> if ( kinzi >= 0 && i > base && ( cc & Mymr_CF_AFTER_KINZI )) { <nl> reordered [ len ] = Mymr_C_NGA ; <nl> reordered [ len + 1 ] = Mymr_C_VIRAMA ; <nl> - properties [ len - 1 ] = AboveForm ; <nl> + if ( len > 0 ) <nl> + properties [ len - 1 ] = AboveForm ; <nl> properties [ len ] = AboveForm ; <nl> len += 2 ; <nl> kinzi = - 1 ;
void RenderThreadImpl :: Shutdown () { <nl>  <nl> // Wait for all databases to be closed . <nl> if ( webkit_platform_support_ ) { <nl> + // WaitForAllDatabasesToClose might run a nested message loop . To avoid <nl> + // processing timer events while we ' re already in the process of shutting <nl> + // down blink , put a ScopePageLoadDeferrer on the stack . <nl> + WebView :: willEnterModalLoop (); <nl> webkit_platform_support_ -> web_database_observer_impl ()-> <nl> WaitForAllDatabasesToClose (); <nl> + WebView :: didExitModalLoop (); <nl> } <nl>  <nl> // Shutdown in reverse of the initialization order .
xsltFormatNumberFunction ( xmlXPathParserContextPtr ctxt , int nargs ) <nl> void <nl> xsltGenerateIdFunction ( xmlXPathParserContextPtr ctxt , int nargs ){ <nl> xmlNodePtr cur = NULL ; <nl> + xmlXPathObjectPtr obj = NULL ; <nl> long val ; <nl> xmlChar str [ 30 ]; <nl> xmlDocPtr doc ; <nl> xsltGenerateIdFunction ( xmlXPathParserContextPtr ctxt , int nargs ){ <nl> if ( nargs == 0 ) { <nl> cur = ctxt -> context -> node ; <nl> } else if ( nargs == 1 ) { <nl> - xmlXPathObjectPtr obj ; <nl> xmlNodeSetPtr nodelist ; <nl> int i , ret ; <nl>  <nl> xsltGenerateIdFunction ( xmlXPathParserContextPtr ctxt , int nargs ){ <nl> if ( ret == - 1 ) <nl> cur = nodelist -> nodeTab [ i ]; <nl> } <nl> - xmlXPathFreeObject ( obj ); <nl> } else { <nl> xsltTransformError ( xsltXPathGetTransformContext ( ctxt ), NULL , NULL , <nl> " generate - id () : invalid number of args % d \ n ", nargs ); <nl> xsltGenerateIdFunction ( xmlXPathParserContextPtr ctxt , int nargs ){ <nl>  <nl> } <nl>  <nl> + if ( obj ) <nl> + xmlXPathFreeObject ( obj ); <nl> + <nl> val = ( long )(( char *) cur - ( char *) doc ); <nl> if ( val >= 0 ) { <nl> sprintf (( char *) str , " idp % ld ", val );
void GpuDataManager :: UpdateGpuInfo ( const GPUInfo & gpu_info ) { <nl> base :: AutoLock auto_lock ( gpu_info_lock_ ); <nl> if (! gpu_info_ . Merge ( gpu_info )) <nl> return ; <nl> + } <nl> + <nl> + RunGpuInfoUpdateCallbacks (); <nl>  <nl> - RunGpuInfoUpdateCallbacks (); <nl> + { <nl> + base :: AutoLock auto_lock ( gpu_info_lock_ ); <nl> content :: GetContentClient ()-> SetGpuInfo ( gpu_info_ ); <nl> } <nl> 
void OverlayWindowViews :: OnGestureEvent ( ui :: GestureEvent * event ) { <nl> TogglePlayPause (); <nl> event -> SetHandled (); <nl> } <nl> - <nl> - views :: Widget :: OnGestureEvent ( event ); <nl> } <nl>  <nl> void OverlayWindowViews :: ButtonPressed ( views :: Button * sender ,
NavigateParams :: NavigateParams ( <nl> source_contents ( NULL ), <nl> disposition ( CURRENT_TAB ), <nl> transition ( a_transition ), <nl> + is_renderer_initiated ( false ), <nl> tabstrip_index (- 1 ), <nl> tabstrip_add_types ( TabStripModel :: ADD_ACTIVE ), <nl> window_action ( NO_ACTION ), <nl> NavigateParams :: NavigateParams ( Browser * a_browser , <nl> source_contents ( NULL ), <nl> disposition ( CURRENT_TAB ), <nl> transition ( content :: PAGE_TRANSITION_LINK ), <nl> + is_renderer_initiated ( false ), <nl> tabstrip_index (- 1 ), <nl> tabstrip_add_types ( TabStripModel :: ADD_ACTIVE ), <nl> window_action ( NO_ACTION ),
# include " core / dom / MessagePort . h " <nl> # include " core / frame / LocalDOMWindow . h " <nl> # include " core / frame / UseCounter . h " <nl> +# include " core / frame / csp / ContentSecurityPolicy . h " <nl> # include " modules / EventTargetModules . h " <nl> # include " modules / serviceworkers / ServiceWorker . h " <nl> # include " modules / serviceworkers / ServiceWorkerContainerClient . h " <nl> void ServiceWorkerContainer :: registerServiceWorkerImpl ( ExecutionContext * executi <nl> return ; <nl> } <nl>  <nl> + ContentSecurityPolicy * csp = executionContext -> contentSecurityPolicy (); <nl> + if ( csp ) { <nl> + if (! csp -> allowWorkerContextFromSource ( scriptURL , ContentSecurityPolicy :: DidNotRedirect , ContentSecurityPolicy :: SendReport )) { <nl> + callbacks -> onError ( WebServiceWorkerError ( WebServiceWorkerError :: ErrorTypeSecurity , String (" Failed to register a ServiceWorker : The provided scriptURL ('" + scriptURL . getString () + "') violates the Content Security Policy ."))); <nl> + return ; <nl> + } <nl> + } <nl> + <nl> m_provider -> registerServiceWorker ( patternURL , scriptURL , callbacks . leakPtr ()); <nl> } <nl> 
static void _ewk_frame_smart_del ( Evas_Object * ewkFrame ) <nl> if ( smartData -> frame ) { <nl> WebCore :: FrameLoaderClientEfl * flc = _ewk_frame_loader_efl_get ( smartData -> frame ); <nl> flc -> setWebFrame ( 0 ); <nl> - smartData -> frame -> loader ()-> detachFromParent (); <nl> - smartData -> frame -> loader ()-> cancelAndClear (); <nl> + EWK_FRAME_SD_GET ( ewk_view_frame_main_get ( smartData -> view ), mainSmartData ); <nl> + if ( mainSmartData -> frame == smartData -> frame ) // applying only for main frame is enough ( will traverse through frame tree ) <nl> + smartData -> frame -> loader ()-> detachFromParent (); <nl> smartData -> frame = 0 ; <nl> } <nl> 
MediaStreamImpl ::~ MediaStreamImpl () { <nl> chrome_worker_thread_ . message_loop ()-> PostTask ( FROM_HERE , base :: Bind ( <nl> & MediaStreamImpl :: DeleteIpcNetworkManager , <nl> base :: Unretained ( this ))); <nl> + // Stopping the thread will wait until all tasks have been <nl> + // processed before returning . We wait for the above task to finish before <nl> + // letting the destructor continue to avoid any potential race issues . <nl> + chrome_worker_thread_ . Stop (); <nl> } else { <nl> NOTREACHED () << " Worker thread not running ."; <nl> }
TEST_PPAPI_IN_PROCESS ( Var ) <nl> TEST_PPAPI_OUT_OF_PROCESS ( Var ) <nl> TEST_PPAPI_NACL_VIA_HTTP ( Var ) <nl>  <nl> +// Flaky on mac , http :// crbug . com / 121107 <nl> +# if defined ( OS_MACOSX ) <nl> +# define MAYBE_VarDeprecated DISABLED_VarDeprecated <nl> +# else <nl> +# define MAYBE_VarDeprecated VarDeprecated <nl> +# endif <nl> + <nl> TEST_PPAPI_IN_PROCESS ( VarDeprecated ) <nl> - TEST_PPAPI_OUT_OF_PROCESS ( VarDeprecated ) <nl> + TEST_PPAPI_OUT_OF_PROCESS ( MAYBE_VarDeprecated ) <nl>  <nl> // Windows defines ' PostMessage ', so we have to undef it . <nl> # ifdef PostMessage
bool RenderFrameDevToolsAgentHost :: AttachSession ( DevToolsSession * session ) { <nl> session -> AddHandler ( base :: WrapUnique ( new protocol :: SchemaHandler ())); <nl> session -> AddHandler ( base :: WrapUnique ( new protocol :: ServiceWorkerHandler ())); <nl> session -> AddHandler ( base :: WrapUnique ( new protocol :: StorageHandler ())); <nl> - session -> AddHandler ( <nl> - base :: WrapUnique ( new protocol :: TargetHandler ( false /* browser_only */))); <nl> + if (! session -> restricted ()) { <nl> + session -> AddHandler ( base :: WrapUnique ( <nl> + new protocol :: TargetHandler ( false /* browser_only */))); <nl> + } <nl> session -> AddHandler ( base :: WrapUnique ( new protocol :: TracingHandler ( <nl> protocol :: TracingHandler :: Renderer , <nl> frame_tree_node_ ? frame_tree_node_ -> frame_tree_node_id () : 0 ,
void StatusBubbleGtk :: Show () { <nl> // If we were going to hide , stop . <nl> hide_timer_ . Stop (); <nl>  <nl> - gtk_widget_show_all ( container_ . get ()); <nl> + gtk_widget_show ( container_ . get ()); <nl> if ( container_ -> window ) <nl> gdk_window_raise ( container_ -> window ); <nl> } <nl> void StatusBubbleGtk :: Hide () { <nl> expand_timer_ . Stop (); <nl> expand_animation_ . reset (); <nl>  <nl> - gtk_widget_hide_all ( container_ . get ()); <nl> + gtk_widget_hide ( container_ . get ()); <nl> } <nl>  <nl> void StatusBubbleGtk :: SetStatusTextTo ( const std :: string & status_utf8 ) { <nl> void StatusBubbleGtk :: InitWidgets () { <nl> kInternalLeftRightPadding + ( ltr ? 0 : kCornerSize ), <nl> kInternalLeftRightPadding + ( ltr ? kCornerSize : 0 )); <nl> gtk_container_add ( GTK_CONTAINER ( padding_ ), label_ ); <nl> + gtk_widget_show_all ( padding_ ); <nl>  <nl> container_ . Own ( gtk_event_box_new ()); <nl> gtk_widget_set_no_show_all ( container_ . get (), TRUE );
void ScreenOrientationDispatcherHost :: OnLockRequest ( <nl> blink :: WebLockOrientationErrorCanceled ); <nl> } <nl>  <nl> - current_lock_ = new LockInformation ( request_id , <nl> - render_frame_host -> GetProcess ()-> GetID (), <nl> - render_frame_host -> GetRoutingID ()); <nl> - <nl> if (! provider_ ) { <nl> NotifyLockError ( request_id , <nl> blink :: WebLockOrientationErrorNotAvailable ); <nl> return ; <nl> } <nl>  <nl> + current_lock_ = new LockInformation ( request_id , <nl> + render_frame_host -> GetProcess ()-> GetID (), <nl> + render_frame_host -> GetRoutingID ()); <nl> + <nl> provider_ -> LockOrientation ( request_id , orientation ); <nl> } <nl>  <nl> void ScreenOrientationDispatcherHost :: OnUnlockRequest ( <nl> blink :: WebLockOrientationErrorCanceled ); <nl> } <nl>  <nl> - if (! provider_ . get ()) <nl> + if (! provider_ ) <nl> return ; <nl>  <nl> provider_ -> UnlockOrientation ();
void AudioOutputController :: DoFlush () { <nl> if (! sync_reader_ ) { <nl> if ( state_ != kPaused ) <nl> return ; <nl> + AutoLock auto_lock ( lock_ ); <nl> buffer_ . Clear (); <nl> } <nl> }
bool SimplifiedBackwardsTextIterator :: handleTextNode () <nl> m_textLength = m_positionEndOffset - m_positionStartOffset ; <nl> m_textCharacters = text . characters () + ( m_positionStartOffset - offsetInNode ); <nl> ASSERT ( m_textCharacters >= text . characters ()); <nl> - ASSERT ( m_textCharacters + m_textLength <= text . characters () + static_cast < int >( text . length ())); <nl> + RELEASE_ASSERT ( m_textCharacters + m_textLength <= text . characters () + static_cast < int >( text . length ())); <nl>  <nl> m_lastCharacter = text [ m_positionEndOffset - 1 ]; <nl> 
class CryptohomeClientImpl : public CryptohomeClient { <nl> const AsyncMethodCallback & callback ) OVERRIDE { <nl> dbus :: MethodCall method_call ( <nl> cryptohome :: kCryptohomeInterface , <nl> - cryptohome :: kCryptohomeAsyncTpmAttestationCreateEnrollRequestNew ); <nl> + cryptohome :: kCryptohomeAsyncTpmAttestationCreateEnrollRequest ); <nl> dbus :: MessageWriter writer (& method_call ); <nl> writer . AppendInt32 ( pca_type ); <nl> proxy_ -> CallMethod (& method_call , dbus :: ObjectProxy :: TIMEOUT_USE_DEFAULT , <nl> class CryptohomeClientImpl : public CryptohomeClient { <nl> const AsyncMethodCallback & callback ) OVERRIDE { <nl> dbus :: MethodCall method_call ( <nl> cryptohome :: kCryptohomeInterface , <nl> - cryptohome :: kCryptohomeAsyncTpmAttestationEnrollNew ); <nl> + cryptohome :: kCryptohomeAsyncTpmAttestationEnroll ); <nl> dbus :: MessageWriter writer (& method_call ); <nl> writer . AppendInt32 ( pca_type ); <nl> writer . AppendArrayOfBytes (
void GLSurfaceOzoneSurfacelessSurfaceImpl :: Destroy () { <nl> } <nl>  <nl> if (! was_current ) { <nl> - previous_context -> MakeCurrent ( previous_surface . get ()); <nl> - } else { <nl> - context_ -> ReleaseCurrent ( this ); <nl> + if ( previous_context ) { <nl> + previous_context -> MakeCurrent ( previous_surface . get ()); <nl> + } else { <nl> + context_ -> ReleaseCurrent ( this ); <nl> + } <nl> } <nl> } <nl> 
Compositor :: Compositor ( const viz :: FrameSinkId & frame_sink_id , <nl> auto * host_frame_sink_manager = <nl> context_factory_private_ -> GetHostFrameSinkManager (); <nl> host_frame_sink_manager -> RegisterFrameSinkId ( <nl> - frame_sink_id_ , this , viz :: ReportFirstSurfaceActivation :: kYes ); <nl> + frame_sink_id_ , this , viz :: ReportFirstSurfaceActivation :: kNo ); <nl> host_frame_sink_manager -> SetFrameSinkDebugLabel ( frame_sink_id_ , <nl> " Compositor "); <nl> } <nl> void Compositor :: DidSubmitCompositorFrame () { <nl>  <nl> void Compositor :: OnFirstSurfaceActivation ( <nl> const viz :: SurfaceInfo & surface_info ) { <nl> - // TODO ( fsamuel ): Once surface synchronization is turned on , the fallback <nl> - // surface should be set here . <nl> + NOTREACHED (); <nl> } <nl>  <nl> void Compositor :: OnFrameTokenChanged ( uint32_t frame_token ) {
bool SubsetterImpl :: ResolveCompositeGlyphs ( const unsigned int * glyph_ids , <nl> glyph_id_remaining . clear (); <nl> glyph_id_remaining = comp_glyph_id ; <nl> } <nl> + <nl> + return true ; <nl> } <nl>  <nl> CALLER_ATTACH Font * SubsetterImpl :: Subset ( const IntegerSet & glyph_ids ) {
